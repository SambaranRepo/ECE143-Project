YEAR,TITLE,Authors,Paper Link,ABSTRACT,Affiliations
2017,Decoupled Neural Interfaces using Synthetic Gradients,"Max Jaderberg, Wojciech Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, koray kavukcuoglu",https://icml.cc/Conferences/2017/Schedule?showEvent=665,"Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled \emph{synthetic gradient} in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously \ie~we realise \emph{decoupled neural interfaces}. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind']"
2017,PixelCNN Models with Auxiliary Variables for Natural Image Modeling,"Alexander Kolesnikov, Christoph H. Lampert",https://icml.cc/Conferences/2017/Schedule?showEvent=606,"We study probabilistic models of natural images and extend the autoregressive family of PixelCNN models by incorporating auxiliary variables. Subsequently, we describe two new generative image models that exploit different image transformations as auxiliary variables: a quantized grayscale view of the image or a multi-resolution image pyramid. The proposed models tackle two known shortcomings of existing PixelCNN models: 1) their tendency to focus on low-level image details, while largely ignoring high-level image information, such as object shapes, and 2) their computationally costly procedure for image sampling. We experimentally demonstrate benefits of our models, in particular showing that they produce much more realistically looking image samples than previous state-of-the-art probabilistic models.
","['IST Austria', 'IST Austria']"
2017,Tight Bounds for Approximate Carathéodory and Beyond,"Vahab Mirrokni, Renato Leme, Adrian Vladu, Sam Wong",https://icml.cc/Conferences/2017/Schedule?showEvent=467,"We present a deterministic nearly-linear time algorithm for approximating any point inside a convex polytope with a sparse convex combination of the polytope's vertices. Our result provides a constructive proof for the Approximate Carathéodory Problem, which states that any point inside a polytope contained in the $\ell_p$ ball of radius $D$ can be approximated to within $\epsilon$ in $\ell_p$ norm by a convex combination of $O\left(D^2 p/\epsilon^2\right)$ vertices of the polytope for $p \geq 2$. While for the particular case of $p=2$, this can be achieved by the well-known Perceptron algorithm, we follow a more principled approach which generalizes to arbitrary $p\geq 2$; furthermore, this naturally extends to domains with more complicated geometry, as it is the case for providing an approximate Birkhoff-von Neumann decomposition. Secondly, we show that the sparsity bound is tight for $\ell_p$ norms, using an argument based on anti-concentration for the binomial distribution, thus resolving an open question posed by Barman. Experimentally, we verify that our deterministic optimization-based algorithms achieve in practice much better sparsity than previously known sampling-based algorithms. We also show how to apply our techniques to SVM training and rounding fractional points in matroid and flow polytopes.","['Google Research', 'Google Research', 'MIT', 'UC Berkeley']"
2017,Robust Adversarial Reinforcement Learning,"Lerrel Pinto, James Davidson, Rahul Sukthankar, Abhinav Gupta",https://icml.cc/Conferences/2017/Schedule?showEvent=754,"Deep neural networks coupled with fast simulation and improved computational speeds have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can just be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. 
We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method  (a) improves training stability;  (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.
","['Carnegie Mellon University', 'Google Brain', 'Google Research', 'Carnegie Mellon University']"
2017,Robust Probabilistic Modeling with Bayesian Data Reweighting,"Yixin Wang, Alp Kucukelbir, David Blei",https://icml.cc/Conferences/2017/Schedule?showEvent=471,"Probabilistic models analyze data by relying on a set of assumptions.
Data that exhibit deviations from these assumptions can undermine
inference and prediction quality. Robust models offer protection
against mismatch between a model's assumptions and reality. We
propose a way to systematically detect and mitigate mismatch of a
large class of probabilistic models. The idea is to raise the
likelihood of each observation to a weight and then to infer both the
latent variables and the weights from data. Inferring the weights
allows a model to identify observations that match its assumptions
and down-weight others. This enables robust inference and improves
predictive accuracy. We study four different forms of mismatch with
reality, ranging from missing latent groups to structure
misspecification. A Poisson factorization analysis of the Movielens
1M dataset shows the benefits of this approach in a practical
scenario.
","['Columbia University', 'Columbia University', 'Columbia University']"
2017,Multi-objective Bandits: Optimizing the Generalized Gini Index,"Robert Busa-Fekete, Balazs Szorenyi, Paul Weng, Shie Mannor",https://icml.cc/Conferences/2017/Schedule?showEvent=677,"We study the multi-armed bandit (MAB) problem where the agent receives a vectorial feedback that encodes many possibly competing objectives to be optimized. The goal of the agent is to find a policy, which can optimize these objectives simultaneously in a fair way. This multi-objective online optimization problem is formalized by using the Generalized Gini Index (GGI) aggregation function. We propose an online gradient descent algorithm which exploits the convexity of the GGI aggregation function, and controls the exploration in a careful way achieving a distribution-free regret $\tilde{\bigO} (T^{-1/2} )$ with high probability. We test our algorithm on synthetic data as well as on an electric battery control problem where the goal is to trade off the use of the different cells of a battery in order to balance their respective degradation rates.","['Yahoo! Research', 'Technion', 'SYSU-CMU JIE', 'Technion']"
2017,Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis,"Dan Garber, Ohad Shamir, Nati Srebro",https://icml.cc/Conferences/2017/Schedule?showEvent=559,"We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of m stores a sample of n points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all mn samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large n. 
We also introduce an iterative distributed algorithm that is applicable in any regime of n, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters.
","['TTIC', 'Weizmann Institute of Science', 'Toyota Technological Institute at Chicago']"
2017,Enumerating Distinct Decision Trees,Salvatore Ruggieri,https://icml.cc/Conferences/2017/Schedule?showEvent=538,"The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features. We provide an exact enumeration procedure of the subsets that lead to all and only the distinct decision trees. The procedure can be adopted to prune the search space of complete and heuristics search methods in wrapper models for feature selection. Based on this, we design a computational optimization of the sequential backward elimination heuristics with a performance improvement of up to 100X.
",['Università di Pisa']
2017,Understanding Synthetic Gradients and Decoupled Neural Interfaces,"Wojciech Czarnecki, Grzegorz Świrszcz, Max Jaderberg, Simon Osindero, Oriol Vinyals, koray kavukcuoglu",https://icml.cc/Conferences/2017/Schedule?showEvent=719,"When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking - without waiting for a true error gradient to be backpropagated -resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison. 
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2017,Parallel Multiscale Autoregressive Density Estimation,"Scott Reed, Aäron van den Oord, Nal Kalchbrenner, Sergio Gómez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov, Nando de Freitas",https://icml.cc/Conferences/2017/Schedule?showEvent=643,"PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.
","['Google Deepmind', 'Google', 'DeepMind', 'Google DeepMind', 'Deep Mind', 'DeepMind', 'Google', 'DeepMind']"
2017,Oracle Complexity of Second-Order Methods for Finite-Sum Problems,"Yossi Arjevani, Ohad Shamir",https://icml.cc/Conferences/2017/Schedule?showEvent=482,"Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in \emph{second-order} methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer -- perhaps surprisingly -- is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result. 
","['Weizmann Institute of Science', 'Weizmann Institute of Science']"
2017,Minimax Regret Bounds for Reinforcement Learning,"Mohammad Gheshlaghi Azar, Ian Osband, Remi Munos",https://icml.cc/Conferences/2017/Schedule?showEvent=774,"We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs.
We show that an optimistic modification to value iteration achieves a regret bound of $\tilde {O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the number of time-steps.
This result improves over the best previous known bound $\tilde {O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm. 
The key significance of our new results is that when  $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor.
Our analysis contain two key insights.
We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use ""exploration bonuses"" built from Bernstein's  inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$).","['Deepmind', 'Google DeepMind', 'DeepMind']"
2017,Post-Inference Prior Swapping,"Willie Neiswanger, Eric Xing",https://icml.cc/Conferences/2017/Schedule?showEvent=532,"While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used. In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior? A potential solution is to use importance sampling (IS). However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior. Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors. Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information “post-inference”. We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.
","['CMU', 'Carnegie Mellon University']"
2017,Online Learning with Local Permutations and Delayed Feedback,"Liran Szlak, Ohad Shamir",https://icml.cc/Conferences/2017/Schedule?showEvent=533,"We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly permute the \emph{order} of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner's responses is not crucial, and on the other hand, might allow better learning and regret performance, by mitigating highly adversarial loss sequences. Also, with random permutations, this can be seen as a setting interpolating between adversarial and stochastic losses. In this paper, we consider the 
applicability of this setting to convex online learning with delayed feedback, in which the feedback on the prediction made in round $t$ arrives with some delay $\tau$. With such delayed feedback, the best possible regret bound is well-known to be $O(\sqrt{\tau T})$. We prove that by being able to permute losses by a distance of at most $M$ (for $M\geq \tau$), the regret can be improved to $O(\sqrt{T}(1+\sqrt{\tau^2/M}))$, using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for $M<\tau/3$, it is impossible to improve the standard $O(\sqrt{\tau T})$ regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm.","['Weizmann Institute of Science', 'Weizmann Institute of Science']"
2017,SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling,"Jun-ichiro Hirayama, Aapo Hyvärinen, Motoaki Kawanabe",https://icml.cc/Conferences/2017/Schedule?showEvent=691,"We present a novel probabilistic framework for a hierarchical extension of independent component analysis (ICA), with a particular motivation in neuroscientific data analysis and modeling. The framework incorporates a general subspace pooling with linear ICA-like layers stacked recursively. Unlike related previous models, our generative model is fully tractable: both the likelihood and the posterior estimates of latent variables can readily be computed with analytically simple formulae. The model is particularly simple in the case of complex-valued data since the pooling can be reduced to taking the modulus of complex numbers. Experiments on electroencephalography (EEG) and natural images demonstrate the validity of the method.
","['RIKEN AIP / ATR', 'UCL', 'ATR / RIKEN']"
2017,Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation,"Yacine Jernite, Anna Choromanska, David Sontag",https://icml.cc/Conferences/2017/Schedule?showEvent=782,"We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static. We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchical predictor. Our approach optimizes an objective function which favors balanced and easily-separable multi-way node partitions. We theoretically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We empirically validate both variants of the algorithm on text classification and language modeling, respectively, and show that they compare favorably to common baselines in terms of accuracy and running time.
","['New York University', 'New York University', 'Massachusetts Institute of Technology']"
2017,meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting,"Xu SUN, Xuancheng REN, Shuming Ma, Houfeng Wang",https://icml.cc/Conferences/2017/Schedule?showEvent=567,"We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.","['Peking University', 'Peking University', 'Peking University', '']"
2017,Video Pixel Networks,"Nal Kalchbrenner, Karen Simonyan, Aäron van den Oord, Ivo Danihelka, Oriol Vinyals, Alex Graves, koray kavukcuoglu",https://icml.cc/Conferences/2017/Schedule?showEvent=720,"We  propose  a  probabilistic  video  model, the Video Pixel Network (VPN), that estimates the  discrete  joint  distribution  of  the  raw  pixel  values  in  a  video.   The  model  and  the  neural  architecture reflect the time, space and color structure  of  video  tensors  and  encode  it  as  a   four-dimensional  dependency  chain.  The  VPN  approaches  the  best  possible  performance  on  the Moving MNIST benchmark, a leap over the previous state of the art,  and the generated  videos show  only  minor  deviations from  the  ground truth. The  VPN  also  produces  detailed  samples  on  the  action-conditional Robotic  Pushing benchmark  and  generalizes  to  the  motion of novel objects.
","['DeepMind', 'DeepMind', 'Google', 'Google DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2017,Global optimization of Lipschitz functions,"Cédric Malherbe, Nicolas Vayatis",https://icml.cc/Conferences/2017/Schedule?showEvent=717,"The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H\""older like condition. An adaptive version of LIPO is also introduced for the more realistic setup where Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.
","['ENS Paris-Saclay', 'ENS Cachan']"
2017,Fairness in Reinforcement Learning,"Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Aaron Roth",https://icml.cc/Conferences/2017/Schedule?showEvent=697,"We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states
to achieve non-trivial approximation to the optimal policy.  We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.
","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']"
2017,Evaluating Bayesian Models with Posterior Dispersion Indices,"Alp Kucukelbir, Yixin Wang, David Blei",https://icml.cc/Conferences/2017/Schedule?showEvent=514,"Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance. Evaluation drives the cycle, as we revise our model based on how it performs. This requires a metric. Traditionally, predictive accuracy prevails. Yet, predictive accuracy does not tell the whole story. We propose to evaluate a model through posterior dispersion. The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure. This highlights datapoints the model struggles to explain and provides complimentary insight to datapoints with low predictive accuracy. We present a family of posterior dispersion indices (PDI) that capture this idea. We show how a PDI identifies patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.
","['Columbia University', 'Columbia University', 'Columbia University']"
2017,Model-Independent Online Learning for Influence Maximization,"Sharan Vaswani, Branislav Kveton, Zheng Wen, Mohammad Ghavamzadeh, Laks V.S Lakshmanan, Mark Schmidt",https://icml.cc/Conferences/2017/Schedule?showEvent=639,"We consider \emph{influence maximization} (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of ``seed'' users to expose the product to. While prior work assumes a known model of information diffusion, we propose a novel parametrization that not only makes our framework agnostic to the underlying diffusion model, but also statistically efficient to learn from data. We give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. We also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, we propose a pairwise-influence semi-bandit feedback model and develop a LinUCB-based bandit algorithm. Our model-independent analysis shows that our regret bound has a better (as compared to previous work) dependence on the size of the network. Experimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-optimal solution.
","['University of British Columbia', 'Adobe Research', 'Adobe Research', 'Adobe Research & INRIA', 'University of British Columbia', 'University of British Columbia']"
2017,Latent Feature Lasso,"En-Hsu Yen, Wei-Cheng Lee, Sung-En Chang, Arun Suggala, Shou-De Lin, Pradeep Ravikumar",https://icml.cc/Conferences/2017/Schedule?showEvent=867,"The latent feature model (LFM), proposed in \cite{griffiths2005infinite}, but possibly with earlier origins, is a generalization of a mixture model, where each instance is generated not from a single latent class but from a combination of \emph{latent features}. Thus, each instance has an associated  latent binary feature incidence vector indicating the presence or absence of a feature. Due to its combinatorial nature, inference of LFMs is considerably intractable, and accordingly, most of the attention has focused on nonparametric LFMs, with priors such as the Indian Buffet Process (IBP) on infinite binary matrices. Recent efforts to tackle this complexity either still have computational complexity that is exponential, or sample complexity that is high-order polynomial w.r.t. the number of latent features. In this paper, we address this outstanding problem of tractable estimation of LFMs via a novel atomic-norm regularization, which gives an algorithm with polynomial run-time and sample complexity without impractical assumptions on the data distribution.
","['Carnegie Mellon University', 'National Taiwan University', 'National Taiwan University', 'Carnegie Mellon University', 'National Taiwan University', 'Carnegie Mellon University']"
2017,Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things,"Ashish Kumar, Saurabh Goyal, Manik Varma",https://icml.cc/Conferences/2017/Schedule?showEvent=696,"This paper develops a novel tree-based algorithm, called Bonsai, for efficient prediction on IoT devices – such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no native floating point support, 2 KB RAM and 32 KB read-only flash. Bonsai maintains prediction accuracy while minimizing model size and prediction costs by: (a) developing a tree model which learns a single, shallow, sparse tree with powerful nodes; (b) sparsely projecting all data into a low-dimensional space in which the tree is learnt; and (c) jointly learning all tree and projection parameters. Experimental results on multiple benchmark datasets demonstrate that Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms while achieving prediction accuracies that can be as much as 30% higher than state-of-the-art methods for resource-efficient machine learning. Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as compared to Bing’s L3 ranker when the model size is restricted to 300 bytes. Bonsai’s code can be downloaded from (http://www.manikvarma.org/code/Bonsai/download.html).
","['Microsoft Research', 'IBM India Pvt Ltd', 'Microsoft Research']"
2017,Learning Important Features Through Propagating Activation Differences,"Avanti Shrikumar, Peyton Greenside, Anshul Kundaje",https://icml.cc/Conferences/2017/Schedule?showEvent=614,"The purported ""black box"" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: \url{http://goo.gl/qKb7pL}, code: \url{http://goo.gl/RM8jvH}.
","['Stanford University', 'Stanford University', 'Stanford University']"
2017,Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks,"Lars Mescheder, Sebastian Nowozin, Andreas Geiger",https://icml.cc/Conferences/2017/Schedule?showEvent=671,"Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders  with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.
","['MPI Tübingen', 'Microsoft Research', 'MPI Tübingen']"
2017,Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions,"Yichen Chen, Dongdong Ge, Mengdi Wang, Zizhuo Wang, Yinyu Ye, Hao Yin",https://icml.cc/Conferences/2017/Schedule?showEvent=726,"Consider the regularized sparse minimization problem, which involves empirical sums of loss functions for $n$ data points (each of dimension $d$) and a nonconvex sparsity penalty. We prove that finding an $\mathcal{O}(n^{c_1}d^{c_2})$-optimal solution to the regularized sparse optimization problem is strongly NP-hard for any $c_1, c_2\in [0,1)$ such that $c_1+c_2$ less than 1. We also prove strong NP-hardness results for the sparsity-constrained optimizaiotn problem. These results apply to a broad class of loss functions and sparse penalty functions. They suggest that one cannot even approximately solve the sparse optimization problem in polynomial time, unless P $=$ NP.","['Princeton University', 'Shanghai University of Finance and Economics', 'Princeton University', 'University of Minnesota', 'Standord', 'Stanford University']"
2017,Boosted Fitted Q-Iteration,"Samuele Tosatto, Matteo Pirotta, Carlo D'Eramo, Marcello Restelli",https://icml.cc/Conferences/2017/Schedule?showEvent=676,"This paper is about the study of B-FQI, an Approximated Value Iteration (AVI) algorithm that exploits a boosting procedure to estimate the action-value function in reinforcement learning problems. B-FQI is an iterative off-line algorithm that, given a dataset of transitions, builds an approximation of the optimal action-value function by summing the approximations of the Bellman residuals across all iterations. The advantage of such approach w.r.t. to other AVI methods is twofold: (1) while keeping the same function space at each iteration, B-FQI can represent more complex functions by considering an
additive model; (2) since the Bellman residual decreases as the optimal value function is approached, regression problems become easier as iterations proceed. We study B-FQI both theoretically, providing also a finite-sample error upper bound for it, and empirically, by comparing its performance to the one of FQI in different domains and using different regression techniques.
","['Politecnico di Milano', 'SequeL - Inria Lille - Nord Europe', 'Politecnico di Milano', 'Politecnico di Milano']"
2017,Automatic Discovery of the Statistical Types of Variables in a Dataset,"Isabel Valera, Zoubin Ghahramani",https://icml.cc/Conferences/2017/Schedule?showEvent=541,"A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known. However, as the availability of real-world data increases, this assumption becomes too restrictive. Data are often heterogeneous, complex, and improperly or incompletely documented. Surprisingly, despite their practical importance, there is still a lack of tools to automatically discover the statistical types of, as well as appropriate likelihood (noise) models for, the variables in a dataset. In this paper, we fill this gap by proposing a Bayesian method, which accurately discovers the statistical data types in both synthetic and real data.
","['University of Cambridge', 'University of Cambridge & Uber']"
2017,Online Learning to Rank in Stochastic Click Models,"Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton, Csaba Szepesvari, Zheng Wen",https://icml.cc/Conferences/2017/Schedule?showEvent=645,"Online learning to rank is a core problem in information retrieval and machine learning. Many provably efficient algorithms have been recently proposed for this problem in specific click models. The click model is a model of how the user interacts with a list of documents. Though these results are significant, their impact on practice is limited, because all proposed algorithms are designed for specific click models and lack convergence guarantees in other models. In this work, we propose BatchRank, the first online learning to rank algorithm for a broad class of click models. The class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the T-step regret of BatchRank and evaluate it on a range of web search queries. We observe that BatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for the cascade model.
","['Independent Researcher', 'Czech Technical University', 'Adobe Research & INRIA', 'Adobe Research', 'University of Alberta', 'Adobe Research']"
2017,Online Partial Least Square Optimization: Dropping Convexity for Better Efficiency and Scalability,"Zhehui Chen, Lin Yang, Chris Junchi Li, Tuo Zhao",https://icml.cc/Conferences/2017/Schedule?showEvent=791,"Multiview representation learning is popular for latent factor analysis. Many existing approaches formulate the multiview representation learning as convex optimization problems, where global optima can be obtained by certain algorithms in polynomial time. However, many evidences have corroborated that heuristic nonconvex approaches also have good empirical computational performance and convergence to the global optima, although there is a lack of theoretical justification. Such a gap between theory and practice motivates us to study a nonconvex formulation for multiview representation learning, which can be efficiently solved by a simple stochastic gradient descent method. By analyzing the dynamics of the algorithm based on diffusion processes, we establish a global rate of convergence to the global optima. Numerical experiments are provided to support our theory.
","['Georgia Institute of Technology', 'Johns Hopkins', 'Princeton University', 'Georgia Institute of Technology']"
2017,Multi-Class Optimal Margin Distribution Machine,"Teng Zhang, Zhi-Hua Zhou",https://icml.cc/Conferences/2017/Schedule?showEvent=823,"Recent studies disclose that maximizing the minimum margin like support vector machines does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution. Although it has been shown that for binary classification, characterizing the margin distribution by the first- and second-order statistics can achieve superior performance. It still remains open for multi-class classification, and due to the complexity of margin for multi-class classification, optimizing its distribution by mean and variance can also be difficult. In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine), which can solve this problem efficiently. We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification. Empirical study further shows that mcODM always outperforms all four versions of multi-class SVMs on all experimental data sets.
","['Nanjing University', 'Nanjing University']"
2017,Evaluating the Variance of Likelihood-Ratio Gradient Estimators,"Seiya Tokui, Issei Sato",https://icml.cc/Conferences/2017/Schedule?showEvent=608,"The likelihood-ratio method is often used to estimate gradients of stochastic computations, for which baselines are required to reduce the estimation variance. Many types of baselines have been proposed, although their degree of optimality is not well understood. In this study, we establish a novel framework of gradient estimation that includes most of the common gradient estimators as special cases. The framework gives a natural derivation of the optimal estimator that can be interpreted as a special case of the likelihood-ratio method so that we can evaluate the optimal degree of practical techniques with it. It bridges the likelihood-ratio method and the reparameterization trick while still supporting discrete variables. It is derived from the exchange property of the differentiation and integration. To be more specific, it is derived by the reparameterization trick and local marginalization analogous to the local expectation gradient. We evaluate various baselines and the optimal estimator for variational learning and show that the performance of the modern estimators is close to the optimal estimator.
","['Preferred Networks / The University of Tokyo', 'University of Tokyo / RIKEN']"
2017,Learning Texture Manifolds with the Periodic Spatial GAN,"Urs M Bergmann, Nikolay Jetchev, Roland Vollgraf",https://icml.cc/Conferences/2017/Schedule?showEvent=664,"This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014), and call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities  which surpass the current state of the art in texture synthesis. First, we can learn multiple textures, periodic or non-periodic, from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources, and the method is highly scalable and can generate output images of arbitrary large size.
","['Zalando Research', 'Zalando Research', 'Zalando Research']"
2017,Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence,"Yi Xu, Qihang Lin, Tianbao Yang",https://icml.cc/Conferences/2017/Schedule?showEvent=505,"In this paper, a new theory is developed for first-order stochastic convex optimization, showing that the global convergence rate is sufficiently  quantified by a local growth rate of the objective function in a neighborhood of the optimal solutions. In particular, if the objective function $F(\w)$ in the $\epsilon$-sublevel set grows as fast as $\|\w - \w_*\|_2^{1/\theta}$, where $\w_*$ represents the closest optimal solution to $\w$ and $\theta\in(0,1]$ quantifies the local growth rate,  the iteration complexity of first-order stochastic optimization for achieving  an $\epsilon$-optimal solution can be $\widetilde O(1/\epsilon^{2(1-\theta)})$, which is {\it optimal at most} up to a logarithmic factor. This result is fundamentally better in contrast with the previous works that either assume a global growth condition in the entire domain or achieve a local faster convergence under the local faster growth condition. To achieve the faster global convergence, we develop two different {\bf accelerated stochastic subgradient} methods by iteratively solving the original problem approximately in a local region around a historical solution with the size of the local region gradually decreasing as the solution approaches the optimal set. Besides the theoretical improvements, this work also include new contributions towards making the proposed algorithms practical: (i) we present practical variants of accelerated stochastic subgradient methods that can run without the knowledge of  multiplicative growth constant and even the growth rate $\theta$; (ii) we consider a broad family of problems in machine learning to demonstrate that the proposed algorithms enjoy faster convergence than traditional stochastic subgradient method.   For example, when applied to the $\ell_1$ regularized empirical polyhedral loss minimization (e.g., hinge loss, absolute loss), the proposed stochastic methods have a logarithmic iteration complexity. ","['The University of Iowa', 'Univ Iowa', 'The University of Iowa']"
2017,Why is Posterior Sampling Better than Optimism for Reinforcement Learning?,"Ian Osband, Benjamin Van Roy",https://icml.cc/Conferences/2017/Schedule?showEvent=589,"Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.","['Deepmind', 'Stanford University']"
2017,Bayesian Models of Data Streams with Hierarchical Power Priors,"Andres Masegosa, Thomas D. Nielsen, Helge Langseth, Dario Ramos-Lopez, Antonio Salmeron, Anders Madsen",https://icml.cc/Conferences/2017/Schedule?showEvent=775,"Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating, and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models. 
","['University of Almeria', 'Aalborg University', 'Norwegian University of Science and Technology', 'University of Almeria', 'University of Almeria', 'Hugin Expert A/S']"
2017,The Sample Complexity of Online One-Class Collaborative Filtering,"Reinhard Heckel, Kannan Ramchandran",https://icml.cc/Conferences/2017/Schedule?showEvent=878,"We consider the online one-class collaborative filtering (CF) problem that consist of recommending items to users over time in an online fashion based on positive ratings only. 
This problem arises when users respond only occasionally to a recommendation with a positive rating, and never with a negative one. 
We study the impact of the probability of a user responding to a recommendation, 
pf, on the sample complexity, and ask whether receiving positive and negative ratings, instead of positive ratings only, improves the sample complexity. 
Both questions arise in the design of recommender systems. 
We introduce a simple probabilistic user model, and analyze the performance of an online user-based CF algorithm. 
We prove that after an initial cold start phase, 
where recommendations are invested in exploring the user's preferences, 
this algorithm makes---up to a fraction of the recommendations required for updating the user's preferences---perfect recommendations. 
The number of ratings required for the cold start phase 
is nearly proportional to 1/pf, 
and that for 
updating the user's preferences is essentially independent of pf. As a consequence we find that, 
receiving positive and negative ratings instead of only positive ones improves the number of ratings required for initial exploration 
by a factor of 1/pf, which can be significant. 
","['UC Berkeley', 'UC Berkeley']"
2017,Kernelized Support Tensor Machines,"Lifang He, Chun-Ta Lu, Guixiang Ma, Shen Wang, Linlin Shen, Philip Yu, Ann Ragin",https://icml.cc/Conferences/2017/Schedule?showEvent=516,"In the context of supervised tensor learning, preserving the structural information and exploiting the discriminative nonlinear relationships of tensor data are crucial for improving the performance of learning tasks. Based on tensor factorization theory and kernel methods, we propose a novel Kernelized Support Tensor Machine (KSTM) which integrates kernelized tensor factorization with maximum-margin criterion. Specifically, the kernelized factorization technique is introduced to approximate the tensor data in kernel space such that the complex nonlinear relationships within tensor data can be explored. Further, dual structural preserving kernels are devised to learn the nonlinear boundary between tensor data. As a result of joint optimization, the kernels obtained in KSTM exhibit better generalization power to discriminative analysis. The experimental results on real-world neuroimaging datasets show the superiority of KSTM over the state-of-the-art techniques.
","['University of Illinios at Chicago/Shenzhen University', 'University of Illinois at Chicago', '', 'University of Illinios at Chicago', '', 'UIC', 'Northwestern University']"
2017,Equivariance Through Parameter-Sharing,"Siamak Ravanbakhsh, Jeff Schneider, Barnabás Póczos",https://icml.cc/Conferences/2017/Schedule?showEvent=686,"We propose to study equivariance in deep neural networks through parameter symmetries. In particular, given a group G that acts discretely on the input and output of a standard neural network layer,
we show that its equivariance is linked to the symmetry group of network parameters. We then propose two parameter-sharing scheme to induce the desirable symmetry on the parameters of the neural network. Under some conditions on the action of G, our procedure for tying the parameters achieves G-equivariance and guarantees sensitivity  to all other permutation groups outside of G.
","['Carnegie Mellon University', 'CMU/Uber', 'CMU']"
2017,Generalization and Equilibrium in Generative Adversarial Nets (GANs),"Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, Yi Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=854,"It is shown that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a natural training objective (Wasserstein) when generator capacity and training set sizes are moderate. This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.
","['Princeton University', 'Duke University', 'Princeton University', 'Princeton University', 'Princeton University']"
2017,GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization,"Li Shen, Wei Liu, Ganzhao Yuan, Shiqian Ma",https://icml.cc/Conferences/2017/Schedule?showEvent=477,"In this paper, we propose a fast {\bf{G}}auss-{\bf{S}}eidel {\bf{O}}perator {\bf{S}}plitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in machine learning, signal processing and statistics. The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the operator splitting technique to reduce the computational complexity. In addition, we develop a new technique to establish the global convergence of the GSOS algorithm. To be specific, we first reformulate the iterations of GSOS as a two-step iterations algorithm by employing the tool of operator optimization theory. Subsequently, we establish the convergence of GSOS based on the two-step iterations algorithm reformulation. At last, we apply the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems. Numerical experiments show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.
","['School of Mathematics, South China University of Technology', 'Tencent AI Lab', 'SYSU', 'The Chinese University of Hong Kong']"
2017,Constrained Policy Optimization,"Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel",https://icml.cc/Conferences/2017/Schedule?showEvent=848,"For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting.
We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety. 
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'OpenAI / UC Berkeley']"
2017,Ordinal Graphical Models: A Tale of Two Approaches,"ARUN SAI SUGGALA, Eunho Yang, Pradeep Ravikumar",https://icml.cc/Conferences/2017/Schedule?showEvent=644,"Undirected graphical models or Markov random fields (MRFs) are widely used for modeling multivariate probability distributions. Much of the work on MRFs has focused on continuous variables, and nominal variables (that is, unordered categorical variables). However, data from many real world applications involve ordered categorical variables also known as ordinal variables, e.g., movie ratings on Netflix which can be ordered from 1 to 5 stars. 
With respect to univariate ordinal distributions, as we detail in the paper, there are two main categories of distributions; while there have been efforts to extend these to multivariate ordinal distributions, the resulting distributions are typically very complex, with either a large number of parameters, or with non-convex likelihoods. While there have been some work on tractable approximations, these do not come with strong statistical guarantees, and moreover are relatively computationally expensive.
In this paper, we theoretically investigate two classes of graphical models for ordinal data, corresponding to the two main categories of univariate ordinal distributions. In contrast to previous work, our theoretical developments allow us to provide correspondingly two classes of estimators that are not only computationally efficient but also have strong statistical guarantees.
","['Carnegie Mellon University', 'KAIST / AItrics', 'Carnegie Mellon University']"
2017,Efficient Regret Minimization in Non-Convex Games,"Elad Hazan, Karan Singh, Cyril Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=581,"We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.
","['Princeton University', 'Princeton University', 'Princeton University']"
2017,Coresets for Vector Summarization with Applications to Network Graphs,"Dan Feldman, Sedat Ozer, Daniela Rus",https://icml.cc/Conferences/2017/Schedule?showEvent=481,"We provide a deterministic data summarization algorithm that approximates the mean $\bar{p}=\frac{1}{n}\sum_{p\in P} p$ of a set $P$ of $n$ vectors in $\REAL^d$, by a weighted mean $\tilde{p}$ of a \emph{subset} of $O(1/\eps)$ vectors, i.e., independent of both $n$ and $d$. We prove that the squared Euclidean distance between $\bar{p}$ and $\tilde{p}$ is at most $\eps$ multiplied by the variance of $P$. We use this algorithm to maintain an approximated sum of vectors from an unbounded stream, using memory that is independent of $d$, and logarithmic in the $n$ vectors seen so far. Our main application is to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. For example, in the case of mobile networks, we can use GPS traces to identify meetings; in the case of social networks, we can use information exchange to identify friend groups. Our algorithm provably identifies the {\it Heavy Hitter} entries in a proximity (adjacency) matrix. The Heavy Hitters can be used to extract and represent in a compact way friend groups and activity summaries of users from underlying data exchanges. We evaluate the algorithm on several large data sets. ","['The University of Haifa', 'MIT', 'MIT CSAIL']"
2017,Recovery Guarantees for One-hidden-layer Neural Networks,"Kai Zhong, Zhao Song, Prateek Jain, Peter Bartlett, Inderjit Dhillon",https://icml.cc/Conferences/2017/Schedule?showEvent=752,"In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs). We distill some properties of activation functions that lead to            \emph{local strong convexity} in the neighborhood of the ground-truth parameters for the 1NN squared-loss objective and most popular nonlinear activation functions     satisfy the distilled properties, including rectified linear units (\ReLU s), leaky \ReLU s, squared \ReLU s and sigmoids. For activation functions that are also       smooth, we show \emph{local linear convergence} guarantees of gradient descent under a resampling rule. For homogeneous activations, we show tensor methods are able to initialize the parameters to fall into the local strong convexity region. As a result, tensor initialization followed by gradient descent is guaranteed to recover the  ground truth with sample complexity $ d \cdot \log(1/\epsilon) \cdot \poly(k,\lambda )$ and computational complexity $n\cdot d \cdot \poly(k,\lambda) $ for smooth      homogeneous activations with high probability, where $d$ is the dimension of the input, $k$ ($k\leq d$) is the number of hidden nodes, $\lambda$ is a conditioning      property of the ground-truth parameter matrix between the input layer and the hidden layer, $\epsilon$ is the targeted precision and $n$ is the number of samples. To   the best of our knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample complexity and computational complexity \emph{linear} in  the input dimension and \emph{logarithmic} in the precision.","['University of Texas at Austin', 'UT-Austin', 'Microsoft Research', 'UC Berkeley', 'UT Austin & Amazon']"
2017,Dual Supervised Learning,"Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, Tie-Yan Liu",https://icml.cc/Conferences/2017/Schedule?showEvent=763,"Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach dual supervised learning. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.
","['University of Science and Technology of China', 'Microsoft Research Asia', 'Microsoft Research', 'Microsoft Research', 'USTC', 'Microsoft']"
2017,Warped Convolutions: Efficient Invariance to Spatial Transformations,"Joao Henriques, Andrea Vedaldi",https://icml.cc/Conferences/2017/Schedule?showEvent=660,"Convolutional Neural Networks (CNNs) are extremely efficient, since they exploit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful spatial transformations. Can the same efficiency be attained when considering other spatial invariances? Such generalized convolutions have been considered in the past, but at a high computational cost. We present a construction that is simple and exact, yet has the same computational complexity that standard convolutions enjoy. It consists of a constant image warp followed by a simple convolution, which are standard blocks in deep learning toolboxes. With a carefully crafted warp, the resulting architecture can be made equivariant to a wide range of two-parameter spatial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle poses in the Google Earth dataset (rotation and scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations under perspective).
","['University of Oxford', 'University of Oxford']"
2017,McGan: Mean and Covariance Feature Matching GAN,"Youssef Mroueh, Tom Sercu, Vaibhava Goel",https://icml.cc/Conferences/2017/Schedule?showEvent=705,"We introduce new families of Integral Probability
Metrics (IPM) for training Generative Adversarial
Networks (GAN). Our IPMs are based on
matching statistics of distributions embedded in
a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable
training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.
","['IBM T.J Watson Research Center', 'IBM Research', 'IBM']"
2017,Breaking Locality Accelerates Block Gauss-Seidel,"Stephen Tu, Shivaram Venkataraman, Ashia Wilson, Alex Gittens, Michael Jordan, Benjamin Recht",https://icml.cc/Conferences/2017/Schedule?showEvent=502,"Recent work by Nesterov and Stich (2016) showed that momentum can be used to accelerate the rate of convergence for block Gauss-Seidel in the setting where a fixed partitioning of the coordinates is chosen ahead of time.  We show that this setting is too restrictive, constructing instances where breaking locality by running non-accelerated Gauss-Seidel with randomly sampled coordinates substantially outperforms accelerated Gauss-Seidel with any fixed partitioning.  Motivated by this finding, we analyze the accelerated block Gauss-Seidel algorithm in the random coordinate sampling setting. Our analysis captures the benefit of acceleration with a new data-dependent parameter which is well behaved when the matrix sub-blocks are well-conditioned.  Empirically, we show that accelerated Gauss-Seidel with random coordinate sampling provides speedups for large scale machine learning tasks when compared to non-accelerated Gauss-Seidel and the classical conjugate-gradient algorithm.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'Berkeley']"
2017,Reinforcement Learning with Deep Energy-Based Policies,"Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine",https://icml.cc/Conferences/2017/Schedule?showEvent=838,"We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.
","['UC Berkeley', 'UC Berkeley', 'OpenAI / UC Berkeley', 'Berkeley']"
2017,Scalable Bayesian Rule Lists,"Hongyu Yang, Cynthia Rudin, Margo Seltzer",https://icml.cc/Conferences/2017/Schedule?showEvent=735,"We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we aim to fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees. In many cases, the computational time is practical and often less than that of decision trees.
","['Massachusetts Institute of Technology', 'Duke University', 'Harvard University']"
2017,Identify the Nash Equilibrium in Static Games with Random Payoffs,"Yichi Zhou, Jialian Li, Jun Zhu",https://icml.cc/Conferences/2017/Schedule?showEvent=681,"We study the problem on how to learn the pure Nash Equilibrium of a two-player zero-sum static game with random payoffs under unknown distributions via efficient payoff queries. We introduce a multi-armed bandit model to this problem due to its ability to find the best arm efficiently among random arms and propose two algorithms for this problem---LUCB-G based on the confidence bounds and a racing algorithm based on successive action elimination. We provide an analysis on the sample complexity lower bound when the Nash Equilibrium exists.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2017,Partitioned Tensor Factorizations for Learning Mixed Membership Models,"Zilong Tan, Sayan Mukherjee",https://icml.cc/Conferences/2017/Schedule?showEvent=551,"We present an efficient algorithm for learning mixed membership models when the number of variables p is much larger than the number of hidden components k. This algorithm reduces the computational complexity of state-of-the-art tensor methods, which require decomposing an O(p^3) tensor, to factorizing O (p/k) sub-tensors each of size O(k^3). In addition, we address the issue of negative entries in the empirical method of moments based estimators. We provide sufficient conditions under which our approach has provable guarantees. Our approach obtains competitive empirical results on both simulated and real data.
","['Duke University', 'Duke University']"
2017,Failures of Gradient-Based Deep Learning,"Shaked Shammah, Shai Shalev-Shwartz, Ohad Shamir",https://icml.cc/Conferences/2017/Schedule?showEvent=694,"In recent years, Deep Learning has become the go-to solution for a
  broad range of applications, often outperforming
  state-of-the-art. However, it is important, for both theoreticians
  and practitioners, to gain a deeper understanding of the
  difficulties and limitations associated with common approaches and
  algorithms. We describe four types of simple problems, for which the
  gradient-based algorithms commonly used in deep learning either fail
  or suffer from significant difficulties. We illustrate the failures
  through practical experiments, and provide theoretical insights
  explaining their source, and how they might be
  remedied.
","['Hebrew University, Jerusalem', '', 'Weizmann Institute of Science']"
2017,Learning Infinite Layer Networks without the Kernel Trick,"Roi Livni, Daniel Carmon, Amir Globerson",https://icml.cc/Conferences/2017/Schedule?showEvent=755,"Infinite Layer Networks (ILN) have been proposed as an architecture that mimics neural networks while enjoying some of the advantages of kernel methods. ILN  are networks that integrate over infinitely many nodes within a single hidden layer. It has been demonstrated by several authors that the problem of learning ILN can be reduced to the kernel trick, implying that whenever a certain integral can be computed analytically they are efficiently learnable. 
In this work we give an online algorithm for ILN, which avoids the kernel trick assumption. More generally and of independent interest, we show that kernel methods in general can be exploited even when the kernel cannot be efficiently computed but can only be estimated via sampling. We provide a regret analysis for our algorithm, showing that it matches the sample complexity of methods which have access to kernel values.  Thus, our method is the first to demonstrate that the kernel trick is not necessary, as such, and random features suffice to obtain comparable performance.
","['Princeton', 'Tel-Aviv University', 'Tel Aviv University']"
2017,Graph-based Isometry Invariant Representation Learning,"Renata Khasanova, Pascal Frossard",https://icml.cc/Conferences/2017/Schedule?showEvent=597,"Learning transformation invariant representations of visual data is an important problem in computer vision. Deep convolutional networks have demonstrated remarkable results for image and video classification tasks. However,  they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we present a novel Transformation Invariant Graph-based Network (TIGraNet), which learns graph-based features that are inherently invariant to isometric transformations such as rotation and translation of input images. In particular, images are represented as signals on graphs, which permits to replace classical convolution and pooling layers in deep networks with graph spectral convolution and dynamic graph pooling layers that together contribute to invariance to isometric transformation. Our experiments show high performance on rotated and translated images from the test set compared to classical architectures that are very sensitive to transformations in the data. The inherent invariance properties of our framework provide key advantages, such as increased resiliency to data variability and sustained performance with limited training sets.
","['Ecole Polytechnique Federale de Lausanne (EPFL)', 'EPFL']"
2017,Conditional Image Synthesis with Auxiliary Classifier GANs,"Augustus Odena, Christopher Olah, Jon Shlens",https://icml.cc/Conferences/2017/Schedule?showEvent=792,"In this paper we introduce new
methods for the improved training of generative
adversarial networks (GANs) for image synthesis.
We construct a variant of GANs employing
label conditioning that results in 128 × 128 resolution
image samples exhibiting global coherence.
We expand on previous work for image
quality assessment to provide two new analyses
for assessing the discriminability and diversity of
samples from class-conditional image synthesis
models. These analyses demonstrate that high
resolution samples provide class information not
present in low resolution samples. Across 1000
ImageNet classes, 128 × 128 samples are more
than twice as discriminable as artificially resized
32×32 samples. In addition, 84.7% of the classes
have samples exhibiting diversity comparable to
real ImageNet data.
","['Google Brain', 'Google Brain', 'Google Brain']"
2017,Stochastic DCA for the Large-sum of Non-convex Functions Problem and its Application to Group Variable Selection in Classification,"Hoai An Le Thi, Hoai Minh Le, Duy Nhat Phan, Bach Tran",https://icml.cc/Conferences/2017/Schedule?showEvent=886,"In this paper, we present a stochastic version of DCA (Difference of Convex functions Algorithm) to solve a class of optimization problems whose objective function is a large sum of non-convex functions and a regularization term. We consider the $\ell_{2,0}$ regularization to deal with the group variables selection. By exploiting the special structure of the problem, we propose an efficient DC decomposition for which the corresponding stochastic DCA scheme is very inexpensive: it only requires the projection of points onto balls that is explicitly computed. As an application, we applied our algorithm for the group variables selection in multiclass logistic regression. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithm and its superiority over well-known methods, with respect to classification accuracy, sparsity of solution as well as running time.","['Theoretical and Applied Computer Science Laboratory, University of Lorraine', 'Laboratory of Theoretical and Applied Computer Science, Univ. of Lorraine, Fr', 'Universite de Lorraine', 'University of Lorraine']"
2017,Prediction and Control with Temporal Segment Models,"Nikhil Mishra, Pieter Abbeel, Igor Mordatch",https://icml.cc/Conferences/2017/Schedule?showEvent=657,"We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.
","['UC Berkeley', 'OpenAI / UC Berkeley', 'OpenAI']"
2017,Learning Determinantal Point Processes with Moments and Cycles,"John C Urschel, Ankur Moitra, Philippe Rigollet, Victor-Emmanuel Brunel",https://icml.cc/Conferences/2017/Schedule?showEvent=721,"Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the cycle sparsity; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings. 
","['Massachusetts Institute of Technology', 'MIT', 'MIT', 'Massachusetts Institute of Technology']"
2017,Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU,"Zeyuan Allen-Zhu, Yuanzhi Li",https://icml.cc/Conferences/2017/Schedule?showEvent=492,"The online problem of computing the top eigenvector is fundamental to machine learning. The famous matrix-multiplicative-weight-update (MMWU) framework solves this online problem and gives optimal regret. However, since MMWU runs very slow due to the computation of matrix exponentials, researchers proposed the follow-the-perturbed-leader (FTPL) framework which is faster, but a factor $\sqrt{d}$ worse than the optimal regret for dimension-$d$ matrices.

We propose a \emph{follow-the-compressed-leader} framework which, not only matches the optimal regret of MMWU (up to polylog factors), but runs no slower than FTPL.

Our main idea is to ``compress'' the MMWU strategy to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This resolves an open question regarding how to obtain both (nearly) optimal and efficient algorithms for the online eigenvector problem.","['Microsoft Research / Princeton / IAS', 'Princeton University']"
2017,On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations,"Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti",https://icml.cc/Conferences/2017/Schedule?showEvent=602,"The problem of finding overlapping communities in networks has gained much attention recently. Optimization-based approaches use non-negative matrix factorization (NMF) or variants, but the global optimum cannot be provably attained in general. Model-based approaches, such as the popular mixed-membership stochastic blockmodel or MMSB (Airoldi et al., 2008), use parameters for each node to specify the overlapping communities, but standard inference techniques cannot guarantee consistency. We link the two approaches, by (a) establishing sufficient conditions for the symmetric NMF optimization to have a unique solution under MMSB, and (b) proposing a computationally efficient algorithm called GeoNMF that is provably optimal and hence consistent for a broad parameter regime. We demonstrate its accuracy on both simulated and real-world datasets.
","['University of Texas at Austin', 'UT Austin', 'University of Texas, Austin']"
2017,Analytical Guarantees on Numerical Precision of Deep Neural Networks,"Charbel Sakr, Yongjune Kim, Naresh Shanbhag",https://icml.cc/Conferences/2017/Schedule?showEvent=743,"The acclaimed successes of neural networks often overshadow their tremendous complexity. We focus on numerical precision - a key parameter defining the complexity of neural networks. First, we present theoretical bounds on the accuracy in presence of limited precision. Interestingly, these bounds can be computed via the back-propagation algorithm. Hence, by combining our theoretical analysis and the back-propagation algorithm, we are able to readily determine the minimum precision needed to preserve accuracy without having to resort to  time-consuming fixed-point simulations. We provide numerical evidence showing how our approach allows us to maintain high accuracy but with lower complexity than state-of-the-art binary networks.
","['University of Illinois at Urbana-Champaign', 'UIUC', 'University of Illinois']"
2017,Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees,"Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, Amir Zandieh",https://icml.cc/Conferences/2017/Schedule?showEvent=800,"Random Fourier features is one of the most popular techniques for scaling up kernel methods, such as kernel ridge regression. However, despite impressive empirical results, the statistical properties of random Fourier features are still not well understood. In this paper we take steps toward filling this gap. Specifically, we approach random Fourier features from a spectral matrix approximation point of view, give tight bounds on the number of Fourier features required to achieve a spectral approximation, and show how spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression.
","['Tel Aviv University', 'EPFL', '', '', '', 'EPFL']"
2017,Deriving Neural Architectures from Sequence and Graph Kernels,"Tao Lei, Wengong Jin, Regina Barzilay, Tommi Jaakkola",https://icml.cc/Conferences/2017/Schedule?showEvent=797,"The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process.
In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations.
We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. 
Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training.
We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.
","['MIT CSAIL', 'MIT Computer Science and Artificial Intelligence Laboratory', 'MIT CSAIL', 'MIT']"
2017,Learning to Discover Cross-Domain Relations with Generative Adversarial Networks,"Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, Jiwon Kim",https://icml.cc/Conferences/2017/Schedule?showEvent=496,"While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on a generative adversarial network that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.
","['SK T-Brain', 'SK T-Brain', 'SK T-Brain', 'SK T-Brain', 'SK T-Brain']"
2017,Gradient Projection Iterative Sketch for Large-Scale Constrained Least-Squares,"Junqi Tang, Mohammad Golbabaee, Michael E Davies",https://icml.cc/Conferences/2017/Schedule?showEvent=578,"We propose a randomized first order optimization algorithm Gradient Projection Iterative Sketch (GPIS) and an accelerated variant for efficiently solving large scale constrained Least Squares (LS). We provide the first theoretical convergence analysis for both algorithms. An efficient implementation using a tailored line-search scheme is also proposed. We demonstrate our methods' computational efficiency compared to the classical accelerated gradient method, and the variance-reduced stochastic gradient methods through numerical experiments in various large synthetic/real data sets.
","['the University of Edinburgh', 'the University of Edinburgh', 'University of Edinburgh']"
2017,An Alternative Softmax Operator for Reinforcement Learning,"Kavosh Asadi, Michael L. Littman",https://icml.cc/Conferences/2017/Schedule?showEvent=781,"A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.
","['Brown University', 'Brown University']"
2017,Deep Bayesian Active Learning with Image Data,"Yarin Gal, Riashat Islam, Zoubin Ghahramani",https://icml.cc/Conferences/2017/Schedule?showEvent=794,"Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).
","['University of Cambridge', 'McGill University', 'University of Cambridge & Uber']"
2017,On Kernelized Multi-armed Bandits,"Sayak Ray Chowdhury, Aditya Gopalan",https://icml.cc/Conferences/2017/Schedule?showEvent=779,"We consider the stochastic bandit problem with a continuous set of arms, with the expected reward function over the arms assumed to be fixed but unknown. We provide two new Gaussian process-based algorithms for continuous bandit optimization -- Improved GP-UCB (IGP-UCB) and GP-Thomson sampling (GP-TS), and derive corresponding regret bounds. Specifically, the bounds hold when the expected reward function belongs to the reproducing kernel Hilbert space (RKHS) that naturally corresponds to a Gaussian process kernel used as input by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vector-valued martingales of arbitrary, possibly infinite, dimension. Finally, experimental evaluation and comparisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favourable gains of the proposed strategies in many cases. 
","['Indian Institute of Science', 'Indian Institute of Science']"
2017,Nonnegative Matrix Factorization for Time Series Recovery From a Few Temporal Aggregates,"Jiali Mei, Yohann De Castro, Yannig Goude, Georges Hébrail",https://icml.cc/Conferences/2017/Schedule?showEvent=522,"Motivated by electricity consumption reconstitution, we propose a new matrix recovery method using nonnegative matrix factorization (NMF).
The task tackled here is to reconstitute electricity consumption time series at a fine temporal scale from measures that are temporal aggregates of individual consumption. 
Contrary to existing NMF algorithms, the proposed method uses temporal aggregates as input data, instead of matrix entries.
Furthermore, the proposed method is extended to take into account individual autocorrelation to provide better estimation, using a recent convex relaxation of quadratically constrained quadratic programs. 
Extensive experiments on synthetic and real-world electricity consumption datasets illustrate the effectiveness of the proposed method.
","['EDF R&D & Université Paris-Sud', 'LMO', 'EDF Lab Paris-Saclay', 'EDF Lab Paris-Saclay']"
2017,Follow the Moving Leader in Deep Learning,"Shuai Zheng, James Kwok",https://icml.cc/Conferences/2017/Schedule?showEvent=810,"Deep networks are highly nonlinear and difficult to optimize. During training, the parameter iterate may move from one local basin to another, or the data distribution may even change. Inspired by the close connection between stochastic optimization and online learning, we propose a variant of the {\em follow the regularized leader} (FTRL) algorithm called {\em follow the moving leader} (FTML). Unlike the FTRL family of algorithms, the recent samples are weighted more heavily in each iteration and so FTML can adapt more quickly to changes. We show that FTML enjoys the nice properties of RMSprop and Adam, while avoiding their pitfalls.  Experimental results on a number of deep learning models and tasks demonstrate that FTML converges quickly, and outperforms other state-of-the-art optimizers.
","['Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology']"
2017,Logarithmic Time One-Against-Some,"Hal Daumé, Nikos Karampatziakis, John Langford, Paul Mineiro",https://icml.cc/Conferences/2017/Schedule?showEvent=617,"We create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes. We show that several simple techniques give rise to an algorithm which is superior to previous logarithmic time classification approaches while competing with one-against-all in space. The core construction is based on using a tree to select a small subset of labels with high recall, which are then scored using a one-against-some structure with high precision.
","['University of Maryland', 'Microsoft', 'Microsoft Research', 'Microsoft']"
2017,Unsupervised Learning by Predicting Noise,"Piotr Bojanowski, Armand Joulin",https://icml.cc/Conferences/2017/Schedule?showEvent=520,"Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision; this paper introduces a generic framework to train such networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of the features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with the state-of-the-arts among unsupervised methods on ImageNet and Pascal VOC.
","['Facebook', 'Facebook']"
2017,Wasserstein Generative Adversarial Networks,"Martin Arjovsky, Soumith Chintala, Léon Bottou",https://icml.cc/Conferences/2017/Schedule?showEvent=799,"We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.
","['New York University', 'Facebook', 'Facebook']"
2017,Connected Subgraph Detection with Mirror Descent on SDPs,"Cem Aksoylar, Orecchia Lorenzo, Venkatesh Saligrama",https://icml.cc/Conferences/2017/Schedule?showEvent=873,"We propose a novel, computationally efficient mirror-descent based optimization framework for subgraph detection in graph-structured data. Our aim is to discover anomalous patterns present in a connected subgraph of a given graph. This problem arises in many applications such as detection of network intrusions, community detection, detection of anomalous events in surveillance videos or disease outbreaks. 
Since optimization over connected subgraphs is a combinatorial and computationally difficult problem, we propose a convex relaxation that offers a principled approach to incorporating connectivity and conductance constraints on candidate subgraphs. We develop a novel efficient algorithm to solve the relaxed problem, establish convergence  guarantees and demonstrate its feasibility and performance with experiments on real and very large simulated networks.
","['', 'Boston', 'Boston University']"
2017,Fake News Mitigation via Point Process Based Intervention,"Mehrdad Farajtabar, Jiachen Yang, Xiaojing Ye, Huan Xu, Rakshit Trivedi, Elias Khalil, Shuang Li, Le Song, Hongyuan Zha",https://icml.cc/Conferences/2017/Schedule?showEvent=857,"We propose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a point process network activity model. The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additional exogenous control terms. By choosing a feature representation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we map the problem of fake news mitigation into the reinforcement learning framework. We develop a policy iteration method unique to the multivariate networked point process, with the goal of optimizing the actions for maximal reward under budget constraints. Our method shows promising performance in real-time intervention experiments on a Twitter network to mitigate a surrogate fake news campaign, and outperforms alternatives on synthetic datasets.
","['Georgia Tech', 'Georgia Institute of Technology', 'Georgia State University', 'Georgia Tech', 'Georgia Institute of Technology', 'Georgia Tech', 'Georgia Tech', 'Georgia Institute of Technology', 'Georgia Institute of Technology']"
2017,Bayesian Boolean Matrix Factorisation,"Tammo Rukat, Christopher Holmes, Michalis Titsias, Christopher Yau",https://icml.cc/Conferences/2017/Schedule?showEvent=716,"Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.
","['University of Oxford', 'University of Oxford', 'Athens University of Economics and Business', 'University of Birmingham']"
2017,Second-Order Kernel Online Convex Optimization with Adaptive Sketching,"Daniele Calandriello, Alessandro Lazaric, Michal Valko",https://icml.cc/Conferences/2017/Schedule?showEvent=812,"Kernel online convex optimization (KOCO) is a framework combining the expressiveness of non-parametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require only $O(t)$ time and space per iteration, and, when the only information on the losses is their convexity, achieve a minimax optimal $O(\sqrt{T})$ regret. Nonetheless, many common losses in kernel problems, such as squared loss, logistic loss, and squared hinge loss posses stronger curvature that can be exploited. In this case, second-order KOCO methods achieve $O(\log(\Det(K)))$ regret, which we show scales as $O(deff \log T)$, where $deff$ is the effective dimension of the problem and is usually much smaller than $O(\sqrt{T})$. The main drawback of second-order methods is their much higher $O(t^2)$ space and time complexity.
In this paper, we introduce kernel online Newton step (KONS), a new second-order KOCO method that also achieves $O(deff\log T)$ regret. To address the computational complexity of second-order methods, we introduce a new matrix sketching algorithm for the kernel matrix~$K$, and show that for a chosen parameter $\gamma \leq 1$ our Sketched-KONS reduces the space and time complexity by a factor of $\gamma^2$ to $O(t^2\gamma^2)$ space and time per iteration, while incurring only $1/\gamma$ times more regret.
","['INRIA Lille', 'FACEBOOK', 'Inria Lille - Nord Europe']"
2017,Frame-based Data Factorizations,"Sebastian Mair, Ahcène Boubekki, Ulf Brefeld",https://icml.cc/Conferences/2017/Schedule?showEvent=722,"Archetypal Analysis is the method of choice to compute interpretable matrix factorizations. Every data point is represented as a convex combination of factors, i.e., points on the boundary of the convex hull of the data. This renders computation inefficient. In this paper, we show that the set of vertices of a convex hull, the so-called frame, can be efficiently computed by a quadratic program. We provide theoretical and empirical results  for our proposed approach and make use of the frame to accelerate Archetypal Analysis. The novel method yields similar reconstruction errors as baseline competitors but is much faster to compute.
","['Leuphana University Lüneburg', 'Leuphana University', 'Leuphana University']"
2017,Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank,"Liang Zhao, Siyu Liao, Yanzhi Wang, Zhe Li, Jian Tang, Bo Yuan",https://icml.cc/Conferences/2017/Schedule?showEvent=832,"Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. This paper gives theoretical study on LDR neural networks. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks.
","['The City University of New York', '', '', 'Syracuse University', 'Syracuse University', 'City College of New York, CUNY']"
2017,Understanding Black-box Predictions via Influence Functions,"Pang Wei Koh, Percy Liang",https://icml.cc/Conferences/2017/Schedule?showEvent=678,"How can we explain the predictions of a black-box model? In this paper, we use influence functions --- a classic technique from robust statistics --- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.
","['Stanford University', 'Stanford University']"
2017,Deep Transfer Learning with Joint Adaptation Networks,"Mingsheng Long, Han Zhu, Jianmin Wang, Michael Jordan",https://icml.cc/Conferences/2017/Schedule?showEvent=470,"Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'UC Berkeley']"
2017,Learning Hierarchical Features from Deep Generative Models,"Shengjia Zhao, Jiaming Song, Stefano Ermon",https://icml.cc/Conferences/2017/Schedule?showEvent=852,"Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge. 
","['Stanford University', 'Stanford University', 'Stanford University']"
2017,Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed Nonconvex Optimization and Learning Over Networks,"Mingyi Hong, Davood Hajinezhad, Ming-Min Zhao",https://icml.cc/Conferences/2017/Schedule?showEvent=749,"In this paper we consider  nonconvex optimization and learning over a network of distributed nodes.  We develop a  Proximal Primal-Dual Algorithm (Prox-PDA),  which enables the network nodes to distributedly and collectively compute the set of first-order stationary solutions in a global sublinear manner  [with a  rate of $\cO(1/r)$, where $r$ is the iteration counter]. To the best of our knowledge, this is the first algorithm that enables distributed nonconvex optimization with global rate guarantees.  Our numerical experiments also demonstrate the effectiveness of the proposed algorithm.   ","['Iowa State University', 'Iowa State University', 'Zhejiang University']"
2017,Curiosity-driven Exploration by Self-supervised Prediction,"Deepak Pathak, Pulkit Agrawal, Alexei Efros, Trevor Darrell",https://icml.cc/Conferences/2017/Schedule?showEvent=478,"In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.
","['UC Berkeley', '', 'UC Berkeley', 'University of California at Berkeley']"
2017,Learning the Structure of Generative Models without Labeled Data,"Stephen Bach, Bryan He, Alexander J Ratner, Christopher Re",https://icml.cc/Conferences/2017/Schedule?showEvent=464,"Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the l1-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100x faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford']"
2017,Dueling Bandits with Weak Regret,"Bangrui Chen, Peter I Frazier",https://icml.cc/Conferences/2017/Schedule?showEvent=587,"We consider online content recommendation with implicit feedback through pairwise comparisons, formalized as the so-called dueling bandit problem. We study the dueling bandit problem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms pulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm pulled is the Condorcet winner. We propose a new algorithm for this problem, Winner Stays (WS), with variations for each kind of regret:
WS for weak regret (WS-W) has expected cumulative weak regret that is $O(N^2)$, and $O(N\log(N))$ if arms have a total order; WS for strong regret (WS-S) has expected cumulative strong regret of $O(N^2 + N \log(T))$, and $O(N\log(N)+N\log(T))$ if arms have a total order.
WS-W is the first dueling bandit algorithm with weak regret that is constant in time.
WS is simple to compute, even for problems with many arms, and we demonstrate through numerical experiments on simulated and real data that WS has significantly smaller regret than existing algorithms in both the weak- and strong-regret settings.","['Cornell University', 'Cornell University']"
2017,Nearly Optimal Robust Matrix Completion,"Yeshwanth Cherapanamjeri, Prateek Jain, Kartik Gupta",https://icml.cc/Conferences/2017/Schedule?showEvent=772,"In this paper, we consider the problem of Robust Matrix Completion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corrupted. We propose a simple projected gradient descent-based method to estimate the low-rank matrix that alternately performs a projected gradient descent step and cleans up a few of the corrupted entries using hard-thresholding. Our algorithm solves RMC using nearly optimal number of observations while tolerating a nearly optimal number of corruptions. Our result also implies significant improvement over the existing time complexity bounds for the low-rank matrix completion problem. Finally, an application of our result to the robust PCA problem (low-rank+sparse matrix separation)  leads to nearly linear time (in matrix dimensions) algorithm for the same; existing state-of-the-art methods require quadratic time. Our empirical results corroborate our theoretical results and show that even for moderate sized problems, our method for robust PCA is an order of magnitude faster than the existing methods.
","['Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2017,Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs,"Alon Brutzkus, Amir Globerson",https://icml.cc/Conferences/2017/Schedule?showEvent=571,"Deep learning models are often successfully trained using gradient descent, despite the worst case hardness of the underlying non-convex optimization problem. The key question is then under what conditions can one prove that optimization will succeed. Here we provide a strong result of this kind. We consider a neural net with one hidden layer and a convolutional structure with no overlap and a ReLU activation function. For this architecture we show that learning is NP-complete in the general case, but that when the input distribution is Gaussian, gradient descent converges to the global optimum in polynomial time. To the best of our knowledge, this is the first global optimality guarantee of gradient descent on a convolutional neural network with ReLU activations.
","['Tel Aviv University', 'Tel Aviv University']"
2017,Re-revisiting Learning on Hypergraphs: Confidence Interval and Subgradient Method,"Chenzi Zhang, Shuguang Hu, Zhihao Gavin Tang, Hubert Chan",https://icml.cc/Conferences/2017/Schedule?showEvent=517,"We revisit semi-supervised learning on hypergraphs. Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable.  We exploit the non-uniqueness of the optimal solutions, and consider  confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution. Moreover, we give a much simpler approach for solving the convex program based on the subgradient method. Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our sub-gradient method gives faster running times when the number of vertices is much larger than the number of edges.
","['HKU', 'University of Hong Kong', 'University of Hong Kong', 'University of Hong Kong']"
2017,Meta Networks,"Tsendsuren Munkhdalai, Hong Yu",https://icml.cc/Conferences/2017/Schedule?showEvent=702,"Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.
","['University of Massachusetts', 'University of Massachusetts']"
2017,Bottleneck Conditional Density Estimation,"Rui Shu, Hung Bui, Mohammad Ghavamzadeh",https://icml.cc/Conferences/2017/Schedule?showEvent=824,"We introduce a new framework for training deep generative models for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are high-dimensional. Crucially, we propose a new hybrid training method that blends the conditional generative model with a joint generative model. Hybrid blending is the key to effective training of the BCDE, which avoids overfitting and provides a novel mechanism for leveraging unlabeled data. We show that our hybrid training procedure enables models to achieve competitive results in the MNIST quadrant prediction task in the fully-supervised setting, and sets new benchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA.
","['Stanford University', 'Adobe Research', 'Adobe Research & INRIA']"
2017,Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms,"Jialei Wang, Lin Xiao",https://icml.cc/Conferences/2017/Schedule?showEvent=863,"We consider empirical risk minimization of linear predictors with convex loss functions. Such problems can be reformulated as convex-concave saddle point problems and solved by primal-dual first-order algorithms. However, primal-dual algorithms often require explicit strongly convex 
regularization in order to obtain fast linear convergence, and the required dual proximal mapping may not admit closed-form or efficient solution.  In this paper, we develop both batch and randomized primal-dual algorithms  that can exploit strong convexity from data adaptively
and are capable of achieving linear convergence even without regularization. We also present dual-free variants of adaptive primal-dual algorithms 
that do not need the dual proximal mapping, 
which are especially suitable for logistic regression.
","['University of Chicago', 'Microsoft Research']"
2017,Interactive Learning from Policy-Dependent Human Feedback,"James MacGlashan, Mark Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts, Matthew E. Taylor, Michael L. Littman",https://icml.cc/Conferences/2017/Schedule?showEvent=846,"This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false---whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot.
","['Cogitai', 'Brown University', 'North Carolina State University', 'Washington State University', 'Brown University', 'North Carolina State University', 'Washington State University', 'Brown University']"
2017,Learning to Discover Sparse Graphical Models,"Eugene Belilovsky, Kyle Kastner, Gael Varoquaux, Matthew B Blaschko",https://icml.cc/Conferences/2017/Schedule?showEvent=706,"We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures.  Popular methods rely on estimating a penalized maximum likelihood of the precision matrix. However, in these approaches structure recovery is an indirect consequence of the data-fit term, the penalty can be difficult to adapt for domain-specific knowledge, and the inference is computationally demanding.
By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this
latter source of information as training data to learn a function, parametrized by a neural network, that
maps empirical covariance matrices to estimated graph structures.  Learning this function brings two benefits: it implicitly models the desired
structure or sparsity properties to form suitable priors, and it can be
tailored to the specific problem of edge structure discovery,
rather than maximizing data likelihood. Applying this framework, we find our learnable graph-discovery method trained on synthetic data generalizes
well: identifying relevant edges in both synthetic and real data,
completely unknown at training time. We find that on
genetics, brain imaging, and simulation data we obtain performance generally superior to analytical methods.
","['CentraleSupelec', '', 'Inria', 'KU Leuven']"
2017,On Context-Dependent Clustering of Bandits,"Claudio Gentile, Shuai Li, Purushottam Kar, Alexandros Karatzoglou, Giovanni Zappella, Evans Etrue Howard",https://icml.cc/Conferences/2017/Schedule?showEvent=583,"We investigate a novel cluster-of-bandit algorithm CAB for collaborative recommendation tasks that implements the underlying feedback sharing mechanism by estimating user neighborhoods in a context-dependent manner. CAB makes sharp departures from the state of the art by incorporating collaborative effects into inference, as well as learning processes in a manner that seamlessly interleaves explore-exploit tradeoffs and collaborative steps. We prove regret bounds for CAB under various data-dependent assumptions which exhibit a crisp dependence on the expected number of clusters over the users, a natural measure of the statistical difficulty of the learning task. Experiments on production and real-world datasets show that CAB offers significantly increased prediction performance against a representative pool of state-of-the-art methods.
","[""Universita dell'Insubria"", 'University of Cambridge', 'Indian Institute of Technology Kanpur', 'Telefonica Research', 'Amazon Dev Center Germany', 'University of Insubria']"
2017,Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations,"Yuanzhi Li, Yingyu Liang",https://icml.cc/Conferences/2017/Schedule?showEvent=635,"Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating minimization framework. 
However, it is unclear whether such algorithms can recover the ground-truth feature matrix when the weights for different features are highly correlated, which is common in applications.
This paper proposes a simple and natural alternating gradient descent based algorithm, and shows that with a mild initialization it provably recovers the ground-truth in the presence of strong correlations. 
In most interesting cases, the correlation can be in the same order as the highest possible. 
Our analysis also reveals its several favorable features including robustness to noise. 
We complement our theoretical results with empirical studies on semi-synthetic datasets, demonstrating its advantage over several popular methods in recovering the ground-truth.
","['Princeton University', 'Princeton University']"
2017,Convexified Convolutional Neural Networks,"Yuchen Zhang, Percy Liang, Martin Wainwright",https://icml.cc/Conferences/2017/Schedule?showEvent=540,"We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem.  For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN.  For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve competitive or better performance than CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.
","['Stanford', 'Stanford University', 'University of California at Berkeley']"
2017,Self-Paced Co-training,"Fan Ma, Deyu Meng, Qi Xie, Zina Li, Xuanyi Dong",https://icml.cc/Conferences/2017/Schedule?showEvent=518,"Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a ""draw without replacement"" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a cotraining process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced cotraining (SPaCo) with a ``draw with replacement"" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly  complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.
","['Xian Jiaotong University', '', '', '', 'University of Technology Sydney']"
2017,SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization,"Juyong Kim, Yookoon Park, Gunhee Kim, Sung Ju Hwang",https://icml.cc/Conferences/2017/Schedule?showEvent=490,"We propose a novel deep neural network that is both lightweight and effectively structured for model parallelization. Our network, which we name as SplitNet, automatically learns to split the network weights into either a set or a hierarchy of multiple groups that use disjoint sets of features, by learning both the class-to-group and feature-to-group assignment matrices along with the network weights. This produces a tree-structured network that involves no connection between branched subtrees of semantically disparate class groups. SplitNet thus greatly reduces the number of parameters and requires significantly less computations, and is also embarrassingly model parallelizable at test time, since the network evaluation for each subnetwork is completely independent except for the shared lower layer weights that can be duplicated over multiple processors. We validate our method with two deep network models (ResNet and AlexNet) on two different datasets (CIFAR-100 and ILSVRC 2012) for image classification, on which our method obtains networks with significantly reduced number of parameters while achieving comparable or superior classification accuracies over original full deep networks, and accelerated test speed with multiple GPUs.
","['Seoul National University', 'Seoul National University', 'Seoul National University', 'UNIST / AItrics']"
2017,Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo,Matthew Hoffman,https://icml.cc/Conferences/2017/Schedule?showEvent=876,"Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation.  In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and
 does not suffer from the variational overpruning effect. MCMC's additional computational overhead proves to be 
significant, but not prohibitive.
",['Google Research']
2017,Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization,"Qi Lei, En-Hsu Yen, Chao-Yuan Wu, Inderjit Dhillon, Pradeep Ravikumar",https://icml.cc/Conferences/2017/Schedule?showEvent=864,"We consider the popular problem of sparse empirical risk minimization with linear predictors and a large number of both features and observations. With a convex-concave saddle point objective reformulation, we propose a Doubly Greedy Primal-Dual Coordinate Descent algorithm that is able to exploit sparsity in both primal and dual variables. It enjoys a low cost per iteration and our theoretical analysis shows that it converges linearly with a good iteration complexity, provided that the set of primal variables is sparse. We then extend this algorithm further to leverage active sets. The resulting new algorithm is even faster, and experiments on large-scale Multi-class data sets show that our algorithm achieves up to 30 times speedup on several state-of-the-art optimization methods.
","['University of Texas at Austin', 'Carnegie Mellon University', 'UT Austin', 'UT Austin & Amazon', 'Carnegie Mellon University']"
2017,End-to-End Differentiable Adversarial Imitation Learning,"Nir Baram, Oron Anschel, Itai Caspi, Shie Mannor",https://icml.cc/Conferences/2017/Schedule?showEvent=586,"Generative Adversarial Networks (GANs) have been successfully applied to the problem of \emph{policy imitation} in a model-free setup. However, the computation graph of GANs, that include a stochastic policy as the generative model, is no longer differentiable end-to-end, which requires the use of high-variance gradient estimation. In this paper, we introduce the Model-based Generative Adversarial Imitation Learning (MGAIL) algorithm. We show how to use a forward model to make the computation fully differentiable, which enables training policies using the exact gradient of the discriminator. The resulting algorithm trains competent policies using relatively fewer expert samples and interactions with the environment. We test it on both discrete and continuous action domains and report results that surpass the state-of-the-art.
","['Technion - Israel Institute of Technology', 'Technion', 'Technion', 'Technion']"
2017,Local-to-Global Bayesian Network Structure Learning,"Tian Gao, Kshitij Fadnis, Murray Campbell",https://icml.cc/Conferences/2017/Schedule?showEvent=688,"We introduce a new local-to-global structure learning algorithm, called graph growing  structure learning (GGSL), to learn Bayesian network (BN) structures.  GGSL starts at a (random) node and then gradually expands the learned structure through a series of local learning steps. At each local learning step, the proposed algorithm only needs to revisit a subset of the learned nodes, consisting of the local neighborhood of a target, and therefore improves on both memory and time efficiency compared to traditional global structure learning approaches. GGSL also improves on the existing local-to-global learning approaches by removing the need for conflict-resolving AND-rules, and achieves better learning accuracy. We provide theoretical analysis for the local learning step, and show  that GGSL outperforms existing algorithms on benchmark datasets. Overall, GGSL demonstrates a novel direction to scale up BN structure learning while limiting accuracy loss.
","['IBM Research', 'IBM', 'IBM']"
2017,Provably Optimal Algorithms for Generalized Linear Contextual Bandits,"Lihong Li, Yu Lu, Dengyong Zhou",https://icml.cc/Conferences/2017/Schedule?showEvent=654,"Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an ~O(sqrt{dT}) regret over T rounds with d dimensional feature vectors.  This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a sqrt{d} factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases.
","['Microsoft Research', 'Yale University', 'Microsoft Research']"
2017,No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis,"Rong Ge, Chi Jin, Yi Zheng",https://icml.cc/Conferences/2017/Schedule?showEvent=777,"In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.
","['Duke University', 'UC Berkeley', 'Duke University']"
2017,On the Expressive Power of Deep Neural Networks,"Maithra Raghu, Ben Poole, Surya Ganguli, Jon Kleinberg, Jascha Sohl-Dickstein",https://icml.cc/Conferences/2017/Schedule?showEvent=841,"We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.
","['Google Brain / Cornell University', 'Stanford University', 'Stanford', 'Cornell University', 'Google Brain']"
2017,Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data,"Tomoya Sakai, Marthinus C du Plessis, Gang Niu, Masashi Sugiyama",https://icml.cc/Conferences/2017/Schedule?showEvent=603,"Most of the semi-supervised classification methods developed so far use unlabeled data for regularization purposes under particular distributional assumptions such as the cluster assumption. In contrast, recently developed methods of classification from positive and unlabeled data (PU classification) use unlabeled data for risk evaluation, i.e., label information is directly extracted from unlabeled data. In this paper, we extend PU classification to also incorporate negative data and propose a novel semi-supervised learning approach. We establish generalization error bounds for our novel methods and show that the bounds decrease with respect to the number of unlabeled data without the distributional assumptions that are required in existing semi-supervised learning methods. Through experiments, we demonstrate the usefulness of the proposed methods.
","['The University of Tokyo / RIKEN', 'N/A', 'University of Tokyo', 'RIKEN / The University of Tokyo']"
2017,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,"Chelsea Finn, Pieter Abbeel, Sergey Levine",https://icml.cc/Conferences/2017/Schedule?showEvent=495,"We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.
","['UC Berkeley', 'OpenAI / UC Berkeley', 'Berkeley']"
2017,Zero-Inflated Exponential Family Embeddings,"Liping Liu, David Blei",https://icml.cc/Conferences/2017/Schedule?showEvent=625,"Word embeddings are a widely-used tool to analyze language, and exponential family embeddings (Rudolph et al., 2016) generalize the technique to other types of data. One challenge to fitting embedding methods is sparse data, such as a document/term matrix that contains many zeros. To address this issue, practitioners typically downweight or subsample the zeros, thus focusing learning on the non-zero entries. In this paper, we develop zero-inflated embeddings, a new embedding method that is designed to learn from sparse observations. In a zero-inflated embedding (ZIE), a zero in the data can come from an interaction to other data (i.e., an embedding) or from a separate process by which many observations are equal to zero (i.e. a probability mass at zero). Fitting a ZIE naturally downweights the zeros and dampens their influence on the model. Across many types of data---language, movie ratings, shopping histories, and bird watching logs---we found that zero-inflated embeddings provide improved predictive performance over standard approaches and find better vector representation of items.
","['Columbia University', 'Columbia University']"
2017,A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates,"Tianbao Yang, Qihang Lin, Lijun Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=511,"This paper focuses on  convex constrained optimization problems, where the solution is subject to a convex inequality constraint. In particular, we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are  time-consuming, which render both projected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) expensive. In this paper, we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates under a certain regularity condition of the constraint function. We first present a general theory of optimization with only one projection. Its application to smooth optimization with only one projection yields $O(1/\epsilon)$ iteration complexity, which  improves over the $O(1/\epsilon^2)$ iteration complexity established before for non-smooth optimization  and can be further reduced under strong convexity. Then we introduce a local error bound condition and develop faster algorithms for non-strongly convex optimization at the price of a logarithmic number of projections. In particular, we achieve  an iteration complexity  of $\widetilde O(1/\epsilon^{2(1-\theta)})$ for non-smooth optimization and $\widetilde O(1/\epsilon^{1-\theta})$ for smooth optimization, where $\theta\in(0,1]$ appearing the local error bound condition characterizes the functional local growth rate around the optimal solutions. Novel applications in solving the constrained $\ell_1$ minimization problem  and a positive semi-definite constrained distance metric learning problem demonstrate that the proposed algorithms achieve significant speed-up compared with previous algorithms. ","['The University of Iowa', 'Univ Iowa', 'Nanjing University']"
2017,Learning in POMDPs with Monte Carlo Tree Search,"Sammie Katt, Frans A Oliehoek, Chris Amato",https://icml.cc/Conferences/2017/Schedule?showEvent=750,"The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BA-POMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several
techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.
","['Northeastern University', 'University of Liverpool', 'Northeastern University']"
2017,Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data,"XIUYAN NI, Novi Quadrianto, Yusu Wang, Chao Chen",https://icml.cc/Conferences/2017/Schedule?showEvent=504,"Clustering data with both continuous and discrete attributes is a challenging task. Existing methods lack a principled probabilistic formulation. In this paper, we propose a clustering method based on a tree-structured graphical model to describe the generation process of mixed-type data. Our tree-structured model factorized into a product of pairwise interactions, and thus localizes the interaction between feature variables of different types. To provide a robust clustering method based on the tree-model, we adopt a topographical view and compute peaks of the density function and their attractive basins for clustering. Furthermore, we leverage the theory from topology data analysis to adaptively merge trivial peaks into large ones in order to achieve meaningful clusterings. Our method outperforms state-of-the-art methods on mixed-type data.
","['THE GRADUATE CENTER, CUNY', 'University of Sussex and National Research University Higher School of Economics', 'Ohio State University', 'City University of New York (CUNY)']"
2017,Safety-Aware Algorithms for Adversarial Contextual Bandit,"Wen Sun, Debadeepta Dey, Ashish Kapoor",https://icml.cc/Conferences/2017/Schedule?showEvent=628,"In this work we study the safe sequential decision making problem under the setting of adversarial contextual bandits with sequential risk constraints. At each round, nature prepares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to pull an arm and receives the corresponding cost and risk associated with the pulled arm. In addition to minimizing the cumulative cost, for safety purposes, the learner needs to make safe decisions such that the average of the cumulative risk from all pulled arms should not be larger than a pre-defined threshold. To address this problem, we first study online convex programming in the full information setting where in each round the learner receives an adversarial convex loss and a convex constraint. We develop a meta algorithm leveraging online mirror descent for the full information setting and then extend it to contextual bandit with sequential risk constraints setting using expert advice. Our algorithms can achieve near-optimal regret in terms of minimizing the total cost, while successfully maintaining a sub- linear growth of accumulative risk constraint violation. We support our theoretical results by demonstrating our algorithm on a simple simulated robotics reactive control task.
","['Carnegie Mellon University', 'Microsoft', 'Microsoft Research']"
2017,"Coherence Pursuit: Fast, Simple, and Robust Subspace Recovery","Mostafa Rahmani, George Atia",https://icml.cc/Conferences/2017/Schedule?showEvent=527,"This paper presents a remarkably simple, yet powerful, algorithm for robust Principal Component Analysis (PCA). In the proposed approach, an outlier is set apart from an inlier by comparing their coherence with the rest of the data points. As inliers lie on a low dimensional subspace, they are likely to have strong mutual coherence provided there are enough inliers. By contrast, outliers do not typically admit low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with a large number of data points. The mutual coherences are computed by forming the Gram matrix of normalized data points. Subsequently, the subspace is recovered from the span of a small subset of the data points that exhibit strong coherence with the rest of the data. As coherence pursuit only involves one simple matrix multiplication, it is significantly faster than the state of-the-art robust PCA algorithms. We provide a mathematical analysis of the proposed algorithm under a random model for the distribution of the inliers and outliers. It is shown that the proposed method can recover the correct subspace even if the data is predominantly outliers. To the best of our knowledge, this is the first provable robust PCA algorithm that is simultaneously non-iterative, can tolerate a large number of outliers and is robust to linearly dependent outliers.
","['University of Central Florida', 'University of Central Florida']"
2017,Depth-Width Tradeoffs in Approximating Natural Functions With Neural Networks,"Itay Safran, Ohad Shamir",https://icml.cc/Conferences/2017/Schedule?showEvent=593,"We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger. This includes indicators of balls and ellipses; non-linear functions which are radial with respect to the $L_1$ norm; and smooth non-linear functions. We also show that these gaps can be observed experimentally: Increasing the depth indeed allows better learning than increasing width, when training neural networks to learn an indicator of a unit ball.","['Weizmann Institute of Science', 'Weizmann Institute of Science']"
2017,Iterative Machine Teaching,"Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda Smith, James Rehg, Le Song",https://icml.cc/Conferences/2017/Schedule?showEvent=858,"In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner. We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.
","['Georgia Tech', 'Georgia Tech', 'Georgia Institute of Technology', 'Indiana University', 'Indiana University', 'Indiana University', 'Georgia Tech', 'Georgia Institute of Technology']"
2017,AdaNet: Adaptive Structural Learning of Artificial Neural Networks,"Corinna Cortes, Xavi Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, Scott Yang",https://icml.cc/Conferences/2017/Schedule?showEvent=682,"We present a new framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accompanied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.
","['Google Research', 'Google Inc.', 'Google', 'Courant Institute and Google Research', 'Courant Institute']"
2017,Convex Phase Retrieval without Lifting via PhaseMax,"Tom Goldstein, Christoph Studer",https://icml.cc/Conferences/2017/Schedule?showEvent=636,"Semidefinite relaxation methods transform a variety of non-convex optimization problems into convex problems, but square the number of variables.  We study a new type of convex relaxation for phase retrieval problems, called PhaseMax, that convexifies the underlying problem without lifting. The resulting problem formulation can be solved using standard convex optimization routines, while still working in the original, low-dimensional variable space.  We prove, using a random spherical distribution measurement model, that PhaseMax succeeds with high probability for a sufficiently large number of measurements.  We compare our approach to other phase retrieval methods and demonstrate that our theory accurately predicts the success of PhaseMax.  
","['University of Maryland', 'Cornell University']"
2017,DARLA: Improving Zero-Shot Transfer in Reinforcement Learning,"Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, Alexander Lerchner",https://icml.cc/Conferences/2017/Schedule?showEvent=711,"Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA’s vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Deepmind', 'DeepMind', 'DeepMind', 'DeepMind']"
2017,On Relaxing Determinism in Arithmetic Circuits,"Arthur Choi, Adnan Darwiche",https://icml.cc/Conferences/2017/Schedule?showEvent=815,"The past decade has seen a significant interest in learning tractable probabilistic representations. Arithmetic circuits (ACs) were among the first proposed tractable representations, with some subsequent representations being instances of ACs with weaker or stronger properties.  In this paper, we provide a formal basis under which variants on ACs can be compared, and where the precise roles and semantics of their various properties can be made more transparent.  This allows us to place some recent developments on ACs in a clearer perspective and to also derive new results for ACs. This includes an exponential separation between ACs with and without determinism; completeness and incompleteness results; and tractability results (or lack thereof) when computing most probable explanations (MPEs).
","['UCLA', 'UCLA']"
2017,Adaptive Multiple-Arm Identification,"Jiecao Chen, Xi Chen, Qin Zhang, Yuan Zhou",https://icml.cc/Conferences/2017/Schedule?showEvent=546,"We study the problem of selecting K arms with the highest expected rewards in a stochastic n-armed bandit game. This problem has a wide range of applications, e.g., A/B testing, crowdsourcing, simulation optimization. Our goal is to develop a PAC algorithm, which, with probability at least 1-\delta, identifies a set of K arms with the aggregate regret at most \epsilon. The notion of aggregate regret for multiple-arm identification was first introduced in Zhou et. al. (2014), which is defined as the difference of the averaged expected rewards between the selected set of arms and the best K arms. In contrast to Zhou et. al. (2014) that only provides instance-independent sample complexity, we introduce a new hardness parameter for characterizing the difficulty of any given instance. We further develop two algorithms and establish the corresponding sample complexity in terms of this hardness parameter. The derived sample complexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-independent lower bound upto a \log(\epsilon^{-1}) factor in the worst case. We also prove a lower bound result showing that the extra \log(\epsilon^{-1}) is necessary for instance-dependent algorithms using the introduced hardness parameter.
","['Indiana University Bloomington', 'New York University', 'Indiana University Bloomington', 'Indiana University Bloomington']"
2017,Tensor Decomposition with Smoothness,"Masaaki Imaizumi, Kohei Hayashi",https://icml.cc/Conferences/2017/Schedule?showEvent=556,"Real data tensors are usually high dimensional but their intrinsic information is preserved in low-dimensional space, which motivates to use tensor decompositions such as Tucker decomposition. Often, real data tensors are not only low dimensional, but also smooth, meaning that the adjacent elements are similar or continuously changing, which typically appear as spatial or temporal data. To incorporate the smoothness property, we propose the smoothed Tucker decomposition (STD). STD leverages the smoothness by the sum of a few basis functions, which reduces the number of parameters. The objective function is formulated as a convex problem and, to solve that, an algorithm based on the alternating direction method of multipliers is derived. We theoretically show that, under the smoothness assumption, STD achieves a better error bound. The theoretical result and performances of STD are numerically verified.
","['Institute of Statistical Mathematics', 'AIST / RIKEN']"
2017,Automated Curriculum Learning for Neural Networks,"Alex Graves, Marc Bellemare, Jacob Menick, Remi Munos, koray kavukcuoglu",https://icml.cc/Conferences/2017/Schedule?showEvent=701,"We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency.
A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus.
We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity.
Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2017,Attentive Recurrent Comparators,"Pranav Shyam, Shubham Gupta, Ambedkar Dukkipati",https://icml.cc/Conferences/2017/Schedule?showEvent=730,"Rapid learning requires flexible representations to quickly adopt to new evidence. We develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form representations of objects by cycling through them and making observations. Using the representations extracted by ARCs, we develop a way of approximating a \textit{dynamic representation space} and use it for one-shot learning. In the task of one-shot classification on the Omniglot dataset, we achieve the state of the art performance with an error rate of 1.5\%. This represents the first super-human result achieved for this task with a generic model that uses only pixel information.
","['R. V. College of Engineering & Indian Institute of Science', 'Indian Institute of Science', 'Indian Institute of Science']"
2017,An Infinite Hidden Markov Model With Similarity-Biased Transitions,"Colin Dawson, Chaofan Huang, Clayton T. Morrison",https://icml.cc/Conferences/2017/Schedule?showEvent=649,"We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between ""nearby"" states.  This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pairwise similarities, thereby inducing correlations among the transition distributions. We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states.  This augmentation restores conditional conjugacy and admits a simple Gibbs sampler. We evaluate the model and inference method on a speaker diarization task and a ""harmonic parsing"" task using four-part chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.
","['Oberlin College', 'Oberlin College', 'University of Arizona']"
2017,Efficient Nonmyopic Active Search,"Shali Jiang, Luiz Gustavo Malkomes, Geoff Converse, Alyssa Shofner, Benjamin Moseley, Roman Garnett",https://icml.cc/Conferences/2017/Schedule?showEvent=553,"Active search is an active learning setting with the goal of identifying as many members of a given class as possible under a labeling budget. In this work, we first establish a theoretical hardness of active search, proving that no polynomial-time policy can achieve a constant factor approximation ratio with respect to the expected utility of the optimal policy. We also propose a novel, computationally efficient active search policy achieving exceptional performance on several real-world tasks. Our policy is nonmyopic, always considering the entire remaining search budget. It also automatically and dynamically balances exploration and exploitation consistent with the remaining budget, without relying on a parameter to control this tradeoff. We conduct experiments on diverse datasets from several domains: drug discovery, materials science, and a citation network. Our efficient nonmyopic policy recovers significantly more valuable points with the same budget than several alternatives from the literature, including myopic approximations to the optimal policy.
","['Washington University in St. Louis', 'Washington University in St. Louis', 'Simpson College', 'University of South Carolina', 'Washington University in St. Louis', 'Washington University in St. Louis']"
2017,Asymmetric Tri-training for Unsupervised Domain Adaptation,"Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada",https://icml.cc/Conferences/2017/Schedule?showEvent=616,"It is important to apply models trained on a large number of labeled samples to different domains because collecting many labeled samples in various domains is expensive. To learn discriminative representations for the target domain, we assume that artificially labeling the target samples can result in a good representation. Tri-training leverages three classifiers equally to provide pseudo-labels to unlabeled samples; however, the method does not assume labeling samples generated from a different domain. In this paper, we propose the use of an \textit{asymmetric} tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train the neural networks as if they are true labels. In our work, we use three networks \textit{asymmetrically}, and by \textit{asymmetric}, we mean that two networks are used to label unlabeled target samples, and one network is trained by the pseudo-labeled samples to obtain target-discriminative representations. Our proposed method was shown to achieve a state-of-the-art performance on the benchmark digit recognition datasets for domain adaptation.
","['The University of Tokyo', 'The University of Tokyo', 'The Univ. of Tokyo / RIKEN']"
2017,State-Frequency Memory Recurrent Neural Networks,"Hao Hu, Guo-Jun Qi",https://icml.cc/Conferences/2017/Schedule?showEvent=515,"Modeling temporal sequences plays a fundamental role in various modern applications and has drawn more and more attentions in the machine learning community. Among those efforts on improving the capability to represent temporal data, the Long Short-Term Memory (LSTM) has achieved great success in many areas. Although the LSTM can capture long-range dependency in the time domain, it does not explicitly model the pattern occurrences in the frequency domain that plays an important role in tracking and predicting data points over various time cycles. We propose the State-Frequency Memory (SFM), a novel recurrent architecture that allows to separate dynamic patterns across different frequency components and their impacts on modeling the temporal contexts of input sequences. By jointly decomposing memorized dynamics into state-frequency components, the SFM is able to offer a fine-grained analysis of temporal sequences by capturing the dependency of uncovered patterns in both time and frequency domains. Evaluations on several temporal modeling tasks demonstrate the SFM can yield competitive performances, in particular as compared with the state-of-the-art LSTM models.
","['University of Central Florida', 'University of Central Florida']"
2017,Batched High-dimensional Bayesian Optimization via Structural Kernel Learning,"Zi Wang, Chengtao Li, Stefanie Jegelka, Pushmeet Kohli",https://icml.cc/Conferences/2017/Schedule?showEvent=739,"Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings. In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method. Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes. Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.
","['MIT', 'MIT', 'MIT', 'Microsoft Research']"
2017,Leveraging Union of Subspace Structure to Improve Constrained Clustering,"John Lipor, Laura Balzano",https://icml.cc/Conferences/2017/Schedule?showEvent=613,"Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present a pairwise-constrained clustering algorithm that actively selects queries based on the union-of-subspaces model. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. We prove that points lying near the intersection of subspaces are points with low margin. Our procedure can be used after any subspace clustering algorithm that outputs an affinity matrix. We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the state-of-the-art active query algorithms on datasets with subspace structure and is competitive on other datasets. 
","['University of Michigan', 'University of Michigan']"
2017,Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression,"PENGFEI WEI, Ramon Sagarna, Yiping Ke, Yew Soon ONG, CHI GOH",https://icml.cc/Conferences/2017/Schedule?showEvent=679,"A key challenge in multi-source transfer learning is to capture the diverse inter-domain similarities. In this paper, we study different approaches based on Gaussian process models to solve the multi-source transfer regression problem. Precisely, we first investigate the feasibility and performance of a family of transfer covariance functions that represent the pairwise similarity of each source and the target domain. We theoretically show that using such a transfer covariance function for general Gaussian process modelling can only capture the same similarity coefficient for all the sources, and thus may result in unsatisfactory transfer performance.
This leads us to propose \emph{TC$_{MS}$Stack}, an integrated strategy incorporating the benefits of the transfer covariance function and stacking. Extensive experiments on one synthetic and two real-world datasets, with learning settings of up to 11 sources for the latter, demonstrate the effectiveness of our proposed \emph{TC$_{MS}$Stack}.","['Nanyang Technological University, Singapore', '', 'Nanyang Technological University', 'Nanyang Technological University', '']"
2017,Delta Networks for Optimized Recurrent Network Computation,"Daniel Neil, Jun Lee, Tobi Delbruck, Shih-Chii Liu",https://icml.cc/Conferences/2017/Schedule?showEvent=535,"Many neural networks exhibit stability in their activation patterns over time in response to inputs from sensors operating under real-world conditions. By capitalizing on this property of natural signals, we propose a Recurrent Neural Network (RNN) architecture called a delta network in which each neuron transmits its value only when the change in its activation exceeds a threshold. The execution of RNNs as delta networks is attractive because their states must be stored and fetched at every timestep, unlike in convolutional neural networks (CNNs). We show that a naive run-time delta network implementation offers modest improvements on the number of memory accesses and computes, but optimized training techniques confer higher accuracy at higher speedup. With these optimizations, we demonstrate a 9X reduction in cost with negligible loss of accuracy for the TIDIGITS audio digit recognition benchmark.  Similarly, on the large Wall Street Journal (WSJ) speech recognition benchmark, pretrained networks can also be greatly accelerated as delta networks and trained delta networks show a 5.7x improvement with negligible loss of accuracy. Finally, on an end-to-end CNN-RNN network trained for steering angle prediction in a driving dataset, the RNN cost can be reduced by a substantial 100X.
","['Institute of Neuroinformatics', 'Samsung Advanced Institute of Technology', 'Institute of Neuroinformatics', 'Institute of Neuroinformatics']"
2017,From Patches to Images: A Nonparametric Generative Model,"Geng Ji, Michael Hughes, Erik Sudderth",https://icml.cc/Conferences/2017/Schedule?showEvent=753,"We propose a hierarchical generative model that captures the self-similar structure of image regions as well as how this structure is shared across image collections. Our model is based on a novel, variational interpretation of the popular expected patch log-likelihood (EPLL) method as a model for randomly positioned grids of image patches. While previous EPLL methods modeled image patches with finite Gaussian mixtures, we use nonparametric Dirichlet process (DP) mixtures to create models whose complexity grows as additional images are observed. An extension based on the hierarchical DP then captures repetitive and self-similar structure via image-specific variations in cluster frequencies. We derive a structured variational inference algorithm that adaptively creates new patch clusters to more accurately model novel image textures. Our denoising performance on standard benchmarks is superior to EPLL and comparable to the state-of-the-art, and provides novel statistical justifications for common image processing heuristics. We also show accurate image inpainting results.
","['Brown University', 'Harvard University', 'University of California, Irvine']"
2017,Active Heteroscedastic Regression,"Kamalika Chaudhuri, Prateek Jain, Nagarajan Natarajan",https://icml.cc/Conferences/2017/Schedule?showEvent=785,"An active learner is given a model class $\Theta$, a large sample of unlabeled data drawn from an underlying distribution and access to a labeling oracle which can provide a label for any of the unlabeled instances. The goal of the learner is to find a model $\theta \in \Theta$ that fits the data to a given accuracy while making as few label queries to the oracle as possible. In this work, we consider a theoretical analysis of the label requirement of active learning for regression under a heteroscedastic noise model.

Previous work has looked at active regression either with no model mismatch~\cite{chaudhuri2015convergence} or with arbitrary model mismatch~\cite{sabato2014active}. In the first case, active learning provided no improvement even in the simple case where the unlabeled examples were drawn from Gaussians. In the second case, under arbitrary model mismatch, the algorithm either required a very high running time or a large number of labels. We provide bounds on the convergence rates of active and passive learning for heteroscedastic regression, where the noise depends on the instance. Our results illustrate that just like in binary classification, some partial knowledge of the nature of the noise can lead to significant gains in the label requirement of active learning. ","['University of California at San Diego', 'Microsoft Research', 'Microsoft Research']"
2017,Multi-task Learning with Labeled and Unlabeled Tasks,"Anastasia Pentina, Christoph H. Lampert",https://icml.cc/Conferences/2017/Schedule?showEvent=690,"In multi-task learning, a learner is given a collection of prediction tasks and needs to solve all of them.  In contrast to previous work, which required that annotated training data must be available for all tasks, we consider a new setting, in which for some tasks, potentially most of them, only unlabeled training data is provided. Consequently, to solve all tasks, information must be transferred between tasks with labels and tasks without labels.  Focusing on an instance-based transfer method we analyze two variants of this setting: when the set of labeled tasks is fixed, and when it can be actively selected by the learner. We state and prove a generalization bound that covers both scenarios and derive from it an algorithm for making the choice of labeled tasks (in the active case) and for transferring information between the tasks in a principled way. We also illustrate the effectiveness of the algorithm on synthetic and real data.
","['IST Austria', 'IST Austria']"
2017,Recurrent Highway Networks,"Julian Zilly, Rupesh Srivastava, Jan Koutnik, Jürgen Schmidhuber",https://icml.cc/Conferences/2017/Schedule?showEvent=656,"Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with ""deep"" transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin’s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.
","['ETH Zurich', 'IDSIA (University of Lugano)', 'NNAISSENSE', 'Swiss AI Lab']"
2017,Fast Bayesian Intensity Estimation for the Permanental Process,"Christian Walder, Adrian N Bishop",https://icml.cc/Conferences/2017/Schedule?showEvent=622,"The Cox process is a stochastic process which generalises the Poisson process by letting the underlying intensity function itself be a stochastic process. In this paper we present a fast Bayesian inference scheme for the permanental process, a Cox process under which the square root of the intensity is a Gaussian process. In particular we exploit connections with reproducing kernel Hilbert spaces, to derive efficient approximate Bayesian inference algorithms based on the Laplace approximation to the predictive distribution and marginal likelihood. We obtain a simple algorithm which we apply to toy and real-world problems, obtaining orders of magnitude speed improvements over previous work.
","['CSIRO Data61', 'Data61/ANU/UTS']"
2017,Active Learning for Cost-Sensitive Classification,"Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daumé III, John Langford",https://icml.cc/Conferences/2017/Schedule?showEvent=795,"We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label’s cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. Our experiment with COAL show significant improvements in labeling effort and test cost over passive and active baselines.
","['UMass', 'Microsoft Research', 'Uber', 'University of Maryland', 'Microsoft Research']"
2017,Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics,"Ken Kansky, Thomas Silver, David A Mély, Mohamed Eldawy, Miguel Lazaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, Dileep George",https://icml.cc/Conferences/2017/Schedule?showEvent=652,"The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.
","['Vicarious AI', 'Vicarious AI', 'Vicarious AI', 'Vicarious AI', 'Vicarious AI', 'Vicarious AI', 'Vicarious AI', 'OpenAI', 'Vicarious AI', 'Vicarious AI']"
2017,A Birth-Death Process for Feature Allocation,"Konstantina Palla, David Knowles, Zoubin Ghahramani",https://icml.cc/Conferences/2017/Schedule?showEvent=692,"We propose a Bayesian nonparametric prior over feature allocations 
for sequential data, the birth-death feature allocation process (BDFP). The BDFP models 
the evolution of the feature allocation of a set of N objects across a covariate (e.g.~time) 
by creating and deleting features. A BDFP is exchangeable, projective, stationary and reversible, 
and its equilibrium distribution is given by the Indian buffet process (IBP). We show that the Beta process 
on an extended space is the de Finetti mixing distribution underlying the BDFP. Finally, we present the finite approximation of the BDFP, the Beta Event Process (BEP), that permits simplified inference. The utility of the BDFP as a prior is demonstrated on real world dynamic genomics and social network data. 
","['Oxford', 'Stanford', 'University of Cambridge & Uber']"
2017,Diameter-Based Active Learning,"Christopher Tosh, Sanjoy Dasgupta",https://icml.cc/Conferences/2017/Schedule?showEvent=804,"To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.
","['University of California, San Diego', 'UCSD']"
2017,Risk Bounds for Transferring Representations With and Without Fine-Tuning,"Daniel McNamara, Nina Balcan",https://icml.cc/Conferences/2017/Schedule?showEvent=869,"A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. We develop sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight transfer, which we validate with experiments.
","['Australian National University and Data61', 'Carnegie Mellon University']"
2017,The loss surface of deep and wide neural networks,"Quynh Nguyen, Matthias Hein",https://icml.cc/Conferences/2017/Schedule?showEvent=724,"While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.
","['Saarland University', 'Saarland University']"
2017,Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks,"David Balduzzi, Brian McWilliams, Tony Butler-Yeoman",https://icml.cc/Conferences/2017/Schedule?showEvent=600,"Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex; standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets, which furthermore matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization. The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets -- that gradients are shattered --  and investigates the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions.
","['Victoria University Wellington', 'Disney Research', 'Victoria University of Wellington']"
2017,Sharp Minima Can Generalize For Deep Nets,"Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio",https://icml.cc/Conferences/2017/Schedule?showEvent=607,"Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. \citet{hochreiter1997flat, keskar2016large}, is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization.  Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Or, depending on the definition of flatness, it is the same for any given minimum. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.
","['University of Montreal', 'DeepMind', 'Google Brain', 'U. Montreal']"
2017,Geometry of Neural Network Loss Surfaces via Random Matrix Theory,"Jeffrey Pennington, Yasaman Bahri",https://icml.cc/Conferences/2017/Schedule?showEvent=655,"Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, $\phi$, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function $1/2(1-\phi)^2$.","['Google Brain', 'Google Brain']"
2017,"The Shattered Gradients Problem: If resnets are the answer, then what is the question?","David Balduzzi, Marcus Frean, Wan-Duo Ma, Brian McWilliams, Lennox Leary, John Lewis",https://icml.cc/Conferences/2017/Schedule?showEvent=601,"A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new ``looks linear'' (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.
","['Victoria University Wellington', 'Victoria University Wellington', 'Victoria University of Wellington', 'Disney Research', 'VUW', 'Frostbite Labs and Victoria University']"
2017,Learning to Learn without Gradient Descent by Gradient Descent,"Yutian Chen, Matthew Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy Lillicrap, Matthew Botvinick, Nando de Freitas",https://icml.cc/Conferences/2017/Schedule?showEvent=687,"We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks.  Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.
","['DeepMind', 'DeepMind', 'Google DeepMind', 'University of Oxford', 'Google DeepMind', 'DeepMind', 'DeepMind']"
2017,"A Semismooth Newton Method for Fast, Generic Convex Programming","Alnur Ali, Eric Wong, Zico Kolter",https://icml.cc/Conferences/2017/Schedule?showEvent=512,"We introduce Newton-ADMM, a method for fast
conic optimization.  The basic idea is to view the residuals of consecutive iterates generated by the
alternating direction method of multipliers (ADMM) as a set of fixed point
equations, and then use a nonsmooth Newton method to find a solution; we apply the basic idea to the Splitting Cone Solver (SCS), a state-of-the-art method for solving generic conic optimization
problems.  We demonstrate theoretically, by extending the theory of 
semismooth operators, that Newton-ADMM converges rapidly (i.e.,
quadratically) to a solution; empirically, Newton-ADMM is significantly faster
than SCS on a number of problems.  The method also has essentially no tuning parameters, generates certificates of primal or dual infeasibility, when appropriate, and can be specialized to solve specific convex problems. 
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Unifying task specification in reinforcement learning,Martha White,https://icml.cc/Conferences/2017/Schedule?showEvent=461,"Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.
",['University of Alberta/Indiana University']
2017,Efficient Online Bandit Multiclass Learning with O(sqrt{T}) Regret,"Alina Beygelzimer, Francesco Orabona, Chicheng Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=557,"We present an efficient second-order algorithm with tilde{O}(1/eta sqrt{T}) regret for the bandit online multiclass problem.  The regret bound holds simultaneously with respect to a family of loss functions parameterized by eta, ranging from hinge loss (eta=0) to squared hinge loss (eta=1).  This provides a solution to the open problem of (Abernethy, J. and Rakhlin, A. An efficient bandit algorithm for sqrt{T}-regret in online multiclass prediction? In COLT, 2009).  We test our algorithm experimentally, showing that it performs favorably against earlier algorithms.
","['Yahoo Research', 'Stony Brook University', 'UCSD']"
2017,Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use,"Vatsal Sharan, Gregory Valiant",https://icml.cc/Conferences/2017/Schedule?showEvent=500,"The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is efficient and easy to implement, but often converges to poor local optima---particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS for a variety of tasks on synthetic data---including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion---and for computing word embeddings from a third-order word tri-occurrence tensor.
","['Stanford University', 'Stanford University']"
2017,Learned Optimizers that Scale and Generalize,"Olga Wichrowska, Niru Maheswaranathan, Matthew Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Nando de Freitas, Jascha Sohl-Dickstein",https://icml.cc/Conferences/2017/Schedule?showEvent=780,"Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse, optimization tasks capturing common properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps, optimization problems that are of a vastly different scale than those it was trained on.
","['Google Brain', 'Stanford University', 'DeepMind', 'Google DeepMind', 'University of Oxford', 'DeepMind', 'Google Brain']"
2017,Approximate Newton Methods and Their Local Convergence,"Haishan Ye, Luo Luo, Zhihua Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=715,"Many machine learning models are reformulated as optimization problems. Thus, it is important to solve a large-scale optimization problem in big data applications. 
Recently, subsampled Newton methods have emerged to attract much attention for optimization
due to their efficiency at
each iteration, rectified a weakness in the ordinary Newton method of suffering a high cost in each
iteration while commanding a high convergence rate. Other efficient stochastic second order methods are also proposed. However, the convergence properties of these methods are still not well understood. There are also several important gaps between the current convergence theory and performance in real applications. In this paper, we aim to fill these gaps. We propose a unifying framework to analyze local convergence properties of second order methods. Based on this framework, our theoretical analysis matches the performance in real applications.
","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Peking University']"
2017,A Distributional Perspective on Reinforcement Learning,"Marc Bellemare, Will Dabney, Remi Munos",https://icml.cc/Conferences/2017/Schedule?showEvent=580,"In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.
","['DeepMind', 'DeepMind', 'DeepMind']"
2017,Active Learning for Accurate Estimation of Linear Models,"Carlos Riquelme Ruiz, Mohammad Ghavamzadeh, Alessandro Lazaric",https://icml.cc/Conferences/2017/Schedule?showEvent=483,"We explore the sequential decision making problem where the goal is to estimate uniformly well a number of linear models, given a shared budget of random contexts independently sampled from a known distribution. The decision maker must query one of the linear models for each incoming context, and receives an observation corrupted by noise levels that are unknown, and depend on the model instance. We present Trace-UCB, an adaptive allocation algorithm that learns the noise levels while balancing contexts accordingly across the different linear functions, and derive guarantees for simple regret in both expectation and high-probability. Finally, we extend the algorithm and its guarantees to high dimensional settings, where the number of linear models times the dimension of the contextual space is higher than the total budget of samples. Simulations with real data suggest that Trace-UCB is remarkably robust, outperforming a number of baselines even when its assumptions are violated.
","['Stanford University', 'Adobe Research & INRIA', 'FACEBOOK']"
2017,Tensor Decomposition via Simultaneous Power Iteration,"Poan Wang, Chi-Jen Lu",https://icml.cc/Conferences/2017/Schedule?showEvent=560,"Tensor decomposition is an important problem with many applications across several disciplines, and a popular approach for this problem
is the tensor power method. However, previous works with theoretical guarantee based on this approach can only find the top eigenvectors one after one, unlike the case for matrices. In this paper, we show how to find the eigenvectors simultaneously with the help of a new initialization procedure. This allows us to achieve a better running
time in the batch setting, as well as a lower sample complexity in the streaming setting.
","['Academia sinica', 'Academia Sinica']"
2017,Learning Gradient Descent: Better Generalization and Longer Horizons,"Kaifeng Lyu, Shunhua Jiang, Jian Li",https://icml.cc/Conferences/2017/Schedule?showEvent=875,"Training deep neural networks is a highly nontrivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.
","['Tsinghua University', 'Tsinghua University', 'IIIS']"
2017,Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values,"Chaoxu Zhou, Wenbo Gao, Donald Goldfarb",https://icml.cc/Conferences/2017/Schedule?showEvent=856,"We propose a novel class of stochastic, adaptive methods for minimizing self-concordant functions which can be expressed as an expected value. These methods generate an estimate of the true objective function by taking the empirical mean over a sample drawn at each step, making the problem tractable.
The use of adaptive step sizes eliminates the need for the user to supply a step size. Methods in this class include extensions of gradient descent (GD) and BFGS. We show that,
given a suitable amount of sampling, the stochastic adaptive GD attains linear convergence in expectation, and with further sampling, the stochastic adaptive BFGS attains R-superlinear convergence. We present experiments showing that these methods compare favorably to SGD.
","['Columbia University', 'Columbia University', 'Columbia University']"
2017,Hierarchy Through Composition with Multitask LMDPs,"Andrew Saxe, Adam Earle, Benjamin Rosman",https://icml.cc/Conferences/2017/Schedule?showEvent=725,"Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme exploits the guaranteed concurrent compositionality provided by the linearly solvable Markov decision process (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time.
","['Harvard University', 'University of the Witwatersrand', 'Council for Scientific and Industrial Research (CSIR)']"
2017,Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP,"Satyen Kale, Zohar Karnin, Tengyuan Liang, David Pal",https://icml.cc/Conferences/2017/Schedule?showEvent=760,"Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.
","['Google Research', 'yahoo', 'UPenn', 'Yahoo']"
2017,A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank Matrix Recovery,"Lingxiao Wang, Xiao Zhang, Quanquan Gu",https://icml.cc/Conferences/2017/Schedule?showEvent=624,"We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an appropriate initial estimator, our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery. Based upon the mild restricted strong convexity and smoothness conditions, we derive a  projected notion of the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity. Moreover, our algorithm can be employed to both noiseless and noisy observations, where the (near) optimal sample complexity and statistical rate can be attained respectively. We further illustrate the superiority of our generic framework through several specific examples, both theoretically and experimentally. 
","['University of Virginia', 'University of Virginia', 'University of Virginia']"
2017,Learning Algorithms for Active Learning,"Philip Bachman, Alessandro Sordoni, Adam Trischler",https://icml.cc/Conferences/2017/Schedule?showEvent=880,"We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a prediction function. Our model uses the item selection heuristic to construct a labeled support set for training the prediction function. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.
","['Maluuba', 'Microsoft Maluuba', 'Maluuba']"
2017,Practical Gauss-Newton Optimisation for Deep Learning,"Aleksandar Botev, Hippolyt Ritter, David Barber",https://icml.cc/Conferences/2017/Schedule?showEvent=789,"We present an efficient block-diagonal approximation to the Gauss-Newton matrix for feedforward neural networks. Our resulting algorithm is competitive against state-of-the-art first-order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a laborious process, our approach can provide good performance even when used with default settings. A side result of our work is that for piecewise linear transfer functions, the network objective function can have no differentiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.
","['University College London', 'University College London', 'University College London']"
2017,A Laplacian Framework for Option Discovery in Reinforcement Learning,"Marlos C. Machado, Marc Bellemare, Michael Bowling",https://icml.cc/Conferences/2017/Schedule?showEvent=577,"Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.
","['University of Alberta', 'DeepMind', 'University of Alberta']"
2017,Emulating the Expert: Inverse Optimization through Online Learning,"Sebastian Pokutta, Andreas Bärmann, Oskar Schneider",https://icml.cc/Conferences/2017/Schedule?showEvent=865,"In this paper, we demonstrate how to learn the objective function of a decision maker while only observing the problem input data and the decision maker's corresponding decisions over multiple rounds. Our approach is based on online learning techniques and works for linear objectives over arbitrary sets for which we have a linear optimization oracle and as such generalizes previous work based on KKT-system decomposition and dualization approaches. The applicability of our framework for learning linear constraints is also discussed briefly. Our algorithm converges at a rate of O(1/sqrt(T)), and we demonstrate its effectiveness and applications in preliminary computational results. 
","['Georgia Tech', 'FAU Erlangen-Nürnberg', '']"
2017,"An Efficient, Sparsity-Preserving, Online Algorithm for Low-Rank Approximation","David Anderson, Ming Gu",https://icml.cc/Conferences/2017/Schedule?showEvent=833,"Low-rank matrix approximation is a fundamental tool in data analysis for processing large datasets, reducing noise, and finding important signals.  In this work, we present a novel truncated LU factorization called Spectrum-Revealing LU (SRLU) for effective low-rank matrix approximation, and develop a fast algorithm to compute an SRLU factorization. We provide both matrix and singular value approximation error bounds for the SRLU approximation computed by our algorithm. Our analysis suggests that SRLU is competitive with the best low-rank matrix approximation methods, deterministic or randomized, in both computational complexity and approximation quality. Numeric experiments illustrate that SRLU preserves sparsity, highlights important data features and variables, can be efficiently updated, and calculates data approximations nearly as accurately as the best possible.  To the best of our knowledge this is the first practical variant of the LU factorization for effective  and efficient low-rank matrix approximation. 
","['UC Berkeley', 'University of California at Berkeley']"
2017,Tensor Balancing on Statistical Manifold,"Mahito Sugiyama, Hiroyuki Nakahara, Koji Tsuda",https://icml.cc/Conferences/2017/Schedule?showEvent=631,"We solve tensor balancing, rescaling an Nth order nonnegative tensor by multiplying N tensors of order N - 1 so that every fiber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton's method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones. To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton's method, can be analytically obtained using the Möbius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing, but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.
","['National Institute of Informatics', 'RIKEN Brain Science Institute', 'University of Tokyo / RIKEN']"
2017,Modular Multitask Reinforcement Learning with Policy Sketches,"Jacob Andreas, Dan Klein, Sergey Levine",https://icml.cc/Conferences/2017/Schedule?showEvent=548,"We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them---specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations).  To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies.  Optimization is accomplished via a decoupled actor--critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions.  We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals.  Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.
","['UC Berkeley', 'UC Berkeley', 'Berkeley']"
2017,Variants of RMSProp and Adagrad with Logarithmic Regret Bounds,"Mahesh Chandra Mukkamala, Matthias Hein",https://icml.cc/Conferences/2017/Schedule?showEvent=884,"Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.","['Saarland University', 'Saarland University']"
2017,Algorithms for $\ell_p$ Low-Rank Approximation,"Flavio Chierichetti, Sreenivas Gollapudi, Ravi Kumar, Silvio Lattanzi, Rina Panigrahy, David Woodruff",https://icml.cc/Conferences/2017/Schedule?showEvent=710,"  We consider the problem of approximating a given matrix by a
  low-rank matrix so as to minimize the entrywise $\ell_p$-approximation error,
  for any $p \geq 1$; the case $p = 2$ is the classical SVD problem.
  We obtain the first provably good approximation algorithms for this
  robust version of low-rank approximation that work for
  every value of $p$.
  Our algorithms are simple, easy to implement, work well in
  practice, and illustrate interesting tradeoffs between the
  approximation quality, the running time, and the rank of the
  approximating matrix.
","['Sapienza University of Rome', '', 'Google', '', 'Google', '']"
2017,Relative Fisher Information and Natural Gradient for Learning Large Modular Models,"Ke Sun, Frank Nielsen",https://icml.cc/Conferences/2017/Schedule?showEvent=458,"Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. However related analysis becomes more and more difficult as the learner's structure turns large and complex. This paper makes a preliminary step towards a new direction. We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system. This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks. We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.
","['KAUST', 'École Polytechnique']"
2017,Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections,"zakaria mhammedi, Andrew Hellicar, James Bailey, Ashfaqur Rahman",https://icml.cc/Conferences/2017/Schedule?showEvent=670,"The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations. 
","['The University of Melbourne', 'CSIRO', 'The University of Melbourne', 'CSIRO']"
2017,Lazifying Conditional Gradient Algorithms,"Gábor Braun, Sebastian Pokutta, Daniel Zink",https://icml.cc/Conferences/2017/Schedule?showEvent=509,"Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are popular due to their simplicity of only requiring a linear optimization oracle and more recently they also gained significant traction for online learning. While simple in principle, in many cases the actual implementation of the linear optimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of speedup in wall-clock time. This is achieved by using a faster separation oracle instead of a linear optimization oracle, relying only on few linear optimization oracle calls.
","['Georgia Institute of Technology', 'Georgia Tech', '']"
2017,Data-Efficient Policy Evaluation Through Behavior Policy Search,"Josiah Hanna, Philip S. Thomas, Peter Stone, Scott Niekum",https://icml.cc/Conferences/2017/Schedule?showEvent=855,"We consider the task of evaluating a policy for a Markov decision process (MDP). The standard unbiased technique for evaluating a policy is to deploy the policy and observe its performance. We show that the data collected from deploying a different policy, commonly called the behavior policy, can be used to produce unbiased estimates with lower mean squared error than this standard technique. We derive an analytic expression for the optimal behavior policy --- the behavior policy that minimizes the mean squared error of the resulting estimates. Because this expression depends on terms that are unknown in practice, we propose a novel policy evaluation sub-problem, behavior policy search: searching for a behavior policy that reduces mean squared error. We present a behavior policy search algorithm and empirically demonstrate its effectiveness in lowering the mean squared error of policy performance estimates.
","['University of Texas at Austin', 'CMU', 'University of Texas at Austin', 'University of Texas at Austin']"
2017,Exact MAP Inference by Avoiding Fractional Vertices,"Erik Lindgren, Alexandros Dimakis, Adam Klivans",https://icml.cc/Conferences/2017/Schedule?showEvent=729,"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time---we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.
","['University of Texas at Austin', 'UT Austin', 'University of Texas at Austin']"
2017,Leveraging Node Attributes for Incomplete Relational Data,"He Zhao, Lan Du, Wray Buntine",https://icml.cc/Conferences/2017/Schedule?showEvent=555,"Relational data are usually highly incomplete in practice, which inspires us to leverage side information to improve the performance of community detection and link prediction. This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood. Our method works flexibly with both directed and undirected relational networks. The inference can be done by efficient Gibbs sampling which leverages sparsity of both networks and node attributes. Extensive experiments show that our models achieve the state-of-the-art link prediction results, especially with highly incomplete relational data.
","['FIT, Monash University', 'Faculty of Information Technology, Monash University', 'Monash University']"
2017,How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?,Andreas Loukas,https://icml.cc/Conferences/2017/Schedule?showEvent=489,"How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply \textit{non-asymptotic} concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from $O(1)$ samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances. 
",['EPFL']
2017,Distributed and Provably Good Seedings for k-Means in Constant Rounds,"Olivier Bachem, Mario Lucic, Andreas Krause",https://icml.cc/Conferences/2017/Schedule?showEvent=472,"The k-Means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) competitive in expectation. However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to massive data sets. The standard remedy is to use the k-Means|| algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting. In this paper, we provide a novel analysis of the k-Means|| algorithm that bounds the expected solution quality for any number of rounds and oversampling factors greater than k, the two parameters one needs to choose in practice. In particular, we show that k-Means|| provides provably good clusterings even for a small, constant number of iterations. This theoretical finding explains the common observation that k-Means|| performs extremely well in practice even if the number of rounds is low. We further provide a hard instance that shows that an additive error term as encountered in our analysis is inevitable if less than k-1 rounds are employed.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2017,Learning Deep Architectures via Generalized Whitened Neural Networks,Ping Luo,https://icml.cc/Conferences/2017/Schedule?showEvent=488,"Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural
networks by whitening their internal hidden representation. However, the whitening transformation increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates
convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties. First, GWNN is able to learn compact representation to reduce computations. Second, it enables whitening transformation to be performed in a short period,
preserving good conditioning. Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency. Extensive experiments on various datasets demonstrate the benefits of GWNN.
",['The Chinese University of Hong Kong']
2017,On orthogonality and learning RNNs with long term dependencies,"Eugene Vorontsov, Chiheb Trabelsi, Christopher Pal, Samuel Kadoury",https://icml.cc/Conferences/2017/Schedule?showEvent=740,"It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.
","['MILA', 'Ecole Polytechnique de Montreal', 'École Polytechnique de Montréal', 'Ecole Polytechnique de Montreal']"
2017,Conditional Accelerated Lazy Stochastic Gradient Descent,"Guanghui , Sebastian Pokutta, Yi Zhou, Daniel Zink",https://icml.cc/Conferences/2017/Schedule?showEvent=510,"In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate O(1 / epsilon^2) improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of (Hazan and Kale, 2012) with convergence rate O(1 / epsilon^4). 
","['George', 'Georgia Tech', 'Georgia Institute of Technology', '']"
2017,Stochastic Variance Reduction Methods for Policy Evaluation,"Simon Du, Jianshu Chen, Lihong Li, Lin Xiao, Dengyong Zhou",https://icml.cc/Conferences/2017/Schedule?showEvent=648,"Policy evaluation is concerned with estimating the value function that predicts long-term values of states under a given policy.  It is a crucial step in many reinforcement-learning algorithms.  In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.
","['Carnegie Mellon University', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2017,Exact Inference for Integer Latent-Variable Models,"Kevin Winner, Debora Sujono, Daniel Sheldon",https://icml.cc/Conferences/2017/Schedule?showEvent=771,"Graphical models with latent count variables arise in a number of areas. However, standard inference algorithms do not apply to these models due to the infinite support of the latent variables. Winner and Sheldon (2016) recently developed a new technique using probability generating functions (PGFs) to perform efficient, exact inference for certain Poisson latent variable models. However, the method relies on symbolic manipulation of PGFs, and it is unclear whether this can be extended to more general models. In this paper we introduce a new approach for inference with PGFs: instead of manipulating PGFs symbolically, we adapt techniques from the autodiff literature to compute the higher-order derivatives necessary for inference. This substantially generalizes the class of models for which efficient, exact inference algorithms are available. Specifically, our results apply to a class of models that includes branching processes, which are widely used in applied mathematics and population ecology, and autoregressive models for integer data. Experiments show that our techniques are more scalable than existing approximate methods and enable new applications.
","['University of Massachusetts, Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst']"
2017,Bayesian inference on random simple graphs with power law degree distributions,"Juho Lee, Creighton Heaukulani, Zoubin Ghahramani, Lancelot F. James, Seungjin Choi",https://icml.cc/Conferences/2017/Schedule?showEvent=570,"We present a model for random simple graphs with power law (i.e., heavy-tailed) degree distributions. To attain this behavior, the edge
probabilities in the graph are constructed from Bertoin–Fujita–Roynette–Yor (BFRY) random variables, which have been recently utilized in
Bayesian statistics for the construction of power law models in several applications. Our construction readily extends to capture the structure of latent factors, similarly to stochastic block-models, while maintaining its power law degree distribution. The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption. By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data. In order to further scale our inference procedure, we adopt
stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph.
","['POSTECH', 'Cambridge University', 'University of Cambridge & Uber', 'Hong Kong University of Science and Technology', 'POSTECH']"
2017,Faster Principal Component Regression and Stable Matrix Chebyshev Approximation,"Zeyuan Allen-Zhu, Yuanzhi Li",https://icml.cc/Conferences/2017/Schedule?showEvent=493,"We solve principal component regression (PCR), up to a multiplicative accuracy $1+\gamma$, by reducing the problem to $\tilde{O}(\gamma^{-1})$ black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires $\tilde{O}(\gamma^{-2})$ such black-box calls.

We obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods.
","['Microsoft Research / Princeton / IAS', 'Princeton University']"
2017,Consistent k-Clustering,"Silvio Lattanzi, Sergei Vassilvitskii",https://icml.cc/Conferences/2017/Schedule?showEvent=588,"The study of online algorithms and competitive analysis provides a solid foundation for studying the quality of irrevocable decision making when the data arrives in an online manner. 
While in some scenarios the decisions are indeed irrevocable, there are many practical situations when changing a previous decision is not impossible, but simply expensive.  In this work we formalize this notion and introduce the consistent k-clustering problem. With points arriving online, the goal is to maintain a constant approximate solution, while minimizing the number of reclusterings necessary. We prove a lower bound, showing that \Omega(k \log n) changes are necessary in the worst case for a wide range of objective functions. On the positive side, we give an algorithm that needs only O(k^2 \log^4n) changes to maintain a constant competitive solution, an exponential improvement on the naive solution of reclustering at every time step. Finally, we show experimentally that our approach performs much better than the theoretical bound, with the number of changes growing approximately as O(\log n). 
","['Google', 'Google']"
2017,Continual Learning Through Synaptic Intelligence,"Friedemann Zenke, Ben Poole, Surya Ganguli",https://icml.cc/Conferences/2017/Schedule?showEvent=598,"While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.
","['Stanford', 'Stanford University', 'Stanford']"
2017,Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs,"Li Jing, Yichen Shen, Tena Dubcek, John E Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, Marin Solja\v{c}i\'{c}",https://icml.cc/Conferences/2017/Schedule?showEvent=813,"Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely $\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.","['Massachusetts Institute of Technology', 'MIT', 'MIT', 'MIT', 'MIT', 'New York University', 'MIT', 'MIT']"
2017,SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient,"Lam Nguyen, Jie Liu, Katya Scheinberg, Martin Takac",https://icml.cc/Conferences/2017/Schedule?showEvent=605,"In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.
","['Lehigh University', 'Lehigh University', 'Lehigh University', 'Lehigh University']"
2017,Optimal and Adaptive Off-policy Evaluation in Contextual Bandits,"Yu-Xiang Wang, Alekh Agarwal, Miroslav Dudik",https://icml.cc/Conferences/2017/Schedule?showEvent=618,"We study the off-policy evaluation problem---estimating the value of a target policy using data collected by another policy---under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of rewards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched up to constants by the inverse propensity scoring (IPS) and doubly robust (DR) estimators. This highlights the difficulty of the agnostic contextual setting, in contrast with multi-armed bandits and contextual bandits with access to a consistent reward model, where IPS is suboptimal. We then propose the SWITCH estimator, which can use an existing reward model (not necessarily consistent) to achieve a better bias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often outperforming prior work by orders of magnitude.
","['Carnegie Mellon University / Amazon AWS', 'Microsoft Research', 'Microsoft Research']"
2017,Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms,"Arturs Backurs, Christos Tzamos",https://icml.cc/Conferences/2017/Schedule?showEvent=728,"The classic algorithm of Viterbi computes the most likely path in a Hidden Markov Model (HMM) that results in a given sequence of observations. It runs in time O(Tn^2) given a sequence of T observations from a HMM with n states. Despite significant interest in the problem and prolonged effort by different communities, no known algorithm achieves more than a polylogarithmic speedup. In this paper, we explain this difficulty by providing matching conditional lower bounds. Our lower bounds are based on assumptions that the best known algorithms for the All-Pairs Shortest Paths problem (APSP) and for the Max-Weight k-Clique problem in edge-weighted graphs are essentially tight. Finally, using a recent algorithm by Green Larsen and Williams for online Boolean matrix-vector multiplication, we get a 2^{Omega(sqrt{log n})} speedup for the Viterbi algorithm when there are few distinct transition probabilities in the HMM.
","['MIT', 'MIT']"
2017,Analogical Inference for Multi-relational Embeddings,"Hanxiao Liu, Yuexin Wu, Yiming Yang",https://icml.cc/Conferences/2017/Schedule?showEvent=620,"Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs. An effective and scalable solution for this problem is crucial for the true success of knowledge-based inference in a broad range of applications. This paper proposes a novel framework for optimizing the latent representations with respect to the \textit{analogical} properties of the embedded entities and relations. By formulating the objective function in a differentiable fashion, our model enjoys both its theoretical power and computational scalability, and significantly outperformed a large number of representative baseline methods on benchmark datasets. Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be proven to be special instantiations of our framework.
","['CMU/Google DeepMind', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Spectral Learning from a Single Trajectory under Finite-State Policies,"Borja de Balle Pigem, Odalric Maillard",https://icml.cc/Conferences/2017/Schedule?showEvent=820,"We present spectral methods of moments for learning sequential models from a single trajectory, in stark contrast with the classical literature that assumes the availability of multiple i.i.d. trajectories. Our approach leverages an efficient SVD-based learning algorithm for weighted automata and provides the first rigorous analysis for learning many important models using dependent data. We state and analyze the algorithm under three increasingly difficult scenarios: probabilistic automata, stochastic weighted automata, and reactive predictive state representations controlled by a finite-state policy. Our proofs include novel tools for studying mixing properties of stochastic weighted automata.
","['Amazon Research Cambridge', '']"
2017,Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering,"Bo Yang, Xiao Fu, Nicholas Sidiropoulos, Mingyi Hong",https://icml.cc/Conferences/2017/Schedule?showEvent=738,"Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the ‘clustering-friendly’ latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network’s ability to approximate any nonlinear function. This way, the pro- posed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using different real datasets are employed to showcase the effectiveness of the proposed approach.
","['University of Minnesota', 'University of Minnesota', 'University of Minnesota', 'Iowa State University']"
2017,Adaptive Neural Networks for Efficient Inference,"Tolga Bolukbasi, Joseph Wang, Ofer Dekel, Venkatesh Saligrama",https://icml.cc/Conferences/2017/Schedule?showEvent=807,"We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem.  Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal ($<1\%$) loss of top5 accuracy. ","['Boston University', 'Amazon', 'Microsoft', 'Boston University']"
2017,The Statistical Recurrent Unit,"Junier Oliva, Barnabás Póczos, Jeff Schneider",https://icml.cc/Conferences/2017/Schedule?showEvent=684,"Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters for both synthetic and real-world tasks.
","['Carnegie Mellon University', 'CMU', 'CMU/Uber']"
2017,Approximate Steepest Coordinate Descent,"Sebastian Stich, Anant Raj, Martin Jaggi",https://icml.cc/Conferences/2017/Schedule?showEvent=723,"We propose a new selection rule for the coordinate selection in coordinate descent methods for huge-scale optimization. The efficiency of this novel scheme is provably better than the efficiency of uniformly random selection, and can reach the efficiency of steepest coordinate descent (SCD), enabling an acceleration of a factor of up to $n$, the number of coordinates. In many practical applications, our scheme can be implemented at no extra cost and computational efficiency very close to the faster uniform selection.
Numerical experiments with Lasso and Ridge regression show promising improvements, in line with our theoretical guarantees.","['EPFL', 'Max-Planck Institute for Intelligent Systems', 'EPFL']"
2017,Consistent On-Line Off-Policy Evaluation,"Assaf Hallak, Shie Mannor",https://icml.cc/Conferences/2017/Schedule?showEvent=480,"The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD($\lambda$, $\beta$)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD($\lambda$, $\beta$) can be designed to converge to the same value that would have been obtained by using on-policy TD($\lambda$) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call log-COP-TD($\lambda$, $\beta$). Both algorithms have favorable empirical results to the current state of the art on-line OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.","['Technion', 'Technion']"
2017,Variational Inference for Sparse and Undirected Models,"John Ingraham, Debora Marks",https://icml.cc/Conferences/2017/Schedule?showEvent=861,"Undirected graphical models are applied in genomics, protein structure prediction, and neuroscience to identify sparse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo sampling. Here, we develop a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and approximations of the partition function. The second is Fadeout, a reparameterization approach for variational inference under sparsity-inducing priors that captures a posteriori correlations between parameters and hyperparameters with noncentered parameterizations. We find that, together, these methods for variational inference substantially improve learning of sparse undirected graphical models in simulated and real problems from physics and biology.
","['Harvard University', 'Harvard Medical School']"
2017,Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs,"Rakshit Trivedi, Hanjun Dai, Yichen Wang, Le Song",https://icml.cc/Conferences/2017/Schedule?showEvent=882,"The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood.
To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.
","['Georgia Institute of Technology', 'Georgia Tech', 'Gatech', 'Georgia Institute of Technology']"
2017,Capacity Releasing Diffusion for Speed and Locality.,"Di Wang, Kimon Fountoulakis, Monika Henzinger, Michael Mahoney, Satish Rao",https://icml.cc/Conferences/2017/Schedule?showEvent=827,"Diffusions and related random walk procedures are of central importance in many areas of machine learning, data analysis, and applied mathematics. Because they spread mass agnostically at each step in an iterative manner, they can sometimes spread mass ``too aggressively,'' thereby failing to find the ``right'' clusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process, which is both faster and stays more local than the classical spectral diffusion process. As an application, we use our CRD Process to develop an improved local algorithm for graph clustering. Our local graph clustering method can find local clusters in a model of clustering where one begins the CRD Process in a cluster whose vertices are connected better internally than externally by an $O(\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus, our CRD Process is the first local graph clustering algorithm that is not subject to the well-known quadratic Cheeger barrier. Our result requires a certain smoothness condition, which we expect to be an artifact of our analysis. Our empirical evaluation demonstrates improved results, in particular for realistic social graphs where there are moderately good---but not very good---clusters.","['UC Berkeley', 'University of California Berkeley and International Computer Science Institute', '', 'UC Berkeley', 'UC Berkeley']"
2017,Hyperplane Clustering Via Dual Principal Component Pursuit,"Manolis Tsakiris, Rene Vidal",https://icml.cc/Conferences/2017/Schedule?showEvent=736,"State-of-the-art methods for clustering data drawn from a union of subspaces are based on sparse and low-rank representation theory and convex optimization algorithms. Existing results guaranteeing the correctness of such methods require the dimension of the subspaces to be small relative to the dimension of the ambient space. When this assumption is violated, as is, e.g., in the case of hyperplanes, existing methods are either computationally too intensive (e.g., algebraic methods) or lack sufficient theoretical support (e.g., K-Hyperplanes or RANSAC). 
In this paper we provide theoretical and algorithmic contributions to the problem of clustering data from a union of hyperplanes, by extending a recent subspace learning method called Dual Principal Component Pursuit (DPCP) to the multi-hyperplane case. We give theoretical guarantees under which, the non-convex $\ell_1$ problem associated with DPCP admits a unique global minimizer equal to the normal vector of the most dominant hyperplane. Inspired by this insight, we propose sequential (RANSAC-style) and iterative (K-Hyperplanes-style) hyperplane learning DPCP algorithms, which, via experiments on synthetic and real data, are shown to outperform or be competitive to the state-of-the-art.","['Johns Hopkins University', 'Johns Hopkins University']"
2017,Combined Group and Exclusive Sparsity for Deep Neural Networks,"jaehong yoon, Sung Ju Hwang",https://icml.cc/Conferences/2017/Schedule?showEvent=474,"The number of parameters in a deep neural network is usually very large, which helps with its learning capacity but also hinders its scalability and practicality due to memory/time inefficiency and overfitting. To resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the features to enforce the network to be sparse, and at the same time remove any redundancies among the features to fully utilize the capacity of the network. Specifically, we propose to use an exclusive sparsity regularization based on (1,2)-norm, which promotes competition for features between different weights, thus enforcing them to fit to disjoint sets of features. We further combine the exclusive sparsity with the group sparsity based on (2,1)-norm, to promote both sharing and competition for features in training of a deep neural network. We validate our method on multiple public datasets, and the results show that our method can obtain more compact and efficient networks while also improving the performance over the base networks with full weights, as opposed to existing sparsity regularizations that often obtain efficiency at the expense of prediction accuracy.
","['UNIST', 'UNIST / AItrics']"
2017,Input Switched Affine Networks: An RNN Architecture Designed for Interpretability,"Jakob Foerster, Justin Gilmer, Jan Chorowski, Jascha Sohl-Dickstein, David Sussillo",https://icml.cc/Conferences/2017/Schedule?showEvent=759,"There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.
","['University of Oxford', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain, Google Inc.']"
2017,StingyCD: Safely Avoiding Wasteful Updates in Coordinate Descent,"Tyler Johnson, Carlos Guestrin",https://icml.cc/Conferences/2017/Schedule?showEvent=764,"Coordinate descent (CD) is a scalable and simple algorithm for solving many optimization problems in machine learning.
Despite this fact, CD can also be very computationally wasteful. 
Due to sparsity in sparse regression problems, for example, the majority of CD updates often result in no progress toward the solution.
To address this inefficiency, we propose a modified CD algorithm named ""StingyCD.""
By skipping over many updates that are guaranteed to not decrease the objective value, StingyCD significantly reduces convergence times.
Since StingyCD only skips updates with this guarantee, however, StingyCD does not fully exploit the problem's sparsity.
For this reason, we also propose StingyCD+, an algorithm that achieves further speed-ups by skipping updates more aggressively.
Since StingyCD and StingyCD+ rely on simple modifications to CD, it is also straightforward to use these algorithms with other approaches to scaling optimization.
In empirical comparisons, StingyCD and StingyCD+ improve convergence times considerably for several L1-regularized optimization problems.
","['University of Washington', '']"
2017,Contextual Decision Processes with low Bellman rank are PAC-Learnable,"Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert Schapire",https://icml.cc/Conferences/2017/Schedule?showEvent=610,"This paper studies systematic exploration for reinforcement learning (RL) with rich observations and function approximation. We introduce contextual decision processes (CDPs), that unify most prior RL settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial in all relevant parameters but independent of the number of unique contexts. Our approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for RL with function approximation.
","['Microsoft Research', 'UMass', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2017,Tensor Belief Propagation,"Andrew Wrigley, Wee Sun Lee, Nan Ye",https://icml.cc/Conferences/2017/Schedule?showEvent=651,"We propose a new approximate inference algorithm for graphical models, tensor belief propagation, based on approximating the messages passed in the junction tree algorithm. Our algorithm represents the potential functions of the graphical model and all messages on the junction tree compactly as mixtures of rank-1 tensors. Using this representation, we show how to perform the operations required for inference on the junction tree efficiently: marginalisation can be computed quickly due to the factored form of rank-1 tensors while multiplication can be approximated using sampling. Our analysis gives sufficient conditions for the algorithm to perform well, including for the case of high-treewidth graphs, for which exact inference is intractable. We compare our algorithm experimentally with several approximate inference algorithms and show that it performs well.
","['Australian National University', 'National University of Singapore', 'Queensland University of Technology']"
2017,Deep Generative Models for Relational Data with Side Information,"Changwei Hu, Piyush Rai, Lawrence Carin",https://icml.cc/Conferences/2017/Schedule?showEvent=712,"We present a probabilistic framework for overlapping community discovery and link prediction for relational data, given as a graph. The proposed framework has: (1) a deep architecture which enables us to infer multiple layers of latent features/communities for each node, providing superior link prediction performance on more complex networks and better interpretability of the latent features; and (2) a regression model which allows directly conditioning the node latent features on the side information available in form of node attributes. Our framework handles both (1) and (2) via a clean, unified model, which enjoys full local conjugacy via data augmentation, and facilitates efficient inference via closed form Gibbs sampling. Moreover, inference cost scales in the number of edges which is attractive for massive but sparse networks. Our framework is also easily extendable to model weighted networks with count-valued edges.  We compare with various state-of-the-art methods and report results, both quantitative and qualitative, on several benchmark data sets.
","['Duke University', 'IIT Kanpur', 'Duke']"
2017,Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition,"Zeyuan Allen-Zhu, Yuanzhi Li",https://icml.cc/Conferences/2017/Schedule?showEvent=485,"We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonical-correlation analysis. We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on k.
Furthermore, our algorithms are \emph{doubly-accelerated}: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both k-GenEV or k-CCA. We also provide the first gap-free results, which provide running times that depend on 1/\sqrt{\varepsilon}$ rather than the eigengap.
","['Microsoft Research / Princeton / IAS', 'Princeton University']"
2017,Multilevel Clustering via Wasserstein Means,"Nhat Ho, XuanLong Nguyen, Mikhail Yurochkin, Hung Bui, Viet Huynh, Dinh Phung",https://icml.cc/Conferences/2017/Schedule?showEvent=788,"We propose a novel approach to the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formulation over several spaces of discrete probability measures, which are endowed with Wasserstein distance metrics. We propose a number of variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters.  Consistency properties are established for the estimates of both local and global clusters. Finally, experiment results with both synthetic and real data are presented to demonstrate the flexibility and scalability of the proposed approach.
","['University of Michigan', 'University of Michigan', 'University of Michigan', 'Adobe Research', 'Deakin University', 'Deakin University']"
2017,Online and Linear-Time Attention by Enforcing Monotonic Alignments,"Colin Raffel, Thang Luong, Peter Liu, Ron Weiss, Douglas Eck",https://icml.cc/Conferences/2017/Schedule?showEvent=866,"Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.
","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']"
2017,Stochastic modified equations and adaptive stochastic gradient algorithms,"Qianxiao Li, Cheng Tai, Weinan E",https://icml.cc/Conferences/2017/Schedule?showEvent=530,"We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms. 
","['Institute of High Performance Computing, A*STAR', 'Peking University', 'Princeton University']"
2017,A Simple Multi-Class Boosting Framework with Theoretical Guarantees and Empirical Proficiency,"Ron Appel, Pietro Perona",https://icml.cc/Conferences/2017/Schedule?showEvent=675,"There is a need for simple yet accurate white-box learning systems that train quickly and with little data. To this end, we showcase REBEL, a multi-class boosting method, and present a novel family of weak learners called localized similarities. Our framework provably minimizes the training error of any dataset at an exponential rate.
We carry out experiments on a variety of synthetic and real datasets, demonstrating a consistent tendency to avoid overfitting. We evaluate our method on MNIST and standard UCI datasets against other state-of-the-art methods, showing the empirical proficiency of our method.
","['caltech.edu', 'caltech.edu']"
2017,Faster Greedy MAP Inference for Determinantal Point Processes,"Insu Han, Prabhanjan Kambadur, Kyoungsoo Park, Jinwoo Shin",https://icml.cc/Conferences/2017/Schedule?showEvent=638,"Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by determinants of their features. In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success. Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration. We present faster implementations of the greedy algorithms by utilizing the orthogonal benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) high-order expansions to the scalar log function with stochastic trace estimators. In our experiments, our algorithms are orders of magnitude faster than their competitors, while sacrificing marginal accuracy.
","['Korea Advanced Institute of Science and Technology', 'Bloomberg', 'KAIST', 'KAIST']"
2017,ChoiceRank: Identifying Preferences from Node Traffic in Networks,"Lucas Maystre, Matthias Grossglauser",https://icml.cc/Conferences/2017/Schedule?showEvent=519,"Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce's axiom. In this case, the O(n) marginal counts of node visits are a sufficient statistic for the O(n^2) transition probabilities. We show how to make the inference problem well-posed regardless of the network's structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to two clickstream datasets and show that it successfully recovers the transition probabilities using only the network structure and marginal (node-level) traffic data. Finally, we also consider an application to mobility networks and apply the model to one year of rides on New York City's bicycle-sharing system.
","['EPFL', 'EPFL']"
2017,On the Iteration Complexity of Support Recovery via Hard Thresholding Pursuit,"Jie Shen, Ping Li",https://icml.cc/Conferences/2017/Schedule?showEvent=663,"Recovering the support of a sparse signal from its compressed samples has been one of the most important problems in high dimensional statistics. In this paper, we present a novel analysis for the hard thresholding pursuit (HTP) algorithm, showing that it exactly recovers the support of an arbitrary s-sparse signal within O(sklogk) iterations via a properly chosen proxy function, where k is the condition number of the problem. In stark contrast to the theoretical results in the literature, the iteration complexity we obtained holds without assuming the restricted isometry property, or relaxing the sparsity, or utilizing the optimality of the underlying signal. We further extend our result to a more challenging scenario, where the subproblem involved in HTP cannot be solved exactly. We prove that even in this setting, support recovery is possible and the computational complexity of HTP is established. Numerical study substantiates our theoretical results.
","['Rutgers University', 'Rugters University']"
2017,Uniform Deviation Bounds for k-Means Clustering,"Olivier Bachem, Mario Lucic, Hamed Hassani, Andreas Krause",https://icml.cc/Conferences/2017/Schedule?showEvent=523,"Uniform deviation bounds limit the difference between a model's expected loss and its loss on an empirical sample uniformly for all models in a learning problem. In this paper, we provide a novel framework to obtain uniform deviation bounds for loss functions which are unbounded. As a result, we obtain competitive uniform deviation bounds for k-Means clustering under weak assumptions on the underlying distribution. If the fourth moment is bounded, we prove a rate of O(m^(-1/2)) compared to the previously known O(m^(-1/4)) rate. Furthermore, we show that the rate also depends on the kurtosis - the normalized fourth moment which measures the ""tailedness"" of a distribution. We also provide improved rates under progressively stronger assumptions, namely, bounded higher moments, subgaussianity and bounded support of the underlying distribution.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2017,Sequence Tutor: Conservative fine-tuning of sequence generation models with KL-control,"Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez-Lobato, Richard E Turner, Douglas Eck",https://icml.cc/Conferences/2017/Schedule?showEvent=814,"This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data. 
","['Massachusetts Institute of Technology', 'Cambridge', 'Université de Montréal', 'University of Cambridge', 'University of Cambridge', 'Google Brain']"
2017,Dissipativity Theory for Nesterov's Accelerated Method,"Bin Hu, Laurent Lessard",https://icml.cc/Conferences/2017/Schedule?showEvent=891,"In this paper, we adapt the control theoretic concept of dissipativity theory to provide a natural understanding of Nesterov's accelerated method. Our theory ties rigorous convergence rate analysis to the physically intuitive notion of energy dissipation. Moreover, dissipativity allows one to efficiently construct Lyapunov functions (either numerically or analytically) by solving a small semidefinite program. Using novel supply rate functions, we show how to recover known rate bounds for Nesterov's method and we generalize the approach to certify both linear and sublinear rates in a variety of settings. Finally, we link the continuous-time version of dissipativity to recent works on algorithm analysis that use discretizations of ordinary differential equations.
","['University of Wisconsin', 'University of Wisconsin-Madison']"
2017,Gradient Boosted Decision Trees for High Dimensional Sparse Output,"Si Si, Huan Zhang, Sathiya Keerthi, Dhruv Mahajan, Inderjit Dhillon, Cho-Jui Hsieh",https://icml.cc/Conferences/2017/Schedule?showEvent=870,"In this paper, we study the gradient boosted decision trees (GBDT) when the output space is high dimensional and sparse. For example, in multilabel classification, the output space is a $L$-dimensional 0/1 vector, where $L$ is number of labels that can grow to millions and beyond in many modern applications. We show that vanilla GBDT can easily run out of memory or encounter near-forever running time in this regime, and propose a new GBDT variant, GBDT-SPARSE, to resolve this problem by employing $L_0$ regularization. We then discuss in detail how to utilize this sparsity to conduct GBDT training, including splitting the nodes, computing the sparse residual, and predicting in sublinear time. Finally, we apply our algorithm to extreme multilabel classification problems, and show that the proposed GBDT-SPARSE achieves an order of magnitude improvements in model size and prediction time over existing methods, while yielding similar performance.","['Google Research', 'UC Davis', 'Microsoft', 'Facebook', 'UT Austin & Amazon', 'University of California, Davis']"
2017,Zonotope hit-and-run for efficient sampling from projection DPPs,"Guillaume Gautier, Rémi Bardenet, Michal Valko",https://icml.cc/Conferences/2017/Schedule?showEvent=680,"Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems.   Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an  effort towards efficient approximate samplers.  We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costly-to-evaluate functions, such as summary extraction in our experiments. 
","['INRIA Lille', 'CNRS and Univ. Lille', 'Inria Lille - Nord Europe']"
2017,Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening,"Mohsen Ahmadi Fahandar, Eyke Hüllermeier, Ines Couso",https://icml.cc/Conferences/2017/Schedule?showEvent=590,"We consider the problem of statistical inference for ranking data, specifically rank aggregation, under the assumption that samples are incomplete in the sense of not comprising all choice alternatives. In contrast to most existing methods, we explicitly model the process of turning a full ranking into an incomplete one, which we call the coarsening process. To this end, we propose the concept of rank-dependent coarsening, which assumes that incomplete rankings are produced by projecting a full ranking to a random subset of ranks. For a concrete instantiation of our model, in which full rankings are drawn from a Plackett-Luce distribution and observations take the form of pairwise preferences, we study the performance of various rank aggregation methods. In addition to predictive accuracy in the finite sample setting, we address the theoretical question of consistency, by which we mean the ability to recover a target ranking when the sample size goes to infinity, despite a potential bias in the observations caused by the (unknown) coarsening.
","['Paderborn University', 'Paderborn University', 'University of Oviedo']"
2017,Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization,"Bo Liu, Xiaotong Yuan, Lezi Wang, Qingshan Liu, Dimitris Metaxas",https://icml.cc/Conferences/2017/Schedule?showEvent=525,"Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form. It remains open to explore duality theory and algorithms in such a non-convex and NP-hard setting. In this article, we bridge the gap by establishing a duality theory for sparsity-constrained minimization with $\ell_2$-regularized objective and proposing an IHT-style algorithm for dual maximization. Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual space. The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective. An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by all the existing primal IHT without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization. Numerical results demonstrate that dual IHT algorithms can achieve more accurate model estimation given small number of training data and have higher computational efficiency than the state-of-the-art primal IHT-style algorithms.","['Rutgers', 'Nanjing University of Information Science & Technology', 'Rutgers', '', 'Rutgers']"
2017,Uniform Convergence Rates for Kernel Density Estimation,Heinrich Jiang,https://icml.cc/Conferences/2017/Schedule?showEvent=545,"Kernel density estimation (KDE) is a popular nonparametric density estimation method. We (1) derive finite-sample high-probability density estimation bounds for multivariate KDE under mild density assumptions which hold uniformly in $x \in \mathbb{R}^d$ and bandwidth matrices. We apply these results to (2) mode, (3) density level set, and (4) class probability estimation and attain optimal rates up to logarithmic factors. We then (5) provide an extension of our results under the manifold hypothesis. Finally, we (6) give uniform convergence results for local intrinsic dimension estimation.",['Google']
2017,Deep Voice: Real-time Neural Text-to-Speech,"Andrew Gibiansky, Mike Chrzanowski, Mohammad Shoeybi, Shubho Sengupta, Gregory Diamos, Sercan Arik, Jonathan Raiman, John Miller, Xian Li, Yongguo Kang, Adam Coates, Andrew Ng",https://icml.cc/Conferences/2017/Schedule?showEvent=646,"We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.
","['Baidu Research Silicon Valley AI Lab', 'Baidu Research', 'Baidu Research', 'Baidu Research', 'Baidu Research', 'Baidu Research', 'Baidu Research', 'Baidu Research', 'Baidu', 'Baidu', 'Baidu SVAIL', 'Baidu']"
2017,An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis,Yuandong Tian,https://icml.cc/Conferences/2017/Schedule?showEvent=550,"In this paper, we explore theoretical properties of training a two-layered ReLU network $g(\vx; \vw) = \sum_{j=1}^K \sigma(\vw_j\trans\vx)$ with centered $d$-dimensional spherical Gaussian input $\vx$ ($\sigma$=ReLU). We train our network with gradient descent on $\vw$ to mimic the output of a teacher network with the same architecture and fixed parameters $\vw\opt$. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (``out-of-plane``) are not isolated and form manifolds, and characterize in-plane critical-point-free regions for two-ReLU case. On the other hand, convergence to $\vw\opt$ for one ReLU node is guaranteed with at least $(1-\epsilon)/2$ probability, if weights are initialized randomly with standard deviation upper-bounded by $O(\epsilon/\sqrt{d})$, in accordance with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards $\vw\opt$ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.",['Facebook AI Research']
2017,Globally Induced Forest: A Prepruning Compression Scheme,"Jean-Michel Begon, Arnaud Joly, Pierre Geurts",https://icml.cc/Conferences/2017/Schedule?showEvent=802,"Tree-based ensemble models are heavy memory-wise. An undesired state of affairs
considering nowadays datasets, memory-constrained environment and
fitting/prediction times.  In this paper, we propose the Globally Induced Forest
(GIF) to remedy this problem. GIF is a fast prepruning approach to build
lightweight ensembles by iteratively deepening the current forest. It mixes
local and global optimizations to produce accurate predictions under memory
constraints in reasonable time.  We show that the proposed method is more than
competitive with standard tree-based ensembles under corresponding constraints,
and can sometimes even surpass much larger models.
","['University of Liege', 'University of Liege', 'University of Liege']"
2017,A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI,Justin Domke,https://icml.cc/Conferences/2017/Schedule?showEvent=825,"Two popular classes of methods for approximate inference are Markov chain Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better approximations at shorter time horizons. However, the amount of time needed for MCMC to exceed the performance of variational methods can be quite high, motivating more fine-grained tradeoffs. This paper derives a distribution over variational parameters, designed to minimize a bound on the divergence between the resulting marginal distribution and the target, and gives an example of how to sample from this distribution in a way that interpolates between the behavior of existing methods based on Langevin dynamics and stochastic gradient variational inference (SGVI).
","['University of Massachusetts, Amherst']"
2017,Just Sort It! A Simple and Effective Approach to Active Preference Learning,"Lucas Maystre, Matthias Grossglauser",https://icml.cc/Conferences/2017/Schedule?showEvent=592,"We address the problem of learning a ranking by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all comparison outcomes are consistent with the ranking, the optimal solution is to use an efficient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking? We give favorable guarantees for Quicksort for the popular Bradley-Terry model, under natural assumptions on the parameters. Furthermore, we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy: repeatedly sort the items. This strategy performs as well as state-of-the-art methods (and much better than random sampling) at a minuscule fraction of the computational cost.
","['EPFL', 'EPFL']"
2017,On The Projection Operator to A Three-view Cardinality Constrained Set,"Haichuan Yang, Shupeng Gui, Chuyang Ke, Daniel Stefankovic, Ryohei Fujimaki, Ji Liu",https://icml.cc/Conferences/2017/Schedule?showEvent=803,"The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardinality constraints. In this paper, we consider the scenario where the overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection into a linear programming, and show that for TVCS, the vertex solution of this linear programming is the solution for the original projection problem. We further prove that such solution can be found with the complexity proportional to the number of variables and constraints. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.
","['University of Rochester', 'University of Rochester', 'University of Rochester', 'University of Rochester', '-', 'University of Rochester']"
2017,Density Level Set Estimation on Manifolds with DBSCAN,Heinrich Jiang,https://icml.cc/Conferences/2017/Schedule?showEvent=552,"We show that DBSCAN can estimate the connected components of the $\lambda$-density level set $\{ x : f(x) \ge \lambda\}$ given $n$ i.i.d. samples from an unknown density $f$. We characterize the regularity of the level set boundaries using parameter $\beta > 0$ and analyze the estimation error under the Hausdorff metric. When the data lies in $\mathbb{R}^D$ we obtain a rate of $\widetilde{O}(n^{-1/(2\beta + D)})$, which matches known lower bounds up to logarithmic factors. When the data lies on an embedded unknown $d$-dimensional manifold in $\mathbb{R}^D$, then we obtain a rate of $\widetilde{O}(n^{-1/(2\beta + d\cdot \max\{1, \beta \})})$. Finally, we provide adaptive parameter tuning in order to attain these rates with no a priori knowledge of the intrinsic dimension, density, or $\beta$.",['Google']
2017,DeepBach: a Steerable Model for Bach Chorales Generation,"Gaëtan HADJERES, François Pachet, Frank Nielsen",https://icml.cc/Conferences/2017/Schedule?showEvent=731,"This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. 
We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach.
DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches  which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.
","['LIP6 / SONY CSL', 'Sony CSL / UPMC', 'Sony CSL, Japan']"
2017,Forward and Reverse Gradient-Based Hyperparameter Optimization,"Luca Franceschi, Michele Donini, Paolo Frasconi, Massimiliano Pontil",https://icml.cc/Conferences/2017/Schedule?showEvent=704,"We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs in terms  of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al (2015) but does not require reversible dynamics. Additionally, we explore the use of constraints on the hyperparameters. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speedup hyperparameter optimization on large datasets. We present a series of experiments on image and phone classification tasks. In the second task, previous gradient-based approaches are prohibitive. We show that our real-time algorithm yields state-of-the-art results in affordable time.
","['IIT and UCL', 'IIT', 'University of Florence', 'University College London']"
2017,Forest-type Regression with General Losses and Robust Forest,"Hanbo Li, Andrew Martin",https://icml.cc/Conferences/2017/Schedule?showEvent=871,"This paper introduces a new general framework for forest-type regression which allows the development of robust forest regressors by selecting from a large family of robust loss functions. In particular, when plugged in the squared error and quantile losses, it will recover the classical random forest and quantile random forest. We then use robust loss functions to develop more robust forest-type regression algorithms. In the experiments, we show by simulation and real data that our robust forests are indeed much more insensitive to outliers, and choosing the right number of nearest neighbors can quickly improve the generalization performance of random forest.
","['UC San Diego', 'Zillow']"
2017,On the Sampling Problem for Kernel Quadrature,"Francois-Xavier Briol, Chris J Oates, Jon Cockayne, Wilson Ye Chen, Mark Girolami",https://icml.cc/Conferences/2017/Schedule?showEvent=695,"The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio s/d, where s and d encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant C is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is well-understood, the sampling distribution that minimises C for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.
","['University of Warwick', 'Newcastle University', 'University of Warwick', 'University of Technology Sydney', 'Imperial College London']"
2017,Maximum Selection and Ranking under Noisy Comparisons,"Moein Falahatgar, Alon Orlitsky, Venkatadheeraj Pichapati, Ananda Theertha Suresh",https://icml.cc/Conferences/2017/Schedule?showEvent=881,"We consider $(\epsilon,\delta)$-PAC maximum-selection and ranking
using pairwise comparisons for \nobreak{general} probabilistic models whose
comparison probabilities satisfy strong stochastic 
transitivity and stochastic triangle inequality.
Modifying the popular knockout tournament, we propose a simple
maximum-selection algorithm that uses $\mathcal{O}\left(\frac{n}{\epsilon^2}
\left(1+\log \frac1{\delta}\right)\right)$ comparisons, optimal up to a constant
factor.  We then derive a general framework that uses noisy binary
search to speed up many ranking algorithms, and
combine it with merge sort to obtain a ranking algorithm that uses
$\mathcal{O}\left(\frac n{\epsilon^2}\log n(\log \log n)^3\right)$ comparisons
for  $\delta=\frac1n$, optimal up to a $(\log \log n)^3$ factor.","['UCSD', 'UCSD', 'University of California San Diego', 'Google Research']"
2017,Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity,"Eunho Yang, Aurelie Lozano",https://icml.cc/Conferences/2017/Schedule?showEvent=647,"Imposing sparse + group-sparse superposition structures in high-dimensional parameter estimation is known to provide flexible regularization  that is more realistic for many real-world problems. For example, such a superposition enables partially-shared support sets in multi-task learning, thereby striking the right balance between parameter overlap across tasks and task specificity. Existing theoretical results on estimation consistency, however, are problematic as they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components.  In this paper, we fill the gap between the practical success and suboptimal analysis of sparse + group-sparse models, by providing the first consistency results that do not require unrealistic assumptions. We also study non-convex counterparts of sparse + group-sparse models. Interestingly, we show that these are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models, which might be critical in practical applications as illustrated by our experiments.
","['KAIST / AItrics', 'IBM']"
2017,Algorithmic Stability and Hypothesis Complexity,"Tongliang Liu, Gábor Lugosi, Gergely Neu, Dacheng Tao",https://icml.cc/Conferences/2017/Schedule?showEvent=650,"We introduce a notion of algorithmic stability of learning algorithms---that we term \emph{hypothesis stability}---that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected. The main result of the paper bounds the generalization error of any learning algorithm in terms of its hypothesis stability. The bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent.
","['The University of Sydney', 'Universitat Pompeu Fabra', 'Universitat Pompeu Fabra / Google Brain Zürich', '']"
2017,Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders,"Cinjon Resnick, Adam Roberts, Jesse Engel, Douglas Eck, Sander Dieleman, Karen Simonyan, Mohammad Norouzi",https://icml.cc/Conferences/2017/Schedule?showEvent=868,"Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.
","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'DeepMind', 'DeepMind', 'Google']"
2017,Adaptive Sampling Probabilities for Non-Smooth Optimization,"Hongseok Namkoong, Aman Sinha, Steven Yadlowsky, John Duchi",https://icml.cc/Conferences/2017/Schedule?showEvent=562,"Standard forms of coordinate and stochastic gradient methods do not adapt to structure in data; their good behavior under random sampling is predicated on uniformity in data. When gradients in certain blocks of features (for coordinate descent) or examples (for SGD) are larger than others, there is a natural structure that can be exploited for quicker convergence.  Yet adaptive variants often suffer nontrivial computational overhead. We present a framework that discovers and leverages such structural properties at a low computational cost. We employ a bandit optimization procedure that ""learns"" probabilities for sampling coordinates or examples in (non-smooth) optimization problems, allowing us to guarantee performance close to that of the optimal stationary sampling distribution. When such structures exist, our algorithms achieve tighter convergence guarantees than their non-adaptive counterparts, and we complement our analysis with experiments on several datasets.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
2017,Confident Multiple Choice Learning,"Kimin Lee, Changho Hwang, KyoungSoo Park, Jinwoo Shin",https://icml.cc/Conferences/2017/Schedule?showEvent=693,"Ensemble methods are arguably the most trustworthy techniques for boosting the performance of machine learning models. Popular independent ensembles (IE) relying on naive averaging/voting scheme have been of typical choice for most applications involving deep neural networks, but they do not consider advanced collaboration among ensemble models. In this paper, we propose new ensemble methods specialized for deep neural networks, called confident multiple choice learning (CMCL): it is a variant of multiple choice learning (MCL) via addressing its overconfidence issue.In particular, the proposed major components of CMCL beyond the original MCL scheme are (i) new loss, i.e., confident oracle loss, (ii) new architecture, i.e., feature sharing and (iii) new training method, i.e., stochastic labeling. We demonstrate the effect of CMCL via experiments on the image classification on CIFAR and SVHN, and the foreground-background segmentation on the iCoseg. In particular, CMCL using 5 residual networks provides 14.05% and 6.60% relative reductions in the top-1 error rates from the corresponding IE scheme for the classification task on CIFAR and SVHN, respectively.
","['KAIST', 'KAIST', 'KAIST', 'KAIST']"
2017,Measuring Sample Quality with Kernels,"Jackson Gorham, Lester Mackey",https://icml.cc/Conferences/2017/Schedule?showEvent=526,"Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein’s method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.
","['STANFORD', 'Microsoft Research']"
2017,Active Learning for Top-$K$ Rank Aggregation from Noisy Comparisons,"Soheil Mohajer, Changho Suh, Adel Elmahdy",https://icml.cc/Conferences/2017/Schedule?showEvent=819,"We explore an active top-$K$ ranking problem based on pairwise comparisons that are collected possibly in a sequential manner as per our design choice. We consider two settings: (1) \emph{top-$K$ sorting} in which the goal is to recover the top-$K$ items in order out of $n$ items; (2) \emph{top-$K$ partitioning} where only the set of  top-$K$ items is desired. Under a fairly general model which subsumes as special cases various models (e.g., Strong Stochastic Transitivity model, BTL model and uniform noise model), we characterize upper bounds on the sample size required for top-$K$ sorting as well as for top-$K$ partitioning. As a consequence, we demonstrate that active ranking can offer significant multiplicative gains in sample complexity over passive ranking. Depending on the underlying stochastic noise model, such gain varies  from around $\frac{\log n}{\log \log n}$ to $\frac{ n^2 \log n }{\log \log n}$. 
We also present an algorithm that is applicable to both settings. ","['University of Minnesota', 'KAIST', 'University of Minnesota']"
2017,Compressed Sensing using Generative Models,"Ashish Bora, Ajil Jalal, Eric Price, Alexandros Dimakis",https://icml.cc/Conferences/2017/Schedule?showEvent=796,"The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all.  Instead, we suppose that vectors lie near the range of a generative model $G: \R^k \to \R^n$.  Our main theorem is that, if $G$ is $L$-Lipschitz, then roughly $\mathcal{O}(k \log L)$ random Gaussian measurements suffice for an $\ell_2/\ell_2$ recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use $5$-$10$x fewer measurements than Lasso for the same accuracy.","['University of Texas at Austin', 'University of Texas at Austin', 'UT-Austin', 'UT Austin']"
2017,Consistency Analysis for Binary Classification Revisited,"Krzysztof Dembczynski, Wojciech Kotlowski, Sanmi Koyejo, Nagarajan Natarajan",https://icml.cc/Conferences/2017/Schedule?showEvent=787,"Statistical learning theory is at an inflection point enabled by recent advances in understanding and optimizing a wide range of metrics. Of particular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples. Non-decomposability is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. In this manuscript we analyze both settings, from statistical and algorithmic points of view, to explore the connections and to highlight differences between them for a wide range of metrics. The analysis complements previous results on this topic, clarifies common confusions around both settings, and provides guidance to the theory and practice of binary classification with complex metrics. 
","['Poznan University of Technology', 'Poznan University of Technology', 'University of Illinois at Urbana-Champaign', 'Microsoft Research']"
2017,A Closer Look at Memorization in Deep Networks,"David Krueger, Yoshua Bengio, Stanislaw Jastrzebski, Maxinder S. Kanwal, Nicolas Ballas, Asja Fischer, Emmanuel Bengio, Devansh Arpit, Tegan Maharaj, Aaron Courville, Simon Lacoste-Julien",https://icml.cc/Conferences/2017/Schedule?showEvent=874,"We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs.~real data. We also demonstrate that for appropriately tuned explicit regularization (e.g.,~dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.
","['MILA', 'U. Montreal', 'Jagiellonian University', 'UC Berkeley', 'Université de Montréal', 'Computer Science Department, University of Bonn', 'McGill University', '', '', 'University of Montreal', 'University of Montreal']"
2017,Learning to Generate Long-term Future via Hierarchical Prediction,"Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, Honglak Lee",https://icml.cc/Conferences/2017/Schedule?showEvent=641,"We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.
","['University of Michigan', 'Adobe Research', 'University of Michigan', 'University of Michigan', '', 'Google / U. Michigan']"
2017,Sub-sampled Cubic Regularization for Non-convex Optimization,"Jonas Kohler, Aurelien Lucchi",https://icml.cc/Conferences/2017/Schedule?showEvent=563,"We consider the minimization of non-convex functions that typically arise in machine learning. Specifically, we focus our attention on a variant of trust region methods known as cubic regularization. This approach is particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than first- and second-order as well as classical trust region methods. However, it suffers from a high computational complexity that makes it impractical for large-scale learning. Here, we propose a novel method that uses sub-sampling to lower this computational cost. By the use of concentration inequalities we provide a sampling scheme that gives sufficiently accurate gradient and Hessian approximations to retain the strong global and local convergence guarantees of cubically regularized methods. To the best of our knowledge this is the first work that gives global convergence guarantees for a sub-sampled variant of cubic regularization on non-convex functions. Furthermore, we provide experimental results supporting our theory.
","['ETH Zurich', 'ETH']"
2017,Regret Minimization in Behaviorally-Constrained Zero-Sum Games,"Gabriele Farina, Christian Kroer, Tuomas Sandholm",https://icml.cc/Conferences/2017/Schedule?showEvent=501,"No-regret learning has emerged as a powerful tool for solving extensive-form games. This was facilitated by the counterfactual-regret minimization (CFR) framework, which relies on the instantiation of regret minimizers for simplexes at each information set of the game. We use an instantiation of the CFR framework to develop algorithms for solving behaviorally-constrained (and, as a special case, perturbed in the Selten sense) extensive-form games, which allows us to compute approximate Nash equilibrium refinements. Nash equilibrium refinements are motivated by a major deficiency in Nash equilibrium: it provides virtually no guarantees on how it will play in parts of the game tree that are reached with zero probability. Refinements can mend this issue, but have not been adopted in practice, mostly due to a lack of scalable algorithms. We show that, compared to standard algorithms, our method finds solutions that have substantially better refinement properties, while enjoying a convergence rate that is comparable to that of state-of-the-art algorithms for Nash equilibrium computation both in theory and practice.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Variational Boosting: Iteratively Refining Posterior Approximations,"Andrew Miller, Nicholas J Foti, Ryan P. Adams",https://icml.cc/Conferences/2017/Schedule?showEvent=558,"We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class.  Our method, variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing a trade-off between computation time and accuracy.  We expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture.  We apply variational boosting to synthetic and real statistical models, and show that the resulting posterior inferences compare favorably to existing variational algorithms.
","['Harvard', 'University of Washington', 'Google Brain and Princeton University']"
2017,Learning to Align the Source Code to the Compiled Object Code,"Dor Levy, Lior Wolf",https://icml.cc/Conferences/2017/Schedule?showEvent=821,"We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code.
Our architecture learns the alignment between the two sequences -- one being the translation of the other -- by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains.
Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines. We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.
","['Tel Aviv University', 'Facebook AI Research and Tel Aviv University']"
2017,Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction,"Weizhong Zhang, Bin Hong, Wei Liu, Jieping Ye, Deng Cai, Xiaofei He, Jie Wang",https://icml.cc/Conferences/2017/Schedule?showEvent=569,"Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVMs remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in both the memory usage and computational cost without sacrificing accuracy. To the best of our knowledge, the proposed method is the \emph{first} \emph{static} feature and sample reduction method for sparse SVMs. Experiments on both synthetic and real datasets (e.g., the kddb dataset with about 20 million samples and 30 million features) demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.
","['Zhejiang University & Tencent AI Lab', 'Zhejiang University', 'Tencent AI Lab', 'University of Michigan', 'Zhejiang University', 'Zhejiang University', 'University of Michigan']"
2017,Distributed Mean Estimation with Limited Communication,"Ananda Theertha Suresh, Felix Xinnan Yu, Sanjiv Kumar, Brendan McMahan",https://icml.cc/Conferences/2017/Schedule?showEvent=714,"Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike previous works, we make no probabilistic assumptions on the data. We first show that for $d$ dimensional data with $n$ clients, a naive stochastic rounding approach yields a mean squared error (MSE) of $\Theta(d/n)$ and uses a constant number of bits per dimension per client. We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to $\mathcal{O}((\log d)/n)$ and a better coding strategy further reduces the error to $\mathcal{O}(1/n)$. We also show that the latter coding strategy  is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd's algorithm for k-means and power iteration for PCA.","['Google Research', 'Google Research', 'Google Research, NY', 'Google']"
2017,Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study,"Samuel Ritter, David GT Barrett, Adam Santoro, Matthew Botvinick",https://icml.cc/Conferences/2017/Schedule?showEvent=536,"Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2017,Sequence to Better Sequence: Continuous Revision of Combinatorial Structures,"Jonas Mueller, David Gifford, Tommi Jaakkola",https://icml.cc/Conferences/2017/Schedule?showEvent=623,"We present a model that, after learning on observations of (sequence, outcome) pairs, can be efficiently used to revise a new sequence in order to improve its associated outcome.  Our framework requires neither example improvements, nor additional evaluation of outcomes for proposed revisions.  To avoid combinatorial-search over sequence elements, we specify a generative model with continuous latent factors, which is learned via joint approximate inference using a recurrent variational autoencoder (VAE) and an outcome-predicting neural network module.  Under this model, gradient methods can be used to efficiently optimize the continuous latent factors with respect to inferred outcomes.  By appropriately constraining this optimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural. These desiderata are proven to hold with high probability under our approach, which is empirically demonstrated for revising natural language sentences.
","['MIT', 'MIT', 'MIT']"
2017,Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter,Zeyuan Allen-Zhu,https://icml.cc/Conferences/2017/Schedule?showEvent=594,"Given a non-convex function $f(x)$ that is an average of $n$ smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue $-\sigma$ of the Hessian. This parameter $\sigma$ captures how strongly non-convex $f(x)$ is, and is analogous to the strong convexity parameter for convex optimization.

At least in theory, our methods outperform known results for a range of parameter $\sigma$, and can also be used to find approximate local minima. Our result implies an interesting dichotomy: there exists a threshold $\sigma_0$ so that the (currently) fastest methods for $\sigma>\sigma_0$ and for $\sigma<\sigma_0$ have different behaviors: the former scales with $n^{2/3}$ and the latter scales with $n^{3/4}$.
",['Microsoft Research / Princeton / IAS']
2017,Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning,"Noam Brown, Tuomas Sandholm",https://icml.cc/Conferences/2017/Schedule?showEvent=862,"Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most popular way to solve large zero-sum imperfect-information games. In this paper we introduce Best-Response Pruning (BRP), an improvement to iterative algorithms such as CFR that allows poorly-performing actions to be temporarily pruned. We prove that when using CFR in zero-sum games, adding BRP will asymptotically prune any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that BRP results in a factor of 7 reduction in space, and the reduction factor increases with game size.
","['Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Lost Relatives of the Gumbel Trick,"Matej Balog, Nilesh Tripuraneni, Zoubin Ghahramani, Adrian Weller",https://icml.cc/Conferences/2017/Schedule?showEvent=463,"The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with so-called low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.
","['University of Cambridge', 'UC Berkeley', 'University of Cambridge & Uber', 'University of Cambridge']"
2017,RobustFill: Neural Program Learning under Noisy I/O,"Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdelrahman Mohammad, Pushmeet Kohli",https://icml.cc/Conferences/2017/Schedule?showEvent=661,"The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for 'automatic program learning' have received significant attention: (1) 'neural program synthesis', where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) 'neural program induction', where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. 
Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs, which achieve 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.
","['Microsoft Research', 'MIT', 'MIT', 'Microsoft Research', 'Microsoft', 'Microsoft Research']"
2017,Efficient Distributed Learning with Sparsity,"Jialei Wang, Mladen Kolar, Nati Srebro, Tong Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=584,"We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines. In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted $\ell_1$ regularized loss minimization problem. After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.","['University of Chicago', 'University of Chicago', 'Toyota Technological Institute at Chicago', 'HKUST']"
2017,Nonparanormal Information Estimation,"Shashank Singh, Barnabás Póczos",https://icml.cc/Conferences/2017/Schedule?showEvent=805,"We study the problem of using i.i.d. samples from an unknown multivariate probability distribution p to estimate the mutual information of p. This problem has recently received attention in two settings: (1) where p is assumed to be Gaussian and (2) where p is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when p is not Gaussian, while estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimension. Hence, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when p is assumed to be a nonparanormal (or Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scalability.
","['Carnegie Mellon University', 'CMU']"
2017,Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing,"Tasha Nagamine, Nima Mesgarani",https://icml.cc/Conferences/2017/Schedule?showEvent=826,"Despite the recent success of deep learning, the nature of the transformations they apply to the input features remains poorly understood. This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform. These methods are used to discern and quantify properties of feed-forward neural networks trained to map acoustic features to phoneme labels. We show a selective and nonlinear warping of the feature space, achieved by forming prototypical functions to account for the possible variation of each class. This study provides a joint framework where the properties of node activations and the functions implemented by the network can be linked together. 
","['Columbia University', 'Columbia University']"
2017,Tensor-Train Recurrent Neural Networks for Video Classification,"Yinchong Yang, Denis Krompass, Volker Tresp",https://icml.cc/Conferences/2017/Schedule?showEvent=770,"The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors. To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves. We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data. 
","['Ludwig-Maximilians-Universität München, Siemens AG', 'Siemens AG', 'University of Munich']"
2017,“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions,"Yair Carmon, John Duchi, Oliver Hinder, Aaron Sidford",https://icml.cc/Conferences/2017/Schedule?showEvent=831,"We develop and analyze a variant of Nesterov's accelerated gradient descent (AGD) for minimization of smooth non-convex functions. We prove that one of two cases occurs: either our AGD variant converges quickly, as if the function was convex, or we produce a certificate that the function is “guilty” of being non-convex. This non-convexity certificate allows us to exploit negative curvature and obtain deterministic, dimension-free acceleration of convergence for non-convex functions. For a function $f$ with Lipschitz continuous gradient and Hessian, we compute a point $x$ with $\|\nabla f(x)\| \le \epsilon$ in $O(\epsilon^{-7/4} \log(1/ \epsilon) )$ gradient and function evaluations.  Assuming additionally that the third derivative is Lipschitz, we require only $O(\epsilon^{-5/3} \log(1/ \epsilon) )$ evaluations.","['Stanford University', 'Stanford University', 'Stanford', 'Stanford']"
2017,Strongly-Typed Agents are Guaranteed to Interact Safely,David Balduzzi,https://icml.cc/Conferences/2017/Schedule?showEvent=599,"As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The paper shows that that gradient descent converges to a Nash equilibrium in safe games. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely, thereby providing sufficient conditions to guarantee safe interactions. A series of examples show that strong-typing generalizes certain key features of convexity, is closely related to blind source separation, and introduces a new perspective on classical multilinear games based on tensor decomposition. 
",['Victoria University Wellington']
2017,Learning to Aggregate Ordinal Labels by Maximizing Separating Width,"Guangyong Chen, Shengyu Zhang, Di Lin, Hui Huang, Pheng Ann Heng",https://icml.cc/Conferences/2017/Schedule?showEvent=503,"While crowdsourcing has been a cost and time efficient method to label massive samples, one critical issue is quality control, for which the key challenge is to infer the ground truth from noisy or even adversarial data by various users. A large class of crowdsourcing problems, such as those involving age, grade, level, or stage, have an ordinal structure in their labels. Based on a technique of sampling estimated label from the posterior distribution, we define a novel separating width among the labeled observations to characterize the quality of sampled labels, and develop an efficient algorithm to optimize it through solving multiple linear decision boundaries and adjusting prior distributions. Our algorithm is empirically evaluated on several real world datasets, and demonstrates its supremacy over state-of-the-art methods.
","['The Chinese University of Hong Kong', 'CUHK', 'Shenzhen University', 'Shenzhen University', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences']"
2017,Programming with a Differentiable Forth Interpreter,"Matko Bošnjak, Tim Rocktäschel, Jason Naradowsky, Sebastian Riedel",https://icml.cc/Conferences/2017/Schedule?showEvent=809,"Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.
","['University College London', 'University of Oxford', 'University of Cambridge', 'UCL']"
2017,Innovation Pursuit: A New Approach to the Subspace Clustering Problem,"Mostafa Rahmani, George Atia",https://icml.cc/Conferences/2017/Schedule?showEvent=579,"This paper presents a new scalable approach, termed Innovation Pursuit (iPursuit), to the problem of subspace clustering. iPursuit rests on a new geometrical idea whereby each subspace is identified based on its novelty with respect to the other subspaces. The subspaces are identified consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data. A detailed mathematical analysis is provided establishing sufficient conditions for the proposed approach to correctly cluster the data points. Moreover, the proposed direction search approach can be integrated with spectral clustering to yield a new variant of spectral-clustering-based algorithms. Remarkably, the proposed approach can provably yield exact clustering even when the subspaces have significant intersections. The numerical simulations demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms – more so for subspaces with significant intersections – along with substantial reductions in computational complexity.
","['University of Central Florida', 'University of Central Florida']"
2017,A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions,"Jayadev Acharya, Hirakendu Das, Alon Orlitsky, Ananda Suresh",https://icml.cc/Conferences/2017/Schedule?showEvent=811,"Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications. Recently, researchers applied different estimators and analysis tools to derive asymptotically sample-optimal approximations for each of these properties. We show that a single, simple, plug-in
estimator---\emph{profile maximum likelihood (PML)}--is sample competitive for all symmetric properties, and in particular is asymptotically sample-optimal for all the above properties.
","['Cornell University', 'Yahoo!', 'UCSD', 'Google']"
2017,Axiomatic Attribution for Deep Networks,"Mukund Sundararajan, Ankur Taly, Qiqi Yan",https://icml.cc/Conferences/2017/Schedule?showEvent=850,"We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other
works. We identify two fundamental axioms—Sensitivity and Implementation Invariance that
attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement;
it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network,
and to enable users to engage with models better.
","['Google Inc.', 'Google Inc.', 'Google Inc.']"
2017,Sequence Modeling via Segmentations,"Chong Wang, Yining Wang, Po-Sen Huang, Abdelrahman Mohammad, Dengyong Zhou, Li Deng",https://icml.cc/Conferences/2017/Schedule?showEvent=513,"Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a probabilistic model for sequences via their segmentations. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments,  where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final probability for the sequence. An efficient dynamic programming algorithm is developed for  forward and backward computations without resorting to any approximation. We demonstrate our approach on text segmentation and speech recognition tasks. In addition to quantitative results, we also show that our approach can discover meaningful segments in their respective application contexts.
","['Microsoft Research', 'CMU', 'Microsoft Research', 'Microsoft', 'Microsoft Research', 'Citadel']"
2017,Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization,"Qunwei Li, Yi Zhou, Yingbin Liang, Pramod K Varshney",https://icml.cc/Conferences/2017/Schedule?showEvent=837,"In this work, we investigate the accelerated proximal gradient method for nonconvex programming (APGnc). The method compares between a usual proximal gradient step and a linear extrapolation step, and accepts the one that has a lower function value to achieve a monotonic decrease. In specific, under a general nonsmooth and nonconvex setting, we provide a rigorous argument to show that the limit points of the sequence generated by APGnc are critical points of the objective function. Then, by exploiting the Kurdyka-Lojasiewicz (KL) property for a broad class of functions, we establish the linear and sub-linear convergence rates of the function value sequence generated by APGnc. We further propose a stochastic variance reduced APGnc (SVRG-APGnc), and establish its linear convergence under a special case of the KL property. We also extend the analysis to the inexact version of these methods and develop an adaptive momentum strategy that improves the numerical performance.
","['Syracuse University', 'Syracuse University', '', 'Syracuse University']"
2017,Coordinated Multi-Agent Imitation Learning,"Hoang Le, Yisong Yue, Peter Carr, Patrick Lucey",https://icml.cc/Conferences/2017/Schedule?showEvent=621,"We study the problem of imitation learning from demonstrations of multiple coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often implicit in the demonstrations and must be inferred as a latent variable.  We propose a joint approach that simultaneously learns a latent coordination model along with the individual policies. In particular, our method integrates unsupervised structure learning with conventional imitation learning. We illustrate the power of our approach on a difficult problem of learning multiple policies for fine-grained behavior modeling in team sports, where different players occupy different roles in the coordinated team strategy.  We show that having a coordination model to infer the roles of players  yields substantially improved imitation loss compared to conventional baselines.
","['Caltech', 'Caltech', 'Disney Research', 'STATS LLC.']"
2017,Uncorrelation and Evenness: a New Diversity-Promoting Regularizer,"Pengtao Xie, Aarti Singh, Eric Xing",https://icml.cc/Conferences/2017/Schedule?showEvent=491,"Latent space models (LSMs) provide a principled and effective way to extract hidden patterns from observed data. To cope with two challenges in LSMs: (1) how to capture infrequent patterns when pattern frequency is imbalanced and (2) how to reduce model size without sacrificing their expressiveness, several studies have been proposed
to ""diversify"" LSMs, which design regularizers to encourage the components therein to be ""diverse"". In light of the limitations of existing approaches, we design a new diversity-promoting regularizer by considering two factors: uncorrelation and evenness, which encourage the components to be uncorrelated and to play equally important roles in modeling data. Formally, this amounts to encouraging the covariance matrix of the components to have more uniform eigenvalues. We apply the regularizer to two LSMs and develop an efficient optimization algorithm. Experiments on healthcare, image and text data demonstrate the effectiveness of the regularizer.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Differentiable Programs with Neural Libraries,"Alex Gaunt, Marc Brockschmidt, Nate Kushman, Daniel Tarlow",https://icml.cc/Conferences/2017/Schedule?showEvent=784,"We develop a framework for combining differentiable programming languages with neural networks. Using this framework we create end-to-end trainable systems that learn to write interpretable algorithms with perceptual components. We explore the benefits of inductive biases for strong generalization and modularity that come from the program-like structure of our models. In particular, modularity allows us to learn a library of (neural) functions which grows and improves as more tasks are solved. Empirically, we show that this leads to lifelong learning systems that transfer knowledge to new tasks more effectively than baselines.
","['Microsoft', 'Microsoft Research', 'Microsoft Research', 'Google Brain']"
2017,Selective Inference for Sparse High-Order Interaction Models,"Shinya Suzumura, Kazuya Nakagawa, Yuta Umezu, Koji Tsuda, Ichiro Takeuchi",https://icml.cc/Conferences/2017/Schedule?showEvent=801,"Finding statistically significant high-order interactions in predictive modeling is important but challenging task because the possible number of high-order interactions is extremely large (e.g., $> 10^{17}$). In this paper we study feature selection and statistical inference for sparse high-order interaction models. Our main contribution is to extend recently developed selective inference framework for linear models to high-order interaction models by developing a novel algorithm for efficiently characterizing the selection event for the selective inference of high-order interactions.  We demonstrate the effectiveness of the proposed algorithm by applying it to an HIV drug response prediction problem.","['Nagoya Institute of Technology', 'Nagoya Institute of Technology', 'Nagoya Institute of Technology', 'University of Tokyo / RIKEN', 'Nagoya Institute of Technology / RIKEN']"
2017,Gradient Coding: Avoiding Stragglers in Distributed Learning,"Rashish Tandon, Qi Lei, Alexandros Dimakis, Nikos Karampatziakis",https://icml.cc/Conferences/2017/Schedule?showEvent=851,"We propose a novel coding theoretic framework for mitigating stragglers in distributed learning. We show how carefully replicating data blocks and coding across gradients can provide tolerance to failures and stragglers for synchronous Gradient Descent. We implement our schemes in python (using MPI) to run on Amazon EC2, and show how we compare against baseline approaches in running time and generalization error.
","['University of Texas at Austin', 'University of Texas at Austin', 'UT Austin', 'Microsoft']"
2017,On Calibration of Modern Neural Networks,"Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Weinberger",https://icml.cc/Conferences/2017/Schedule?showEvent=808,"Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.
","['Cornell University', 'Cornell University', 'Cornell University', 'Cornell University']"
2017,Latent LSTM Allocation: Joint clustering and non-linear dynamic modeling of sequence data,"Manzil Zaheer, Amr Ahmed, Alex Smola",https://icml.cc/Conferences/2017/Schedule?showEvent=817,"Recurrent neural networks, such as long-short term memory (LSTM) networks, are powerful tools for modeling sequential data like user browsing history (Tan et al., 2016; Korpusik et al., 2016) or natural language text (Mikolov et al., 2010). However, to generalize across different user types, LSTMs require a large number of parameters, notwithstanding the simplicity of the underlying dynamics, rendering it uninterpretable, which is highly undesirable in user modeling. The increase in complexity and parameters arises due to a large action space in which many of the actions have similar intent or topic. In this paper, we introduce Latent LSTM Allocation (LLA) for user modeling combining hierarchical Bayesian models with LSTMs. In LLA, each user is modeled as a sequence of actions, and the model jointly groups actions into topics and learns the temporal dynamics over the topic sequence, instead of action space directly. This leads to a model that is highly interpretable, concise, and can capture intricate dynamics. We present an efficient Stochastic EM inference algorithm for our model that scales to millions of users/documents. Our experimental evaluations show that the proposed model compares favorably with several state-of-the-art baselines.
","['Carnegie Mellon University', 'Google', 'Amazon']"
2017,How to Escape Saddle Points Efficiently,"Chi Jin, Rong Ge, Praneeth Netrapalli, Sham Kakade, Michael Jordan",https://icml.cc/Conferences/2017/Schedule?showEvent=640,"This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost ``dimension-free''). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.
","['UC Berkeley', 'Duke University', 'Microsoft Research', 'University of Washington', 'UC Berkeley']"
2017,Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability,"Shayegan Omidshafiei, Jason Pazis, Chris Amato, Jonathan How, John L Vian",https://icml.cc/Conferences/2017/Schedule?showEvent=845,"Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.
","['MIT', 'Amazon', 'Northeastern University', 'MIT', 'The Boeing Company']"
2017,Learning Latent Space Models with Angular Constraints,"Pengtao Xie, Yuntian Deng, Yi Zhou, Abhimanu Kumar, Yaoliang Yu, James Zou, Eric Xing",https://icml.cc/Conferences/2017/Schedule?showEvent=499,"The large model capacity of latent space models (LSMs) enables them to achieve great performance on various applications, but meanwhile renders LSMs to be prone to overfitting. Several recent studies investigate a new type of regularization approach, which encourages components in LSMs to be diverse, for the sake of alleviating overfitting. While they have shown promising empirical effectiveness, in theory why larger ""diversity"" results in less overfitting is still unclear. To bridge this gap, we propose a new diversity-promoting approach that is both theoretically analyzable and empirically effective. Specifically, we use near-orthogonality to characterize ""diversity"" and impose angular constraints (ACs) on the components of LSMs to promote diversity. A generalization error analysis shows that larger diversity results in smaller estimation error and larger approximation error. An efficient ADMM algorithm is developed to solve the constrained LSM problems. Experiments demonstrate that ACs improve generalization performance of LSMs and outperform other diversity-promoting approaches.
","['Carnegie Mellon University', 'Harvard University', 'Syracuse University', 'IMC Financial Markets', 'University of Waterloo', 'Stanford', 'Carnegie Mellon University']"
2017,Developing Bug-Free Machine Learning Systems With Formal Mathematics,"Daniel Selsam, Percy Liang, David L Dill",https://icml.cc/Conferences/2017/Schedule?showEvent=849,"Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems.  As a result, detecting actual implementation errors can be extremely difficult.  We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct.  The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail.  As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients.  We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.
","['Stanford University', 'Stanford University', 'Stanford University']"
2017,Dictionary Learning Based on Sparse Distribution Tomography,"Pedram Pad, Farnood Salehi, L. Elisa Celis, Patrick Thiran, Michael Unser",https://icml.cc/Conferences/2017/Schedule?showEvent=662,"We propose a new statistical dictionary learning algorithm for sparse signals that is based on an $\alpha$-stable innovation model. The parameters of the underlying model---that is, the atoms of the dictionary, the sparsity index $\alpha$ and the dispersion of the transform-domain coefficients---are recovered
using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm.
Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily $\alpha$-stable. 
We evaluate our algorithm by performing two types of experiments: image in-painting and image denoising. In both cases, we find that our approach is competitive with state-of-the-art dictionary learning techniques. 
Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of $\ell_p$-norms that constitutes the foundation of our approach.
","['Ecole Polytechnique Federale de Lausanne (EPFL)', 'EPFL', 'Yale', 'EPFL', '']"
2017,Learning Discrete Representations via Information Maximizing Self-Augmented Training,"Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, Masashi Sugiyama",https://icml.cc/Conferences/2017/Schedule?showEvent=574,"Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invariance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the information-theoretic dependency between data and their predicted discrete representations. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.
","['The University of Tokyo / RIKEN', 'Preferred Networks, Inc., ATR', 'Preferred Networks / The University of Tokyo', 'Preferred Networks Inc.', 'RIKEN / The University of Tokyo']"
2017,"Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging","Shusen Wang, Alex Gittens, Michael Mahoney",https://icml.cc/Conferences/2017/Schedule?showEvent=486,"We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR---namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the ``mass'' in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.
","['UC Berkeley', 'Rensselaer Polytechnic Institute', 'UC Berkeley']"
2017,Estimating the unseen from multiple populations,"Aditi Raghunathan, Greg Valiant, James Zou",https://icml.cc/Conferences/2017/Schedule?showEvent=885,"Given samples from a distribution, how many new elements should we expect to find if we keep on sampling this distribution? This is an important and actively studied problem, with many applications ranging from species estimation to genomics. We generalize this extrapolation and related unseen estimation problems to the multiple population setting, where population $j$ has an unknown distribution $D_j$ from which we observe $n_j$ samples. We derive an optimal estimator for the total number of elements we expect to find among new samples across the populations. Surprisingly, we prove that our estimator's accuracy is independent of the number of populations. We also develop an efficient optimization algorithm to solve the more general problem of estimating multi-population frequency distributions. We validate our methods and theory through extensive experiments. Finally, on a real dataset of human genomes across multiple ancestries, we demonstrate how our approach for unseen estimation can enable cohort designs that can discover interesting mutations with greater efficiency.","['Stanford', '', 'Stanford']"
2017,Meritocratic Fairness for Cross-Population Selection,"Michael Kearns, Aaron Roth, Steven Wu",https://icml.cc/Conferences/2017/Schedule?showEvent=744,"We consider the problem of selecting a strong pool of individuals from several populations with incomparable skills (e.g. soccer players, mathematicians, and singers) in a fair manner.  The quality of an individual is defined to be their relative rank (by cumulative distribution value) within their own population, which permits cross-population comparisons.  We study algorithms which attempt to select the highest quality subset despite the fact that true CDF values are not known, and can only be estimated from the finite pool of candidates.  Specifically, we quantify the regret in quality imposed by ""meritocratic"" notions of fairness, which require that individuals are selected with probability that is monotonically increasing in their true quality. We give algorithms with provable fairness and regret guarantees, as well as lower bounds, and provide empirical results which suggest that our algorithms perform better than the theory suggests.  We extend our results to a sequential batch setting, in which an algorithm must repeatedly select subsets of individuals from new pools of applicants, but has the benefit of being able to compare them to the accumulated data from previous rounds.
","['University of Pennsylvania', 'University of Pennsylvania', 'UPenn']"
2017,Neural networks and rational functions,Matus Telgarsky,https://icml.cc/Conferences/2017/Schedule?showEvent=761,"Neural networks and rational functions efficiently approximate each other.  In more detail, it is shown here that for any ReLU network, there exists a rational function of degree $O(polylog(1/\epsilon))$ which is $\epsilon$-close, and similarly for any rational function there exists a ReLU network of size $O(polylog(1/\epsilon))$ which is $\epsilon$-close.  By contrast, polynomials need degree $\Omega(poly(1/\epsilon))$ to approximate even a single ReLU.  When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight;  in other words, a compositional representation can be beneficial even for rational functions.
",['UIUC']
2017,Input Convex Neural Networks,"Brandon Amos, Lei Xu, Zico Kolter",https://icml.cc/Conferences/2017/Schedule?showEvent=835,"This paper presents the input convex neural network
architecture. These are scalar-valued (potentially deep) neural
networks with constraints on the network parameters such that the
output of the network is a convex function of (some of) the inputs.
The networks allow for efficient inference via optimization over some
inputs to the network given others, and can be applied to settings
including structured prediction, data imputation, reinforcement
learning, and others.  In this paper we lay the basic groundwork for
these models, proposing methods for inference, optimization and
learning, and analyze their representational power.  We show that many
existing neural network architectures can be made input-convex with
a minor modification, and develop specialized optimization
algorithms tailored to this setting. Finally, we highlight the
performance of the methods on multi-label prediction, image
completion, and reinforcement learning problems, where we show
improvement over the existing state of the art in many cases.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Co-clustering through Optimal Transport,"Charlotte Laclau, Ievgen Redko, Basarab Matei, Younès Bennani, Vincent Brault",https://icml.cc/Conferences/2017/Schedule?showEvent=591,"In this paper, we present a novel method for co-clustering, an unsupervised learning approach that aims at discovering homogeneous groups of data instances and features by grouping them simultaneously. The proposed method uses the entropy regularized optimal transport between empirical measures defined on data instances and features in order to obtain an estimated joint probability density function represented by the optimal coupling matrix. This matrix is further factorized to obtain the induced row and columns partitions using multiscale representations approach. To justify our method theoretically, we show how the solution of the regularized optimal transport can be seen from the variational inference perspective thus motivating its use for co-clustering. The algorithm derived for the proposed method and its kernelized version based on the notion of Gromov-Wasserstein distance are fast, accurate and can determine automatically the number of both row and column clusters. These features are vividly demonstrated through extensive experimental evaluations. 
","['LIG', 'Université Lyon 1 – INSA Lyon - Université Jean Monnet Saint-Etienne.', '', '', 'Univ. Grenoble Alpes']"
2017,OptNet: Differentiable Optimization as a Layer in Neural Networks,"Brandon Amos, Zico Kolter",https://icml.cc/Conferences/2017/Schedule?showEvent=844,"This paper presents OptNet, a network architecture that integrates
  optimization problems (here, specifically in the form of quadratic programs)
  as individual layers in larger end-to-end trainable deep networks.
  These layers encode constraints and complex dependencies
  between the hidden states that traditional convolutional and
  fully-connected layers often cannot capture.
  In this paper, we explore the foundations for such an architecture:
  we show how techniques from sensitivity analysis, bilevel
  optimization, and implicit differentiation can be used to
  exactly differentiate through these layers and with respect
  to layer parameters;
  we develop a highly efficient solver for these layers that exploits fast
  GPU-based batch solves within a primal-dual interior point method, and which
  provides backpropagation gradients with virtually no additional cost on top of
  the solve;
  and we highlight the application of these approaches in several problems.
  In one notable example, we show that the method is
  capable of learning to play mini-Sudoku (4x4) given just input and output games,
  with no a priori information about the rules of the game;
  this highlights the ability of our architecture to learn hard
  constraints better than other neural architectures.
","['Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Multiple Clustering Views from Multiple Uncertain Experts,"Yale Chang, Junxiang Chen, Michael Cho, Peter Castaldi, Edwin Silverman, Jennifer Dy",https://icml.cc/Conferences/2017/Schedule?showEvent=484,"Expert input can improve clustering performance. In today's collaborative 
environment, the availability of crowdsourced multiple expert input is 
becoming common. Given multiple experts' inputs, most existing approaches can 
only discover one clustering structure. However, data is multi-faced by nature 
and can be clustered in different ways (also known as views). 
In an exploratory analysis problem where ground truth is not known, different 
experts may have diverse views on how to cluster data.
In this paper, we address the problem on how to automatically discover 
multiple ways to cluster data given potentially diverse inputs from multiple 
uncertain experts. We propose a novel Bayesian probabilistic model that 
automatically learns the multiple expert views and the clustering structure 
associated with each view. The benefits of learning the experts' views include 
1) enabling the discovery of multiple diverse clustering structures, and
2) improving the quality of clustering solution in each view by assigning 
higher weights to experts with higher confidence.
In our approach, the expert views, multiple clustering structures and 
expert confidences are jointly learned via variational inference.
Experimental results on synthetic datasets, benchmark datasets and a
real-world disease subtyping problem show that our proposed approach outperforms
competing baselines, including meta clustering, semi-supervised clustering,
semi-crowdsourced clustering and consensus clustering.
","['Northeastern University', 'Northeastern University', 'Harvard Medical School', 'Harvard Medical School', 'Harvard Medical School', 'Northeastern University']"
2017,Parseval Networks: Improving Robustness to Adversarial Examples,"Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, Nicolas Usunier",https://icml.cc/Conferences/2017/Schedule?showEvent=830,"We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than $1$. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art regarding accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.","['Facebook AI Research', 'Facebook', 'Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research']"
2017,"Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery","Ashkan Panahi, Devdatt Dubhashi, Fredrik D Johansson, Chiranjib Bhattacharya",https://icml.cc/Conferences/2017/Schedule?showEvent=699,"Standard clustering methods such as K-means,  Gaussian mixture models, and hierarchical clustering are beset by local minima, which are sometimes drastically suboptimal. Moreover the number of clusters K must be known in advance. The recently introduced the sum-of-norms (SON) or Clusterpath convex relaxation of k-means and hierarchical clustering shrinks cluster centroids toward one another and ensure a unique global minimizer. We give a scalable stochastic incremental algorithm based on proximal iterations to solve the SON problem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which have a similar form to the unifying proximity condition introduced in the approximation algorithms community (that covers paradigm cases such as Gaussian mixtures and planted partition models). We give experimental results to confirm that our algorithm scales much better than previous methods while producing clusters of comparable quality.
","['NC state university', 'Chalmers University', 'MIT', '']"
2017,Regularising Non-linear Models Using Feature Side-information,"Amina Mollaysa, Pablo Strasser, Alexandros Kalousis",https://icml.cc/Conferences/2017/Schedule?showEvent=758,"Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features' similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.
","['University of Geneva, HES', 'HES-UNIGE', 'HES-UNIGE']"
2017,Clustering High Dimensional Dynamic Data Streams,"Lin Yang, Harry Lang, Christian Sohler, Vladimir Braverman, Gereon Frahling",https://icml.cc/Conferences/2017/Schedule?showEvent=626,"We present data streaming algorithms for the $k$-median problem in high-dimensional dynamic geometric data streams, i.e. streams allowing both insertions and deletions of points from a discrete Euclidean space $\{1, 2, \ldots \Delta\}^d$.
Our algorithms use $k \epsilon^{-2} \poly(d \log \Delta)$ space/time and maintain with high probability a small weighted set 
of points (a coreset) such that for every set of $k$ centers the cost of the coreset $(1+\epsilon)$-approximates the cost of the streamed point set.
We also provide algorithms that guarantee only positive weights in the coreset with additional logarithmic factors in the space and time complexities.
We can use this positively-weighted coreset to compute a $(1+\epsilon)$-approximation for the $k$-median problem 
by any efficient offline $k$-median algorithm.  
All previous algorithms for computing a $(1+\epsilon)$-approximation for the $k$-median problem over dynamic data streams required space and time exponential in $d$.
Our algorithms can be generalized to metric spaces of bounded doubling dimension.","['Johns Hopkins', 'Johns Hopkins University', 'TU Dortmund', 'Johns Hopkins University', 'Linguee GmbH']"
2017,Fast k-Nearest Neighbour Search via Prioritized DCI,"Ke Li, Jitendra Malik",https://icml.cc/Conferences/2017/Schedule?showEvent=468,"Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) offers a promising way of circumventing the curse by avoiding space partitioning and achieves a query time that grows sublinearly in the intrinsic dimensionality. In this paper, we propose a variant of DCI, which we call Prioritized DCI, and show a remarkable improvement in the dependence of query time on intrinsic dimensionality. In particular, a linear increase in intrinsic dimensionality, which could mean an exponential increase in the number of points near a query, can be mostly counteracted with just a linear increase in space. We also demonstrate empirically that Prioritized DCI significantly outperforms prior methods. In particular, relative to Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumption by a factor of 21. 
","['UC Berkeley', 'University of California at Berkeley']"
2017,Deep Spectral Clustering Learning,"Marc Law, Raquel Urtasun, Richard Zemel",https://icml.cc/Conferences/2017/Schedule?showEvent=494,"Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is fixed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is efficient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets confirm state-of-the-art Recall@K performance.
","['University of Toronto', 'University of Toronto', 'University of Toronto']"
2017,Joint Dimensionality Reduction and Metric Learning: A Geometric Take,"Mehrtash Harandi, Mathieu Salzmann, Richard I Hartley",https://icml.cc/Conferences/2017/Schedule?showEvent=561,"To be tractable and robust to data noise, existing metric learning algorithms commonly rely on PCA as a pre-processing step. How can we know, however, that PCA, or any other specific dimensionality reduction technique, is the method of choice for the problem at hand? The answer is simple: We cannot! To address this issue, in this paper, 
we develop a Riemannian framework to jointly learn a mapping performing dimensionality reduction and a metric in the induced space.
Our experiments evidence that, while we directly work on high-dimensional features, our approach yields competitive runtimes with and higher accuracy than state-of-the-art metric learning algorithms.
","['Data61', 'EPFL', 'Australian National University']"
2017,ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices,"Chirag Gupta, ARUN SUGGALA, Ankit Goyal, Saurabh Goyal, Ashish Kumar, Bhargavi Paranjape, Harsha Vardhan Simhadri, Raghavendra Udupa, Manik Varma, Prateek Jain",https://icml.cc/Conferences/2017/Schedule?showEvent=683,"Several real-world applications require real-time prediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such applications demand prediction models with small storage and computational complexity that do not compromise significantly on accuracy.  In this work, we propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. ProtoNN is inspired by k-Nearest Neighbor (KNN) but has several orders lower storage and prediction complexity. ProtoNN models can be deployed even on devices with puny storage and computational power (e.g. an Arduino UNO with 2kB RAM) to get excellent prediction accuracy. 
ProtoNN derives its strength from three key ideas: a) learning a small number of prototypes to represent the entire training set, b) sparse low dimensional projection of data, c) joint discriminative learning of the projection and prototypes with explicit model size constraint. We conduct systematic empirical evaluation of ProtoNN on a variety of supervised learning tasks (binary, multi-class, multi-label classification) and show that it gives nearly state-of-the-art prediction accuracy on resource-scarce devices while consuming several orders lower storage, and using minimal working memory. 
","['Microsoft Research, India', 'Carnegie Mellon University', 'University of Michigan', 'IBM India Pvt Ltd', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2017,Device Placement Optimization with Reinforcement Learning,"Azalia Mirhoseini, Hieu Pham, Quoc Le, benoit steiner, Mohammad Norouzi, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Samy Bengio, Jeff Dean",https://icml.cc/Conferences/2017/Schedule?showEvent=888,"The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we pro- pose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algo-rithmic methods.
","['Google', 'Google', 'Google Brain', 'Google', 'Google', 'Google', 'Google Brain', 'Google', 'Google Brain', 'Google Brain']"
2017,Dynamic Word Embeddings,"Robert Bamler, Stephan Mandt",https://icml.cc/Conferences/2017/Schedule?showEvent=497,"We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec [Mikolov et al., 2013]. These embedding vectors are connected in time through a latent  diffusion process. We describe two scalable variational inference algorithms--skip-gram smoothing and skip-gram filtering--that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.
","['Disney Research Pittsburgh', 'Disney Research']"
2017,Asynchronous Stochastic Gradient Descent with Delay Compensation,"Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhiming Ma, Tie-Yan Liu",https://icml.cc/Conferences/2017/Schedule?showEvent=564,"With the fast development of deep learning, it has become common to learn big neural networks using massive training data. Asynchronous Stochastic Gradient Descent (ASGD) is widely adopted to fulfill this task for its efficiency, which is, however, known to suffer from the problem of delayed gradients. That is, when a local worker adds its gradient to the global model, the global model may have been updated by other workers and this gradient becomes ``delayed''. We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is achieved by leveraging Taylor expansion of the gradient function and efficient approximators to the Hessian matrix of the loss function. We call the new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and the experimental results demonstrate that DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly approaches the performance of sequential SGD.
","['University of Science and Technology of China', 'Peking University', '', 'Microsoft Research', 'USTC', '', 'Microsoft']"
2017,Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution,"Po-Wei Chou, Daniel Maturana, Sebastian Scherer",https://icml.cc/Conferences/2017/Schedule?showEvent=733,"Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaussian distribution is used as the stochastic policy. In this work, we propose to use the Beta distribution as an alternative and analyze the bias and variance of the policy gradients of both policies. We show that the Beta policy is bias-free and provides significantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimization (TRPO) and actor critic with experience replay (ACER), the state-of-the-art on- and off-policy stochastic methods respectively, on OpenAI Gym's and MuJoCo's continuous control environments.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2017,Fractional Langevin Monte Carlo: Exploring Levy Driven Stochastic Differential Equations for MCMC,Umut Simsekli,https://icml.cc/Conferences/2017/Schedule?showEvent=462,"Along with the recent advances in scalable Markov Chain Monte Carlo methods, sampling techniques that are based on Langevin diffusions have started receiving increasing attention. These so called Langevin Monte Carlo (LMC) methods are based on diffusions driven by a Brownian motion, which gives rise to Gaussian proposal distributions in the resulting algorithms. Even though these approaches have proven successful in many applications, their performance can be limited by the light-tailed nature of the Gaussian proposals. In this study, we extend classical LMC and develop a novel Fractional LMC (FLMC) framework that is based on a family of heavy-tailed distributions, called alpha-stable Levy distributions. As opposed to classical approaches, the proposed approach can possess large jumps while targeting the correct distribution, which would be beneficial for efficient exploration of the state space. We develop novel computational methods that can scale up to large-scale problems and we provide formal convergence analysis of the proposed scheme. Our experiments support our theory: FLMC can provide superior performance in multi-modal settings, improved convergence rates, and robustness to algorithm parameters. 
",['Telecom ParisTech']
2017,Preferential Bayesian Optmization,"Javier González, Zhenwen Dai, Andreas Damianou, Neil Lawrence",https://icml.cc/Conferences/2017/Schedule?showEvent=565,"Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimize black-box functions where direct queries of the objective are expensive. We consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as  A/B tests or recommender systems. 
We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) and that allows to find the optimum of a latent function that can only be queried through pairwise comparisons, so-called duels. PBO extend the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the the winner of each duel  by means of Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of experiments in which we show how the way correlations are modeled is the key ingredient to drastically reduce the number of comparisons to find the optimum of the latent function of interest.
","['Amazon', 'Amazon.com', 'Amazon.com', 'Amazon.com']"
2017,Being Robust (in High Dimensions) Can Be Practical,"Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, Alistair Stewart",https://icml.cc/Conferences/2017/Schedule?showEvent=460,"Robust estimation is much more challenging in high-dimensions than it is in one-dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. 
Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.  
","['USC', 'MIT', 'UCSD', 'MIT', 'MIT', 'USC']"
2017,Differentially Private Ordinary Least Squares,Or Sheffet,https://icml.cc/Conferences/2017/Schedule?showEvent=611,"Linear regression is one of the most prevalent techniques in machine learning;
however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. 
Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income)  in the presence of other (potentially correlated) features. 
OLS assumes a particular model that randomly generates the data, and derives t-values --- representing the likelihood of each real value to be the true correlation. 
Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation is non-zero.
Our work aims at achieving similar guarantees on data under differentially private estimators.
First, we show that for well-spread data, the  Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l_2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the ``Analyze Gauss'' algorithm (Dwork et al 2014).
",['University of Alberta']
2017,"When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests, $\ell_2$-consistency and Neuroscience Applications","Hao Zhou, Yilin Zhang, Vamsi Krishna Ithapu, Sterling Johnson, Grace Wahba, Vikas Singh",https://icml.cc/Conferences/2017/Schedule?showEvent=487,"Many studies in biomedical and health sciences involve small sample sizes due to logistic or financial constraints. Often, identifying weak (but scientifically interesting) associations between a set of predictors and a response necessitates pooling datasets from multiple diverse labs or groups. While there is a rich literature in statistical machine learning to address distributional shifts and inference in multi-site datasets, it is less clear when such pooling is guaranteed to help (and when it does not) – independent of the inference algorithms we use. In this paper, we present a hypothesis test to answer this question, both for classical and high dimensional linear regression. We precisely identify regimes where pooling datasets across multiple sites is sensible, and how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens. With a focus on Alzheimer’s disease studies, we present empirical results showing that in regimes suggested by our analysis, pooling a local dataset with data from an international study improves power.
","['University of Wisconsin - Madison', '', 'Univresity of Wisconsin Madiso', 'UW Madison', 'University of Wisconsin-Madison', 'University of Wisconsin Madison']"
2017,Deep Tensor Convolution on Multicores,"David Budden, Alexander Matveev, Shibani Santurkar, Shraman Ray Chaudhuri, Nir Shavit",https://icml.cc/Conferences/2017/Schedule?showEvent=465,"Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of spatiotemporal features. These networks have improved performance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Existing CPU implementations overcome this constraint but are impractically slow. Here we extend and optimize the faster Winograd-class of  convolutional algorithms to the $N$-dimensional case and specifically for CPU hardware. First, we remove the need to manually hand-craft algorithms by exploiting the relaxed constraints and cheap sparse access of CPU memory. Second, we maximize CPU utilization and multicore scalability by transforming data matrices to be cache-aware, integer multiples of AVX vector widths. Treating 2-dimensional ConvNets as a special (and the least beneficial) case of our approach, we demonstrate a 5 to 25-fold improvement in throughput compared to previous state-of-the-art.","['MIT / DeepMind', 'MIT', 'MIT', 'MIT', 'MIT']"
2017,Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling,"Hairong Liu, Zhenyao Zhu, Xiangang Li, Sanjeev Satheesh",https://icml.cc/Conferences/2017/Schedule?showEvent=742,"Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: $1$) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and $2$) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called \textit{Gram-CTC}. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences. 
Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency.
We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.","['Baidu Silicon Valley AI Lab', 'Baidu Silicon Valley AI Lab', 'Baidu AI Lab', 'Baidu SVAIL']"
2017,Adaptive Consensus ADMM for Distributed Optimization,"Zheng Xu, Gavin Taylor, Hao Li, Mario Figueiredo, Xiaoming Yuan, Tom Goldstein",https://icml.cc/Conferences/2017/Schedule?showEvent=632,"The alternating direction method of multipliers (ADMM) is commonly used for distributed model fitting problems, but its performance and reliability depend strongly on user-defined penalty parameters.   We study distributed ADMM methods that boost performance by using different fine-tuned algorithm parameters on each worker node. We present a O(1/k) convergence rate for adaptive ADMM methods with node-specific parameters, and propose adaptive consensus ADMM (ACADMM), which automatically tunes parameters without user oversight.
","['University of Maryland', 'US Naval Academy', 'University of Maryland at College Park', 'Instituto Superior Tecnico', '', 'University of Maryland']"
2017,Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning,"Yevgen Chebotar, Karol Hausman, Marvin Zhang, Gaurav Sukhatme, Stefan Schaal, Sergey Levine",https://icml.cc/Conferences/2017/Schedule?showEvent=853,"Reinforcement learning algorithms for real-world robotic applications must be able to handle complex, unknown dynamical systems while maintaining data-efficient learning. These requirements are handled well by model-free and model-based RL approaches, respectively. In this work, we aim to combine the advantages of these approaches. By focusing on time-varying linear-Gaussian policies, we enable a model-based algorithm based on the linear-quadratic regulator that can be integrated into the model-free framework of path integral policy improvement. We can further combine our method with guided policy search to train arbitrary parameterized policies such as deep neural networks. Our simulation and real-world experiments demonstrate that this method can solve challenging manipulation tasks with comparable or better performance than model-free methods while maintaining the sample efficiency of model-based methods.
","['University of Southern California', 'University of Southern California', 'UC Berkeley', 'University of Southern California', '', 'Berkeley']"
2017,Stochastic  Bouncy  Particle Sampler,"Ari Pakman, Dar Gilboa, David Carlson, Liam Paninski",https://icml.cc/Conferences/2017/Schedule?showEvent=890,"We introduce a stochastic version of the non-reversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear, to efficiently sample  Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias, in exchange for faster mixing. We introduce a simple method that controls this trade-off. We illustrate these ideas in several examples  which outperform previous approaches.
","['Columbia University', 'Columbia University', 'Duke University', '']"
2017,Max-value Entropy Search for Efficient Bayesian Optimization,"Zi Wang, Stefanie Jegelka",https://icml.cc/Conferences/2017/Schedule?showEvent=630,"Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the $\arg\max$ of the unknown function; yet, both are plagued by the expensive computation  for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.","['MIT', 'MIT']"
2017,Multilabel Classification with Group Testing and Codes,"Shashanka Ubaru, Arya Mazumdar",https://icml.cc/Conferences/2017/Schedule?showEvent=713,"In recent years, the multiclass and mutlilabel classification problems we encounter in many applications have very large (10^3-10^6) number of classes. However, each instance belongs to only one or few classes, i.e., the label vectors are sparse. In this work, we propose a novel approach based on group testing to solve such large multilabel classification problems with sparse label vectors. We describe various group testing constructions, and advocate the use of concatenated Reed Solomon codes and unbalanced bipartite expander graphs for extreme classification problems. The proposed approach has several advantages theoretically and practically over existing popular methods. Our method operates on the binary alphabet and can utilize the well-established binary classifiers for learning. The error correction capabilities of the codes are leveraged for the first time in the learning problem to correct prediction errors. Even if a linearly growing number of classifiers mis-classify, these errors are fully corrected. We establish Hamming loss error bounds for the approach. More importantly, our method utilizes a simple prediction algorithm and does not require matrix inversion or solving optimization problems making the algorithm very inexpensive. Numerical experiments with various datasets illustrate the superior performance of our method.
","['University of Minnesota', 'University of Massachusetts Amherst']"
2017,Priv’IT: Private and Sample Efficient Identity Testing,"Bryan Cai, Constantinos Daskalakis, Gautam Kamath",https://icml.cc/Conferences/2017/Schedule?showEvent=459,"We develop differentially private hypothesis testing methods for the small sample regime. Given a sample D from a categorical distribution p over some domain Sigma, an explicitly described distribution q over Sigma, some privacy parameter epsilon, accuracy parameter alpha, and requirements betaI$ and  betaII for the type I and type II errors of our test, the goal is to distinguish between p=q and dtv(p,q) > alpha. We provide theoretical bounds for the sample size |D| so that our method both satisfies (epsilon,0)-differential privacy, and guarantees betaI and betaII type I and type II errors. We show that differential privacy may come for free in some regimes of parameters, and we always beat the sample complexity resulting from running the chi^2-test with noisy counts, or standard approaches such as repetition for endowing non-private chi^2-style statistics with differential privacy guarantees. We experimentally compare  the sample complexity of our method to that of recently proposed methods for private hypothesis testing.
","['MIT', 'MIT', 'MIT']"
2017,Learning from Clinical Judgments: Semi-Markov-Modulated Marked Hawkes Processes for Risk Prognosis,"Ahmed M. Alaa, Scott B Hu, Mihaela van der Schaar",https://icml.cc/Conferences/2017/Schedule?showEvent=573,"Critically ill patients in regular wards are vulnerable to unanticipated adverse events which require prompt transfer to the intensive care unit (ICU). To allow for accurate prognosis of deteriorating patients, we develop a novel continuous-time probabilistic model for a monitored patient's temporal sequence of physiological data. Our model captures ""informatively sampled"" patient episodes: the clinicians' decisions on when to observe a hospitalized patient's vital signs and lab tests over time are represented by a marked Hawkes process, with intensity parameters that are modulated by the patient's latent clinical states, and with observable physiological data (mark process) modeled as a switching multi-task Gaussian process. In addition, our model captures ""informatively censored"" patient episodes by representing the patient's latent clinical states as an absorbing semi-Markov jump process. The model parameters are learned from offline patient episodes in the electronic health records via an EM-based algorithm. Experiments conducted on a cohort of patients admitted to a major medical center over a 3-year period show that risk prognosis based on our model significantly outperforms the currently deployed medical risk scores and other baseline machine learning algorithms.
","['UCLA', 'UCLA', 'Oxford University and UCLA']"
2017,MEC: Memory-efficient Convolution for Deep Neural Network,"Minsik Cho, Daniel Brand",https://icml.cc/Conferences/2017/Schedule?showEvent=568,"Convolution  is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed.
Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect
methods have been proposed including im2col-based convolution,
FFT-based convolution, or Winograd-based algorithm.
However, all these indirect methods have high memory overhead,
which  creates performance degradation and offers a poor trade-off between performance and memory consumption.
In this work, we propose a memory-efficient convolution or MEC with compact lowering,
which  reduces memory overhead substantially and accelerates convolution process.
MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory overhead), and then
executes  multiple small matrix multiplications in parallel to get convolution completed.
Additionally, the reduced memory footprint improves memory sub-system efficiency, improving performance.
Our experimental results show that MEC reduces memory consumption significantly with
good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.
","['IBM Research', 'IBM Research']"
2017,Coupling Distributed and Symbolic Execution for Natural Language Queries,"Lili Mou, Zhengdong Lu, Hang Li, Zhi Jin",https://icml.cc/Conferences/2017/Schedule?showEvent=572,"Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor's intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.
","['Peking University', 'DeeplyCurious.ai', 'Huawei', 'Peking University']"
2017,Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks,"Kevin Scaman, Francis Bach, Sebastien Bubeck, Yin Tat Lee, Laurent Massoulié",https://icml.cc/Conferences/2017/Schedule?showEvent=685,"In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov's accelerated gradient descent is optimal and achieves a precision $\varepsilon > 0$ in time $O(\sqrt{\kappa_g}(1+\Delta\tau)\ln(1/\varepsilon))$, where $\kappa_g$ is the condition number of the (global) function to optimize, $\Delta$ is the diameter of the network, and $\tau$ (resp. $1$) is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based on gossip, we provide the first optimal algorithm, called the multi-step dual accelerated (MSDA) method, that achieves a precision $\varepsilon > 0$ in time $O(\sqrt{\kappa_l}(1+\frac{\tau}{\sqrt{\gamma}})\ln(1/\varepsilon))$, where $\kappa_l$ is the condition number of the local functions and $\gamma$ is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two problems: least-squares regression and classification by logistic regression.","['MSR-INRIA Joint Center', 'INRIA', 'Microsoft Research', 'Microsoft Research', 'MSR-INRIA Joint Center']"
2017,Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control,"Yunpeng Pan, Xinyan Yan, Evangelos Theodorou, Byron Boots",https://icml.cc/Conferences/2017/Schedule?showEvent=786,"Sparse Spectrum Gaussian Processes (SSGPs) are a powerful tool for scaling Gaussian processes (GPs) to large datasets. Existing SSGP algorithms for regression assume deterministic inputs, precluding their use in many real-world robotics and engineering applications where accounting for input uncertainty is crucial. We address this problem by proposing two analytic moment-based approaches with closed-form expressions for SSGP regression with uncertain inputs. Our methods are more general and scalable than their standard GP counterparts, and are naturally applicable to multi-step prediction or uncertainty propagation. We show that efficient algorithms for Bayesian filtering and stochastic model predictive control can use these methods, and we evaluate our algorithms with comparative analyses and both real-world and simulated experiments.
","['Georgia Tech', 'Georgia Institute of Technology', 'Georgia Tech', 'Georgia Tech']"
2017,Canopy --- Fast Sampling with Cover Trees,"Manzil Zaheer, Satwik Kottur, Amr Ahmed, Jose Moura, Alex Smola",https://icml.cc/Conferences/2017/Schedule?showEvent=507,"Hierarchical Bayesian models often capture distributions over a very large number of distinct atoms.
The need for these models arises when organizing huge amount of unsupervised data, for instance, features extracted using deep convnets that can be exploited to organize abundant unlabeled images.
Inference for hierarchical Bayesian models in such cases can be rather nontrivial, leading to approximate approaches.
In this work, we propose \emph{Canopy}, a sampler based on Cover Trees that is exact, has guaranteed runtime logarithmic in the number of atoms, and is provably polynomial in the inherent dimensionality of the underlying parameter space.
In other words, the algorithm is as fast as search over a hierarchical data structure.
We provide theory for Canopy and demonstrate its effectiveness on both synthetic and real datasets, consisting of over 100 million images.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Google', 'CMU', 'Amazon']"
2017,Bayesian Optimization with Tree-structured Dependencies,"Rodolphe Jenatton, Cedric Archambeau, Javier González, Matthias Seeger",https://icml.cc/Conferences/2017/Schedule?showEvent=666,"Bayesian optimization has been successfully used to optimize complex black-box functions whose evaluations are expensive. In many applications, like in deep learning and predictive analytics, the optimization domain is itself complex and structured. In this work, we focus on use cases where this domain exhibits a known dependency structure. The benefit of leveraging this structure is twofold: we explore the search space more efficiently and posterior inference scales more favorably with the number of observations than Gaussian Process-based approaches published in the literature. We introduce a novel surrogate model for Bayesian optimization which combines independent Gaussian Processes with a linear model that encodes a tree-based dependency structure and can transfer information between overlapping decision sequences. We also design a specialized  two-step acquisition function that explores the search space more effectively. Our experiments on synthetic tree-structured functions and the tuning of feedforward neural networks trained on a range of binary classification datasets show that our method compares favorably with competing approaches.
","['Amazon', 'Amazon', 'Amazon', 'Amazon.com']"
2017,High-Dimensional Structured Quantile Regression,"Vidyashankar Sivakumar, Arindam Banerjee",https://icml.cc/Conferences/2017/Schedule?showEvent=741,"Quantile regression aims at modeling the conditional median and quantiles of a response variable given certain predictor variables. In this work we consider the problem of linear quantile regression in high dimensions where the number of predictor variables is much higher than the number of samples available for parameter estimation. We assume the true parameter to have some structure characterized as having a small value according to some atomic norm R(.) and consider the norm regularized quantile regression estimator. We characterize the sample complexity for consistent recovery and give non-asymptotic bounds on the estimation error. While this problem has been previously considered, our analysis reveals geometric and statistical characteristics of the problem not available in prior literature. We perform experiments on synthetic data which support the theoretical results.
","['University of Minnesota', 'University of Minnesota']"
2017,Differentially Private Submodular Maximization: Data Summarization in Disguise,"Marko Mitrovic, Mark Bun, Andreas Krause, Amin Karbasi",https://icml.cc/Conferences/2017/Schedule?showEvent=637,"Many data summarization applications are captured by the general framework of submodular maximization. As a consequence,  a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacy-preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal. Along the way, we analyze a new algorithm for non-monotone submodular maximization, which is the first (even non-privately) to achieve a constant approximation ratio while running in linear time. We additionally provide two concrete experiments to validate the efficacy of these algorithms. 
","['Yale University', 'Princeton University', 'ETH Zurich', 'Yale']"
2017,Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier,"Joseph Futoma, Sanjay Hariharan, Katherine Heller",https://icml.cc/Conferences/2017/Schedule?showEvent=757,"We present a scalable end-to-end classifier that uses streaming physiological and medication data to accurately predict the onset of sepsis, a life-threatening complication from infections that has high mortality and morbidity.  Our proposed framework models the multivariate trajectories of continuous-valued physiological time series using multitask Gaussian processes, seamlessly accounting for the high uncertainty, frequent missingness, and irregular sampling rates typically associated with real clinical data.  The Gaussian process is directly connected to a black-box classifier that predicts whether a patient will become septic, chosen in our case to be a recurrent neural network to account for the extreme variability in the length of patient encounters.  We show how to scale the computations associated with the Gaussian process in a manner so that the entire system can be discriminatively trained end-to-end using backpropagation.   In a large cohort of heterogeneous inpatient encounters at our university health system we find that it outperforms several baselines at predicting sepsis, and yields 19.4\% and 55.5\% improved areas under the Receiver Operating Characteristic and Precision Recall curves as compared to the NEWS score currently used by our hospital.
","['Duke University', 'Duke University', 'Duke University']"
2017,Beyond Filters: Compact Feature Map for Portable Deep Model,"Yunhe Wang, Chang Xu, Chao Xu, Dacheng Tao",https://icml.cc/Conferences/2017/Schedule?showEvent=466,"Convolutional neural networks (CNNs) have shown extraordinary performance in a number of applications, but they are usually of heavy design for the accuracy reason. Beyond compressing the filters in CNNs, this paper focuses on the redundancy in the feature maps derived from the large number of filters in a layer. We propose to extract intrinsic representation of the feature maps and preserve the discriminability of the features. Circulant matrix is employed to formulate the feature map transformation, which only requires O(dlog d) computation complexity to embed a d-dimensional feature map. The filter is then re-configured to establish the mapping from original input to the new compact feature map, and the resulting network can preserve intrinsic information of the original network with significantly fewer parameters, which not only decreases the online memory for launching CNN but also accelerates the computation speed. Experiments on benchmark image datasets demonstrate the superiority of the proposed algorithm over state-of-the-art methods.
","['Peking University', 'The University of Sydney', 'Peking University', '']"
2017,Image-to-Markup Generation with Coarse-to-Fine Attention,"Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, Alexander Rush",https://icml.cc/Conferences/2017/Schedule?showEvent=498,"We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.
","['Harvard University', 'University of Eastern Finland', 'Harvard University', 'Harvard University']"
2017,Projection-free Distributed Online Learning in Networks,"Wenpeng Zhang, Peilin Zhao, wenwu zhu, Steven Hoi, Tong Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=700,"The conditional gradient algorithm has regained a surge of research interest in recent years due to its high efficiency in handling large-scale machine learning problems. However, none of existing studies has explored it in the distributed online learning setting, where locally light computation is assumed. In this paper, we fill this gap by proposing the distributed online conditional gradient algorithm, which eschews the expensive projection operation needed in its counterpart algorithms by exploiting much simpler linear optimization steps. We give a regret bound for the proposed algorithm as a function of the network size and topology, which will be smaller on smaller graphs or ""well-connected"" graphs. Experiments on two large-scale real-world datasets for a multiclass classification task confirm the computational benefit of the proposed algorithm and also verify the theoretical regret bound.
","['Tsinghua University', 'Artificial Intelligence Department, Ant \u200bFinancial', 'Tsinghua University', 'Singapore Management University', 'HKUST']"
2017,Learning Stable Stochastic Nonlinear Dynamical Systems,"Jonas Umlauft, Sandra Hirche",https://icml.cc/Conferences/2017/Schedule?showEvent=531,"A data-driven identification of dynamical systems requiring only minimal prior knowledge is promising whenever no analytically derived model structure is available, e.g., from first principles in physics. However,    meta-knowledge on the system's behavior is often given and should be exploited: Stability as fundamental property is essential when the model is used for controller design or movement generation. Therefore, this paper  proposes a framework for learning stable stochastic systems from data. We focus on identifying a state-dependent coefficient form of the nonlinear stochastic model which is globally asymptotically stable according to probabilistic Lyapunov methods. We compare our approach to other state of the art methods on real-world datasets in terms of flexibility and   stability.
","['Technical University of Munich', 'Technical University of Munich']"
2017,A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization,"Jianbo Ye, James Wang, Jia Li",https://icml.cc/Conferences/2017/Schedule?showEvent=585,"Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated annealing for solving WLMs. Particularly, we have developed a Gibbs sampler to approximate effectively and efficiently the partial gradients of a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which the oracle for computing the value and gradient of a loss function is embedded. We applied the method to optimal transport with Coulomb cost and the Wasserstein non-negative matrix factorization problem, and made comparisons with the existing method of entropy regularization. 
","['Penn State University', 'Penn State University', 'Penn State University']"
2017,Multi-fidelity Bayesian Optimisation with Continuous Approximations,"kirthevasan kandasamy, Gautam Dasarathy, Barnabás Póczos, Jeff Schneider",https://icml.cc/Conferences/2017/Schedule?showEvent=596,"Bandit methods for black-box optimisation, such as Bayesian optimisation,
are used in a variety of applications including hyper-parameter tuning and
experiment design.
Recently, \emph{multi-fidelity} methods have garnered
considerable attention since function evaluations have become increasingly expensive in
such applications.
Multi-fidelity methods use cheap approximations to the function of
interest to speed up the overall optimisation process.
However, most multi-fidelity methods assume only a finite number of approximations.
On the other hand, in many practical applications, a continuous spectrum of approximations might be
available.
For instance, when tuning an expensive neural network, one might choose to approximate the
cross validation performance using less data $N$ and/or few training iterations $T$.
Here, the approximations are best viewed as arising out of a continuous two dimensional
space $(N,T)$.
In this work, we develop a Bayesian optimisation method, \boca, for this setting.
We characterise its theoretical properties and show that it achieves better regret than
than strategies which ignore the approximations.
\bocas outperforms several other baselines in synthetic and real experiments.","['CMU', 'Rice University', 'CMU', 'CMU/Uber']"
2017,High-dimensional Non-Gaussian Single Index Models via Thresholded Score Function Estimation,"Zhuoran Yang, Krishnakumar Balasubramanian, Han Liu",https://icml.cc/Conferences/2017/Schedule?showEvent=737,"We consider estimating the parametric component​ ​of single index models in high dimensions.​ ​Compared with existing work,​ ​we do not require the covariate to be normally​ ​distributed. Utilizing Stein’s Lemma,​ ​we propose estimators based on the score​ ​function of the covariate. Moreover, to handle​ ​​score function and response variables​ ​that are heavy-tailed, our estimators are constructed​ ​via carefully thresholding their empirical​ ​counterparts. Under a bounded fourth​ ​moment condition, we establish optimal statistical​ ​rates of convergence for the proposed​ ​estimators. Extensive numerical experiments​ ​are provided to back up our theory.
","['Princeton University', 'Princeton', 'Princeton University']"
2017,Differentially Private Learning of Graphical Models using CGMs,"Garrett Bernstein, Ryan McKenna, Tao Sun, Daniel Sheldon, Michael Hay, Gerome Miklau",https://icml.cc/Conferences/2017/Schedule?showEvent=612,"We investigate the problem of learning discrete graphical models in a differentially private way. Approaches to this problem range from privileged algorithms that conduct learning completely behind the privacy barrier to schemes that release private summary statistics paired with algorithms to learn parameters from those statistics. We show that the approach of releasing noisy sufficient statistics using the Laplace mechanism achieves a good trade-off between privacy, utility, and practicality. A naive learning algorithm that uses the noisy sufficient statistics ``as is'' outperforms general-purpose differentially private learning algorithms. However, it has three limitations: it ignores knowledge about the data generating process, rests on uncertain theoretical foundations, and exhibits certain pathologies. We develop a more principled approach that applies the formalism of collective graphical models to perform inference over the true sufficient statistics within an expectation-maximization framework. We show that this learns better models than competing approaches on both synthetic data and on real human mobility data used as a case study.
","['University of Massachusetts Amherst', 'UMass Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'Colgate University', 'University of Massachusetts, Amherst']"
2017,"iSurvive: An Interpretable, Event-time Prediction Model for mHealth","Walter Dempsey, Alexander Moreno, James Rehg, Susan Murphy, Chris Scott, Michael Dennis, David Gustafson",https://icml.cc/Conferences/2017/Schedule?showEvent=732,"An important mobile health (mHealth) task is the use of multimodal data, such as sensor streams and self-report, to construct interpretable time-to-event predictions of, for example, lapse to alcohol or illicit drug use.  Interpretability of the prediction model is important for acceptance and adoption by domain scientists, enabling model outputs and parameters to inform theory and guide intervention design.  Temporal latent state models are therefore attractive, and so we adopt the continuous time hidden Markov model (CT-HMM) due to its ability to describe irregular arrival times of event data.  Standard CT-HMMs, however, are not specialized for predicting the time to a future event, the key variable for mHealth interventions.  Also, standard emission models lack a sufficiently rich structure to describe multimodal data and incorporate domain knowledge.  We present iSurvive, an extension of classical survival analysis to a CT-HMM.  We present a parameter learning method for GLM emissions and survival model fitting, and present promising results on both synthetic data and an mHealth drug use dataset.
","['University of Michigan', 'Georgia Institute of Technology', 'Georgia Tech', 'University of Michigan', 'Chestnut Health Systems', 'Lighthouse Institute', 'University of Wisconsin-Madison']"
2017,Efficient softmax approximation for GPUs,"Edouard Grave, Armand Joulin, Moustapha Cisse, David Grangier, Herve Jegou",https://icml.cc/Conferences/2017/Schedule?showEvent=762,"We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further
reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.
","['Facebook AI Research', 'Facebook', '', 'Facebook', 'Facebook AI Research']"
2017,Multichannel End-to-end Speech Recognition,"Tsubasa Ochiai, Shinji Watanabe, Takaaki Hori, John Hershey",https://icml.cc/Conferences/2017/Schedule?showEvent=734,"The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.
","['Doshisha University', 'MITSUBISHI ELECTRIC RESEARCH LABORATORIES', 'MITSUBISHI ELECTRIC RESEARCH LABORATORIES', 'MITSUBISHI ELECTRIC RESEARCH LABORATORIES']"
2017,Local Bayesian Optimization of Motor Skills,"Riadh Akrour, Dmitry Sorokin, Jan Peters, Gerhard Neumann",https://icml.cc/Conferences/2017/Schedule?showEvent=748,"Bayesian optimization is renowned for its sample efficiency but its application to higher dimensional tasks is impeded by its focus on global optimization. To scale to higher dimensional problems, we leverage the sample efficiency of Bayesian optimization in a local context. The optimization of the acquisition function is restricted to the vicinity of a Gaussian search distribution which is moved towards high value areas of the objective. The proposed information-theoretic update of the search distribution results in a Bayesian interpretation of local stochastic search: the search distribution encodes prior
knowledge on the optimum’s location and is weighted at each iteration by the likelihood of this location’s optimality. We demonstrate the
effectiveness of our algorithm on several benchmark objective functions as well as a continuous robotic task in which an informative prior is obtained by imitation learning.
","['TU Darmstadt', '', 'TU Darmstadt', 'University of Lincoln']"
2017,Improving Gibbs Sampler Scan Quality with DoGS,"Ioannis Mitliagkas, Lester Mackey",https://icml.cc/Conferences/2017/Schedule?showEvent=765,"The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling. In this work, we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a Gibbs sampler. Our Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection orders for a given sampling budget and variable subset of interest, explicit bounds on total variation distance to stationarity, and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers. In our experiments with image segmentation, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.
","['Stanford University', 'Microsoft Research']"
2017,Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space,"Jose Miguel Hernandez-Lobato, James Requeima, Edward Pyzer-Knapp, alan Aspuru-Guzik",https://icml.cc/Conferences/2017/Schedule?showEvent=767,"Chemical space is so large that brute force searches for new interesting
molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, epsilon-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.
","['University of Cambridge', 'University of Cambridge', 'IBM', '']"
2017,Robust Structured Estimation with Single-Index Models,"Sheng Chen, Arindam Banerjee",https://icml.cc/Conferences/2017/Schedule?showEvent=828,"In this paper, we investigate general single-index models (SIMs) in high dimensions. Based on U-statistics, we propose two types of robust estimators for the recovery of model parameters, which can be viewed as generalizations of several existing algorithms for one-bit compressed sensing (1-bit CS). With minimal assumption on noise, the statistical guarantees are established for the generalized estimators under suitable conditions, which allow general structures of underlying parameter. Moreover, the proposed estimator is novelly instantiated for SIMs with monotone transfer function, and the obtained estimator can better leverage the monotonicity. Experimental results are provided to support our theoretical analyses.
","['University of Minnesota', 'University of Minnesota']"
2017,Minimizing Trust Leaks for Robust Sybil Detection,"János Höner, Shinichi Nakajima, Alexander Bauer, Klaus-robert Mueller, Nico Görnitz",https://icml.cc/Conferences/2017/Schedule?showEvent=669,"Sybil detection is a crucial task to protect online social networks (OSNs) against 
intruders who try to manipulate automatic services provided by OSNs to their customers.
In this paper, 
we first discuss the robustness of graph-based Sybil detectors SybilRank and Integro and refine theoretically their security guarantees 
towards more realistic assumptions. 
After that, we formally introduce adversarial settings for the graph-based Sybil detection problem 
and derive a corresponding optimal attacking strategy by exploitation of trust leaks.
Based on our analysis, we propose transductive Sybil ranking (TSR), a robust extension to SybilRank and Integro that directly minimizes trust leaks. 
Our empirical evaluation shows significant advantages of TSR 
over state-of-the-art competitors on a variety of attacking scenarios on artificially generated data and real-world datasets.
","['TU Berlin / MathPlan', 'TU Berlin', 'TU Berlin', 'Technische Universität Berlin', 'TU Berlin']"
2017,Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture,"Mingmin Zhao, Shichao Yue, Dina Katabi, Tommi Jaakkola, Matt Bianchi",https://icml.cc/Conferences/2017/Schedule?showEvent=889,"We focus on predicting sleep stages from radio measurements without any attached sensors on subjects. 
We introduce a new predictive model that combines convolutional and recurrent neural networks to extract sleep-specific subject-invariant features from RF signals and capture the temporal progression of sleep.  A key innovation underlying our approach is a modified adversarial training regime that discards extraneous information specific to individuals or measurement conditions, while retaining all information relevant to the predictive task.
We analyze our game theoretic setup and empirically demonstrate that our model achieves significant improvements over state-of-the-art solutions.
","['MIT', 'MIT', 'MIT', 'MIT', 'Massachusetts General Hospital']"
2017,Dropout Inference in Bayesian Neural Networks with Alpha-divergences,"Yingzhen Li, Yarin Gal",https://icml.cc/Conferences/2017/Schedule?showEvent=539,"To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.
","['University of Cambridge', 'University of Cambridge']"
2017,Latent Intention Dialogue Models,"Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, Stephen J Young",https://icml.cc/Conferences/2017/Schedule?showEvent=746,"Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. 
The traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture the conversational stochasticity.
In this paper, however, we propose a Latent Intention Dialogue Model that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference.
Additionally, in a goal-oriented dialogue scenario, the latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning.
The experiments demonstrate the effectiveness of discrete latent variable models on learning goal-oriented dialogues, and the results outperform the published benchmarks on both corpus-based evaluation and human evaluation.
","['University of Cambridge', 'University of Oxford', 'Oxford University and DeepMind', 'University of Cambridge']"
2017,Robust Guarantees of Stochastic Greedy Algorithms,"Yaron Singer, Avinatan Hassidim",https://icml.cc/Conferences/2017/Schedule?showEvent=475,"In this paper we analyze the robustness of stochastic variants of the greedy algorithm for submodular maximization. Our main result shows that for maximizing a monotone submodular
function under a cardinality constraint, iteratively selecting an element whose marginal contribution is approximately maximal in expectation is a sufficient condition to obtain the optimal approximation guarantee with exponentially high probability, assuming the cardinality is sufficiently large. One consequence of our result is that the linear-time STOCHASTIC-GREEDY algorithm recently proposed in (Mirzasoleiman et al.,2015) achieves the optimal running time while maintaining an optimal approximation guarantee. We also show that high probability guarantees cannot be obtained for stochastic greedy algorithms under matroid constraints, and prove an approximation guarantee which holds in expectation.  In contrast to the guarantees of the greedy algorithm, we show that the approximation ratio of stochastic local search is arbitrarily bad, with high probability, as well as in expectation.
","['Harvard', 'Bar Ilan University']"
2017,Count-Based Exploration with Neural Density Models,"Georg Ostrovski, Marc Bellemare, Aäron van den Oord, Remi Munos",https://icml.cc/Conferences/2017/Schedule?showEvent=839,"Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. 
This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge.
We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.
","['Google DeepMind', 'DeepMind', 'Google', 'DeepMind']"
2017,Magnetic Hamiltonian Monte Carlo,"Nilesh Tripuraneni, Mark Rowland, Zoubin Ghahramani, Richard E Turner",https://icml.cc/Conferences/2017/Schedule?showEvent=508,"Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics. 
We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. 
We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. 
Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC.
","['UC Berkeley', 'University of Cambridge', 'University of Cambridge & Uber', 'University of Cambridge']"
2017,Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference,"Aditya Chaudhry, Pan Xu, Quanquan Gu",https://icml.cc/Conferences/2017/Schedule?showEvent=479,"Causal inference among high-dimensional time series data proves an important research problem in many fields. While in the classical regime one often establishes causality among time series via a concept known as “Granger causality,” exist- ing approaches for Granger causal inference in high-dimensional data lack the means to char- acterize the uncertainty associated with Granger causality estimates (e.g., p-values and confidence intervals). We make two contributions in this work. First, we introduce a novel asymptotically unbiased Granger causality estimator with corre- sponding test statistics and confidence intervals to allow, for the first time, uncertainty characteriza- tion in high-dimensional Granger causal inference. Second, we introduce a novel method for false dis- covery rate control that achieves higher power in multiple testing than existing techniques and that can cope with dependent test statistics and depen- dent observations. We corroborate our theoretical results with experiments on both synthetic data and real-world climatological data.
","['University of Virginia', 'University of Virginia', 'University of Virginia']"
2017,Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data,"Xixian Chen, Michael Lyu, Irwin King",https://icml.cc/Conferences/2017/Schedule?showEvent=473,"Estimating covariance matrices is a fundamental technique in various domains, most notably in machine learning and signal processing. To tackle the challenges of extensive communication costs, large storage capacity requirements, and high processing time complexity when handling massive high-dimensional and distributed data, we propose an efficient and accurate covariance matrix estimation method via data compression. In contrast to previous data-oblivious compression schemes, we leverage a data-aware weighted sampling method to construct low-dimensional data for such estimation. We rigorously prove that our proposed estimator is unbiased and requires smaller data to achieve the same accuracy with specially designed sampling distributions. Besides, we depict that the computational procedures in our algorithm are efficient. All achievements imply an improved tradeoff between the estimation accuracy and computational costs. Finally, the extensive experiments on synthetic and real-world datasets validate the superior property of our method and illustrate that it significantly outperforms the state-of-the-art algorithms.
","['The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'CUHK']"
2017,The Price of Differential Privacy For Online Learning,"Naman Agarwal, Karan Singh",https://icml.cc/Conferences/2017/Schedule?showEvent=582,"We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal O(T^0.5) regret bounds. In the full-information setting, our results demonstrate that ε-differential privacy may be ensured for free -- in particular, the regret bounds scale as O(T^0.5+1/ε). For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of O(T^0.5/ε), while the previously best known regret bound was O(T^{2/3}/ε).
","['Princeton University', 'Princeton University']"
2017,Bidirectional learning for time-series models with hidden units,"Takayuki Osogami, Hiroshi Kajino, Taro Sekiyama",https://icml.cc/Conferences/2017/Schedule?showEvent=528,"Hidden units can play essential roles in modeling time-series having long-term dependency or on-linearity but make it difficult to learn associated parameters.  Here we propose a way to learn such a time-series model by training a backward model for the time-reversed time-series, where the backward model has a common set of parameters as the original (forward) model.  Our key observation is that only a subset of the parameters is hard to learn, and that subset is complementary between the forward model and the backward model.  By training both of the two models, we can effectively learn the values of the parameters that are hard to learn if only either of the two models is trained.  We apply bidirectional learning to a dynamic Boltzmann machine extended with hidden units.  Numerical experiments with synthetic and real datasets clearly demonstrate advantages of bidirectional learning.
","['IBM Research - Tokyo', 'IBM Research - Tokyo', 'IBM Research - Tokyo']"
2017,Multiplicative Normalizing Flows for Variational Bayesian Neural Networks,"Christos Louizos, Max Welling",https://icml.cc/Conferences/2017/Schedule?showEvent=575,"We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.
","['University of Amsterdam', 'University of Amsterdam']"
2017,Discovering Discrete Latent Topics with Neural Variational Inference,"Yishu Miao, Edward Grefenstette, Phil Blunsom",https://icml.cc/Conferences/2017/Schedule?showEvent=776,"Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.
","['University of Oxford', 'Deepmind', 'Oxford University and DeepMind']"
2017,Guarantees for Greedy Maximization of Non-submodular Functions with Applications,"Yatao Bian, Joachim Buhmann, Andreas Krause, Sebastian Tschiatschek",https://icml.cc/Conferences/2017/Schedule?showEvent=521,"We investigate the performance of the standard Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by a combination of the (generalized) curvature $\alpha$ and the submodularity ratio $\gamma$. In particular, we prove that Greedy enjoys a tight approximation guarantee of $\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and  real-world applications.","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH']"
2017,Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning,"Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli",https://icml.cc/Conferences/2017/Schedule?showEvent=822,"As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.
","['University of Michigan', 'University of Michigan', 'Google / U. Michigan', 'Microsoft Research']"
2017,Probabilistic Path Hamiltonian Monte Carlo,"Vu Dinh, Arman Bilge, Cheng Zhang, Frederick Matsen",https://icml.cc/Conferences/2017/Schedule?showEvent=615,"Hamiltonian Monte Carlo (HMC) is an efficient and effective means of sampling posterior distributions on Euclidean space, which has been extended to manifolds with boundary. However, some applications require an extension to more general spaces. For example, phylogenetic (evolutionary) trees are defined in terms of both a discrete graph and associated continuous parameters; although one can represent these aspects using a single connected space, this rather complex space is not suitable for existing HMC algorithms. In this paper, we develop Probabilistic Path HMC (PPHMC) as a first step to sampling distributions on spaces with intricate combinatorial structure. We define PPHMC on orthant complexes, show that the resulting Markov chain is ergodic, and provide a promising implementation for the case of phylogenetic trees in open-source software. We also show that a surrogate function to ease the transition across a boundary on which the log-posterior has discontinuous derivatives can greatly improve efficiency.
","['Fred Hutchinson Cancer Center', 'University of Washington', 'Fred Hutchinson Cancer Center', 'Fred Hutchinson Cancer Center']"
2017,Uncovering Causality from Multivariate Hawkes Integrated Cumulants,"Massil Achab, Emmanuel Bacry, Stéphane Gaïffas, Iacopo Mastromatteo, Jean-François Muzy",https://icml.cc/Conferences/2017/Schedule?showEvent=673,"We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process.
This matrix not only encodes the mutual influences of each
node of the process, but also disentangles the causality relationships between them.
Our approach is the first that leads to an estimation of this matrix \emph{without any parametric modeling and estimation of the kernels themselves}.
A consequence is that it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime.
For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process.
A theoretical analysis allows to prove that this new estimation technique is consistent.
Moreover, we show on numerical experiments that our approach is indeed very robust to the shape of the kernels, and
gives appealing results on the MemeTracker database and on financial order book data.
","['Ecole Polytechnique', 'Ecole Polytechnique', 'CMAP CNRS UMR 7641', 'Capital Fund Management', 'Université de Corse']"
2017,Robust Gaussian Graphical Model Estimation with Arbitrary Corruption,"Lingxiao Wang, Quanquan Gu",https://icml.cc/Conferences/2017/Schedule?showEvent=718,"We study the problem of estimating the high-dimensional Gaussian graphical model where the data are arbitrarily corrupted. We propose a robust estimator for the sparse precision matrix in the high-dimensional regime.  At the core of our method is a robust covariance matrix estimator, which is based on truncated inner product. We establish the statistical guarantee of our estimator on both estimation error and model selection consistency. In particular, we show that provided that the number of corrupted samples $n_2$ for each variable satisfies $n_2 \lesssim \sqrt{n}/\sqrt{\log d}$, where $n$ is the sample size and $d$ is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models. In addition, we propose a hypothesis testing procedure to assess the uncertainty of our robust estimator. We demonstrate the effectiveness of our method through extensive experiments on both synthetic data and real-world genomic data.","['University of Virginia', 'University of Virginia']"
2017,Pain-Free Random Differential Privacy with Sensitivity Sampling,"Benjamin Rubinstein, Francesco Aldà",https://icml.cc/Conferences/2017/Schedule?showEvent=727,"Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-private function. Bounding such sensitivity is often a prohibitively complex analytic calculation. As an alternative, we propose a straightforward sampler for estimating sensitivity of non-private mechanisms. Since our sensitivity estimates hold with high probability, any mechanism that would be (epsilon,delta)-differentially private under bounded global sensitivity automatically achieves (epsilon,delta,gamma)-random differential privacy (Hall et al. 2012), without any target-specific calculations required. We demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee, while achieving more accurate releases even for non-private functions that are black-box computer programs.
","['University\u200b of Melbourne', 'Ruhr-Universität Bochum']"
2017,Learning Hawkes Processes from Short Doubly-Censored Event Sequences,"Hongteng Xu, Dixin Luo, Hongyuan Zha",https://icml.cc/Conferences/2017/Schedule?showEvent=543,"Many real-world applications require robust algorithms to learn point process models based on a type of incomplete data --- the so-called short doubly-censored (SDC) event sequences. 
In this paper, we study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the general idea of data synthesis. 
In particular, given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method --- sampling predecessor and successor for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences. 
The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood. 
Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both time-invariant and time-varying Hawkes processes.
","['Georgia Institute of Technology', 'University of Toronto', 'Georgia Institute of Technology']"
2017,Variational Dropout Sparsifies Deep Neural Networks,"Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov",https://icml.cc/Conferences/2017/Schedule?showEvent=793,"We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.
","['Skoltech', 'HSE, MIPT', 'HSE']"
2017,Toward Controlled Generation of Text,"Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric Xing",https://icml.cc/Conferences/2017/Schedule?showEvent=816,"Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible text sentences, whose attributes are controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders (VAEs) and holistic attribute discriminators for effective imposition of semantic structures. The model can alternatively be seen as enhancing VAEs with the wake-sleep algorithm for leveraging fake samples as extra training data. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns interpretable representations from even only word annotations, and produces short sentences with desired attributes of sentiment and tenses. Quantitative experiments using trained classifiers as evaluators validate the accuracy of sentence and attribute generation.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellen University', 'Carnegie Mellon University']"
2017,Robust Submodular Maximization: A Non-Uniform Partitioning Approach,"Ilija Bogunovic, Slobodan Mitrovic, Jonathan Scarlett, Volkan Cevher",https://icml.cc/Conferences/2017/Schedule?showEvent=674,"We study the problem of maximizing a monotone submodular function subject to a cardinality constraint $k$, with the added twist that a number of items $\tau$ from the returned set may be removed. We focus on the worst-case setting considered in \cite{orlin2016robust}, in which a constant-factor approximation guarantee was given for $\tau = o(\sqrt{k})$.  In this paper, we solve a key open problem raised therein, presenting a new Partitioned Robust (PRo) submodular maximization algorithm  that achieves the same guarantee for more general $\tau = o(k)$.  Our algorithm constructs partitions consisting of buckets with exponentially increasing sizes, and applies standard submodular optimization subroutines on the buckets in order to construct the robust solution. We numerically demonstrate the performance of PRo in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of \cite{orlin2016robust}.","['EPFL', 'EPFL', 'EPFL', 'EPFL']"
2017,Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning,"Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Phil Torr, Pushmeet Kohli, Shimon Whiteson",https://icml.cc/Conferences/2017/Schedule?showEvent=778,"Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent's value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.
","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'Oxford', 'Microsoft Research', 'University of Oxford']"
2017,Stochastic Gradient Monomial Gamma Sampler,"Yizhe Zhang, Changyou Chen, Zhe Gan, Ricardo Henao, Lawrence Carin",https://icml.cc/Conferences/2017/Schedule?showEvent=768,"Scaling Markov Chain Monte Carlo (MCMC) to estimate posterior distributions from large datasets has been made possible as a result of advances in stochastic gradient techniques.
Despite their success, mixing performance of existing methods when sampling from multimodal distributions can be less efficient with insufficient Monte Carlo samples; this is evidenced by slow convergence and insufficient exploration of posterior distributions.
We propose a generalized framework to improve the sampling efficiency of stochastic gradient MCMC, by leveraging a generalized kinetics that delivers superior stationary mixing, especially in multimodal distributions, and propose several techniques to overcome the practical issues.
We show that the proposed approach is better at exploring a complicated multimodal posterior distribution, and demonstrate improvements over other stochastic gradient MCMC methods on various applications.
","['Duke university', 'Duke', 'Duke University', 'Duke University', 'Duke']"
2017,Cost-Optimal Learning of Causal Graphs,"Murat Kocaoglu, Alexandros Dimakis, Sriram Vishwanath",https://icml.cc/Conferences/2017/Schedule?showEvent=747,"We consider the problem of learning a causal graph over a set of variables with interventions. We study the cost-optimal causal graph learning problem: For a given skeleton (undirected version of the causal graph), design the set of interventions with minimum total cost, that can uniquely identify any causal graph with the given skeleton. We show that this problem is solvable in polynomial time. Later, we consider the case when the number of interventions is limited. For this case, we provide polynomial time algorithms when the skeleton is a tree or a clique tree. For a general chordal skeleton, we develop an efficient greedy algorithm, which can be improved when the causal graph skeleton is an interval graph.
","['University of Texas at Austin', 'UT Austin', '']"
2017,Algebraic Variety Models for High-Rank Matrix Completion,"Greg Ongie, Laura Balzano, Rebecca Willett, Robert Nowak",https://icml.cc/Conferences/2017/Schedule?showEvent=879,"We consider a non-linear generalization of low-rank matrix completion to the case where the data belongs to an algebraic variety, i.e., each data point is a solution to a system of polynomial equations. In this case the original matrix is possibly high-rank, but it becomes low-rank after mapping each column to a higher dimensional space of monomial features. Algebraic varieties capture a range of well-studied linear models, including affine subspaces and their union, but also quadratic and higher degree curves and surfaces. We study the sampling requirements for a general variety model with a focus on the union of affine subspaces. We propose an efficient matrix completion algorithm that minimizes a convex or non-convex surrogate of the rank of the lifted matrix. Our algorithm uses the well-known ""kernel trick'' to avoid working directly with the high-dimensional lifted data matrix and scales efficiently with data size. We show the proposed algorithm is able to recover synthetically generated data up to the predicted sampling complexity bounds. The algorithm also outperforms standard techniques in experiments with real data.
","['University of Michigan', 'University of Michigan', 'UW Madison', 'University of Wisconsion-Madison']"
2017,Differentially Private Clustering in High-Dimensional Euclidean Spaces,"Nina Balcan, Travis Dick, Yingyu Liang, Wenlong Mou, Hongyang Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=756,"We study the problem of clustering sensitive data while preserving the privacy of individuals represented in the dataset, which has broad applications in practical machine learning and data analysis tasks. Although the problem has been widely studied in the context of low-dimensional, discrete spaces, much remains unknown concerning private clustering in high-dimensional Euclidean spaces $\R^d$. In this work, we give differentially private and efficient algorithms achieving strong guarantees for $k$-means and $k$-median clustering when $d=\Omega(\polylog(n))$. Our algorithm achieves clustering loss at most $\log^3(n)\OPT+\poly(\log n,d,k)$, advancing the state-of-the-art result of $\sqrt{d}\OPT+\poly(\log n,d^d,k^d)$. We also study the case where the data points are $s$-sparse and show that the clustering loss can scale logarithmically with $d$, i.e., $\log^3(n)\OPT+\poly(\log n,\log d,k,s)$. Experiments on both synthetic and real datasets verify the effectiveness of the proposed method.","['Carnegie Mellon University', 'CMU', 'Princeton University', 'Peking University', 'Carnegie Mellon University']"
2017,Coherent probabilistic forecasts for hierarchical time series,"Souhaib Ben Taieb, James Taylor, Rob Hyndman",https://icml.cc/Conferences/2017/Schedule?showEvent=633,"Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good prediction accuracy at each level of the hierarchy, but also the coherency between different levels --- the property that forecasts add up appropriately across the hierarchy. A fundamental limitation of prior research is the focus on forecasting the mean of each time series. We consider the situation where probabilistic forecasts are needed for each series in the hierarchy, and propose an algorithm to compute predictive distributions rather than mean forecasts only. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through a sparse forecast combination and a probabilistic hierarchical aggregation. We evaluate the accuracy of our forecasting algorithm on both simulated data and large-scale electricity smart meter data. The results show consistent performance gains compared to state-of-the art methods.
","['Monash University', 'University of Oxford', 'Monash University']"
2017,Unimodal Probability Distributions for Deep Ordinal Classification,"Christopher Beckham, Christopher Pal",https://icml.cc/Conferences/2017/Schedule?showEvent=672,"Probability distributions produced by the cross- entropy loss for ordinal classification problems can possess undesired properties. We propose a straightforward technique to constrain discrete ordinal probability distributions to be unimodal via the use of the Poisson and binomial probability distributions. We evaluate this approach in the context of deep learning on two large ordinal image datasets, obtaining promising results.
","['MILA', 'MILA']"
2017,Adversarial Feature Matching for Text Generation,"Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, Lawrence Carin",https://icml.cc/Conferences/2017/Schedule?showEvent=773,"The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.
","['Duke university', 'Duke University', '', 'Nanjing University', 'Duke University', 'Duke University', 'Duke']"
2017,Probabilistic Submodular Maximization in Sub-Linear Time,"Serban A Stan, Morteza Zadimoghaddam, Andreas Krause, Amin Karbasi",https://icml.cc/Conferences/2017/Schedule?showEvent=840,"In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution.  This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function.  In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough.  As a remedy, we introduce the problem of {\em sublinear time probabilistic submodular maximization}: Given training examples of functions (e.g., via user feature vectors), we seek to reduce the ground set so that optimizing new functions drawn from the same distribution will provide almost as much value when restricted to the reduced ground set as when using the full set.  
We cast this problem as a two-stage submodular maximization and develop  a novel efficient algorithm for this problem which offers 1/2(1 - 1/e^2) approximation ratio 
for general monotone submodular functions and general matroid constraints. We demonstrate the effectiveness of our approach on several real-world problem instances where running the maximization problem on the reduced ground set leads to two folds speed-up while incurring almost no loss. 
","['Yale', 'Google', 'ETH Zurich', 'Yale']"
2017,The Predictron:  End-To-End Learning and Planning,"David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris",https://icml.cc/Conferences/2017/Schedule?showEvent=707,"One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple ""imagined"" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.
","['Google DeepMind', 'DeepMind', 'Deep Mind', 'DeepMind', 'Google DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind']"
2017,Stochastic Gradient MCMC Methods for Hidden Markov Models,"Yi-An Ma, Nicholas J Foti, Emily Fox",https://icml.cc/Conferences/2017/Schedule?showEvent=829,"Stochastic gradient MCMC (SG-MCMC) algorithms have proven useful in scaling Bayesian inference to large datasets under an assumption of i.i.d data. We instead develop an SG-MCMC algorithm to learn the parameters of hidden Markov models (HMMs) for time-dependent data. There are two challenges to applying SG-MCMC in this setting: The latent discrete states, and needing to break dependencies when considering minibatches. We consider a marginal likelihood representation of the HMM and propose an algorithm that harnesses the inherent memory decay of the process. We demonstrate the effectiveness of our algorithm on synthetic experiments and an ion channel recording data, with runtimes significantly outperforming batch MCMC. 
","['University of Washington', 'University of Washington', 'University of Washington']"
2017,Identification and Model Testing in Linear Structural Equation Models using Auxiliary Variables,"Bryant Chen, Daniel Kumor, Elias Bareinboim",https://icml.cc/Conferences/2017/Schedule?showEvent=783,"We developed a novel approach to identification and model testing in linear structural equation models (SEMs) based on auxiliary variables (AVs), which generalizes a widely-used family of methods known as instrumental variables. The identification problem is concerned with the conditions under which causal parameters can be uniquely estimated from an observational, non-causal covariance matrix. In this paper, we provide an algorithm for the identification of causal parameters in linear structural models that subsumes previous state-of-the-art methods. In other words, our algorithm identifies strictly more coefficients and models than methods previously known in the literature. Our algorithm builds on a graph-theoretic characterization of conditional independence relations between auxiliary and model variables, which is developed in this paper. Further, we leverage this new characterization for allowing identification when limited experimental data or new substantive knowledge about the domain is available. Lastly, we develop a new procedure for model testing using AVs.
","['IBM Research', 'Purdue University', 'Purdue']"
2017,High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm,"Rongda Zhu, Lingxiao Wang, Chengxiang Zhai, Quanquan Gu",https://icml.cc/Conferences/2017/Schedule?showEvent=547,"We propose a generic
stochastic expectation-maximization (EM) algorithm for the estimation of high-dimensional latent variable models. At the core of our algorithm is a novel semi-stochastic variance-reduced gradient designed for the $Q$-function in the EM algorithm. Under a mild condition on the initialization, our algorithm is guaranteed to attain a linear convergence rate to the unknown parameter of the latent variable model, and achieve an optimal statistical rate up to a logarithmic factor for parameter estimation. Compared with existing high-dimensional EM algorithms, our algorithm enjoys a better computational complexity and is therefore more efficient. We apply our generic algorithm to two illustrative latent variable models: Gaussian mixture model and mixture of linear regression, and demonstrate the advantages of our algorithm by both theoretical analysis and numerical experiments. 
We believe that the proposed semi-stochastic gradient is of independent interest for general nonconvex optimization problems with bivariate structures.","['Facebook', 'University of Virginia', 'University of Illinois at Urbana-Champaign', 'University of Virginia']"
2017,Differentially Private Chi-squared Test by Unit Circle Mechanism,"Kazuya Kakizaki, Kazuto Fukuchi, Jun Sakuma",https://icml.cc/Conferences/2017/Schedule?showEvent=847,"This paper develops differentially private mechanisms for $\chi^2$ test of independence. While existing works put their effort into properly controlling the type-I error, in addition to that, we investigate the type-II error of differentially private mechanisms. Based on the analysis, we present unit circle mechanism: a novel differentially private mechanism based on the geometrical property of the test statistics. Compared to existing output perturbation mechanisms, our mechanism improves the dominated term of the type-II error from $O(1)$ to $O(\exp(-\sqrt{\numSample}))$ where $\numSample$ is the sample size. Furthermore, we introduce novel procedures for multiple $\chi^2$ tests by incorporating the unit circle mechanism into the sparse vector technique and the exponential mechanism. These procedures can control the family-wise error rate (FWER) properly, which has never been attained by existing mechanisms.

","['University of Tsukuba / NEC', 'University of Tsukuba', 'University of Tsukuba / RIKEN AIP']"
2017,Soft-DTW: a Differentiable Loss Function for Time-Series,"Marco Cuturi, Mathieu Blondel",https://icml.cc/Conferences/2017/Schedule?showEvent=769,"We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation
of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a \emph{differentiable} loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal
significantly outperforms existing baselines~\citep{petitjean2011global}. Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. Source code is available at \url{https://github.com/mblondel/soft-dtw}.
","['ENSAE / CREST', 'NTT']"
2017,Learning Continuous Semantic Representations of Symbolic Expressions,"Miltiadis Allamanis, pankajan Chanthirasegaran, Pushmeet Kohli, Charles Sutton",https://icml.cc/Conferences/2017/Schedule?showEvent=658,"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence network, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that  semantic representations must be computed in a syntax-directed manner,  because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.
","['Microsoft Research', '', 'Microsoft Research', 'University of Edinburgh']"
2017,On Approximation Guarantees for Greedy Low Rank Optimization,"RAJIV KHANNA, Ethan R. Elenberg, Alexandros Dimakis, Joydeep Ghosh, Sahand Negahban",https://icml.cc/Conferences/2017/Schedule?showEvent=877,"We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions of restricted strong convexity and smoothness. Our novel analysis  also uncovers previously unknown connections between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of corresponding approximation bounds in submodular maximization. Additionally, we provide also provide statistical recovery guarantees. Finally, we present empirical comparison of greedy estimation with established baselines on two important real-world problems. 
","['UT Austin', 'The University of Texas at Austin', 'UT Austin', 'The University of Texas at Austin', 'YALE']"
2017,Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning,"Oron Anschel, Nir Baram, Nahum Shimkin",https://icml.cc/Conferences/2017/Schedule?showEvent=566,"Instability and variability of Deep Reinforcement
Learning (DRL) algorithms tend to adversely affect
their performance. Averaged-DQN is a simple
extension to the DQN algorithm, based on
averaging previously learned Q-values estimates,
which leads to a more stable training procedure
and improved performance by reducing approximation
error variance in the target values. To understand
the effect of the algorithm, we examine
the source of value function estimation errors and
provide an analytical comparison within a simplified
model. We further present experiments
on the Arcade Learning Environment benchmark
that demonstrate significantly improved stability
and performance due to the proposed extension.
","['Technion', 'Technion - Israel Institute of Technology', 'Technion']"
2017,Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC,"Yulai Cong, Bo Chen, Hongwei Liu, Mingyuan Zhou",https://icml.cc/Conferences/2017/Schedule?showEvent=703,"It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.
","['Xidian University', 'National Lab of Radar Signal Processing, School of Electronic Engineering, Xidian University', 'Xidian University', 'University of Texas at Austin']"
2017,Estimating individual treatment effect: generalization bounds and algorithms,"Uri Shalit, Fredrik D Johansson, David Sontag",https://icml.cc/Conferences/2017/Schedule?showEvent=790,"There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a ``balanced'' representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.
","['NYU', 'MIT', 'Massachusetts Institute of Technology']"
2017,"Collect at Once, Use Effectively: Making Non-interactive Locally Private Learning Possible","Kai Zheng, Wenlong Mou, Liwei Wang",https://icml.cc/Conferences/2017/Schedule?showEvent=860,"Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated from non-interactive LDP channel using Chebyshev expansion, which is combined with inexact gradient methods to obtain an efficient algorithm with quasi-polynomial sample complexity bound. For the high-dimensional world, we discover that under $\ell_2$-norm assumption on data points, high-dimensional sparse linear regression and mean estimation can be achieved with logarithmic dependence on dimension, using random projection and approximate recovery.  We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation possible for a broad range of learning tasks under non-interactive LDP model.","['Peking University', 'Peking University', 'Peking University']"
2017,Variational Policy for Guiding Point Processes,"Yichen Wang, Grady Williams, Evangelos Theodorou, Le Song",https://icml.cc/Conferences/2017/Schedule?showEvent=798,"Temporal point processes have been widely applied to model event sequence data generated by online users. In this paper, we consider the problem of how to design the optimal control policy for point processes, such that the stochastic system driven by the point process is steered to a target state. In particular, we exploit the key insight to view the stochastic optimal control problem from the perspective of optimal measure and variational inference. We further propose a convex optimization framework and an efficient algorithm to update the policy adaptively to the current system state. Experiments on synthetic and real-world data show that our algorithm can steer the user activities much more accurately and efficiently than other stochastic control methods. 
","['Gatech', 'Georgia Tech', 'Georgia Tech', 'Georgia Institute of Technology']"
2017,Dance Dance Convolution,"Christopher Donahue, Zachary Lipton, Julian McAuley",https://icml.cc/Conferences/2017/Schedule?showEvent=653,"Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, players may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.
","['University of California, San Diego', 'UCSD', 'UCSD']"
2017,Language Modeling with Gated Convolutional Networks,"Yann Dauphin, Angela Fan, Michael Auli, David Grangier",https://icml.cc/Conferences/2017/Schedule?showEvent=887,"The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al. (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.
","['Facebook AI Research', 'Facebook AI Research', 'Facebook', 'Facebook']"
2017,"Deletion-Robust Submodular Maximization: Data Summarization with ""the Right to be Forgotten""","Baharan Mirzasoleiman, Amin Karbasi, Andreas Krause",https://icml.cc/Conferences/2017/Schedule?showEvent=698,"How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an important challenge in online services, where the users generating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization problem. We develop the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor approximation guarantee to the optimum solution. We evaluate the effectiveness of our approach on several real-world applica tions, including summarizing (1) streams of geo-coordinates (2); streams of images; and (3) click-stream log data, consisting of 45 million feature vectors from a news recommendation task.
","['ETH Zurich', 'Yale', 'ETH Zurich']"
2017,FeUdal Networks for Hierarchical Reinforcement Learning,"Alexander Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, koray kavukcuoglu",https://icml.cc/Conferences/2017/Schedule?showEvent=542,"We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a slower time scale and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.
","['DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind']"
2017,Distributed Batch Gaussian Process Optimization,"Erik Daxberger, Bryan Kian Hsiang Low",https://icml.cc/Conferences/2017/Schedule?showEvent=689,"This paper presents a novel distributed batch Gaussian process upper confidence bound (DB-GP-UCB) algorithm for performing batch Bayesian optimization (BO) of highly complex, costly-to-evaluate black-box objective functions. In contrast to existing batch BO algorithms, DB-GP-UCB can jointly optimize a batch of inputs (as opposed to selecting the inputs of a batch one at a time) while still preserving scalability in the batch size. To realize this, we generalize GP-UCB to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size. Our DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order. We provide a theoretical guarantee for the convergence rate of DB-GP-UCB via bounds on its cumulative regret. Empirical evaluation on synthetic benchmark objective functions and a real-world optimization problem shows that DB-GP-UCB outperforms the state-of-the-art batch BO algorithms.
","['Ludwig-Maximilians-Universität München', 'National University of Singapore']"
2017,Recursive Partitioning for Personalization using Observational Data,Nathan Kallus,https://icml.cc/Conferences/2017/Schedule?showEvent=818,"We study the problem of learning to choose from $m$ discrete treatment options (e.g., news item or medical drug) the one with best causal effect for a particular instance (e.g., user or patient) where the training data consists of passive observations of covariates, treatment, and the outcome of the treatment. The standard approach to this problem is regress and compare: split the training data by treatment, fit a regression model in each split, and, for a new instance, predict all $m$ outcomes and pick the best. By reformulating the problem as a single learning task rather than $m$ separate ones, we propose a new approach based on recursively partitioning the data into regimes where different treatments are optimal. We extend this approach to an optimal partitioning approach that finds a globally optimal partition, achieving a compact, interpretable, and impactful personalization model. We develop new tools for validating and evaluating personalization models on observational data and use these to demonstrate the power of our novel approaches in a personalized medicine and a job training application.",['Cornell University']
2017,Optimal Densification for Fast and Accurate Minwise Hashing,Anshumali Shrivastava,https://icml.cc/Conferences/2017/Schedule?showEvent=627,"Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing.  Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques.  As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.",['Rice University']
2017,An Adaptive Test of Independence with Analytic Kernel Embeddings,"Wittawat Jitkrittum, Zoltan Szabo, Arthur Gretton",https://icml.cc/Conferences/2017/Schedule?showEvent=469,"A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.
","['UCL', 'École Polytechnique', 'Gatsby Computational Neuroscience Unit']"
2017,Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs,"Michael Gygli, Mohammad Norouzi, Anelia Angelova",https://icml.cc/Conferences/2017/Schedule?showEvent=534,"We approach structured output prediction by optimizing a deep value network (DVN) to precisely estimate the task loss on different output configurations for a given input. Once the model is trained, we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network. When applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks. For multi-label classification, the DVN's objective is to correctly predict the F1 score for any potential label configuration. The
DVN framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks.
","['Gifs.com', 'Google', 'Google Brain']"
2017,World of Bits: An Open-Domain Platform for Web-Based Agents,"Tim Shi, Andrej Karpathy, Jim Fan, Jonathan Hernandez, Percy Liang",https://icml.cc/Conferences/2017/Schedule?showEvent=843,"While simulated game environments have greatly accelerated research in reinforcement learning, existing environments lack the open-domain realism of tasks in computer vision or natural language processing, which operate on artifacts created by humans in natural, organic settings. To foster reinforcement learning research in such settings, we introduce the World of Bits (WoB), a platform in which agents complete tasks on the Internet by performing low-level keyboard and mouse actions. The two main challenges are: (i) to curate a large, diverse set of interesting web-based tasks, and (ii) to ensure that these tasks have a well-defined reward structure and are reproducible despite the transience of the web. To do this, we develop a methodology in which crowdworkers create tasks defined by natural language questions and provide demonstrations of how to answer the question on real websites using keyboard and mouse; HTTP traffic is cached to create a reproducible offline approximation of the web site. Finally, we show that agents trained via behavioral cloning and reinforcement learning can successfully complete a range of our web-based tasks.
","['Stanford University', 'OpenAI', 'Stanford University', '', 'Stanford University']"
2017,Convolutional Sequence to Sequence Learning,"Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann Dauphin",https://icml.cc/Conferences/2017/Schedule?showEvent=806,"The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.
","['Facebook AI Research', 'Facebook', 'Facebook', 'Facebook AI Research', 'Facebook AI Research']"
2017,Analysis and Optimization of Graph Decompositions by Lifted Multicuts,"Andrea Hornakova, Jan-Hendrik Lange, Bjoern Andres",https://icml.cc/Conferences/2017/Schedule?showEvent=476,"We study the set of all decompositions (clusterings) of a graph through its characterization as a set of lifted multicuts. This leads us to practically relevant insights related to the definition of classes of decompositions by must-join and must-cut constraints and related to the comparison of clusterings by metrics. To find optimal decompositions defined by minimum cost lifted multicuts, we establish some properties of some facets of lifted multicut polytopes, define efficient separation procedures and apply these in a branch-and-cut algorithm.
","['Max Planck Institute for Informatics', 'MPI for Informatics', 'MPI for Informatics']"
2017,Deciding How to Decide: Dynamic Routing in Artificial Neural Networks,"Mason McGill, Pietro Perona",https://icml.cc/Conferences/2017/Schedule?showEvent=642,"We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.
","['California Institute of Technology', 'caltech.edu']"
2017,Scalable Multi-Class Gaussian Process Classification using Expectation Propagation,"Carlos Villacampa-Calvo, Daniel Hernandez-Lobato",https://icml.cc/Conferences/2017/Schedule?showEvent=506,"This paper describes an expectation propagation (EP) method for multi-class classification with Gaussian processes that scales well to very large datasets. In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances. This enables efficient training using stochastic gradients and mini-batches. When this type of training is used, the computational cost does not depend on the number of data instances N. Furthermore, extra assumptions in the approximate inference process make the memory cost independent of N. The  consequence is that the proposed EP method can be used on datasets with millions of instances. We compare empirically this method with alternative approaches that approximate the required computations using variational inference. The results show that it performs similar or even better than these techniques, which sometimes give significantly worse predictive distributions in terms of the test log-likelihood. Besides this, the training process of the proposed approach also seems to converge in a smaller number of iterations.
","['Universidad Autónoma de Madrid', 'Universidad Autonoma de Madrid']"
2017,Identifying Best Interventions through Online Importance Sampling,"Rajat Sen, Karthikeyan Shanmugam, Alexandros Dimakis, Sanjay Shakkottai",https://icml.cc/Conferences/2017/Schedule?showEvent=619,"Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node $V$ in an acyclic causal directed graph, to maximize the expected value of a target node $Y$ (located downstream of $V$). Our setting imposes a fixed total budget for sampling under various interventions, along with cost constraints on different types of interventions. We pose this as a best arm identification bandit problem with $K$ arms, where each arm is a soft intervention at $V$ and leverage the information leakage among the arms to provide the first gap dependent error and simple regret bounds for this problem. Our results are a significant improvement over the traditional best arm identification results. We empirically show that our algorithms outperform the state of the art in the Flow Cytometry data-set, and also apply our algorithm for model interpretation of the Inception-v3 deep net that classifies images.","['University of Texas at Austin', 'IBM Research, T. J. Watson Research Center', 'UT Austin', 'University of Texas at Austin']"
2017,Stochastic Generative Hashing,"Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, Le Song",https://icml.cc/Conferences/2017/Schedule?showEvent=766,"Learning-based binary hashing has become a powerful paradigm for fast search and retrieval in massive databases. However, due to the requirement of discrete outputs for the hash functions, learning such functions is known to be very challenging. In addition, the objective functions adopted by existing hashing techniques are mostly chosen heuristically. In this paper, we propose a novel generative approach to learn hash functions through Minimum Description Length principle such that the learned hash codes maximally compress the dataset and can also be used to regenerate the inputs. We also develop an efficient learning algorithm based on the stochastic distributional gradient, which avoids the notorious difficulty caused by binary output constraints, to jointly optimize the parameters of the hash function and the associated generative model. Extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval results than the existing state-of-the-art methods. 
","['Georgia Tech', 'Google Research', 'Google Research, NY', 'UIUC', 'Georgia Institute of Technology']"
2017,Sliced Wasserstein Kernel for Persistence Diagrams,"Mathieu Carrière, Marco Cuturi, Steve Oudot",https://icml.cc/Conferences/2017/Schedule?showEvent=524,"Persistence diagrams (PDs) play a key role in topological data analysis (TDA), in which they are routinely used to describe succinctly complex topological properties of complicated shapes. PDs enjoy strong stability properties and have proven their utility in various learning contexts.  They do not, however, live in a space naturally endowed with a Hilbert structure and are usually compared with specific distances, such as the bottleneck distance. To incorporate PDs in a learning pipeline, several kernels have been proposed for PDs with a strong emphasis on the stability of the RKHS distance w.r.t. perturbations of the PDs.  In this article, we use the Sliced Wasserstein approximation $\SW$ of the Wasserstein distance to define a new kernel for PDs, which is not only provably stable but also provably discriminative w.r.t. the Wasserstein distance $W^1_\infty$ between PDs. We also demonstrate its practicality, by developing an approximation technique to reduce kernel computation time, and show that our proposal compares favorably to existing kernels for PDs on several benchmarks.
","['Inria Saclay', 'ENSAE / CREST', '']"
2017,Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction,"Wen Sun, Arun Venkatraman, Geoff Gordon, Byron Boots, Drew Bagnell",https://icml.cc/Conferences/2017/Schedule?showEvent=709,"Recently, researchers have demonstrated state-of-the-art performance on sequential prediction problems using deep neural networks and Reinforcement Learning (RL). For some of these problems, oracles that can demonstrate good performance may be available during training, but are not used by plain RL methods. To take advantage of this extra information, we propose AggreVaTeD, an extension of the Imitation Learning (IL) approach of Ross & Bagnell (2014). AggreVaTeD allows us to use expressive differentiable policy representations such as deep networks, while leveraging training-time oracles to achieve faster and more accurate solutions with less training data. Specifically, we present two gradient procedures that can learn neural network policies for several problems, including a sequential prediction task and several high-dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates that we can expect up to exponentially-lower sample complexity for learning with AggreVaTeD than with plain RL algorithms. Our results and theory indicate that IL (and AggreVaTeD in particular) can be a more effective strategy for sequential prediction than plain RL. 
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Georgia Tech', 'Carnegie Mellon University']"
2017,Real-Time Adaptive Image Compression,"Oren Rippel, Lubomir Bourdev",https://icml.cc/Conferences/2017/Schedule?showEvent=544,"We present a machine learning-based approach to lossy image compression which outperforms all existing codecs, while running in real-time.
Our algorithm typically produces file sizes 3 times smaller than JPEG, 2.5 times smaller than JPEG 2000, and 2.3 times smaller than WebP on datasets of generic images across a spectrum of quality levels. At the same time, our codec is designed to be lightweight and deployable: for example, it can encode or decode the Kodak dataset in less than 10ms per image on GPU. 
Our architecture is an autoencoder featuring pyramidal analysis, an adaptive coding module, and regularization of the expected codelength. We also supplement our approach with adversarial training specialized towards use in a compression setting: this enables us to produce visually pleasing reconstructions for very low bitrates.
","['WaveOne, Inc.', 'WaveOne, Inc.']"
2017,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,"Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, Taylor Berg-Kirkpatrick",https://icml.cc/Conferences/2017/Schedule?showEvent=834,"Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning informa- tion from the encoder. In this paper, we ex- periment with a new type of decoder for VAE: a dilated CNN. By changing the decoder’s di- lation architecture, we control the size of con- text from previously generated words. In ex- periments, we find that there is a trade-off be- tween contextual capacity of the decoder and ef- fective use of encoding information. We show that when carefully managed, VAEs can outper- form LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding archi- tecture) for semi-supervised and unsupervised la- beling tasks, demonstrating gains over several strong baselines.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellen University', '']"
2017,Near-Optimal Design of Experiments via Regret Minimization,"Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, Yining Wang",https://icml.cc/Conferences/2017/Schedule?showEvent=609,"We consider computationally tractable methods for the experimental design problem, where k out of n design points of dimension p are selected so that certain optimality criteria are approximately satisfied. Our algorithm finds a (1+eps)-approximate optimal design when k is a linear function of p; in contrast, existing results require k to be super-linear in p. Our algorithm also handles all popular optimality criteria, while existing ones only handle one or two such criteria. Numerical results on synthetic and real-world design problems verify the practical effectiveness of the proposed algorithm.
","['Microsoft Research / Princeton / IAS', 'Princeton University', 'Carnegie Mellon University', 'CMU']"
2017,Neural Episodic Control,"Alexander Pritzel, Benigno Uria, Srinivasan Sriram, Adrià Puigdomenech Badia, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell",https://icml.cc/Conferences/2017/Schedule?showEvent=745,"Deep reinforcement learning methods attain super-human performance in a wide range of environments.
Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance.
We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. 
Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function.
We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.
","['Deepmind', 'Deepmind', 'DeepMind', 'Deepmind', 'DeepMind', 'Deepmind', 'Google DeepMind', 'DeepMind']"
2017,Random Feature Expansions for Deep Gaussian Processes,"Kurt Cutajar, Edwin Bonilla, Pietro Michiardi, Maurizio Filippone",https://icml.cc/Conferences/2017/Schedule?showEvent=576,"The composition of multiple Gaussian Processes as a Deep Gaussian Process DGP enables a deep probabilistic nonparametric approach to flexibly tackle complex machine learning problems with sound quantification of uncertainty. Existing inference approaches for DGP models have limited scalability and are notoriously cumbersome to construct. In this work we introduce a novel formulation of DGPs based on random feature expansions that we train using stochastic variational inference. This yields a practical learning framework which significantly advances the state-of-the-art in inference for DGPs, and enables accurate quantification of uncertainty. We extensively showcase the scalability and performance of our proposal on several datasets with up to 8 million observations, and various DGP architectures with up to 30 hidden layers. 
","['EURECOM', 'UNSW', 'EURECOM', 'Eurecom']"
2017,Deep IV: A Flexible Approach for Counterfactual Prediction,"Jason Hartford, Greg Lewis, Kevin Leyton-Brown, Matt Taddy",https://icml.cc/Conferences/2017/Schedule?showEvent=883,"Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs) -- sources of treatment randomization that are conditionally independent from the outcomes.  Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose  loss function involves integration over the conditional treatment distribution.  This Deep IV framework allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches. 
","['University of British Columbia', 'Microsoft Research', 'University of British Columbia', 'MICROSOFT']"
2017,"ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning","Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, Ce Zhang",https://icml.cc/Conferences/2017/Schedule?showEvent=659,"Recently there has been significant interest in training machine-learning models at low precision: by reducing precision, one can reduce computation and communication by one order of magnitude. We examine training at reduced precision, both from a theoretical and practical perspective, and ask: is it possible to train models at end-to-end low precision with provable guarantees? Can this lead to consistent order-of-magnitude speedups? We mainly focus on linear models, and the answer is yes for linear models. We develop a simple framework called ZipML based on one simple but novel strategy called double sampling. Our ZipML framework is able to execute training at low precision with no bias, guaranteeing convergence, whereas naive quantization would introduce significant bias. We validate our framework across a range of applications, and show that it enables an FPGA prototype that is up to 6.5× faster than an implementation using full 32-bit precision. We further develop a variance-optimal stochastic quantization strategy and show that it can make a significant difference in a variety of settings. When applied to linear models together with double sampling, we save up to another 1.7× in data movement compared with uniform quantization. When training deep networks with quantized models, we achieve higher accuracy than the state-of-the-art XNOR-Net.
","['ETH Zurich', 'MIT', 'ETH Zurich', 'IST Austria & ETH Zurich', 'University of Rochester', 'ETH Zurich']"
2017,Adapting Kernel Representations Online Using Submodular Maximization,"Matthew Schlegel, Yangchen Pan, Jiecao Chen, Martha White",https://icml.cc/Conferences/2017/Schedule?showEvent=668,"Kernel representations provide a nonlinear representation, through similarities to prototypes, but require only simple linear learning algorithms given those prototypes. In a continual learning setting, with a constant stream of observations, it is critical to have an efficient mechanism for sub-selecting prototypes amongst observations. In this work, we develop an approximately submodular criterion for this setting, and an efficient online greedy submodular maximization algorithm for optimizing the criterion. We extend streaming submodular maximization algorithms to continual learning, by removing the need for multiple passes---which is infeasible---and instead introducing the idea of coverage time. We propose a general block-diagonal approximation for the greedy update with our criterion, that enables updates linear in the number of prototypes. We empirically demonstrate the effectiveness of this approximation, in terms of approximation quality, significant runtime improvements, and effective prediction performance.
","['Indiana University', 'Indiana University', 'Indiana University Bloomington', 'University of Alberta/Indiana University']"
2017,End-to-End Learning for Structured Prediction Energy Networks,"David Belanger, Bishan Yang, Andrew McCallum",https://icml.cc/Conferences/2017/Schedule?showEvent=836,"Structured Prediction Energy Networks (SPENs) are a simple, yet expressive family of structured prediction models (Belanger and McCallum, 2016).  An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization. This paper presents end-to-end learning for SPENs, where the energy function is discriminatively trained by back-propagating through gradient-based prediction. In our experience, the approach is substantially more accurate than the structured SVM method of Belanger and McCallum (2016), as it allows us to use more sophisticated non-convex energies. We provide a collection of techniques for improving the speed, accuracy, and memory requirements of end-to-end SPENs, and demonstrate the power of our method on 7-Scenes image denoising and CoNLL-2005 semantic role labeling tasks. In both, inexact minimization of non-convex SPEN energies is superior to baseline methods that use simplistic energy functions that can be minimized exactly.
","['Google Brain', 'Carnegie Mellon University', 'UMass Amherst']"
2017,Neural Message Passing for Quantum Chemistry,"Justin Gilmer, Samuel Schoenholz, Patrick F Riley, Oriol Vinyals, George Dahl",https://icml.cc/Conferences/2017/Schedule?showEvent=529,"Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.
","['Google Brain', 'Google Brain', 'Google', 'DeepMind', 'Google Brain']"
2017,Grammar Variational Autoencoder,"Matt J. Kusner, Brooks Paige, Jose Miguel Hernandez-Lobato",https://icml.cc/Conferences/2017/Schedule?showEvent=859,"Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.
","['Alan Turing Institute', 'Alan Turing Institute', 'University of Cambridge']"
2017,Robust Budget Allocation via Continuous Submodular Functions,"Matthew J Staib, Stefanie Jegelka",https://icml.cc/Conferences/2017/Schedule?showEvent=667,"The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past years, in particular in machine learning and data mining. But in applications, the parameters of the problem are rarely known exactly, and using wrong parameters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allocation or Bipartite Influence Maximization problem introduced by Alon et al. (2012) from a robust optimization perspective, where an adversary may choose the least favorable parameters within a confidence set. The resulting problem is a nonconvex-concave saddle point problem (or game). We show that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization problem. Although constrained submodular minimization is hard in general, here, we establish conditions under which such a problem can be solved to arbitrary precision epsilon.
","['MIT', 'MIT']"
2017,Neural Optimizer Search using Reinforcement Learning,"Irwan Bello, Barret Zoph, Vijay Vasudevan, Quoc Le",https://icml.cc/Conferences/2017/Schedule?showEvent=842,"We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a specific domain language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. These optimizers can also be transferred to perform well on different neural network architectures, including Google’s neural machine translation system.
","['Google Brain', 'Google', 'Google', 'Google Brain']"
2017,Asynchronous Distributed Variational Gaussian Processes for Regression,"Hao Peng, Shandian Zhe, Xiao Zhang, Yuan Qi",https://icml.cc/Conferences/2017/Schedule?showEvent=629,"Gaussian processes (GPs) are powerful non-parametric function estimators. However, their applications are largely limited by the expensive computational cost of the inference procedures. Existing stochastic or distributed synchronous variational inferences, although have alleviated this issue by scaling up GPs to millions of samples, are still far from satisfactory for real-world large applications, where the data sizes are often orders of magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the first Asynchronous Distributed Variational Gaussian Process inference for regression, on the recent large-scale machine learning platform, PARAMETER SERVER. ADVGP uses a novel, flexible variational framework based on a weight space augmentation, and implements the highly efficient, asynchronous proximal gradient optimization. While maintaining comparable or better predictive performance, ADVGP greatly improves upon the efficiency of the existing variational methods. With ADVGP, we effortlessly scale up GP regression to a real-world application with billions of samples and demonstrate an excellent, superior prediction accuracy to the popular linear models.
","['Purdue University', 'Purdue University', 'Purdue University', 'Ant Financial']"
2017,Counterfactual Data-Fusion for Online Reinforcement Learners,"Andrew Forney, Judea Pearl, Elias Bareinboim",https://icml.cc/Conferences/2017/Schedule?showEvent=872,"The Multi-Armed Bandit problem with Unobserved Confounders (MABUC) considers decision-making settings where unmeasured variables can influence both the agent's decisions and received rewards (Bareinboim et al., 2015). Recent findings showed that unobserved confounders (UCs) pose a unique challenge to algorithms based on standard randomization (i.e., experimental data); if UCs are naively averaged out, these algorithms behave sub-optimally, possibly incurring infinite regret. In this paper, we show how counterfactual-based decision-making circumvents these problems and leads to a coherent fusion of observational and experimental data. We then demonstrate this new strategy in an enhanced Thompson Sampling bandit player, and support our findings' efficacy with extensive simulations.
","['UCLA', 'UCLA', 'Purdue']"
2017,Large-Scale Evolution of Image Classifiers,"Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, Alexey Kurakin",https://icml.cc/Conferences/2017/Schedule?showEvent=634,"Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.
","['Google Inc.', 'Google Inc.', 'Google Inc.', 'Google Inc.', 'Google Inc.', 'Google Inc.', 'Google Brain', 'Google Brain']"
2017,Spherical Structured Feature Maps for Kernel Approximation,Yueming LYU,https://icml.cc/Conferences/2017/Schedule?showEvent=537,"We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as $b^{th}$-order arc-cosine kernels~\cite{cho2009kerneldeeplearning}.  We construct SSF maps based on the point set on $d-1$ dimensional sphere $\mathbb{S}^{d-1}$. We prove that the inner product of SSF maps are unbiased estimates for above kernels if asymptotically uniformly distributed point set  on $\mathbb{S}^{d-1}$ is given. According to ~\cite{brauchart2015distributing}, optimizing
the discrete Riesz s-energy can generate asymptotically uniformly distributed point set on $\mathbb{S}^{d-1}$. Thus, we propose 
an efficient coordinate decent method to find a local optimum of the discrete Riesz s-energy for SSF maps construction. Theoretically, SSF maps construction achieves linear space complexity and loglinear time complexity.  Empirically, SSF maps achieve superior performance compared with  other methods. ",['city university of hong kong']
2017,A Unified View of Multi-Label Performance Measures,"Xi-Zhu Wu, Zhi-Hua Zhou",https://icml.cc/Conferences/2017/Schedule?showEvent=751,"Multi-label classification deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classification is more complicated than single-label setting, a number of performance measures have been proposed. It is noticed that an algorithm usually performs differently on different measures. Therefore, it is important to understand which algorithms perform well on which measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification. In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures are to be optimized. Based on the defined margins, a max-margin approach called LIMO is designed and empirical results validate our theoretical findings.
","['Nanjing University', 'Nanjing University']"
2017,Accelerating Eulerian Fluid Simulation With Convolutional Networks,"Jonathan Tompson, Kristofer D Schlachter, Pablo Sprechmann, Ken Perlin",https://icml.cc/Conferences/2017/Schedule?showEvent=549,"Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large sparse linear system with many free parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.
","['Google Brain', 'New York University', 'NYU', 'New York University']"
2017,Rule-Enhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement,"Jonathan Eckstein, Noam Goldberg, Ai Kagawa",https://icml.cc/Conferences/2017/Schedule?showEvent=604,"We describe a learning procedure enhancing L1-penalized regression by adding dynamically generated rules describing multidimensional “box” sets. Our rule-adding procedure is based on the classical column generation method for high-dimensional linear programming. The pricing problem for our column generation procedure reduces to the NP-hard rectangular maximum agreement (RMA) problem of finding a box that best discriminates between two weighted datasets. We solve this problem exactly using a parallel branch-and-bound procedure. The resulting rule-enhanced regression procedure is computation-intensive, but has promising prediction performance.
","['Rutgers University', 'Bar-Ilan University', 'Rutgers Univeristy']"
2017,High Dimensional Bayesian Optimization with Elastic Gaussian Process,"Santu Rana, Cheng Li, Sunil Gupta, Vu Nguyen, Svetha Venkatesh",https://icml.cc/Conferences/2017/Schedule?showEvent=554,"Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or hyperparameter tuning of a machine learning algorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension - having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function as we leverage two underlying facts - a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method at high dimension using both benchmark test functions and real-world case studies. 
","['Deakin University', 'Deakin University', 'Deakin University', 'Deakin University', 'Deakin University']"
2017,Nyström Method with Kernel K-means++ Samples as Landmarks,"Dino Oglic, Thomas Gaertner",https://icml.cc/Conferences/2017/Schedule?showEvent=595,"We investigate, theoretically and empirically, the effectiveness of kernel K-means++ samples as landmarks in the Nyström method for low-rank approximation of kernel matrices. Previous empirical studies (Zhang et al., 2008; Kumar et al.,2012) observe that the landmarks obtained using (kernel) K-means clustering define a good low-rank approximation of kernel matrices. However, the existing work does not provide a theoretical guarantee on the approximation error for this approach to landmark selection. We close this gap and provide the first bound on the approximation error of the Nystrom method with kernel K-means++ samples as landmarks. Moreover, for the frequently used Gaussian kernel we provide a theoretically sound motivation for performing Lloyd refinements of kernel K-means++ landmarks in the instance space. We substantiate our theoretical results empirically by comparing the approach to several state-of-the-art algorithms.
","['University of Bonn', 'The University of Nottingham']"
2017,Scalable Generative Models for Multi-label Learning with Missing Labels,"Vikas Jain, Nirbhay Modhe, Piyush Rai",https://icml.cc/Conferences/2017/Schedule?showEvent=708,"We present a scalable, generative framework for multi-label learning with missing labels. Our framework consists of a latent factor model for the binary label matrix, which is coupled with an exposure model to account for label missingness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation). The underlying latent factor model also assumes that the low-dimensional embeddings of each label vector are directly conditioned on the respective feature vector of that example. Our generative framework admits a simple inference procedure, such that the parameter estimation reduces to a sequence of simple weighted least-square regression problems, each of which can be solved easily, efficiently, and in parallel. Moreover, inference can also be performed in an online fashion using mini-batches of training examples, which makes our framework scalable for large data sets, even when using moderate computational resources. We report both quantitative and qualitative results for our framework on several benchmark data sets, comparing it with a number of state-of-the-art methods.
","['Indian Institute of Technology Kanpur', 'Georgia Institute of Technology', 'IIT Kanpur']"
2018,Spline Filters For End-to-End Deep Learning,"Randall Balestriero, Romain Cosentino, Herve Glotin, Richard Baraniuk",https://icml.cc/Conferences/2018/Schedule?showEvent=2291,"We propose to tackle the problem of end-to-end learning for raw waveforms signals by introducing learnable continuous time-frequency atoms. The derivation of these filters is achieved by first, defining a functional space with a given smoothness order and boundary conditions. From this space, we derive the parametric analytical filters. Their differentiability property allows gradient-based optimization. As such, one can equip any Deep Neural Networks (DNNs) with these filters. This enables us to tackle in a front-end fashion a large scale bird detection task based on the freefield1010 dataset known to contain key challenges, such as high dimensional inputs ($>100000$) and the presence of multiple sources and soundscapes.","['Rice University', 'Rice University', 'Universite de Toulon', 'OpenStax / Rice University']"
2018,Non-linear motor control by local learning in spiking neural networks,"Aditya Gilra, Wulfram Gerstner",https://icml.cc/Conferences/2018/Schedule?showEvent=2311,"Learning weights in a spiking neural network with hidden neurons, using local, stable and online rules, to control non-linear body dynamics is an open problem. Here, we employ a supervised scheme, Feedback-based Online Local Learning Of Weights (FOLLOW), to train a heterogeneous network of spiking neurons with hidden layers, to control a two-link arm so as to reproduce a desired state trajectory. We show that the network learns an inverse model of the non-linear dynamics, i.e. it infers from state trajectory as input to the network, the continuous-time command that produced the trajectory. Connection weights are adjusted via a local plasticity rule that involves pre-synaptic firing and post-synaptic feedback of the error in the inferred command. We propose a network architecture, termed differential feedforward, and show that it gives a lower test error than other feedforward and recurrent architectures. We demonstrate the performance of the inverse model to control a two-link arm along a desired trajectory.
","['University of Bonn', 'EPFL']"
2018,Implicit Quantile Networks for Distributional Reinforcement Learning,"Will Dabney, Georg Ostrovski, David Silver, Remi Munos",https://icml.cc/Conferences/2018/Schedule?showEvent=2450,"In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.
","['DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind']"
2018,An Inference-Based Policy Gradient Method for Learning Options,"Matthew Smith, Herke van Hoof, Joelle Pineau",https://icml.cc/Conferences/2018/Schedule?showEvent=2432,"In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment.  Despite several efforts, this remains a difficult problem. In this work we develop a novel policy gradient method for the automatic learning of policies with options. This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. The differentiable inference procedure employed yields options that can be easily interpreted. Empirical results confirm these attributes, and indicate that our algorithm has an improved sample efficiency relative to state-of-the-art in learning options end-to-end.
","['McGill University', 'McGill University', 'McGill University / Facebook']"
2018,Predict and Constrain: Modeling Cardinality in Deep Structured Prediction,"Nataly Brukhim, Amir Globerson",https://icml.cc/Conferences/2018/Schedule?showEvent=2368,"Many machine learning problems require the prediction of multi-dimensional labels. Such structured prediction models can benefit from modeling dependencies between labels. Recently, several deep learning approaches to structured prediction have been proposed. Here we focus on capturing cardinality constraints in such models. Namely, constraining the number of non-zero labels that the model outputs. Such constraints have proven very useful in previous structured prediction methods, but it is a challenge to introduce them into a deep learning approach. Here we show how to do this via a novel deep architecture. Our approach outperforms strong baselines, achieving state-of-the-art results on multi-label classification benchmarks.
","['Tel Aviv University', 'Tel Aviv University, Google']"
2018,Differentially Private Matrix Completion Revisited,"Prateek Jain, Om Dipakbhai Thakkar, Abhradeep Thakurta",https://icml.cc/Conferences/2018/Schedule?showEvent=2400,"We provide the first provably joint differentially private algorithm with formal utility guarantees for the problem of user-level privacy-preserving collaborative filtering. Our algorithm is based on the Frank-Wolfe method, and it consistently estimates the underlying preference matrix as long as the number of users $m$ is $\omega(n^{5/4})$, where $n$ is the number of items, and each user provides her preference for at least $\sqrt{n}$ randomly selected items. Along the way, we provide an optimal differentially private algorithm for singular vector computation, based on the celebrated Oja's method, that provides significant savings in terms of space and time while operating on sparse matrices. We also empirically evaluate our algorithm on a suite of datasets, and show that it consistently outperforms the state-of-the-art private algorithms.","['Microsoft Research', 'Boston University', 'UCSC']"
2018,Differentiable plasticity: training plastic neural networks with backpropagation,"Thomas Miconi, Kenneth Stanley, Jeff Clune",https://icml.cc/Conferences/2018/Schedule?showEvent=2078,"How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains: synaptic plasticity, carefully tuned by evolution to produce efficient lifelong learning. We show that plasticity, just like connection weights, can be optimized by gradient descent in large (millions of parameters) recurrent networks with Hebbian plastic connections. First, recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel, high-dimensional (1000+ pixels) natural images not seen during training. Crucially,   traditional non-plastic recurrent networks fail to solve this task. Furthermore, trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task, with competitive results and little parameter overhead. Finally, in reinforcement learning settings, plastic networks outperform non-plastic equivalent in a maze exploration task. We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem.
","['Uber AI Labs', 'Uber AI Labs & University of Central Florida', 'Uber AI Labs']"
2018,Model-Level Dual Learning,"Yingce Xia, Xu Tan, Fei Tian, Tao Qin, Nenghai Yu, Tie-Yan Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2172,"Many artificial intelligence tasks appear in dual forms like English$\leftrightarrow$French translation and speech$\leftrightarrow$text transformation. Existing dual learning schemes, which are proposed to solve a pair of such dual tasks, explore how to leverage such dualities from data level. In this work, we propose a new learning framework, model-level dual learning, which takes duality of tasks into consideration while designing the architectures for the primal/dual models, and ties the model parameters that playing similar roles in the two tasks. We study both symmetric and asymmetric model-level dual learning. Our algorithms achieve significant improvements on neural machine translation and sentiment analysis.","['University of Science and Technology of China', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research Asia', 'USTC', 'Microsoft']"
2018,CoVeR: Learning Covariate-Specific Vector Representations with Tensor Decompositions,"Kevin Tian, Teng Zhang, James Zou",https://icml.cc/Conferences/2018/Schedule?showEvent=2309,"Word embedding is a useful approach to capture co-occurrence structures in large text corpora. However, in addition to the text data itself, we often have additional covariates associated with individual corpus documents---e.g. the demographic of the author, time and venue of publication---and we would like the embedding to naturally capture this information. We propose CoVeR, a new tensor decomposition model for vector embeddings with covariates. CoVeR jointly learns a \emph{base} embedding for all the words as well as a weighted diagonal matrix to model how each covariate affects the base embedding. To obtain author or venue-specific embedding, for example, we can then simply multiply the base embedding by the associated transformation matrix. The main advantages of our approach are data efficiency and interpretability of the covariate transformation. Our experiments demonstrate that our joint model learns substantially better covariate-specific embeddings compared to the standard approach of learning a separate embedding for each covariate using only the relevant subset of data, as well as other related methods. Furthermore, CoVeR encourages the embeddings to be ``topic-aligned'' in that the dimensions have specific independent meanings. This allows our covariate-specific embeddings to be compared by topic, enabling downstream differential analysis. We empirically evaluate the benefits of our algorithm on datasets, and demonstrate how it can be used to address many natural questions about covariate effects.
","['Stanford University', 'Stanford University', 'Stanford University']"
2018,Tree Edit Distance Learning via Adaptive Symbol Embeddings,"Benjamin Paaßen, Claudio Gallicchio, Alessio Micheli, CITEC Barbara Hammer",https://icml.cc/Conferences/2018/Schedule?showEvent=2180,"Metric learning has the aim to improve classification accuracy by learning a distance measure which brings data points from the same class closer together and pushes data points from different classes further apart. Recent research has demonstrated that metric learning approaches can also be applied to trees, such as molecular structures, abstract syntax trees of computer programs, or syntax trees of natural language, by learning the cost function of an edit distance, i.e. the costs of replacing, deleting, or inserting nodes in a tree. However, learning such costs directly may yield an edit distance which violates metric axioms, is challenging to interpret, and may not generalize well. In this contribution, we propose a novel metric learning approach for trees which learns an edit distance indirectly by embedding the tree nodes as vectors, such that the Euclidean distance between those vectors supports class discrimination. We learn such embeddings by reducing the distance to prototypical trees from the same class and increasing the distance to prototypical trees from different classes. In our experiments, we show that our proposed metric learning approach improves upon the state-of-the-art in metric learning for trees on six benchmark data sets, ranging from computer science over biomedical data to a natural-language processing data set containing over 300,000 nodes.
","['Bielefeld University', 'University of Pisa', 'Universita di Pisa', 'CITEC, Bielefeld University']"
2018,Gradually Updated Neural Networks for Large-Scale Image Recognition,"Siyuan Qiao, Zhishuai Zhang, Wei Shen, Bo Wang, Alan Yuille",https://icml.cc/Conferences/2018/Schedule?showEvent=1905,"Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition. The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks. In this paper, we present an alternative method to increase the depth. Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner. The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better. Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.
","['Johns Hopkins University', 'Johns Hopkins University', 'Shanghai University', 'Hikvision Research Institue', 'Johns Hopkins University']"
2018,One-Shot Segmentation in Clutter,"Claudio Michaelis, Matthias Bethge, Alexander Ecker",https://icml.cc/Conferences/2018/Schedule?showEvent=2303,"We tackle the problem of one-shot segmentation: finding and segmenting a previously unseen object in a cluttered scene based on a single instruction example. We propose a novel dataset, which we call {\it cluttered Omniglot}. Using a baseline architecture combining a Siamese embedding for detection with a U-net for segmentation we show that increasing levels of clutter make the task progressively harder. Using oracle models with access to various amounts of ground-truth information, we evaluate different aspects of the problem and show that in this kind of visual search task, detection and segmentation are two intertwined problems, the solution to each of which helps solving the other. We therefore introduce {\it MaskNet}, an improved model that attends to multiple candidate locations, generates segmentation proposals to mask out background clutter and selects among the segmented objects. Our findings suggest that such image recognition models based on an iterative refinement of object detection and foreground segmentation may provide a way to deal with highly cluttered scenes.
","['Universty of Tübingen', 'University of Tübingen', 'University of Tübingen']"
2018,Active Testing: An Efficient and Robust Framework for Estimating Accuracy,"Phuc Nguyen, Deva Ramanan, Charless Fowlkes",https://icml.cc/Conferences/2018/Schedule?showEvent=1946,"Much recent work on large-scale visual recogni- tion aims to scale up learning to massive, noisily- annotated datasets. We address the problem of scaling-up the evaluation of such models to large- scale datasets with noisy labels. Current protocols for doing so require a human user to either vet (re-annotate) a small fraction of the testset and ignore the rest, or else correct errors in annotation as they are found through manual inspection of results. In this work, we re-formulate the problem as one of active testing, and examine strategies for efficiently querying a user so as to obtain an accurate performance estimate with minimal vet- ting. We demonstrate the effectiveness of our proposed active testing framework on estimating two performance metrics, Precision@K and mean Average Precisions, for two popular Computer Vi- sion tasks, multilabel classification and instance segmentation, respectively. We further show that our approach is able to siginificantly save human annotation effort and more robust than alterna- tive evaluation protocols.
","['UC Irvine', 'Carnegie Mellon University', 'UC Irvine']"
2018,Learning Deep ResNet Blocks Sequentially using Boosting Theory,"Furong Huang, Jordan Ash, John Langford, Robert Schapire",https://icml.cc/Conferences/2018/Schedule?showEvent=2107,"We prove a \emph{multi-channel telescoping sum boosting} theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast with labels) and provides a new algorithm for ResNet-style architectures.  Our proposed training algorithm, \emph{BoostResNet}, is particularly suitable in non-differentiable architectures.  Our method only requires the relatively inexpensive sequential training of $T$ ``shallow ResNets''. We prove that the training error decays exponentially with the depth $T$ if the weak module classifiers that we train perform slightly better than some weak baseline.  In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with $l_1$ norm bounded weights.","['University of Maryland College Park', 'Princeton University', 'Microsoft Research', 'Microsoft Research']"
2018,Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings,"John Co-Reyes, Yu Xuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, Sergey Levine",https://icml.cc/Conferences/2018/Schedule?showEvent=2337,"In this work, we take a representation learning perspective on hierarchical reinforcement learning, where the problem of learning lower layers in a hierarchy is transformed into the problem of learning trajectory-level generative models. We show that we can learn continuous latent representations of trajectories, which are effective in solving temporally extended and multi-stage problems. Our proposed model, SeCTAR, draws inspiration from variational autoencoders, and learns latent representations of trajectories. A key component of this method is to learn both a latent-conditioned policy and a latent-conditioned model which are consistent with each other. Given the same latent, the policy generates a trajectory which should match the trajectory predicted by the model. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration. This model provides a built-in prediction mechanism, by predicting the outcome of closed loop policy behavior. We propose a novel algorithm for performing hierarchical RL with this model, combining model-based planning in the learned latent space with an unsupervised exploration objective. We show that our model is effective at reasoning over long horizons with sparse rewards for several simulated tasks, outperforming standard reinforcement learning methods and prior methods for hierarchical reasoning, model-based planning, and exploration.
","['UC Berkeley', 'University of California, Berkeley', 'UC Berkeley', 'Google', 'OpenAI / UC Berkeley', 'Berkeley']"
2018,Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs,"Andrea Zanette, Emma Brunskill",https://icml.cc/Conferences/2018/Schedule?showEvent=2433,"In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs).  In this paper, we study whether there exist algorithms for the more general framework (MDP) which automatically provide the best performance bounds for the specific problem at hand without user intervention and without modifying the algorithm. In particular, it is found that a very minor variant of a recently proposed reinforcement learning algorithm for MDPs already matches the best possible regret bound $\tilde O (\sqrt{SAT})$ in the dominant term if deployed on a tabular Contextual Bandit problem despite the agent being agnostic to such setting.","['Stanford University', 'Stanford University']"
2018,Stochastic PCA with $\ell_2$ and $\ell_1$ Regularization,"Poorya Mianjy, Raman Arora",https://icml.cc/Conferences/2018/Schedule?showEvent=2479,"We revisit convex relaxation based methods for stochastic optimization of principal component analysis (PCA). While methods that directly solve the nonconvex problem have been shown to be optimal in terms of statistical and computational efficiency, the methods based on convex relaxation have been shown to enjoy comparable, or even superior, empirical performance -- this motivates the need for a  deeper formal understanding of the latter. Therefore, in this paper, we study variants of stochastic gradient descent for a convex relaxation of PCA with (a) $\ell_2$, (b) $\ell_1$, and (c) elastic net ($\ell_1+\ell_2)$ regularization in the hope that these variants yield (a) better iteration complexity, (b) better control on the rank of the intermediate iterates, and (c) both, respectively. We show, theoretically and empirically, that compared to previous work on convex relaxation based methods, the proposed variants yield faster convergence and improve overall runtime  to achieve a certain user-specified $\epsilon$-suboptimality on the PCA objective. Furthermore, the proposed methods are shown to converge both in terms of the PCA objective as well as the distance between subspaces. However, there still remains a gap in computational requirements for the proposed methods when compared with existing nonconvex approaches.","['Johns Hopkins University', 'Johns Hopkins University']"
2018,Subspace Embedding and Linear Regression with Orlicz Norm,"Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, Ruiqi Zhong",https://icml.cc/Conferences/2018/Schedule?showEvent=2451,"We consider a generalization of the classic linear regression problem to the case when the loss is an Orlicz norm. An Orlicz norm is parameterized by a non-negative convex function G: R+ - > R+ with G(0) = 0: the Orlicz norm of a n-dimensional vector x is defined as |x|G = inf{ alpha > 0 | sum{i = 1}^n G( |xi| / alpha ) < = 1 }. We consider the cases where the function G grows subquadratically. Our main result is based on a new oblivious embedding which embeds the column space of a given nxd matrix A with Orlicz norm into a lower dimensional space with L2 norm. Specifically, we show how to efficiently find an mxn embedding matrix S (m < n), such that for every d-dimensional vector x, we have Omega(1/(d log n)) |Ax|G < = |SAx|2 < = O(d^2 log n) |Ax|G. By applying this subspace embedding technique, we show an approximation algorithm for the regression problem minx |Ax-b|G, up to a O( d log^2 n ) factor. As a further application of our techniques, we show how to also use them to improve on the algorithm for the Lp low rank matrix approximation problem for 1 < = p < 2.
","['', 'Columbia University', '', 'Columbia University', 'Columbia University in the City of New York']"
2018,Signal and  Noise Statistics Oblivious Orthogonal Matching Pursuit,"Sreejith Kallummil, Sheetal Kalyani",https://icml.cc/Conferences/2018/Schedule?showEvent=2315,"Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimensional vectors in linear regression models. The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vector or noise statistics. Both these statistics are rarely known a priori and are very difficult to estimate.  In this paper, we present a novel technique called residual ratio thresholding (RRT) to operate OMP without any a priori knowledge of sparsity and noise statistics and establish  finite sample and large sample support recovery guarantees for the same. Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a performance comparable to OMP with a priori knowledge of sparsity and noise statistics.
","['Qualcomm India Private Limited', 'IIT Madras']"
2018,Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope,"Eric Wong, Zico Kolter",https://icml.cc/Conferences/2018/Schedule?showEvent=2136,"We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$).","['Carnegie Mellon University', 'Carnegie Mellon University / Bosch Center for AI']"
2018,Learning the Reward Function for a Misspecified Model,Erik Talvitie,https://icml.cc/Conferences/2018/Schedule?showEvent=1913,"In model-based reinforcement learning it is typical to decouple the problems of learning the dynamics model and learning the reward function. However, when the dynamics model is flawed, it may generate erroneous states that would never occur in the true environment. It is not clear a priori what value the reward function should assign to such states. This paper presents a novel error bound that accounts for the reward model's behavior in states sampled from the model. This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical performance guarantees in deterministic MDPs that do not assume a perfect model can be learned. Empirically, this approach to reward learning can yield dramatic improvements in control performance when the dynamics model is flawed.
",['Franklin & Marshall College']
2018,Deep Reinforcement Learning in Continuous Action Spaces: a Case Study in the Game of Simulated Curling,"kyowoon Lee, Sol-A Kim, Jaesik Choi, Seong-Whan Lee",https://icml.cc/Conferences/2018/Schedule?showEvent=2461,"Many real-world applications of reinforcement learning require an agent to select optimal actions from continuous action spaces. Recently, deep neural networks have successfully been applied to games with discrete actions spaces. However, deep neural networks for discrete actions are not suitable for devising strategies for games in which a very small change in an action can dramatically affect the outcome. In this paper, we present a new framework which incorporates a deep neural network that can be used to learn game strategies based on a kernel-based Monte Carlo tree search that finds actions within a continuous space. To avoid hand-crafted features, we train our network using supervised learning followed by reinforcement learning with a high-fidelity simulator for the Olympic sport of curling. The program trained under our framework outperforms existing programs equipped with several hand-crafted features and won an international digital curling competition.
","['UNIST', 'UNIST', 'Ulsan National Institute of Science and Technology', 'Korea University']"
2018,Do Outliers Ruin Collaboration?,Mingda Qiao,https://icml.cc/Conferences/2018/Schedule?showEvent=1876,"We consider the problem of learning a binary classifier from $n$ different data sources, among which at most an $\eta$ fraction are adversarial. The overhead is defined as the ratio between the sample complexity of learning in this setting and that of learning the same hypothesis class on a single data distribution. We present an algorithm that achieves an $O(\eta n + \ln n)$ overhead, which is proved to be worst-case optimal. We also discuss the potential challenges to the design of a computationally efficient learning algorithm with a small overhead.","['IIIS, Tsinghua University']"
2018,"Dropout Training, Data-dependent Regularization, and Generalization Bounds","Wenlong Mou, Yuchen Zhou, Jun Gao, Liwei Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=2000,"We study the problem of generalization guarantees for dropout training. A general framework is first proposed for learning procedures with random perturbation on model parameters. The generalization error is bounded by sum of two offset Rademacher complexities: the main term is Rademacher complexity of the hypothesis class with minus offset induced by the perturbation variance, which characterizes data-dependent regularization by the random perturbation; the auxiliary term is offset Rademacher complexity for the variance class, controlling the degree to which this regularization effect can be weakened. For neural networks, we estimate upper and lower bounds for the variance induced by truthful dropout, a variant of dropout that we propose to ensure unbiased output and fit into our framework, and the variance bounds exhibits connection to adaptive regularization methods. By applying our framework to ReLU networks with one hidden layer, a generalization upper bound is derived with no assumptions on the parameter norms or data distribution, with $O(1/n)$ fast rate and adaptivity to geometry of data points being achieved at the same time.","['UC Berkeley', 'University of Wisconsin, Madison', 'Peking University', 'Peking University']"
2018,Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations,"IEMS Xingyu Wang, Diego Klabjan",https://icml.cc/Conferences/2018/Schedule?showEvent=2082,"This paper considers the problem of inverse reinforcement learning in zero-sum stochastic games when expert demonstrations are known to be suboptimal. Compared to previous works that decouple agents in the game by assuming optimality in expert policies, we introduce a new objective function that directly pits experts against Nash Equilibrium policies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with deep neural networks as model approximations. To ?nd Nash Equilibrium in large-scale games, we also propose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical appeal of non-existence of local optima in its objective function. In numerical experiments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to existing benchmark algorithms. Moreover, our algorithm successfully recovers reward and policy functions regardless of the quality of the sub-optimal expert demonstration set.
","['IEMS, Northwestern University', 'Northwestern University']"
2018,Continual Reinforcement Learning with Complex Synapses,"Christos Kaplanis, Murray Shanahan, Claudia Clopath",https://icml.cc/Conferences/2018/Schedule?showEvent=2165,"Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna & Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.
","['Imperial College London', 'Imperial College London', 'Imperial College London']"
2018,Equivalence of Multicategory SVM and Simplex Cone SVM: Fast Computations and Statistical Theory,Guillaume Pouliot,https://icml.cc/Conferences/2018/Schedule?showEvent=2427,"The multicategory SVM (MSVM) of Lee et al. (2004) is a natural generalization of the classical, binary support vector machines (SVM).  However, its use has been limited by computational difficulties.  The simplex-cone SVM (SCSVM) of Mroueh et al. (2012) is a computationally efficient multicategory classifier, but its use has been limited by a seemingly opaque interpretation.  We show that MSVM and SCSVM are in fact exactly equivalent, and provide a bijection between their tuning parameters.  MSVM may then be entertained as both a natural and computationally efficient multicategory extension of SVM.  We further provide a Donsker theorem for finite-dimensional kernel MSVM and partially answer the open question pertaining to the very competitive performance of One-vs-Rest methods against MSVM.  Furthermore, we use the derived asymptotic covariance formula to develop an inverse-variance weighted classification rule which improves on the One-vs-Rest approach.
",['University of Chicago']
2018,Quickshift++: Provably Good Initializations for Sample-Based Mean Shift,"Heinrich Jiang, Jennifer Jang, Samory Kpotufe",https://icml.cc/Conferences/2018/Schedule?showEvent=1949,"We provide initial seedings to the Quick Shift clustering algorithm, which approximate the locally high-density regions of the data. Such seedings act as more stable and expressive cluster-cores than the singleton modes found by Quick Shift. We establish statistical consistency guarantees for this modification. We then show strong clustering performance on real datasets as well as promising applications to image segmentation.
","['Google', 'Uber', 'Princeton University']"
2018,Learning Diffusion using Hyperparameters,"Dimitrios Kalimeris, Yaron Singer, Karthik Subbian, Udi Weinsberg",https://icml.cc/Conferences/2018/Schedule?showEvent=1880,"In this paper we advocate for a hyperparametric approach to learn diffusion in the independent cascade (IC) model. The sample complexity of this model is a function of the number of edges in the network and consequently learning becomes infeasible when the network is large. We study a natural restriction of the hypothesis class using additional information available in order to dramatically reduce the sample complexity of the learning process. In particular we assume that diffusion probabilities can be described as a function of a global hyperparameter and features of the individuals in the network. One of the main challenges with this approach is that training a model reduces to optimizing a non-convex objective. Despite this obstacle, we can shrink the best-known sample complexity bound for learning IC by a factor of |E|/d where |E| is the number of edges in the graph and d is the dimension of the hyperparameter. We show that under mild assumptions about the distribution generating the samples one can provably train a model with low generalization error. Finally, we use large-scale diffusion data from Facebook to show that a hyperparametric model using approximately 20 features per node achieves remarkably high accuracy.
","['Harvard University', 'Harvard', 'Facebook', 'Facebook']"
2018,Learning a Mixture of Two Multinomial Logits,"Flavio Chierichetti, Ravi Kumar, Andrew Tomkins",https://icml.cc/Conferences/2018/Schedule?showEvent=2238,"The classical Multinomial Logit (MNL) is a behavioral model for user   choice.  In this model, a user is offered a slate of choices (a   subset of a finite universe of $n$ items), and selects exactly one   item from the slate, each with probability proportional to its   (positive) weight.  Given a set of observed slates and choices, the   likelihood-maximizing item weights are easy to learn at scale, and   easy to interpret.  However, the model fails to represent common   real-world behavior.   As a result, researchers in user choice often turn to mixtures of   MNLs, which are known to approximate a large class of models of   rational user behavior.  Unfortunately, the only known algorithms   for this problem have been heuristic in nature.  In this paper we   give the first polynomial-time algorithms for exact learning of   uniform mixtures of two MNLs.  Interestingly, the parameters of the   model can be learned for any $n$ by sampling the behavior of random   users only on slates  of sizes 2 and 3; in contrast, we show that slates   of size 2 are insufficient by themselves.","['Sapienza University', 'Google', 'Google']"
2018,Crowdsourcing with Arbitrary Adversaries,"Matthäus Kleindessner, Pranjal Awasthi",https://icml.cc/Conferences/2018/Schedule?showEvent=2054,"Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its special case, where every worker makes mistakes independently of other workers and with the same error probability for every task. We study a significant extension of this restricted model. We allow almost half of the workers to deviate from the one-coin model and for those workers, their probabilities of making an error to be task-dependent and to be arbitrarily correlated. In other words, we allow for arbitrary adversaries, for which not only error probabilities can be high, but which can also perfectly collude. In this adversarial scenario, we design an efficient algorithm to consistently estimate the workers’ error probabilities.
","['Rutgers University', 'Rutgers University']"
2018,Deep Density Destructors,"David Inouye, Pradeep Ravikumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2437,"We propose a unified framework for deep density models by formally defining density destructors. A density destructor is an invertible function that transforms a given density to the uniform density---essentially destroying any structure in the original density. This destructive transformation generalizes Gaussianization via ICA and more recent autoregressive models such as MAF and Real NVP. Informally, this transformation can be seen as a generalized whitening procedure or a multivariate generalization of the univariate CDF function. Unlike Gaussianization, our destructive transformation has the elegant property that the density function is equal to the absolute value of the Jacobian determinant. Thus, each layer of a deep density can be seen as a shallow density---uncovering a fundamental connection between shallow and deep densities. In addition, our framework provides a common interface for all previous methods enabling them to be systematically combined, evaluated and improved. Leveraging the connection to shallow densities, we also propose a novel tree destructor based on tree densities and an image-specific destructor based on pixel locality. We illustrate our framework on a 2D dataset, MNIST, and CIFAR-10. Code is available on first author's website.
","['Carnegie Mellon University', 'Carnegie Mellon University']"
2018,Programmatically Interpretable Reinforcement Learning,"Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, Swarat Chaudhuri",https://icml.cc/Conferences/2018/Schedule?showEvent=2203,"We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.
","['Rice University', 'Rice University', 'Google Brain', 'DeepMind', 'Rice University']"
2018,Structured Evolution with Compact Architectures for Scalable Policy Optimization,"Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard E Turner, Adrian Weller",https://icml.cc/Conferences/2018/Schedule?showEvent=1907,"We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al. (2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.
","['Google Brain Robotics', 'University of Cambridge', 'Google', 'University of Cambridge', 'University of Cambridge, Alan Turing Institute']"
2018,The Weighted Kendall and High-order Kernels for Permutations,"Yunlong Jiao, Jean-Philippe Vert",https://icml.cc/Conferences/2018/Schedule?showEvent=2305,"We propose new positive definite kernels for permutations. First we introduce a weighted version of the Kendall kernel, which allows to weight unequally the contributions of different item pairs in the permutations depending on their ranks. Like the Kendall kernel, we show that the weighted version is invariant to relabeling of items and can be computed efficiently in O(n ln(n)) operations, where n is the number of items in the permutation. Second, we propose a supervised approach to learn the weights by jointly optimizing them with the function estimated by a kernel machine. Third, while the Kendall kernel considers pairwise comparison between items, we extend it by considering higher-order comparisons among tuples of items and show that the supervised approach of learning the weights can be systematically generalized to higher-order permutation kernels.
","['University of Oxford', 'ENS Paris']"
2018,"The Limits of Maxing, Ranking, and Preference Learning","Moein Falahatgar, Ayush Jain, Alon Orlitsky, Venkatadheeraj Pichapati, Vaishakh Ravindrakumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2103,"We present a comprehensive understanding of three important problems in PAC preference learning: maximum selection (maxing), ranking, and estimating \emph{all} pairwise preference probabilities, in the adaptive setting. With just Weak Stochastic Transitivity, we show that maxing requires $\Omega(n^2)$ comparisons and with slightly more restrictive Medium Stochastic Transitivity, we present a linear complexity maxing algorithm. With Strong Stochastic Transitivity and Stochastic Triangle Inequality, we derive a ranking algorithm with optimal $\mathcal{O}(n\log n)$ complexity and an optimal algorithm that estimates all pairwise preference probabilities.","['UC San Diego', 'UC San Diego', 'UCSD', 'University of California San Diego', 'UC San Diego']"
2018,Black Box FDR,"Wesley Tansey, Yixin Wang, David Blei, Raul Rabadan",https://icml.cc/Conferences/2018/Schedule?showEvent=1954,"Analyzing large-scale, multi-experiment studies requires scientists to test each experimental outcome for statistical significance and then assess the results as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes method for analyzing multi-experiment studies when many covariates are gathered per experiment. BB-FDR learns a series of black box predictive models to boost power and control the false discovery rate (FDR) at two stages of study analysis. In Stage 1, it uses a deep neural network prior to report which experiments yielded significant outcomes. In Stage 2, a separate black box model of each covariate is used to select features that have significant predictive power across all experiments. In benchmarks, BB-FDR outperforms competing state-of-the-art methods in both stages of analysis. We apply BB-FDR to two real studies on cancer drug efficacy. For both studies, BB-FDR increases the proportion of significant outcomes discovered and selects variables that reveal key genomic drivers of drug sensitivity and resistance in cancer.
","['Columbia University', 'Columbia University', 'Columbia University', 'Columbia University Medical Center']"
2018,Variable Selection via Penalized Neural Network: a Drop-Out-One Loss Approach,"Mao Ye, Yan Sun",https://icml.cc/Conferences/2018/Schedule?showEvent=1995,"We propose a variable selection method for high dimensional regression models, which allows for complex, nonlinear, and high-order interactions among variables. The proposed method approximates this complex system using a penalized neural network and selects explanatory variables by measuring their utility in explaining the variance of the response variable. This measurement is based on a novel statistic called Drop-Out-One Loss. The proposed method also allows (overlapping) group variable selection. We prove that the proposed method can select relevant variables and exclude irrelevant variables with probability one as the sample size goes to infinity, which is referred to as the Oracle Property. Experimental results on simulated and real world datasets show the efficiency of our method in terms of variable selection and prediction accuracy.
","['PURDUE UNIVERSITY', 'Purdue University']"
2018,Clustering Semi-Random Mixtures of Gaussians,"Aravindan Vijayaraghavan, Pranjal Awasthi",https://icml.cc/Conferences/2018/Schedule?showEvent=2323,"Gaussian mixture models (GMM) are the most widely used statistical model for the k-means clustering problem and form a popular framework for clustering in machine learning and data analysis. In this paper, we propose a natural robust model for k-means clustering that generalizes the Gaussian mixture model, and that we believe will be useful in identifying robust algorithms. Our first contribution is a polynomial time algorithm that provably recovers the ground-truth up to small classification error w.h.p., assuming certain separation between the components. Perhaps surprisingly, the algorithm we analyze is the popular Lloyd's algorithm for k-means clustering that is the method-of-choice in practice. Our second result complements the upper bound by giving a nearly matching lower bound on the number of misclassified points incurred by any k-means clustering algorithm on the semi-random model.
","['Northwestern University', 'Rutgers University']"
2018,Leveraging Well-Conditioned Bases: Streaming and Distributed Summaries in Minkowski $p$-Norms,"Charlie Dickens, Graham Cormode, David Woodruff",https://icml.cc/Conferences/2018/Schedule?showEvent=2163,"Work on approximate linear algebra   has led to efficient distributed and streaming   algorithms for    problems such as approximate matrix multiplication, low rank approximation,   and regression, primarily for the Euclidean norm $\ell_2$.   We study other   $\ell_p$ norms, which are more robust for $p < 2$, and can be used   to find outliers for $p > 2$.    Unlike previous algorithms for such norms,  we give algorithms that are (1) deterministic, (2) work simultaneously for every $p \geq 1$, including $p = \infty$, and (3) can be   implemented in both    distributed and streaming environments. We study $\ell_p$-regression,    entrywise $\ell_p$-low rank approximation,    and versions of approximate matrix multiplication.","['Alan Turing Institute & University of Warwick', 'University of Warwick', 'Carnegie Mellon University']"
2018,Learning by Playing - Solving Sparse Reward Tasks from Scratch,"Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Vlad Mnih, Nicolas Heess, Jost Springenberg",https://icml.cc/Conferences/2018/Schedule?showEvent=2179,"We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.
","['DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'Google', 'DeepMind', 'Google Deepmind', 'DeepMind', 'DeepMind']"
2018,Structured Control Nets for Deep Reinforcement Learning,"Mario Srouji, Jian Zhang, Ruslan Salakhutdinov",https://icml.cc/Conferences/2018/Schedule?showEvent=2282,"In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.
","['Stanford University', 'Apple Inc.', 'Carnegie Mellen University']"
2018,Stagewise Safe Bayesian Optimization with Gaussian Processes,"Yanan Sui, Vincent Zhuang, Joel Burdick, Yisong Yue",https://icml.cc/Conferences/2018/Schedule?showEvent=2466,"Enforcing safety is a key aspect of many problems pertaining to sequential decision making under uncertainty, which require the decisions made at every step to be both informative of the optimal decision and also safe. For example, we value both efficacy and comfort in medical therapy, and efficiency and safety in robotic control. We consider this problem of optimizing an unknown utility function with absolute feedback or preference feedback subject to unknown safety constraints. We develop an efficient safe Bayesian optimization algorithm, StageOpt, that separates safe region expansion and utility function maximization into two distinct stages. Compared to existing approaches which interleave between expansion and optimization, we show that StageOpt is more efficient and naturally applicable to a broader class of problems. We provide theoretical guarantees for both the satisfaction of safety constraints as well as convergence to the optimal utility value. We evaluate StageOpt on both a variety of synthetic experiments, as well as in clinical practice.  We demonstrate that StageOpt is more effective than existing safe optimization approaches, and is able to safely and effectively optimize spinal cord stimulation therapy in our clinical experiments.
","['Stanford', 'Caltech', 'Caltech', 'Caltech']"
2018,Bayesian Optimization of Combinatorial Structures,"Ricardo Baptista, Matthias Poloczek",https://icml.cc/Conferences/2018/Schedule?showEvent=2444,"The optimization of expensive-to-evaluate black-box functions over combinatorial structures is an ubiquitous task in machine learning, engineering and the natural sciences. The combinatorial explosion of the search space and costly evalu- ations pose challenges for current techniques in discrete optimization and machine learning, and critically require new algorithmic ideas (NIPS BayesOpt 2017).   This article proposes, to the best of our knowledge, the first algorithm to overcome these challenges, based on an adaptive, scal able model that identifies useful combinatorial structure even when data is scarce. Our acquisition function pioneers the use of semidefinite programming to achieve efficiency and scalability.  Experimental evaluations demonstrate that this algorithm consistently outperforms other methods from combinatorial and Bayesian optimization.
","['Massachusetts Institute of Technology', 'University of Arizona']"
2018,GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models,"Jiaxuan You, Rex (Zhitao) Ying, Xiang Ren, Will Hamilton, Jure Leskovec",https://icml.cc/Conferences/2018/Schedule?showEvent=2373,"Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure.  GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far.  In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.
","['Stanford University', 'Stanford University', 'University of Southern California', 'Stanford University', 'Stanford University']"
2018,Dependent Relational Gamma Process Models for Longitudinal Networks,"Sikun Yang, Heinz Koeppl",https://icml.cc/Conferences/2018/Schedule?showEvent=1942,"A probabilistic framework based on the covariate-dependent relational gamma process is developed to analyze relational data arising from longitudinal networks. The proposed framework characterizes networked nodes by nonnegative node-group memberships, which allow each node to belong to multiple latent groups simultaneously, and encodes edge probabilities between each pair of nodes using a Bernoulli Poisson link to the embedded latent space. Within the latent space, our framework models the birth and death dynamics of individual groups via a thinning function. Our framework also captures the evolution of individual node-group memberships over time using gamma Markov processes. Exploiting the recent advances in data augmentation and marginalization techniques, a simple and efficient Gibbs sampler is proposed for posterior computation. Experimental results on a simulation study and three real-world temporal network data sets demonstrate the model’s capability, competitive performance and scalability compared to state-of-the-art methods.
","['TU Darmstadt', 'TU Darmstadt']"
2018,K-means clustering using random matrix sparsification,Kaushik Sinha,https://icml.cc/Conferences/2018/Schedule?showEvent=2072,"K-means clustering algorithm using Lloyd's heuristic  is one of the most commonly used tools in data mining and machine learning that shows promising performance. However, it suffers from a high computational cost resulting from pairwise Euclidean distance computations between data points and cluster centers in each iteration of Lloyd's heuristic. Main contributing factor of this computational bottle neck is a matrix-vector multiplication step, where the matrix contains all the data points and the vector is a cluster center. In this paper we show that we can randomly sparsify the original data matrix resulting in a sparse data matrix which can significantly speed up the above mentioned matrix vector multiplication step without significantly affecting cluster quality. In particular, we show that optimal k-means clustering solution of the sparse data matrix, obtained by applying random matrix sparsification, results in an approximately optimal k-means clustering objective of the original data matrix. Our empirical studies on three real world datasets corroborate our theoretical findings and demonstrate that our proposed sparsification method can  indeed achieve satisfactory clustering performance.
",['Wichita State University']
2018,Hierarchical Clustering with Structural Constraints,"Evangelos Chatziafratis, Rad Niazadeh, Moses Charikar",https://icml.cc/Conferences/2018/Schedule?showEvent=2058,"Hierarchical clustering is a popular unsupervised data analysis method. For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm. This gives rise to the problem of hierarchical clustering with structural constraints. Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output. In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information (Dasgupta, 2016). We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective. Furthemore, we explore a variation of this objective for dissimilarity information (Cohen-Addad et al., 2018) and improve upon current techniques. Finally, we demonstrate our approach on a real dataset for the taxonomy application.
","['Stanford University', 'Stanford University', 'Stanford University']"
2018,Kronecker Recurrent Units,"Cijo Jose, Mouhamadou Moustapha Cisse, Francois Fleuret",https://icml.cc/Conferences/2018/Schedule?showEvent=2150,"Our work addresses two important issues with recurrent neural networks: (1) they are over-parametrized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem.  We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on seven standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance.  These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.
","['Idiap Research Institute', 'Google', 'Idiap research institute']"
2018,Semi-Supervised Learning via Compact Latent Space Clustering,"Konstantinos Kamnitsas, Daniel C. Castro, Loic Le Folgoc, Ian Walker, Ryutaro Tanno, Daniel Rueckert, Ben Glocker, Antonio Criminisi, Aditya Nori",https://icml.cc/Conferences/2018/Schedule?showEvent=2417,"We present a novel cost function for semi-supervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization  with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data.
","['Imperial College London', 'Imperial College London', 'Imperial College London', 'Imperial College London', 'University College London', 'Imperial College London', 'Imperial College London', 'Microsoft', 'Microsoft Research Cambridge']"
2018,Dynamic Evaluation of Neural Sequence Models,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",https://icml.cc/Conferences/2018/Schedule?showEvent=2197,"We explore dynamic evaluation, where sequence models are adapted to the recent sequence history using gradient descent, assigning higher probabilities to re-occurring sequential patterns. We develop a dynamic evaluation approach that outperforms existing adaptation approaches in our comparisons. We apply dynamic evaluation to outperform all previous word-level perplexities on the Penn Treebank and WikiText-2 datasets (achieving 51.1 and 44.3 respectively) and all previous character-level cross-entropies on the text8 and Hutter Prize datasets (achieving 1.19 bits/char and 1.08 bits/char respectively).
","['University of Edinburgh', 'Edinburgh University', 'University of Edinburgh', 'University of Edinburgh']"
2018,TACO: Learning Task Decomposition via Temporal Alignment for Control,"Kyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, Ingmar Posner",https://icml.cc/Conferences/2018/Schedule?showEvent=2465,"Many advanced Learning from Demonstration (LfD) methods consider the decomposition of complex, real-world tasks into simpler sub-tasks. By reusing the corresponding sub-policies within and between tasks, we can provide training data for each policy from different high-level tasks and compose them to perform novel ones. Existing approaches to modular LfD focus either on learning a single high-level task or depend on domain knowledge and temporal segmentation. In contrast, we propose a weakly supervised, domain-agnostic approach based on task sketches, which include only the sequence of sub-tasks performed in each demonstration. Our approach simultaneously aligns the sketches with the observed demonstrations and learns the required sub-policies. This improves generalisation in comparison to separate optimisation procedures. We evaluate the approach on multiple domains, including a simulated 3D robot arm control task using purely image-based observations.  The results show that our approach performs commensurately with fully supervised approaches, while requiring significantly less annotation effort.
","['Latent Logic LTD', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford']"
2018,A Spectral Approach to Gradient Estimation for Implicit Distributions,"Jiaxin Shi, Shengyang Sun, Jun Zhu",https://icml.cc/Conferences/2018/Schedule?showEvent=2490,"Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr{\""o}m method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{\""o}m method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.
","['Tsinghua University', 'University of Toronto', 'Tsinghua University']"
2018,Quasi-Monte Carlo Variational Inference,"Alexander Buchholz, Florian Wenzel, Stephan Mandt",https://icml.cc/Conferences/2018/Schedule?showEvent=2288,"Many machine learning problems involve Monte Carlo gradient estimators. As a prominent example,  we focus on Monte Carlo variational inference (MCVI)  in this paper. The performance of MCVI crucially depends on the variance of its stochastic gradients. We propose variance  reduction by means of Quasi-Monte Carlo (QMC) sampling.  QMC replaces N i.i.d. samples from a uniform probability  distribution by a deterministic sequence of samples of length N. This sequence covers the underlying random variable space more evenly than i.i.d. draws, reducing the variance of the gradient estimator. With our novel approach, both the score function and the reparameterization  gradient estimators lead to much faster convergence.  We also propose a new algorithm for Monte Carlo objectives, where we operate with a constant learning rate and increase the number of QMC samples per iteration. We prove that this way, our algorithm can converge asymptotically at a faster rate than SGD . We furthermore provide theoretical guarantees on qmc for Monte Carlo objectives that go beyond MCVI , and support our findings by several experiments on large-scale data sets from various domains.
","['ENSAE-CREST Paris', 'University of Kaiserslautern', 'UC Irvine']"
2018,Learning to Optimize Combinatorial Functions,"Nir Rosenfeld, Eric Balkanski, Amir Globerson, Yaron Singer",https://icml.cc/Conferences/2018/Schedule?showEvent=1881,"Submodular functions have become a ubiquitous tool in machine learning. They are learnable from data, and can be optimized efficiently and with guarantees. Nonetheless, recent negative results show that optimizing learned surrogates of submodular functions can result in arbitrarily bad approximations of the true optimum. Our goal in this paper is to highlight the source of this hardness, and propose an alternative criterion for optimizing general combinatorial functions from sampled data. We prove a tight equivalence showing that a class of functions is optimizable if and only if it can be learned. We provide efficient and scalable optimization algorithms for several function classes of interest, and demonstrate their utility on the task of optimally choosing trending social media items.
","['Harvard University', 'Harvard', 'Tel Aviv University, Google', 'Harvard']"
2018,"Proportional Allocation: Simple, Distributed, and Diverse Matching with High Entropy","Shipra Agarwal, Morteza Zadimoghaddam, Vahab Mirrokni",https://icml.cc/Conferences/2018/Schedule?showEvent=2388,"Inspired by many applications of bipartite matching in online advertising and machine learning, we study a simple and natural iterative proportional allocation algorithm: Maintain a priority score $\priority_a$ for each node $a\in\advertisers$ on one side of the bipartition, initialized as $\priority_a=1$. Iteratively allocate the nodes $i\in \impressions$ on the other side to eligible nodes in $\advertisers$ in proportion of their priority scores. After each round, for each node $a\in \advertisers$, decrease or increase the score $\priority_a$ based on whether it is over- or under- allocated.  Our first result is that this simple, distributed algorithm converges to a $(1-\epsilon)$-approximate fractional $b$-matching solution in $O({\log n\over \epsilon^2} )$ rounds. We also extend the proportional allocation algorithm and convergence results to the maximum weighted matching problem, and show that the algorithm can be naturally tuned to produce maximum matching with {\em high entropy}. High entropy, in turn, implies additional desirable properties of this matching, e.g., it satisfies certain diversity and fairness (aka anonymity) properties that are desirable in a variety of applications in online advertising and machine learning.","['Columbia', 'Google', 'Google Research']"
2018,Representation Learning on Graphs with Jumping Knowledge Networks,"Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, Stefanie Jegelka",https://icml.cc/Conferences/2018/Schedule?showEvent=2453,"Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ""neighboring"" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.
","['MIT', 'MIT', 'MIT', 'National Institute of Informatics', 'National Institute of Informatics', 'MIT']"
2018,NetGAN: Generating Graphs via Random Walks,"Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zügner, Stephan Günnemann",https://icml.cc/Conferences/2018/Schedule?showEvent=2193,"We propose NetGAN - the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.
","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich']"
2018,INSPECTRE: Privately Estimating the Unseen,"Jayadev Acharya, Gautam Kamath, Ziteng Sun, Huanyu Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=1872,"We develop differentially private methods for estimating various distributional properties. Given a sample from a discrete distribution p, some functional f, and accuracy and privacy parameters alpha and epsilon, the goal is to estimate f(p) up to accuracy alpha, while maintaining epsilon-differential privacy of the sample. We prove almost-tight bounds on the sample size required for this problem for several functionals of interest, including support size, support coverage, and entropy. We show that the cost of privacy is negligible in a variety of settings, both theoretically and experimentally. Our methods are based on a sensitivity analysis of several state-of-the-art methods for estimating these properties with sublinear sample complexities
","['Cornell University', 'MIT', 'Cornell University', 'Cornell University']"
2018,Locally Private Hypothesis Testing,Or Sheffet,https://icml.cc/Conferences/2018/Schedule?showEvent=2071,"We initiate the study of differentially private hypothesis testing in the local-model, under both the standard (symmetric) randomized-response mechanism (Warner 1965, Kasiviswanathan et al, 2008} and the newer (non-symmetric) mechanisms (Bassily & Smith, 2015, Bassily et al, 2017). First, we study the general framework of mapping each user's type into a signal and show that the problem of finding the maximum-likelihood distribution over the signals is feasible. Then we discuss the randomized-response mechanism and show that, in essence, it maps the null- and alternative-hypotheses onto new sets, an affine translation of the original sets. We then give sample complexity bounds for identity and independence testing under randomized-response. We then move to the newer non-symmetric mechanisms and show that there too the problem of finding the maximum-likelihood distribution is feasible. Under the mechanism of Bassily et al we give identity and independence testers with better sample complexity than the testers in the symmetric case, and we also propose a $\chi^2$-based identity tester which we investigate empirically.",['University of Alberta']
2018,Latent Space Policies for Hierarchical Reinforcement Learning,"Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, Sergey Levine",https://icml.cc/Conferences/2018/Schedule?showEvent=2334,"We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.
","['UC Berkeley', 'UC Berkeley', 'OpenAI / UC Berkeley', 'Berkeley']"
2018,More Robust Doubly Robust Off-policy Evaluation,"Mehrdad Farajtabar, Yinlam Chow, Mohammad Ghavamzadeh",https://icml.cc/Conferences/2018/Schedule?showEvent=2068,"We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t. the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.
","['Georgia Tech', 'DeepMind', 'Facebook AI Research']"
2018,Learning to Explain: An Information-Theoretic Perspective on Model Interpretation,"Jianbo Chen, Le Song, Martin Wainwright, Michael Jordan",https://icml.cc/Conferences/2018/Schedule?showEvent=1957,"We introduce instancewise feature selection as a methodology for model interpretation.  Our method is based on learning a function to extract a subset of features that are most informative for each given example.  This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.
","['University of California, Berkeley', 'Georgia Institute of Technology', 'University of California at Berkeley', 'UC Berkeley']"
2018,End-to-end Active Object Tracking via Reinforcement Learning,"Wenhan Luo, Peng Sun, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=1889,"We study active object tracking, where a tracker takes as input the visual observation (\ie, frame sequence) and produces the camera control signal (\eg, move forward, turn left, \etc). Conventional methods tackle the tracking and the camera control separately, which is challenging to tune jointly. It also incurs many human efforts for labeling and many expensive trial-and-errors in real-world. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for a successful training. The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving path, unseen object appearance, unseen background, and distracting object. It can restore tracking when occasionally losing the target. With the experiments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios.
","['Tencent AI Lab', 'Tencent AI Lab', 'Peking University', 'Tencent AI Lab', 'Tecent AI Lab', 'Peking University']"
2018,Efficient and Consistent Adversarial Bipartite Matching,"Rizal Fathony, Sima Behpour, Xinhua Zhang, Brian Ziebart",https://icml.cc/Conferences/2018/Schedule?showEvent=2162,"Many important structured prediction problems, including learning to rank items, correspondence-based natural language processing, and multi-object tracking, can be formulated as weighted bipartite matching optimizations. Existing structured prediction approaches have significant drawbacks when applied under the constraints of perfect bipartite matchings. Exponential family probabilistic models, such as the conditional random field (CRF), provide statistical consistency guarantees, but suffer computationally from the need to compute the normalization term of its distribution over matchings, which is a #P-hard matrix permanent computation. In contrast, the structured support vector machine (SSVM) provides computational efficiency, but lacks Fisher consistency, meaning that there are distributions of data for which it cannot learn the optimal matching even under ideal learning conditions (i.e., given the true distribution and selecting from all measurable potential functions). We propose adversarial bipartite matching to avoid both of these limitations. We develop this approach algorithmically, establish its computational efficiency and Fisher consistency properties, and apply it to matching problems that demonstrate its empirical benefits.
","['University of Illinois at Chicago', 'University of Illinois at Chicago', 'University of Illinois at Chicago', 'University of Illinois at Chicago']"
2018,SparseMAP: Differentiable Sparse Structured Inference,"Vlad Niculae, Andre Filipe Torres Martins, Mathieu Blondel, Claire Cardie",https://icml.cc/Conferences/2018/Schedule?showEvent=2043,"Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP, a new method for sparse structured inference, together with corresponding loss functions. SparseMAP inference is able to automatically select only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, hence it is applicable even to problems where marginal inference is intractable, such as linear assignment. Moreover, thanks to the solution sparsity, gradient backpropagation is efficient regardless of the structure. SparseMAP thus enables us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.
","['Cornell University', 'Instituto de Telecomunicacoes', 'NTT', 'Cornell University']"
2018,Bilevel Programming for Hyperparameter Optimization and Meta-Learning,"Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, Massimiliano Pontil",https://icml.cc/Conferences/2018/Schedule?showEvent=2248,"We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective.  Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of  deep learning where representation layers are treated as hyperparameters shared across a set of training episodes.  In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.
","['Istituto Italiano di Tecnologia - University College London', 'University of Florence', 'Istituto Italiano di Tecnologia', 'Istituto Italiano di Tecnologia', 'University College London']"
2018,Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory,"Ron Amit, Ron Meir",https://icml.cc/Conferences/2018/Schedule?showEvent=1976,"In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learning of novel future tasks. Under the assumption that future tasks are `related’ to previous tasks,  representations should be learned in such a way that they capture the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of a new task. We present a framework for meta-learning that is based on generalization error bounds, allowing us to extend various PAC-Bayes bounds to meta-learning. Learning takes place through the construction of a distribution over hypotheses based on the observed tasks, and its utilization for learning a new task. Thus, prior knowledge is incorporated through setting an experience-dependent prior for novel tasks. We develop a gradient-based algorithm, and implement it for deep neural networks, based on minimizing an objective function derived from the bounds, and demonstrate its effectiveness numerically. In addition to establishing the improved performance available through meta-learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.
","['Technion – Israel Institute of Technology', 'Technion Israeli Institute of Technology']"
2018,Parameterized Algorithms for the Matrix Completion Problem,"Robert Ganian, DePaul Iyad Kanj, Sebastian Ordyniak, Stefan Szeider",https://icml.cc/Conferences/2018/Schedule?showEvent=2254,"We consider two matrix completion problems, in which we are given a matrix with missing entries and the task is to complete the matrix in a way that (1) minimizes the rank, or (2) minimizes the number of distinct rows. We study the parameterized complexity of the two aforementioned problems with respect to several parameters of interest, including the minimum number of matrix rows, columns, and rows plus columns needed to cover all missing entries. We obtain new algorithmic results showing that, for the bounded domain case, both problems are fixed-parameter tractable with respect to all aforementioned parameters. We complement these results with a lower-bound result for the unbounded domain case that rules out fixed-parameter tractability w.r.t. some of the parameters under consideration.
","['TU Wien', 'DePaul University, Chicago', 'University of Sheffield', 'TU Wien']"
2018,Nearly Optimal Robust Subspace Tracking,"Praneeth Narayanamurthy, Namrata Vaswani",https://icml.cc/Conferences/2018/Schedule?showEvent=2285,"Robust subspace tracking (RST) can  be simply understood as a dynamic (time-varying) extension of robust PCA.  More precisely, it is the problem of tracking data lying in a fixed or slowly-changing low-dimensional subspace while being robust to sparse outliers. This work develops a recursive projected compressive sensing algorithm called  ``Nearly Optimal RST (NORST)'', and obtains one of the first guarantees for it. We show that NORST provably solves RST under weakened standard RPCA assumptions, slow subspace change, and a lower bound on (most) outlier magnitudes.  Our guarantee shows that (i) NORST is online (after initialization) and enjoys near-optimal values of tracking delay, lower bound on  required delay between subspace change times, and of memory complexity; and (ii) it has a significantly improved worst-case outlier tolerance compared with all previous robust PCA or RST methods without requiring any model on how the outlier support is generated.
","['Iowa State University', 'Iowa State University']"
2018,Katyusha X: Simple Momentum Method for Stochastic Sum-of-Nonconvex Optimization,Zeyuan Allen-Zhu,https://icml.cc/Conferences/2018/Schedule?showEvent=2017,"The problem of minimizing sum-of-nonconvex functions (i.e., convex functions that are average of non-convex ones) is becoming increasing important in machine learning, and is the core machinery for PCA, SVD, regularized Newton's method, accelerated non-convex optimization, and more. We show how to provably obtain an accelerated stochastic algorithm for minimizing sum-of-nonconvex functions, by adding one additional line to the well-known SVRG method. This line corresponds to momentum, and shows how to directly apply momentum to the finite-sum stochastic minimization of sum-of-nonconvex functions. As a side result, our method enjoys linear parallel speed-up using mini-batch.
",['Microsoft Research AI']
2018,signSGD: Compressed Optimisation for Non-Convex Problems,"Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, Anima Anandkumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2356,"Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck.  signSGD alleviates this problem by  transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. The relative $\ell_1/\ell_2$ geometry of gradients, noise and curvature informs whether signSGD or SGD is theoretically better suited to a particular problem. On the practical side we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss we prove that majority vote can achieve the same reduction in variance as full precision distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve fast communication and fast convergence. Code to reproduce experiments is to be found at https://github.com/jxbz/signSGD.","['Caltech', 'UC Santa Barbara', 'UC Irvine/Caltech', 'Amazon AI & Caltech']"
2018,Synthesizing Robust Adversarial Examples,"Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok",https://icml.cc/Conferences/2018/Schedule?showEvent=2307,"Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.
","['MIT CSAIL', 'MIT', 'Massachusetts Institute of Technology', 'LabSix']"
2018,Differentiable Abstract Interpretation for Provably Robust Neural Networks,"Matthew Mirman, Timon Gehr, Martin Vechev",https://icml.cc/Conferences/2018/Schedule?showEvent=2477,"We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.
","['ETH Zürich', 'ETH Zurich', 'ETH Zurich']"
2018,Stochastic Training of Graph Convolutional Networks with Variance Reduction,"Jianfei Chen, Jun Zhu, Le Song",https://icml.cc/Conferences/2018/Schedule?showEvent=2169,"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms with new theoretical guarantee to converge to a local optimum of GCN regardless of the neighbor sampling size. Empirical results show that our algorithms enjoy similar convergence rate and model quality with the exact algorithm using only two neighbors per node. The running time of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.
","['Tsinghua University', 'Tsinghua University', 'Georgia Institute of Technology']"
2018,Neural Relational Inference for Interacting Systems,"Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, Richard Zemel",https://icml.cc/Conferences/2018/Schedule?showEvent=2083,"Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.
","['University of Amsterdam', 'University of Toronto', 'Univeristy of Toronto', 'University of Amsterdam', 'Vector Institute']"
2018,Which Training Methods for GANs do actually Converge?,"Lars Mescheder, Andreas Geiger, Sebastian Nowozin",https://icml.cc/Conferences/2018/Schedule?showEvent=1900,"Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distributions lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.
","['MPI Tübingen', 'MPI-IS and University of Tuebingen', 'Microsoft Research']"
2018,Learning Independent Causal Mechanisms,"Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, Bernhard Schölkopf",https://icml.cc/Conferences/2018/Schedule?showEvent=2049,"Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between observables.  Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.
","['Max Planck Institute for Intelligent Systems and ETH Zurich', 'MPI Tübingen & Cambridge', 'Cambridge/MPI', 'MPI for Intelligent Systems Tübingen, Germany']"
2018,Nonconvex Optimization for Regression with Fairness Constraints,"Junpei Komiyama, Akiko Takeda, Junya Honda, Hajime Shimao",https://icml.cc/Conferences/2018/Schedule?showEvent=2037,"The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer.  Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Furthermore, we propose a nonlinear extension of the method by kernel representation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.
","['U-Tokyo', 'The Institute of Statistical Mathematics', 'University of Tokyo / RIKEN', 'Purdue University']"
2018,Fairness Without Demographics in Repeated Loss Minimization,"Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, Percy Liang",https://icml.cc/Conferences/2018/Schedule?showEvent=2434,"Machine learning models (e.g., speech recognizers) trained on average loss suffer from representation disparity---minority groups (e.g., non-native speakers) carry less weight in the training objective, and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even turn initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.
","['Stanford', 'Stanford University', 'Stanford University', 'Stanford University']"
2018,MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning,"Bo Zhao, Xinwei Sun, Yanwei Fu, Yuan Yao, Yizhou Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=1897,"It is one typical and general topic of learning a good embedding model to efficiently learn the representation coefficients between two spaces/subspaces. To solve this task, $L_{1}$ regularization is widely used for the pursuit of feature selection and avoiding overfitting, and yet the sparse estimation of features in $L_{1}$ regularization may cause the underfitting of training data. $L_{2}$ regularization is also frequently used, but it is a biased estimator. In this paper, we propose the idea that the features consist of three orthogonal parts, \emph{namely} sparse strong signals, dense weak signals and random noise, in which both strong and weak signals contribute to the fitting of data. To facilitate such novel decomposition, \emph{MSplit} LBI is for the first time proposed to realize feature selection and dense estimation simultaneously. We provide theoretical and simulational verification that our method exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental results show that our method achieves state-of-the-art performance in the few-shot and zero-shot learning.","['Peking University', 'Peking University', 'Fudan university', 'Hong Kong Science Tech', 'Peking University']"
2018,Nonoverlap-Promoting Variable Selection,"Pengtao Xie, Hongbao Zhang, Yichen Zhu, Eric Xing",https://icml.cc/Conferences/2018/Schedule?showEvent=1886,"Variable selection is a classic problem in machine learning (ML), widely used to find important explanatory factors, and improve generalization performance and interpretability of ML models. In this paper, we consider variable selection for models where multiple responses are to be predicted based on the same set of covariates. Since each response is relevant to a unique subset of covariates, we desire the selected variables for different responses have small overlap. We propose a regularizer that simultaneously encourage orthogonality and sparsity, which jointly brings in an effect of reducing overlap. We apply this regularizer to four model instances and develop efficient algorithms to solve the regularized problems. We provide a formal analysis on why the proposed regularizer can reduce generalization error. Experiments on both simulation studies and real-world datasets demonstrate the effectiveness of the proposed regularizer in selecting less-overlapped variables and improving generalization performance.
","['Carnegie Mellon University', 'Petuum Inc', 'Peking University', 'Petuum Inc. and CMU']"
2018,Towards More Efficient Stochastic Decentralized Learning: Faster Convergence and Sparse Communication,"Zebang Shen, Aryan Mokhtari, Tengfei Zhou, Peilin Zhao, Hui Qian",https://icml.cc/Conferences/2018/Schedule?showEvent=2229,"Recently, the decentralized optimization problem is attracting growing attention. Most existing methods are deterministic with high per-iteration cost and have a convergence rate quadratically depending on the problem condition number. Besides, the dense communication is necessary to ensure the convergence even if the dataset is sparse. In this paper, we generalize the decentralized optimization problem to a monotone operator root finding problem, and propose a stochastic algorithm named DSBA that (1) converges geometrically with a rate linearly depending on the problem condition number, and (2) can be implemented using sparse communication only. Additionally, DSBA handles important learning problems like AUC-maximization which can not be tackled efficiently in the previous problem setting. Experiments on convex minimization and AUC-maximization validate the efficiency of our method.
","['Zhejiang University', 'MIT', 'Zhejiang University', 'Artificial Intelligence Department, Ant \u200bFinancial', 'Zhejiang University']"
2018,Graph Networks as Learnable Physics Engines for Inference and Control,"Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Springenberg, Josh Merel, Martin Riedmiller, Raia Hadsell, Peter Battaglia",https://icml.cc/Conferences/2018/Schedule?showEvent=2220,"Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models--based on graph networks--which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2018,An Alternative View: When Does SGD Escape Local Minima?,"Bobby Kleinberg, Yuanzhi Li, Yang Yuan",https://icml.cc/Conferences/2018/Schedule?showEvent=1941,"Stochastic gradient descent (SGD) is widely used in machine learning. Although being commonly viewed as a fast but not accurate version of gradient descent (GD), it always finds better solutions than GD for modern neural networks. In order to understand this phenomenon, we take an alternative view that SGD is working on the convolved (thus smoothed) version of the loss function. We show that, even if the function $f$ has many bad local minima or saddle points, as long as for every point $x$, the weighted average of the gradients of its neighborhoods is one point convex with respect to the desired solution $x^*$, SGD will get close to, and then stay around $x^*$ with constant probability. Our result identifies a set of functions that SGD provably works, which is much larger than the set of convex functions. Empirically, we observe that the loss surface of neural networks enjoys nice one point convexity properties locally, therefore our theorem helps explain why SGD works so well for neural networks.","['Cornell', 'Princeton University', 'Cornell University']"
2018,Asynchronous Decentralized Parallel Stochastic Gradient Descent,"Xiangru Lian, Wei Zhang, Ce Zhang, Ji Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2423,"Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal $O(1/\sqrt{K})$ rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8x faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.","['University of Rochester', 'IBM Research', 'ETH Zurich', 'University of Rochester']"
2018,An Estimation and Analysis Framework for the Rasch Model,"Andrew Lan, Mung Chiang, Christoph Studer",https://icml.cc/Conferences/2018/Schedule?showEvent=1977,"The Rasch model is widely used for item response analysis in applications ranging from recommender systems to psychology, education, and finance. While a number of estimators have been proposed for the Rasch model over the last decades, the associated analytical performance guarantees are mostly asymptotic. This paper provides a framework that relies on a novel linear minimum mean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error under the Rasch model. The proposed framework provides guidelines on the number of items and responses required to attain low estimation errors in tests or surveys. We furthermore demonstrate its efficacy on a number of real-world collaborative filtering datasets, which reveals that the proposed L-MMSE estimator performs on par with state-of-the-art nonlinear estimators in terms of predictive performance.
","['Princeton University', 'Purdue University', 'Cornell University']"
2018,Mitigating Bias in Adaptive Data Gathering via Differential Privacy,"Seth Neel, Aaron Roth",https://icml.cc/Conferences/2018/Schedule?showEvent=2339,"Data that is gathered adaptively --- via bandit algorithms, for example --- exhibits bias. This is true both when gathering simple numeric valued data --- the empirical means kept track of by stochastic bandit algorithms are biased downwards --- and when gathering more complicated data --- running hypothesis tests on complex data gathered via contextual bandit algorithms leads to false discovery. In this paper, we show that this problem is mitigated if the data collection procedure is differentially private. This lets us both bound the bias of simple numeric valued quantities (like the empirical means of stochastic bandit algorithms), and correct the p-values of hypothesis tests run on the adaptively gathered data. Moreover, there exist differentially private bandit algorithms with near optimal regret bounds: we apply existing theorems in the simple stochastic case, and give a new analysis for linear contextual bandits. We complement our theoretical results with experiments validating our theory.
","['University of Pennsylvania', 'University of Pennsylvania']"
2018,Local Private Hypothesis Testing: Chi-Square Tests,"Marco Gaboardi, Ryan Rogers",https://icml.cc/Conferences/2018/Schedule?showEvent=2346,"The local model for differential privacy is emerging as the reference model for practical applications of collecting  and sharing sensitive information while satisfying strong privacy guarantees.  In the local model, there is no trusted entity which is allowed to have each individual's raw data as is assumed in the traditional curator model. Individuals' data are usually perturbed before sharing them. We explore the design of private hypothesis tests in the local model, where each data entry is perturbed to ensure the privacy of each participant.  Specifically, we analyze locally private chi-square tests for goodness of fit and independence testing.
","['Univeristy at Buffalo', 'Apple']"
2018,Disentangling by Factorising,"Hyunjik Kim, Andriy Mnih",https://icml.cc/Conferences/2018/Schedule?showEvent=2146,"We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.
","['DeepMind, University of Oxford', 'DeepMind']"
2018,Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning,"Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, Ronald Ortner",https://icml.cc/Conferences/2018/Schedule?showEvent=2036,"We introduce SCAL, an algorithm designed to perform efficient exploration-exploration in any unknown weakly-communicating Markov Decision Process (MDP) for which an upper bound c on the span of the optimal bias function is known. For an MDP with $S$ states, $A$ actions and $\Gamma \leq S$ possible next states, we prove a regret bound of $O(c\sqrt{\Gamma SAT})$, which significantly improves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter $D$. In fact, the optimal bias span is finite and often much smaller than $D$ (e.g., $D=+\infty$ in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this paper, we relax the optimization problem at the core of REGAL.C, we carefully analyze its properties, and we provide the first computationally efficient algorithm to solve it. Finally, we report numerical simulations supporting our theoretical findings and showing how SCAL significantly outperforms UCRL in MDPs with large diameter and small span.","['Inria Lille Nord-Europe', 'SequeL - Inria Lille - Nord Europe', 'Facebook AI Research', 'Montanuniversitaet Leoben']"
2018,Learning to search with MCTSnets,"Arthur Guez, Theophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Remi Munos, David Silver",https://icml.cc/Conferences/2018/Schedule?showEvent=2233,"Planning problems are among the most important and well-studied problems in artificial intelligence. They are most typically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-up those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, powerful and widely used. A typical implementation of MCTS uses cleverly designed rules, optimised to the particular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-up those evaluations. In this paper we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incorporates simulation-based search inside a neural network, by expanding, evaluating and backing-up a vector embedding. The parameters of the network are trained end-to-end using gradient-based optimisation. When applied to small searches in the well-known planning problem Sokoban, the learned search algorithm significantly outperformed MCTS baselines.
","['Google DeepMind', 'DeepMind', 'Deepmind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind', 'Google DeepMind']"
2018,Decoupled Parallel Backpropagation with Convergence Guarantee,"Zhouyuan Huo, Bin Gu, Qian Yang, Heng Huang",https://icml.cc/Conferences/2018/Schedule?showEvent=2106,"Backpropagation algorithm is indispensable for the training of feedforward neural networks. It requires propagating error gradients sequentially from the output layer all the way back to the input layer. The backward locking in backpropagation algorithm constrains us from updating network layers in parallel and fully leveraging the computing resources. Recently, several algorithms have been proposed for breaking the backward locking. However, their performances degrade seriously when networks are deep. In this paper, we propose decoupled parallel backpropagation algorithm for deep learning optimization with convergence guarantee. Firstly, we decouple the backpropagation algorithm using delayed gradients, and show that the backward locking is removed when we split the networks into multiple modules. Then, we utilize decoupled parallel backpropagation in two stochastic methods and prove that our method guarantees convergence to critical points for the non-convex problem. Finally, we perform experiments for training deep convolutional neural networks on benchmark datasets. The experimental results not only confirm our theoretical analysis, but also demonstrate that the proposed method can achieve significant speedup without loss of accuracy.
","['University of Pittsburgh', 'University of Pittsburgh', 'University of Pittsburgh', 'University of Pittsburgh']"
2018,On Learning Sparsely Used Dictionaries from Incomplete Samples,"Thanh Nguyen, Akshay Soni, Chinmay Hegde",https://icml.cc/Conferences/2018/Schedule?showEvent=2099,"Existing algorithms for dictionary learning assume that the entries of the (high-dimensional) input data are fully observed. However, in several practical applications, only an incomplete fraction of the data entries may be available. For incomplete settings, no provably correct and polynomial-time algorithm has been reported in the dictionary learning literature. In this paper, we provide provable approaches for learning -- from incomplete samples -- a family of dictionaries whose atoms have sufficiently ``spread-out'' mass. First, we propose a descent-style iterative algorithm that linearly converges to the true dictionary when provided a sufficiently coarse initial estimate. Second, we propose an initialization algorithm that utilizes a small number of extra fully observed samples to produce such a coarse initial estimate. Finally, we theoretically analyze their performance and provide asymptotic statistical and computational guarantees.
","['Iowa State University', 'Yahoo Research', 'Iowa State University']"
2018,Variational Network Inference: Strong and Stable with Concrete Support,"Amir Dezfouli, Edwin Bonilla, Richard Nock",https://icml.cc/Conferences/2018/Schedule?showEvent=1878,"Traditional methods for the discovery of latent network structures are limited in two ways: they either assume that all the signal comes from the network (i.e. there is no source of signal outside the network) or they place constraints on the network parameters to ensure model or algorithmic stability. We address these limitations by proposing a  model that incorporates a Gaussian process prior on a network-independent component and formally proving that we get algorithmic stability for free while providing a novel perspective on model stability as well as robustness results and precise intervals for key inference parameters. We show that, on three applications, our approach outperforms previous methods consistently.
","['UNSW', 'UNSW', 'Data61, The Australian National University and the University of Sydney']"
2018,Weakly Submodular Maximization Beyond Cardinality Constraints: Does Randomization Help Greedy?,"Lin Chen, Moran Feldman, Amin Karbasi",https://icml.cc/Conferences/2018/Schedule?showEvent=2031,"Submodular functions are a broad class of set functions that naturally arise in many machine learning applications. Due to their combinatorial structures, there has been a myriad of algorithms for maximizing such functions under various constraints. Unfortunately, once a function deviates from submodularity (even slightly), the known algorithms may perform arbitrarily poorly. Amending this issue, by obtaining approximation results for functions obeying properties that generalize submodularity, has been the focus of several recent works. One such class, known as weakly submodular functions, has received a lot of recent attention from the machine learning community due to its strong connections to restricted strong convexity and sparse reconstruction. In this paper, we prove that a randomized version of the greedy algorithm achieves an approximation ratio of $(1 + 1/\gamma )^{-2}$ for weakly submodular maximization subject to a general matroid constraint, where $\gamma$ is a parameter measuring the distance from submodularity. To the best of our knowledge, this is the first algorithm with a non-trivial approximation guarantee for this constrained optimization problem. Moreover, our experimental results show that our proposed algorithm performs well in a variety of real-world problems, including regression, video summarization, splice site detection, and black-box interpretation.","['Yale University', 'The Open University of Israel', 'Yale']"
2018,Data Summarization at Scale: A Two-Stage Submodular Approach,"Marko Mitrovic, Ehsan Kazemi, Morteza Zadimoghaddam, Amin Karbasi",https://icml.cc/Conferences/2018/Schedule?showEvent=2144,"The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify representative elements in a dataset. Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-optimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that optimizing new functions (drawn from the same distribution) over the reduced set provides almost as much value as optimizing them over the entire ground set. In this paper, we develop the first streaming and distributed solutions to this problem. In addition to providing strong theoretical guarantees, we demonstrate both the utility and efficiency of our algorithms on real-world tasks including image summarization and ride-share optimization.
","['Yale University', 'Yale', 'Google', 'Yale']"
2018,Best Arm Identification in Linear Bandits with Linear Dimension Dependency,"Chao Tao, Saúl A. Blanco, Yuan Zhou",https://icml.cc/Conferences/2018/Schedule?showEvent=1983,"We study the best arm identification problem in linear bandits, where the mean reward of each arm depends linearly on an unknown $d$-dimensional parameter vector $\theta$, and the goal is to identify the arm with the largest expected reward. We first design and analyze a novel randomized $\theta$ estimator based on the solution to the convex relaxation of an optimal $G$-allocation experiment design problem. Using this estimator, we describe an algorithm whose sample complexity depends linearly on the dimension $d$, as well as an algorithm with sample complexity dependent on the reward gaps of the best $d$ arms, matching the lower bound arising from the ordinary top-arm identification problem. We finally compare the empirical performance of our algorithms with other state-of-the-art algorithms in terms of both sample complexity and computational time.","['Indiana University Bloomington', 'Indiana University', 'Indiana University Bloomington']"
2018,Learning with Abandonment,"Sven Schmit, Ramesh Johari",https://icml.cc/Conferences/2018/Schedule?showEvent=1965,"Consider a platform that wants to learn a personalized policy for each user, but the platform faces the risk of a user abandoning the platform if they are dissatisfied with the actions of the platform.  For example, a platform is interested in personalizing the number of newsletters it sends, but faces the risk that the user unsubscribes forever.  We propose a general thresholded learning model for scenarios like this, and discuss the structure of optimal policies.  We describe salient features of optimal personalization algorithms and how feedback the platform receives impacts the results.  Furthermore, we investigate how the platform can efficiently learn the heterogeneity across users by interacting with a population and provide performance guarantees.
","['Stanford University', 'Stanford University']"
2018,Hyperbolic Entailment Cones for Learning Hierarchical Embeddings,"Octavian-Eugen Ganea, Gary Becigneul, Thomas Hofmann",https://icml.cc/Conferences/2018/Schedule?showEvent=2487,"Learning graph representations via low-dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces, and they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.
","['ETH Zurich', 'ETHZ', 'ETH Zurich']"
2018,Generative Temporal Models with Spatial Memory for Partially Observed Environments,"Marco Fraccaro, Danilo J. Rezende, Yori Zwols, Alexander Pritzel, S. M. Ali Eslami, Fabio Viola",https://icml.cc/Conferences/2018/Schedule?showEvent=2128,"In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent's representations during training or via use as part of an explicit planning mechanism. However, their application in practice has been limited to simplistic environments, due to the difficulty of training such models in larger, potentially partially-observed and 3D environments. In this work we introduce a novel action-conditioned generative model of such challenging environments. The model features a non-parametric spatial memory system in which we store learned, disentangled representations of the environment. Low-dimensional spatial updates are computed using a state-space model that makes use of knowledge on the prior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder. The result is a scalable architecture capable of performing coherent predictions over hundreds of time steps across a range of partially observed 2D and 3D environments.
","['Technical University of Denmark', 'DeepMind', 'DeepMind', 'Deepmind', 'DeepMind', 'DeepMind']"
2018,DiCE: The Infinitely Differentiable Monte Carlo Estimator,"Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rocktäschel, Eric Xing, Shimon Whiteson",https://icml.cc/Conferences/2018/Schedule?showEvent=2266,"The score function estimator is widely used for estimating gradients of stochastic objectives in stochastic computation graphs (SCG), eg., in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is computationally and conceptually simple, using the same approach for higher-order derivatives is more challenging. Firstly, analytically deriving and implementing such estimators is laborious and not compliant with automatic differentiation. Secondly, repeatedly applying SL to construct new objectives for each order derivative involves increasingly cumbersome graph manipulations. Lastly, to match the first-order gradient under differentiation, SL treats part of the cost as a fixed sample, which we show leads to missing and wrong terms for estimators of higher-order derivatives. To address all these shortcomings in a unified way, we introduce DiCE, which provides a single objective that can be differentiated repeatedly, generating correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for performing the requisite graph manipulations. We verify the correctness of DiCE both through a proof and numerical evaluation of the DiCE derivative estimates. We also use DiCE to propose and evaluate a novel approach for multi-agent learning. Our code is available at https://goo.gl/xkkGxN.
","['Facebook AI Research', 'University of Oxford', 'Carnegie Mellon University', 'University of Oxford', 'Petuum Inc. and CMU', 'University of Oxford']"
2018,Orthogonal Recurrent Neural Networks with Scaled Cayley Transform,"Kyle Helfrich, Devin Willmott, Qiang Ye",https://icml.cc/Conferences/2018/Schedule?showEvent=2397,"Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients.  Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs).  We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform; such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.  The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.
","['University of Kentucky', 'University of Kentucky', 'University of Kentucky']"
2018,Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator,"Stephen Tu, Benjamin Recht",https://icml.cc/Conferences/2018/Schedule?showEvent=2004,"Reinforcement learning (RL) has been successfully used to solve many continuous control tasks. Despite its impressive results however, fundamental questions regarding the sample complexity of RL on continuous problems remain open.  We study the performance of RL in this setting by considering the behavior of the Least-Squares Temporal Difference (LSTD) estimator on the classic Linear Quadratic Regulator (LQR) problem from optimal control.  We give the first finite-time analysis of the number of samples needed to estimate the value function for a fixed static state-feedback policy to within epsilon-relative error.  In the process of deriving our result, we give a general characterization for when the minimum eigenvalue of the empirical covariance matrix formed along the sample path of a fast-mixing stochastic process concentrates above zero, extending a result by Koltchinskii and Mendelson in the independent covariates setting.  Finally, we provide experimental evidence indicating that our analysis correctly captures the qualitative behavior of LSTD on several LQR instances.
","['UC Berkeley', 'Berkeley']"
2018,Spotlight: Optimizing Device Placement for Training Deep Neural Networks,"Yuanxiang Gao, Li Chen, Baochun Li",https://icml.cc/Conferences/2018/Schedule?showEvent=2032,"Training deep neural networks (DNNs) requires an increasing amount of computation resources, and it becomes typical to use a mixture of GPU and CPU devices. Due to the heterogeneity of these devices, a recent challenge is how each operation in a neural network can be optimally placed on these devices, so that the training process can take the shortest amount of time possible. The current state-of-the-art solution uses reinforcement learning based on the policy gradient method, and it suffers from suboptimal training times.  In this paper, we propose Spotlight, a new reinforcement learning algorithm based on proximal policy optimization, designed specifically for finding an optimal device placement for training DNNs. The design of our new algorithm relies upon a new model of the device placement problem: by modeling it as a Markov decision process with multiple stages, we are able to prove that Spotlight achieves a theoretical guarantee on performance improvements.  We have implemented Spotlight in the CIFAR-10 benchmark and deployed it on the Google Cloud platform. Extensive experiments have demonstrated that the training time with placements recommended by Spotlight is 60.9% of that recommended by the policy gradient method.
","['University of Toronto', 'Department of Electrical and Computer Engineering, University of Toronto', 'University of Toronto']"
2018,Universal Planning Networks: Learning Generalizable Representations for Visuomotor Control,"Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea Finn",https://icml.cc/Conferences/2018/Schedule?showEvent=2340,"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities. Visit https://sites.google. com/view/upn-public/home for video highlights.
","['UC Berkeley', 'UC Berkeley', 'OpenAI / UC Berkeley', 'Berkeley', 'Stanford, Google, UC Berkeley']"
2018,Coordinated Exploration in Concurrent Reinforcement Learning,"Maria Dimakopoulou, Benjamin Van Roy",https://icml.cc/Conferences/2018/Schedule?showEvent=1943,"We consider a team of reinforcement learning agents that concurrently learn to operate in a common environment. We identify three properties - adaptivity, commitment, and diversity - which are necessary for efficient coordinated exploration and demonstrate that straightforward extensions to single-agent optimistic and posterior sampling approaches fail to satisfy them. As an alternative, we propose seed sampling, which extends posterior sampling in a manner that meets these requirements. Simulation results investigate how per-agent regret decreases as the number of agents grows, establishing substantial advantages of seed sampling over alternative exploration schemes.
","['Stanford', 'Stanford University']"
2018,A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks,"Akifumi Okuno, Tetsuya Hada, Hidetoshi Shimodaira",https://icml.cc/Conferences/2018/Schedule?showEvent=2020,"A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is proposed for multi-view feature learning with many-to-many associations so that it generalizes various existing multi-view methods. PMvGE is a probabilistic model for predicting new associations via graph embedding of the nodes of data vectors with links of their associations. Multi-view data vectors with many-to-many associations are transformed by neural networks to feature vectors in a shared space, and the probability of new association between two data vectors is modeled by the inner product of their feature vectors. While existing multi-view feature learning techniques can treat only either of many-to-many association or non-linear transformation, PMvGE can treat both simultaneously. By combining Mercer's theorem and the universal approximation theorem, we prove that PMvGE learns a wide class of similarity measures across views. Our likelihood-based estimator enables efficient computation of non-linear transformations of data vectors in large-scale datasets by minibatch SGD, and numerical experiments illustrate that PMvGE outperforms existing multi-view methods.
","['Kyoto University / RIKEN AIP', 'Recruit Technologies Co. Ltd.', 'Kyoto University / RIKEN AIP']"
2018,Learning Steady-States of Iterative Algorithms over Graphs,"Hanjun Dai, Zornitsa Kozareva, Bo Dai, Alex Smola, Le Song",https://icml.cc/Conferences/2018/Schedule?showEvent=2424,"Many graph analytics problems can be solved via iterative algorithms where the solutions are often characterized by a set of steady-state conditions. Different algorithms respect to different set of fixed point constraints, so instead of using these traditional algorithms, can we learn an algorithm which can obtain the same steady-state solutions automatically from examples, in an effective and scalable way? How to represent the meta learner for such algorithm and how to carry out the learning? In this paper, we propose an embedding representation for iterative algorithms over graphs, and design a learning method which alternates between updating the embeddings and projecting them onto the steady-state constraints. We demonstrate the effectiveness of our framework using a few commonly used graph algorithms, and show that in some cases, the learned algorithm can handle graphs with more than 100,000,000 nodes in a single machine.
","['Georgia Tech', '', 'Georgia Institute of Technology', 'Amazon', 'Georgia Institute of Technology']"
2018,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,"Anish Athalye, Nicholas Carlini, David Wagner",https://icml.cc/Conferences/2018/Schedule?showEvent=2217,"We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.
","['MIT CSAIL', 'University of California, Berkeley', 'UC Berkeley']"
2018,Fair and Diverse DPP-Based Data Summarization,"L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Jayant Deshpande, Tarun Kathuria, Nisheeth Vishnoi",https://icml.cc/Conferences/2018/Schedule?showEvent=2176,"Sampling methods that choose a subset of the data proportional to its diversity in the feature space are popular for data summarization. However, recent studies have noted the occurrence of bias – e.g., under or over representation of a particular gender or ethnicity – in such data summarization methods. In this paper we initiate a study of the problem of outputting a diverse and fair summary of a given dataset. We work with a well-studied determinantal measure of diversity and corresponding distributions (DPPs) and present a framework that allows us to incorporate a general class of fairness constraints into such distributions. Designing efficient algorithms to sample from these constrained determinantal distributions, however, suffers from a complexity barrier; we present a fast sampler that is provably good when the input vectors satisfy a natural property. Our empirical results on both real-world and synthetic datasets show that the diversity of the samples produced by adding fairness constraints is not too far from the unconstrained case.
","['Yale', 'EPFL', 'EPFL', 'Microsoft Research', 'UC Berkeley', 'EPFL']"
2018,Learning Implicit Generative Models with the Method of Learned Moments,"Suman Ravuri, Shakir Mohamed, Mihaela Rosca, Oriol Vinyals",https://icml.cc/Conferences/2018/Schedule?showEvent=2251,"We propose a method of moments (MoM) algorithm for training large-scale implicit generative models. Moment estimation in this setting encounters two problems: it is often difficult to define the millions of moments needed to learn the model parameters, and it is hard to determine which properties are useful when specifying moments. To address the first issue, we introduce a moment network, and define the moments as the network's hidden units and the gradient of the network's output with respect to its parameters. To tackle the second problem, we use asymptotic theory to highlight desiderata for moments -- namely they should minimize the asymptotic variance of estimated model parameters -- and introduce an objective to learn better moments. The sequence of objectives created by this Method of Learned Moments (MoLM) can train high-quality neural image samplers. On CIFAR-10, we demonstrate that MoLM-trained generators achieve significantly higher Inception Scores and lower Frechet Inception Distances than those trained with gradient penalty-regularized and spectrally-normalized adversarial objectives. These generators also achieve nearly perfect Multi-Scale Structural Similarity Scores on CelebA, and can create high-quality samples of 128x128 images.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2018,Chi-square Generative Adversarial Network,"Chenyang Tao, Liqun Chen, Ricardo Henao, Jianfeng Feng, Lawrence Carin",https://icml.cc/Conferences/2018/Schedule?showEvent=2369,"To assess the difference between real and synthetic data, Generative Adversarial Networks (GANs) are trained using a distribution discrepancy measure. Three widely employed measures are information-theoretic divergences, integral probability metrics, and Hilbert space discrepancy metrics. We elucidate the theoretical connections between these three popular GAN training criteria and propose a novel procedure, called $\chi^2$ (Chi-square) GAN, that is conceptually simple, stable at training and resistant to mode collapse. Our procedure naturally generalizes to address the problem of simultaneous matching of multiple distributions. Further, we propose a resampling strategy that significantly improves sample quality, by repurposing the trained critic function via an importance weighting mechanism. Experiments show that the proposed procedure improves stability and convergence, and yields state-of-art results on a wide range of generative modeling tasks.","['Duke University', 'Duke University', 'Duke University', 'Fudan University', 'Duke']"
2018,Streaming Principal Component Analysis in Noisy Setting,"Teodor Vanislavov Marinov, Poorya Mianjy, Raman Arora",https://icml.cc/Conferences/2018/Schedule?showEvent=2459,"We study streaming algorithms for principal component analysis (PCA) in noisy settings. We present computationally efficient algorithms with sub-linear regret bounds for PCA in the presence of noise, missing data, and gross outliers.
","['Johns Hopkins University', 'Johns Hopkins University', 'Johns Hopkins University']"
2018,Partial Optimality and Fast Lower Bounds for Weighted Correlation Clustering,"Jan-Hendrik Lange, Andreas Karrenbauer, Bjoern Andres",https://icml.cc/Conferences/2018/Schedule?showEvent=2206,"Weighted correlation clustering is hard to solve and hard to approximate for general graphs. Its applications in network analysis and computer vision call for efficient algorithms. To this end, we make three contributions: We establish partial optimality conditions that can be checked efficiently, and doing so recursively solves the problem for series-parallel graphs to optimality, in linear time. We exploit the packing dual of the problem to compute a heuristic, but non-trivial lower bound faster than that of a canonical linear program relaxation. We introduce a re-weighting with the dual solution by which efficient local search algorithms converge to better feasible solutions. The effectiveness of our methods is demonstrated empirically on a number of benchmark instances.
","['Max Planck Institute for Informatics', 'Max Planck Institute for Informatics', 'MPI for Informatics']"
2018,SGD and Hogwild! Convergence Without the Bounded Gradients Assumption,"Lam Nguyen, PHUONG_HA NGUYEN, Marten van Dijk, Peter Richtarik, Katya Scheinberg, Martin Takac",https://icml.cc/Conferences/2018/Schedule?showEvent=2313,"Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption  that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for  cases where the objective  function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.
","['Lehigh University & IBM T.J. Watson Research Center', 'University of Connecticut', 'University of Connecticut', 'King Abdullah University of Science and Technology (KAUST)', 'Lehigh University', 'Lehigh University']"
2018,Computational Optimal Transport: Complexity by Accelerated Gradient Descent Is Better Than by Sinkhorn's Algorithm,"Pavel Dvurechenskii, Alexander Gasnikov, Alexey Kroshnin",https://icml.cc/Conferences/2018/Schedule?showEvent=2468,"We analyze two algorithms for approximating the general optimal transport (OT) distance between two discrete distributions of size $n$, up to accuracy $\varepsilon$.  For the first algorithm, which is based on the celebrated Sinkhorn's algorithm, we prove the complexity bound $\widetilde{O}\left(\frac{n^2}{\varepsilon^2}\right)$ arithmetic operations ($\widetilde{O}$ hides polylogarithmic factors $(\ln n)^c$, $c>0$).  For the second one, which is based on our novel Adaptive Primal-Dual Accelerated Gradient Descent (APDAGD) algorithm, we prove the complexity bound $\widetilde{O}\left(\min\left\{\frac{n^{9/4}}{\varepsilon}, \frac{n^{2}}{\varepsilon^2} \right\}\right)$ arithmetic operations.  Both bounds have better dependence on $\varepsilon$ than the state-of-the-art result given by $\widetilde{O}\left(\frac{n^2}{\varepsilon^3}\right)$.  Our second algorithm not only has better dependence on $\varepsilon$ in the complexity bound, but also is not specific to entropic regularization and can solve the OT problem with different regularizers.","['Weierstrass Institute for Applied Analysis and Stochastics', 'Moscow Institute of Physics and Technology', 'Institute for Information Transmission Problems']"
2018,Stability and Generalization of Learning Algorithms that Converge to Global Optima,"Zachary Charles, Dimitris Papailiopoulos",https://icml.cc/Conferences/2018/Schedule?showEvent=2024,"We establish novel generalization bounds for learning algorithms that converge to global minima. We derive black-box stability results that only depend on the convergence of a learning algorithm and the geometry around the minimizers of the empirical risk function. The results are shown for non-convex loss functions satisfying the Polyak-Lojasiewicz (PL) and the quadratic growth (QG) conditions, which we show arise for 1-layer neural networks with leaky ReLU activations and deep neural networks with linear activations. We use our results to establish the stability of first-order methods such as  stochastic gradient descent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and the stochastic variance reduced gradient method (SVRG), in both the PL and the strongly convex setting. Our results match or improve state-of-the-art generalization bounds and can easily extend to similar optimization algorithms. Finally, although our results imply comparable stability for SGD and GD in the PL setting, we show that there exist simple quadratic models with multiple local minima where SGD is stable but GD is not.
","['University of Wisconsin-Madison', 'ECE at University of Wisconsin-Madison']"
2018,Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces,"Junhong Lin, Volkan Cevher",https://icml.cc/Conferences/2018/Schedule?showEvent=2379,"We investigate regularized algorithms combining with projection for least-squares regression problem over a Hilbert space, covering nonparametric regression over a reproducing kernel Hilbert space. We prove convergence results with respect to variants of norms, under a capacity assumption on the hypothesis space and a regularity condition on the target function.  As a result, we obtain optimal rates for regularized algorithms with randomized sketches, provided that the sketch dimension is proportional to the effective dimension up to a logarithmic factor. As a byproduct, we obtain similar results for Nystr\""{o}m regularized algorithms. Our results provide optimal, distribution-dependent rates for sketched/Nystr\""{o}m regularized algorithms, considering both the attainable and non-attainable cases.
","['EPFL', 'EPFL']"
2018,Adafactor: Adaptive Learning Rates with Sublinear Memory Cost,"Noam Shazeer, Mitchell Stern",https://icml.cc/Conferences/2018/Schedule?showEvent=2446,"In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters.  For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer.  Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.
","['Google', 'UC Berkeley']"
2018,Fast Parametric Learning with Activation Memorization,"Jack Rae, Chris Dyer, Peter Dayan, Timothy Lillicrap",https://icml.cc/Conferences/2018/Schedule?showEvent=2299,"Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which stores recent activations and class labels into an external memory. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) - the latter achieving a state-of-the-art perplexity of 29.2.
","['DeepMind', 'DeepMind', 'UCL', 'Google DeepMind']"
2018,Essentially No Barriers in Neural Network Energy Landscape,"Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred Hamprecht",https://icml.cc/Conferences/2018/Schedule?showEvent=2349,"Training neural networks involves finding minima of a high-dimensional non-convex loss function. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that minima are perhaps best seen as points on a single connected manifold of low loss, rather than as the bottoms of distinct valleys.
","['Heidelberg University', 'University of Heidelberg', 'Heidelberg University', 'Heidelberg Collaboratory for Image Processing']"
2018,Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global,"Thomas Laurent, James von Brecht",https://icml.cc/Conferences/2018/Schedule?showEvent=1993,"We consider deep linear networks with arbitrary convex differentiable  loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the input layer, or 2) at least as wide as the output layer. This result is the strongest possible in the following sense: If the loss is convex and Lipschitz but not differentiable then deep linear networks can have sub-optimal local minima.
","['Loyola Marymount University', 'CSULB']"
2018,Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression,"Haitao Liu, Jianfei Cai, Yi Wang, Yew Soon ONG",https://icml.cc/Conferences/2018/Schedule?showEvent=1958,"In order to scale standard Gaussian process (GP) regression to large-scale datasets, aggregation models employ factorized training process and then combine predictions from distributed experts. The state-of-the-art aggregation models, however, either provide inconsistent predictions or require time-consuming aggregation process. We first prove the inconsistency of typical aggregations using disjoint or random data partition, and then present a consistent yet efficient aggregation model for large-scale GP. The proposed model inherits the advantages of aggregations, e.g., closed-form inference and aggregation, parallelization and distributed computing. Furthermore, theoretical and empirical analyses reveal that the new aggregation model performs better due to the consistent predictions that converge to the true underlying function when the training size approaches infinity.
","['Rolls-Royce@NTU Corp Lab', 'Nanyang Technological University', 'Rolls-Royce Singapore', 'Nanyang Technological University']"
2018,Bayesian Quadrature for Multiple Related Integrals,"Xiaoyue Xi, Francois-Xavier Briol, Mark Girolami",https://icml.cc/Conferences/2018/Schedule?showEvent=1890,"Bayesian probabilistic numerical methods are a set of tools providing posterior distributions on the output of numerical methods. The use of these methods is usually motivated by the fact that they can represent our uncertainty due to incomplete/finite information about the continuous mathematical problem being approximated. In this paper, we demonstrate that this paradigm can provide additional advantages, such as the possibility of transferring information between several numerical methods. This allows users to represent uncertainty in a more faithful manner and, as a by-product, provide increased numerical efficiency. We propose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in computing the integral of several related functions. We then prove convergence rates for the method in the well-specified and misspecified cases, and demonstrate its efficiency in the context of multi-fidelity models for complex engineering systems and a problem of global illumination in computer graphics.
","['Imperial College London', 'University of Warwick', 'Imperial College London']"
2018,Deep Predictive Coding Network for Object Recognition,"Haiguang Wen, Kuan Han, Junxing Shi, Yizhen Zhang, Eugenio Culurciello, Zhongming Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2447,"Based on the predictive coding theory in neuro- science, we designed a bi-directional and recur- rent neural net, namely deep predictive coding networks (PCN), that has feedforward, feedback, and recurrent connections. Feedback connections from a higher layer carry the prediction of its lower-layer representation; feedforward connec- tions carry the prediction errors to its higher-layer. Given image input, PCN runs recursive cycles of bottom-up and top-down computation to update its internal representations and reduce the differ- ence between bottom-up input and top-down pre- diction at every layer. After multiple cycles of recursive updating, the representation is used for image classification. With benchmark datasets (CIFAR-10/100, SVHN, and MNIST), PCN was found to always outperform its feedforward-only counterpart: a model without any mechanism for recurrent dynamics, and its performance tended to improve given more cycles of computation over time. In short, PCN reuses a single architecture to recursively run bottom-up and top-down pro- cesses to refine its representation towards more accurate and definitive object recognition.
","['Purdue University', 'Purdue University', 'Purdue University', '', 'Nil', 'Purdue University']"
2018,Neural Inverse Rendering for General Reflectance Photometric Stereo,"Tatsunori Taniai, Takanori Maehara",https://icml.cc/Conferences/2018/Schedule?showEvent=1901,"We present a novel convolutional neural network architecture for photometric stereo (Woodham, 1980), a problem of recovering 3D object surface normals from multiple images observed under varying illuminations. Despite its long history in computer vision, the problem still shows fundamental challenges for surfaces with unknown general reflectance properties (BRDFs). Leveraging deep neural networks to learn complicated reflectance models is promising, but studies in this direction are very limited due to difficulties in acquiring accurate ground truth for training and also in designing networks invariant to permutation of input images. In order to address these challenges, we propose a physics based unsupervised learning framework where surface normals and BRDFs are predicted by the network and fed into the rendering equation to synthesize observed images. The network weights are optimized during testing by minimizing reconstruction loss between observed and synthesized images. Thus, our learning process does not require ground truth normals or even pre-training on external images. Our method is shown to achieve the state-of-the-art performance on a challenging real-world scene benchmark.
","['RIKEN AIP', 'RIKEN AIP']"
2018,On the Relationship between Data Efficiency and Error for Uncertainty Sampling,"Stephen Mussmann, Percy Liang",https://icml.cc/Conferences/2018/Schedule?showEvent=2390,"While active learning offers potential cost savings, the actual data efficiency---the reduction in amount of labeled data needed to obtain the same error rate---observed in practice is mixed. This paper poses a basic question: when is active learning actually helpful? We provide an answer for logistic regression with the popular active learning algorithm, uncertainty sampling. Empirically, on 21 datasets from OpenML, we find a strong inverse correlation between data efficiency and the error rate of the final classifier. Theoretically, we show that for a variant of uncertainty sampling, the asymptotic data efficiency is within a constant factor of the inverse error rate of the limiting classifier.
","['Stanford University', 'Stanford University']"
2018,Selecting Representative Examples for Program Synthesis,"Yewen Pu, Zachery Miranda, Armando Solar-Lezama, Leslie Kaelbling",https://icml.cc/Conferences/2018/Schedule?showEvent=2384,"Program synthesis is a class of regression problems where one seeks a solution, in the form of a source-code program, mapping the inputs to their corresponding outputs exactly. Due to its precise and combinatorial nature, program synthesis is commonly formulated as a constraint satisfaction problem, where input-output examples are encoded as constraints and solved with a constraint solver. A key challenge of this formulation is scalability: while constraint solvers work well with a few well-chosen examples, a large set of examples can incur significant overhead in both time and memory. We describe a method to discover a subset of examples that is both small and representative: the subset is constructed iteratively, using a neural network to predict the probability of unchosen examples conditioned on the chosen examples in the subset, and greedily adding the least probable example. We empirically evaluate the representativeness of the subsets constructed by our method, and demonstrate such subsets can significantly improve synthesis time and stability.
","['MIT', 'MIT', 'MIT', '(organization)']"
2018,Conditional Neural Processes,"Marta Garnelo, Dan Rosenbaum, Chris Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Teh, Danilo J. Rezende, S. M. Ali Eslami",https://icml.cc/Conferences/2018/Schedule?showEvent=2253,"Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet, GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.
","['DeepMind', 'DeepMind', 'Oxford, DeepMind', 'DeepMind', 'DeepMind', 'DeepMind / Imperial College London', 'DeepMind', 'DeepMind', 'DeepMind']"
2018,Hierarchical Long-term Video Prediction without Supervision,"Nevan Wichers, Ruben Villegas, Dumitru Erhan, Honglak Lee",https://icml.cc/Conferences/2018/Schedule?showEvent=2381,"Much of recent research has been devoted to video prediction and generation, yet most of the previous works have demonstrated only limited success in generating videos on short-term horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state-of-the-art method for long-term video prediction, but their method is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time. Our network encodes the input frame, predicts a high-level encoding into the future, and then a decoder with access to the first frame produces the predicted image from the predicted encoding. The decoder also produces a mask that outlines the predicted foreground object (e.g., person) as a by-product. Unlike Villegas et al. (2017), we develop a novel training method that jointly trains the encoder, the predictor, and the decoder together without highlevel supervision; we further improve upon this by using an adversarial loss in the feature space to train the predictor. Our method can predict about 20 seconds into the future and provides better results compared to Denton and Fergus (2018) and Finn et al. (2016) on the Human 3.6M dataset.
","['Google', 'University of Michigan', 'Google Brain', 'Google Brain']"
2018,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,"Jonathan Uesato, Brendan O'Donoghue, Pushmeet Kohli, Aäron van den Oord",https://icml.cc/Conferences/2018/Schedule?showEvent=2138,"This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate \emph{adversarial risk} as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as \textit{obscurity to an adversary}, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.
","['DeepMind', 'DeepMind', 'DeepMind', 'Google Deepmind']"
2018,A Classification-Based Study of Covariate Shift in GAN Distributions,"Shibani Santurkar, Ludwig Schmidt, Aleksander Madry",https://icml.cc/Conferences/2018/Schedule?showEvent=2413,"A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift:  mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied  to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.
","['MIT', 'UC Berkeley', 'MIT']"
2018,Gated Path Planning Networks,"Lisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric Xing, Ruslan Salakhutdinov",https://icml.cc/Conferences/2018/Schedule?showEvent=2488,"Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Petuum Inc. and CMU', 'Carnegie Mellen University']"
2018,Automatic Goal Generation for Reinforcement Learning Agents,"Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel",https://icml.cc/Conferences/2018/Schedule?showEvent=2287,"Reinforcement learning (RL) is a powerful technique to train an agent to perform a task; however, an agent that is trained using RL is only capable of achieving the single task that is specified via its reward function.   Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations.  Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment.  We use a generator network to propose tasks for the agent to try to accomplish, each task being specified as reaching a certain parametrized subset of the state-space.  The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent, thus automatically producing a curriculum.  We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment, even when only sparse rewards are available. Videos and code available at https://sites.google.com/view/goalgeneration4rl.
","['UC Berkeley', 'Carnegie Mellon University', 'UC Berkeley', 'OpenAI / UC Berkeley']"
2018,ADMM and Accelerated ADMM as Continuous Dynamical Systems,"Guilherme Franca, Daniel Robinson, Rene Vidal",https://icml.cc/Conferences/2018/Schedule?showEvent=2396,"Recently, there has been an increasing interest in using tools from dynamical systems to analyze the behavior of simple optimization algorithms such as gradient descent and accelerated variants. This paper strengthens such connections by deriving the differential equations that model the continuous limit of the sequence of iterates generated by the alternating direction method of multipliers, as well as an accelerated variant. We employ the direct method of Lyapunov to analyze the stability of critical points of the dynamical systems and to obtain associated convergence rates.
","['Johns Hopkins University', 'Johns Hopkins University', 'Johns Hopkins University']"
2018,Dissipativity Theory for Accelerating Stochastic Variance Reduction: A Unified Analysis of SVRG and Katyusha Using Semidefinite Programs,"Bin Hu, Stephen Wright, Laurent Lessard",https://icml.cc/Conferences/2018/Schedule?showEvent=2471,"Techniques for reducing the variance of gradient estimates used in stochastic programming algorithms for convex finite-sum problems have received a great deal of attention in recent years. By leveraging dissipativity theory from control, we provide a new perspective on two important variance-reduction algorithms: SVRG and its direct accelerated variant Katyusha. Our perspective provides a physically intuitive understanding of the behavior of SVRG-like methods via a principle of energy conservation. The tools discussed here allow us to automate the convergence analysis of SVRG-like methods by capturing their essential properties in small semidefinite programs amenable to standard analysis and computational techniques. Our approach recovers existing convergence results for SVRG and Katyusha and generalizes the theory to alternative parameter choices. We also discuss how our approach complements the linear coupling technique. Our combination of perspectives leads to a better understanding of accelerated variance-reduced stochastic methods for finite-sum problems.
","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison']"
2018,Contextual Graph Markov Model: A Deep and Generative Approach to Graph Processing,"Davide Bacciu, Federico Errica, Alessio Micheli",https://icml.cc/Conferences/2018/Schedule?showEvent=2084,"We introduce the Contextual Graph Markov Model, an approach combining ideas from generative models and neural networks for the processing of graph data. It founds on a constructive methodology to build a deep architecture comprising layers of probabilistic models that learn to encode the structured information in an incremental fashion. Context is diffused in an efficient and scalable way across the graph vertexes and edges. The resulting graph encoding is used in combination with discriminative models to address structure classification benchmarks.
","['University of Pisa', 'University of Pisa', 'Universita di Pisa']"
2018,Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry,"Maximilian Nickel, Douwe Kiela",https://icml.cc/Conferences/2018/Schedule?showEvent=2370,"We are concerned with the discovery of hierarchical relationships from large-scale unstructured similarity scores. For this purpose, we study different models of hyperbolic space and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincaré-ball model. We show that the proposed approach allows us to learn high-quality embeddings of large taxonomies which yield improvements over Poincaré embeddings, especially in low dimensions. Lastly, we apply our model to discover hierarchies in two real-world datasets: we show that an embedding in hyperbolic space can reveal important aspects of a company's organizational structure as well as reveal historical relationships between language families.
","['Facebook AI Research', 'Facebook AI Research']"
2018,Fast Variance Reduction Method with Stochastic Batch Size,"University of California Xuanqing Liu, Cho-Jui Hsieh",https://icml.cc/Conferences/2018/Schedule?showEvent=2295,"In this paper we study a family of variance reduction methods with randomized batch size---at each step, the algorithm first randomly chooses the batch size and then selects a batch of samples to conduct a variance-reduced stochastic update. We give the linear converge rate for this framework for composite functions, and show that the optimal strategy to achieve the best converge rate per data access is to always choose batch size equalling to 1, which is equivalent to the SAGA algorithm. However, due to the presence of cache/disk IO effect in computer architecture, number of data access cannot reflect the running time because of 1) random memory access is much slower than sequential access, 2) when data is too big to fit into memory, disk seeking takes even longer time. After taking these into account, choosing batch size equals to 1 is no longer optimal, so we propose a new algorithm called SAGA++ and theoretically show how to calculate the optimal average batch size. Our algorithm outperforms SAGA and other existing batch and stochastic solvers on real datasets. In addition, we also conduct a precise analysis to compare different update rules for variance reduction methods, showing that SAGA++ converges faster than SVRG in theory.
","['University of California, Davis', 'University of California, Davis']"
2018,Lyapunov Functions for First-Order Methods: Tight Automated Convergence Guarantees,"Adrien Taylor, Bryan Van Scoy, Laurent Lessard",https://icml.cc/Conferences/2018/Schedule?showEvent=2168,"We present a novel way of generating Lyapunov functions for proving linear convergence rates of first-order optimization methods. Our approach provably obtains the fastest linear convergence rate that can be verified by a quadratic Lyapunov function (with given states), and only relies on solving a small-sized semidefinite program. Our approach combines the advantages of performance estimation problems (PEP, due to Drori and Teboulle (2014)) and integral quadratic constraints (IQC, due to Lessard et al. (2016)), and relies on convex interpolation (due to Taylor et al. (2017c;b)).
","['INRIA/ENS', 'University of Wisconsin--Madison', 'University of Wisconsin-Madison']"
2018,Nonparametric Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information,"Yichong Xu, Hariank Muthakana, Sivaraman Balakrishnan, Aarti Singh, Artur Dubrawski",https://icml.cc/Conferences/2018/Schedule?showEvent=2095,"In supervised learning, we leverage a labeled dataset to design methods for function estimation. In many practical situations, we are able to obtain alternative feedback, possibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exploit, this alternative feedback. We focus on a semi-supervised setting where we obtain additional ordinal (or comparison) information for potentially unlabeled samples. We consider ordinal feedback of varying qualities where we have either a perfect ordering of the samples, a noisy ordering of the samples or noisy pairwise comparisons between the samples. We provide a precise quantification of the usefulness of these types of ordinal feedback in non-parametric regression, showing that in many cases it is possible to accurately estimate an underlying function with a very small labeled set, effectively escaping the curse of dimensionality. We develop an algorithm called Ranking-Regression (RR) and analyze its accuracy as a function of size of the labeled and unlabeled datasets and various noise parameters. We also present lower bounds, that establish fundamental limits for the task and show that RR is optimal in a variety of settings. Finally, we present experiments that show the efficacy of RR and investigate its robustness to various sources of noise and model-misspecification.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'CMU']"
2018,The Well-Tempered Lasso,"Yuanzhi Li, Yoram Singer",https://icml.cc/Conferences/2018/Schedule?showEvent=2401,"We study the complexity of the entire regularization path for least squares regression with 1-norm penalty, known as the Lasso. Every regression parameter in the Lasso changes linearly as a function of the regularization value. The number of changes is regarded as the Lasso's complexity. Experimental results using exact path following exhibit polynomial complexity of the Lasso in the problem size. Alas, the path complexity of the Lasso on artificially designed regression problems is exponential  We use smoothed analysis as a mechanism for bridging the gap between worst case settings and the de facto low complexity. Our analysis assumes that the observed data has a tiny amount of intrinsic noise. We then prove that the Lasso's complexity is polynomial in the problem size.
","['Princeton University', 'Princeton University and Google Brain']"
2018,Transfer Learning via Learning to Transfer,"Ying WEI, Yu Zhang, Junzhou Huang, Qiang Yang",https://icml.cc/Conferences/2018/Schedule?showEvent=2145,"In transfer learning, what and how to transfer are two primary issues to be addressed, as different transfer learning algorithms applied between a source and a target domain result in different knowledge transferred and thereby the performance improvement in the target domain. Determining the optimal one that maximizes the performance improvement requires either exhaustive exploration or considerable expertise. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer are the best for a future pair of domains by optimizing the reflection function. We also theoretically analyse the algorithmic stability and generalization bound of L2T, and empirically demonstrate its superiority over several state-of-the-art transfer learning algorithms.
","['Tencent AI Lab', 'Hong Kong UST', 'University of Texas at Arlington / Tencent AI Lab', 'Hong Kong UST']"
2018,Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing—and Back,"Elliot Meyerson, Risto Miikkulainen",https://icml.cc/Conferences/2018/Schedule?showEvent=2256,"Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learning to the setting where only a single task is available. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect of training towards closely-related tasks drawn from the same universe. In a suite of experiments, pseudo-task augmentation is shown to improve performance on single-task learning problems. When combined with multitask learning, further improvements are achieved, including state-of-the-art performance on the CelebA dataset, showing that pseudo-task augmentation and multitask learning have complementary value. All in all, pseudo-task augmentation is a broadly applicable and efficient way to boost performance in deep learning systems.
","['UT Austin - Sentient Technologies', 'UT Austin - Sentient Technologies']"
2018,Analysis of Minimax Error Rate for Crowdsourcing and Its Application to Worker Clustering Model,"Hideaki Imamura, Issei Sato, Masashi Sugiyama",https://icml.cc/Conferences/2018/Schedule?showEvent=2211,"While crowdsourcing has become an important means to label data, there is great interest in estimating the ground truth from unreliable labels produced by crowdworkers.  The Dawid and Skene (DS) model is one of the most well-known models in the study of crowdsourcing. Despite its practical popularity, theoretical error analysis for the DS model has been conducted only under restrictive assumptions on class priors, confusion matrices, or the number of labels each worker provides. In this paper, we derive a minimax error rate under more practical setting for a broader class of crowdsourcing models including the DS model as a special case. We further propose the worker clustering model, which is more practical than the DS model under real crowdsourcing settings. The wide applicability of our theoretical analysis allows us to immediately investigate the behavior of this proposed model, which can not be analyzed by existing studies. Experimental results showed that there is a strong similarity between the lower bound of the minimax error rate derived by our theoretical analysis and the empirical error of the estimated value.
","['The University of Tokyo', 'University of Tokyo / RIKEN', 'RIKEN / The University of Tokyo']"
2018,Deep One-Class Classification,"Lukas Ruff, Nico Görnitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert Vandermeulen, Alexander Binder, Emmanuel Müller, Marius Kloft",https://icml.cc/Conferences/2018/Schedule?showEvent=2483,"Despite the great advances made by deep learning in many machine learning problems, there is a  relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method---Deep Support Vector Data Description---, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.
","['Hasso Plattner Institute', 'TU Berlin', 'University of Edinburgh', 'German Research Center for Artificial Intelligence', 'TU Kaiserslautern', 'Singapore University of Technology and Design', 'Hasso Plattner Institute', 'TU Kaiserslautern']"
2018,Binary Partitions with Approximate  Minimum Impurity,"Eduardo Laber, Marco Molinaro, Felipe de A. Mello Pereira",https://icml.cc/Conferences/2018/Schedule?showEvent=1929,"The problem of splitting attributes is one of the main steps in the construction of decision trees. In order to decide the best split,  impurity measures such as Entropy and Gini are widely used.  In practice, decision-tree inducers use heuristics for finding splits with small impurity when they consider nominal attributes with a large number of distinct values. However, there are no known guarantees for the quality of the splits obtained by these heuristics. To fill this gap, we propose two new splitting procedures that provably achieve near-optimal impurity. We also report experiments that provide  evidence that the proposed methods are interesting candidates  to be employed in splitting nominal attributes with many values during decision tree/random forest induction.
","['PUC-RIO', 'PUC-RIO', 'PUC-Rio']"
2018,Beyond 1/2-Approximation for Submodular Maximization on Massive Data Streams,"Ashkan Norouzi-Fard, Jakub Tarnawski, Slobodan Mitrovic, Amir Zandieh, Aidasadat Mousavifar, Ola Svensson",https://icml.cc/Conferences/2018/Schedule?showEvent=2040,"Many tasks in machine learning and data mining, such as data diversification, non-parametric learning, kernel machines, clustering etc., require extracting a small but representative summary from a massive dataset. Often, such problems can be posed as maximizing a submodular set function subject to a cardinality constraint. We consider this question in the streaming setting, where elements arrive over time at a fast pace and thus we need to design an efficient, low-memory algorithm. One such method, proposed by Badanidiyuru et al. (2014), always finds a 0.5-approximate solution. Can this approximation factor be improved? We answer this question affirmatively by designing a new algorithm Salsa for streaming submodular maximization. It is the first low-memory, singlepass algorithm that improves the factor 0.5, under the natural assumption that elements arrive in a random order. We also show that this assumption is necessary, i.e., that there is no such algorithm with better than 0.5-approximation when elements arrive in arbitrary order. Our experiments demonstrate that Salsa significantly outperforms the state of the art in applications related to exemplar-based clustering, social graph analysis, and recommender systems.
","['EPFL', 'EPFL', 'EPFL', 'EPFL', 'EPFL', 'EPFL']"
2018,"Yes, but Did It Work?: Evaluating Variational Inference","Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman",https://icml.cc/Conferences/2018/Schedule?showEvent=2055,"While it's always possible to compute a variational approximation to a posterior distribution, it can be difficult to discover problems with this approximation"". We propose two  diagnostic algorithms to alleviate this problem. The  Pareto-smoothed importance sampling (PSIS) diagnostic  gives a goodness of fit measurement for joint distributions, while simultaneously improving the error in the estimate. The variational   simulation-based calibration (VSBC) assesses the average performance  of point estimates.
","['Columbia University', 'Aalto University', 'University of Toronto', 'Columbia University']"
2018,Black-Box Variational Inference for Stochastic Differential Equations,"Tom Ryder, Andrew Golightly, Stephen McGough, Dennis Prangle",https://icml.cc/Conferences/2018/Schedule?showEvent=2034,"Parameter inference for stochastic differential equations is challenging due to the presence of a latent diffusion process. Working with an Euler-Maruyama discretisation for the diffusion, we use variational inference to jointly learn the parameters and the diffusion paths. We use a standard mean-field variational approximation of the parameter posterior, and introduce a recurrent neural network to approximate the posterior for the diffusion paths conditional on the parameters. This neural network learns how to provide Gaussian state transitions which bridge between observations in a very similar way to the conditioned diffusion process. The resulting black-box inference method can be applied to any SDE system with light tuning requirements. We illustrate the method on a Lotka-Volterra system and an epidemic model, producing accurate parameter estimates in a few hours.
","['Newcastle University', 'Newcastle University', 'Newcastle University', 'Newcastle University']"
2018,Online Convolutional Sparse Coding with Sample-Dependent Dictionary,"Yaqing WANG, Quanming Yao, James Kwok, Lionel NI",https://icml.cc/Conferences/2018/Schedule?showEvent=2192,"Convolutional sparse coding (CSC) has been popularly used for the learning of shift-invariant dictionaries in image and signal processing. However, existing methods have limited scalability. In this paper, instead of convolving with a dictionary shared by all samples, we propose the use of a sample-dependent dictionary in which each filter is a linear combination of a small set of base filters learned from data. This added flexibility allows a large number of sample-dependent patterns to be captured, which is especially useful in the handling of large or high-dimensional data sets. Computationally, the resultant model can be efficiently learned by online learning. Extensive experimental results on a number of data sets show that the proposed method outperforms existing CSC algorithms with significantly reduced time and space complexities.
","['Hong Kong University of Science and Technology', '4Paradigm', 'Hong Kong University of Science and Technology', 'University of Macau']"
2018,Learning to Speed Up Structured Output Prediction,"Xingyuan Pan, Vivek Srikumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2489,"Predicting structured outputs can be computationally onerous due to the combinatorially large output spaces. In this paper, we focus on reducing the prediction time of a trained black-box structured classifier without losing accuracy. To do so, we train a speedup classifier that learns to mimic a black-box classifier under the learning-to-search approach. As the structured classifier predicts more examples, the speedup classifier will operate as a learned heuristic to guide search to favorable regions of the output space. We present a mistake bound for the speedup classifier and identify inference situations where it can independently make correct judgments without input features. We evaluate our method on the task of entity and relation extraction and show that the speedup classifier outperforms even greedy search in terms of speed without loss of accuracy.
","['University of Utah', 'University of Utah']"
2018,Differentially Private Identity and Equivalence Testing of Discrete Distributions,"Maryam Aliakbarpour, Ilias Diakonikolas, MIT Ronitt Rubinfeld",https://icml.cc/Conferences/2018/Schedule?showEvent=2415,"We study the fundamental problems of identity and equivalence testing over a discrete population from random samples.  Our goal is to develop efficient testers while guaranteeing differential privacy to the individuals of the population.  We provide  sample-efficient differentially private testers for these problems. Our theoretical results significantly improve over the best known  algorithms for identity testing, and are the first results  for private equivalence testing.  The conceptual message of our work is that there exist private hypothesis testers that are nearly as sample-efficient as their non-private counterparts.  We perform an experimental evaluation of our algorithms  on synthetic data. Our experiments illustrate that our private testers achieve small type \rom{1}  and type \rom{2} errors with sample size {\em sublinear} in the domain size of the underlying distributions.
","['MIT', 'USC', 'MIT, TAU']"
2018,Information Theoretic Guarantees for Empirical Risk Minimization with Applications to Model Selection and Large-Scale Optimization,Ibrahim Alabdulmohsin,https://icml.cc/Conferences/2018/Schedule?showEvent=1914,"In this paper, we derive bounds on the mutual information of the empirical risk minimization (ERM) procedure for both 0-1 and strongly-convex loss classes. We prove that under the Axiom of Choice, the existence of an ERM learning rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging  information theory with statistical learning theory. Similarly, an asymptotic bound on the mutual information is established for strongly-convex loss classes in terms of the number of model parameters. The latter result rests on a central limit theorem (CLT) that we derive in this paper. In addition, we use our results to analyze the excess risk in stochastic convex optimization and unify previous works. Finally, we present two important applications. First, we show that the ERM  of strongly-convex loss classes can be trivially scaled to big data using a naive parallelization algorithm with provable guarantees. Second, we propose a simple information criterion for model selection and demonstrate experimentally that it outperforms the popular Akaike's information criterion (AIC) and Schwarz's Bayesian information criterion (BIC).
",['Saudi Aramco']
2018,BOCK : Bayesian Optimization with Cylindrical Kernels,"ChangYong Oh, Efstratios Gavves, Max Welling",https://icml.cc/Conferences/2018/Schedule?showEvent=2230,"A major challenge in Bayesian Optimization is the boundary issue where an algorithm spends too many evaluations near the boundary of its search space. In this paper, we propose BOCK, Bayesian Optimization with Cylindrical Kernels, whose basic idea is to transform the ball geometry of the search space using a cylindrical transformation. Because of the transformed geometry, the Gaussian Process-based surrogate model spends less budget searching near the boundary, while concentrating its efforts relatively more near the center of the search region, where we expect the solution to be located. We evaluate BOCK extensively, showing that it is not only more accurate and efficient, but it also scales successfully to problems with a dimensionality as high as 500. We show that the better accuracy and scalability of BOCK even allows optimizing modestly sized neural network layers, as well as neural network hyperparameters.
","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']"
2018,BOHB: Robust and Efficient Hyperparameter Optimization at Scale,"Stefan Falkner, Aaron Klein, Frank Hutter",https://icml.cc/Conferences/2018/Schedule?showEvent=2387,"Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.
","['University of Freiburg', 'University of Freiburg', 'University of Freiburg']"
2018,Distributed Nonparametric Regression under Communication Constraints,"Yuancheng Zhu, John Lafferty",https://icml.cc/Conferences/2018/Schedule?showEvent=2046,"This paper studies the problem of nonparametric estimation of a smooth function with data distributed across multiple machines.  We assume an independent sample from a white noise model is collected at each machine, and an estimator of the underlying true function needs to be constructed at a central machine.  We place limits on the number of bits that each machine can use to transmit information to the central machine.  Our results give both asymptotic lower bounds and matching upper bounds on the statistical risk under various settings. We identify three regimes, depending on the relationship among the number of machines, the size of data available at each machine, and the communication budget.  When the communication budget is small, the statistical risk depends solely on this communication bottleneck, regardless of the sample size. In the regime where the communication budget is large, the classic minimax risk in the non-distributed estimation setting is recovered.  In an intermediate regime, the statistical risk depends on both the sample size and the communication budget.
","['University of Pennsylvania', 'Yale University']"
2018,Optimal Tuning for Divide-and-conquer Kernel Ridge Regression with Massive Data,"Ganggang Xu, Zuofeng Shang, Guang Cheng",https://icml.cc/Conferences/2018/Schedule?showEvent=2205,"Divide-and-conquer is a powerful approach for large and massive data analysis.  In the nonparameteric regression setting, although various theoretical frameworks have been established to achieve optimality in estimation or hypothesis testing, how to choose the tuning parameter in a practically effective way is still an open problem. In this paper, we propose a data-driven procedure based on divide-and-conquer for selecting the tuning parameters in kernel ridge regression by modifying the popular Generalized Cross-validation (GCV, Wahba, 1990). While the proposed criterion is computationally scalable for massive data sets, it is also shown under mild conditions to be asymptotically optimal in the sense that minimizing the proposed distributed-GCV (dGCV) criterion is equivalent to minimizing the true global conditional empirical loss of the averaged function estimator, extending the existing optimality results of GCV to the divide-and-conquer framework.
","['SUNY-Binghamton University', 'Indiana University–Purdue University Indianapolis', 'Purdue University']"
2018,WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models,"Marine LE MORVAN, Jean-Philippe Vert",https://icml.cc/Conferences/2018/Schedule?showEvent=2255,"Learning sparse linear models with two-way interactions is desirable in many application domains such as genomics. $\ell_1$-regularised linear models are popular to estimate sparse models, yet standard implementations fail to address specifically the quadratic explosion of candidate two-way interactions in high dimensions, and typically do not scale to genetic data with hundreds of thousands of features. Here we present WHInter, a working set algorithm to solve large $\ell_1$-regularised problems with two-way interactions for binary design matrices. The novelty of WHInter stems from a new bound to efficiently identify working sets while avoiding to scan all features, and on fast computations inspired from solutions to the maximum inner product search problem. We apply WHInter to simulated and real genetic data and show that it is more scalable and two orders of magnitude faster than the state of the art.","['Mines Paristech', 'ENS Paris']"
2018,Safe Element Screening for Submodular Function Minimization,"Weizhong Zhang, Bin Hong, Lin Ma, Wei Liu, Tong Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=1883,"Submodular functions are discrete analogs of convex functions, which have applications in various fields, including machine learning and computer vision. However, in large-scale applications, solving Submodular Function Minimization (SFM) problems remains challenging. In this paper, we make the first attempt to extend the emerging technique named screening in large-scale sparse learning to SFM for accelerating its optimization process. We first conduct a careful studying of the relationships between SFM and the corresponding convex proximal problems, as well as the accurate primal optimum estimation of the proximal problems. Relying on this study, we subsequently propose a novel safe screening method to quickly identify the elements guaranteed to be included (we refer to them as active) or excluded (inactive) in the final optimal solution of SFM during the optimization process. By removing the inactive elements and fixing the active ones, the problem size can be dramatically reduced, leading to great savings in the computational cost without sacrificing any accuracy. To the best of our knowledge, the proposed method is the first screening method in the fields of SFM and even combinatorial optimization, thus pointing out a new direction for accelerating SFM algorithms. Experiment results on both synthetic and real datasets demonstrate the significant speedups gained by our approach.
","['Tencent AI Lab', 'Zhejiang University', 'Tencent AI Lab', 'Tencent AI Lab', 'Tecent AI Lab']"
2018,Feedback-Based Tree Search for Reinforcement Learning,"Daniel Jiang, Emmanuel Ekwedike, Han Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2429,"Inspired by recent successes of Monte-Carlo tree search (MCTS) in a number of artificial intelligence (AI) application domains, we propose a reinforcement learning (RL) technique that iteratively applies MCTS on batches of small, finite-horizon versions of the original infinite-horizon Markov decision process. The terminal condition of the finite-horizon problems, or the leaf-node evaluator of the decision tree generated by MCTS, is specified using a combination of an estimated value function and an estimated policy function. The recommendations generated by the MCTS procedure are then provided as feedback in order to refine, through classification and regression, the leaf-node evaluator for the next iteration. We provide the first sample complexity bounds for a tree search-based RL algorithm. In addition, we show that a deep neural network implementation of the technique can create a competitive AI agent for the popular multi-player online battle arena (MOBA) game King of Glory.
","['Facebook', 'Princeton University', 'Northwestern']"
2018,Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement,"Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel J. Mankowitz, Augustin Zidek, Remi Munos",https://icml.cc/Conferences/2018/Schedule?showEvent=2081,"The ability to transfer skills across tasks has the potential to scale up reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised policy improvement (GPI), has been introduced as a principled way of transferring skills. In this paper we extend the SF&GPI framework in two ways. One of the basic assumptions underlying the original formulation of SF&GPI is that rewards for all tasks of interest can be computed as linear combinations of a fixed set of features. We relax this constraint and show that the theoretical guarantees supporting the framework can be extended to any set of tasks that only differ in the reward function. Our second contribution is to show that one can use the reward functions themselves as features for future tasks, without any loss of expressiveness, thus removing the need to specify a set of features beforehand. This makes it possible to combine SF&GPI with deep learning in a more stable way. We empirically verify this claim on a complex 3D environment where observations are images from a first-person perspective. We show that the transfer promoted by SF&GPI leads to very good policies on unseen tasks almost instantaneously. We also describe how to learn policies specialised to the new tasks in a way that allows them to be added to the agent's set of skills, and thus be reused in the future.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'Deep Mind', 'Technion', '', 'DeepMind']"
2018,Data-Dependent Stability of Stochastic Gradient Descent,"Ilja Kuzborskij, Christoph H. Lampert",https://icml.cc/Conferences/2018/Schedule?showEvent=1984,"We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error depends on the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization. As a corollary, our results allow us to show optimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing empirical risk and low noise of stochastic gradient.
","['University of Milan', 'IST Austria']"
2018,LeapsAndBounds: A Method for Approximately Optimal Algorithm Configuration,"Gellért Weisz, András György, Csaba Szepesvari",https://icml.cc/Conferences/2018/Schedule?showEvent=2263,"We consider the problem of configuring general-purpose solvers to run efficiently on problem instances drawn from an unknown distribution. The goal of the configurator is to find a configuration that runs fast on average on most instances, and do so with the least amount of total work. It can run a chosen solver on a random instance until the solver finishes or a timeout is reached. We propose LeapsAndBounds, an algorithm that tests configurations on randomly selected problem instances for longer and longer time. We prove that the capped expected runtime of the configuration returned by LeapsAndBounds is close to the optimal expected runtime, while our algorithm’s running time is near-optimal. Our results show that LeapsAndBounds is more efficient than the recent algorithm of Kleinberg et al. (2017), which, to our knowledge, is the only other algorithm configuration method with non-trivial theoretical guarantees. Experimental results on configuring a public SAT solver on a new benchmark dataset also stand witness to the superiority of our method.
","['DeepMind', 'DeepMind', 'Deepmind']"
2018,Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,"Ehsan Kazemi, Morteza Zadimoghaddam, Amin Karbasi",https://icml.cc/Conferences/2018/Schedule?showEvent=1927,"Can we efficiently extract useful information from a large user-generated dataset while protecting the privacy of the users and/or ensuring fairness in representation? We cast this problem as an instance of a deletion-robust submodular maximization where part of the data may be deleted or masked due to privacy concerns or fairness criteria. We propose the first memory-efficient centralized, streaming, and distributed methods with constant-factor approximation guarantees against \textit{any} number of adversarial deletions. We extensively evaluate the performance of our algorithms on real-world applications, including (i) Uber-pick up locations with location privacy constraints;  (ii) feature selection with fairness constraints for income prediction and crime rate prediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors. Our experiments show that our solution is robust against even $80\%$ of data deletion.","['Yale', 'Google', 'Yale']"
2018,Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization,"Jinghui Chen, Pan Xu, Lingxiao Wang, Jian Ma, Quanquan Gu",https://icml.cc/Conferences/2018/Schedule?showEvent=2478,"We propose a nonconvex estimator for the covariate adjusted precision matrix estimation problem in the high dimensional regime, under sparsity constraints. To solve this estimator, we propose an alternating gradient descent algorithm with hard thresholding. Compared with existing methods along this line of research, which lack theoretical guarantees in optimization error and/or statistical error, the proposed algorithm not only is computationally much more efficient with a linear rate of convergence, but also attains the optimal statistical rate up to a logarithmic factor. Thorough experiments on both synthetic and real data support our theory.
","['University of Virginia', 'University of California, Los Angeles', 'UCLA', 'Carnegie Mellon University', 'UCLA']"
2018,Comparing Dynamics: Deep Neural Networks versus Glassy Systems,"Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart, Giulio Biroli",https://icml.cc/Conferences/2018/Schedule?showEvent=2190,"We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are the complexity of the loss-landscape and of the dynamics within it, and to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and data-sets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, thus showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.
","['Columbia University', 'ENS/CEA', 'EPFL', 'EPFL', '', ""King's College London"", 'New York University', '', '']"
2018,An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks,"Qianxiao Li, IHPC Shuji Hao",https://icml.cc/Conferences/2018/Schedule?showEvent=1992,"Deep learning is formulated as a discrete-time optimal control problem. This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters. In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin's maximum principle, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set. We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.
","['Institute of High Performance Computing, A*STAR, Singapore', 'IHPC, A*STAR']"
2018,Not All Samples Are Created Equal: Deep Learning with Importance Sampling,"Angelos Katharopoulos, Francois Fleuret",https://icml.cc/Conferences/2018/Schedule?showEvent=2178,"Deep Neural Network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on ""informative"" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5% and 17%.
","['Idiap', 'Idiap research institute']"
2018,"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks","Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, Jeffrey Pennington",https://icml.cc/Conferences/2018/Schedule?showEvent=2421,"In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.
","['Google AI', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']"
2018,Path Consistency Learning in Tsallis Entropy Regularized MDPs,"Yinlam Chow, Ofir Nachum, Mohammad Ghavamzadeh",https://icml.cc/Conferences/2018/Schedule?showEvent=2344,"We study the sparse entropy-regularized reinforcement learning (ERL) problem in which the entropy term is a special form of the Tsallis entropy. The optimal policy of this formulation is sparse, i.e., at each state, it has non-zero probability for only a small number of actions. This addresses the main drawback of the standard Shannon entropy-regularized RL (soft ERL) formulation, in which the optimal policy is softmax, and thus, may assign a non-negligible probability mass to non-optimal actions. This problem is aggravated as the number of actions is increased. In this paper, we follow the work of Nachum et al. (2017) in the soft ERL setting, and propose a class of novel path consistency learning (PCL) algorithms, called sparse PCL, for the sparse ERL problem that can work with both on-policy and off-policy data. We first derive a sparse consistency equation that specifies a relationship between the optimal value function and policy of the sparse ERL along any system trajectory. Crucially, a weak form of the converse is also true, and we quantify the sub-optimality of a policy which satisfies sparse consistency, and show that as we increase the number of actions, this sub-optimality is better than that of the soft ERL optimal policy. We then use this result to derive the sparse PCL algorithms. We empirically compare sparse PCL with its soft counterpart, and show its advantage, especially in problems with a large number of actions.
","['DeepMind', 'Google Brain', 'Facebook AI Research']"
2018,Lipschitz Continuity in Model-based Reinforcement Learning,"Kavosh Asadi, Dipendra Misra, Michael L. Littman",https://icml.cc/Conferences/2018/Schedule?showEvent=2467,"We examine the impact of learning Lipschitz continuous models in the context of model-based reinforcement learning. We provide a novel bound on multi-step prediction error of Lipschitz models where we quantify the error using the Wasserstein metric. We go on to prove an error bound for the value-function estimate arising from Lipschitz models and show that the estimated value function is itself Lipschitz. We conclude with empirical results that show the benefits of controlling the Lipschitz constant of neural-network models.
","['Brown University', 'Cornell University', 'Brown University']"
2018,Bounds on the Approximation Power of Feedforward Neural Networks,"Mohammad Mehrabi, Aslan Tchamkerten, MANSOOR I YOUSEFI",https://icml.cc/Conferences/2018/Schedule?showEvent=2210,"The approximation power of general feedforward neural networks with piecewise linear activation functions is investigated. First, lower bounds on the size of a network are established in terms of the approximation error and network depth and width. These bounds improve upon state-of-the-art bounds for certain classes of functions, such as strongly convex functions. Second, an upper bound is established on the difference of two neural networks with identical weights but different activation functions.
","['Sharif University of Technology', 'Telecom ParisTech', 'Telecom ParisTech']"
2018,Linear Spectral Estimators and an Application to Phase Retrieval,"Ramina Ghods, Andrew Lan, Tom Goldstein, Christoph Studer",https://icml.cc/Conferences/2018/Schedule?showEvent=2280,"Phase retrieval refers to the problem of recovering real-  or complex-valued vectors from magnitude measurements. The best-known algorithms for this problem are iterative in nature and rely on so-called spectral initializers that provide accurate initialization vectors. We propose a novel class of estimators suitable for general nonlinear measurement systems, called linear spectral estimators (LSPEs), which can be used to compute accurate initialization vectors for phase retrieval problems. The proposed LSPEs not only provide accurate initialization vectors for noisy phase retrieval systems with structured or random measurement matrices, but also enable the derivation of sharp and nonasymptotic mean-squared error bounds. We demonstrate the efficacy of LSPEs on synthetic and real-world phase retrieval problems, and we show that our estimators  significantly outperform existing methods for  structured measurement systems that arise in practice.
","['Cornell University', 'Princeton University', 'University of Maryland', 'Cornell University']"
2018,Testing Sparsity over Known and Unknown Bases,"Siddharth Barman, Arnab Bhattacharyya, Suprovat Ghoshal",https://icml.cc/Conferences/2018/Schedule?showEvent=2164,"Sparsity is a basic property of real vectors that is exploited in a wide variety of ma- chine learning applications. In this work, we describe property testing algorithms for spar- sity that observe a low-dimensional projec- tion of the input. We consider two settings. In the first setting, we test sparsity with re- spect to an unknown basis: given input vec- tors $y_1 ,...,y_p \in R^d$ whose concatenation as columns forms $Y \in R^{d \times p}$ , does $Y = AX$ for matrices $A \in R^{d\times m}$ and $X \in R^{m \times p}$ such that each column of $X$ is $k$-sparse, or is $Y$ “far” from having such a decomposition? In the second setting, we test sparsity with re- spect to a known basis: for a fixed design ma- trix $A \in R^{d \times m}$ , given input vector $y \in R^d$ , is $y = Ax$ for some $k$-sparse vector $x$ or is $y$ “far” from having such a decomposition? We analyze our algorithms using tools from high-dimensional geometry and probability.","['Indian Institute of Science', 'Indian Institute of Science', 'Indian Institute of Science']"
2018,Inference Suboptimality in Variational Autoencoders,"Chris Cremer, Xuechen Li, David Duvenaud",https://icml.cc/Conferences/2018/Schedule?showEvent=2425,"Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.
","['University of Toronto', 'University of Toronto', 'University of Toronto']"
2018,Semi-Implicit Variational Inference,"Mingzhang Yin, Mingyuan Zhou",https://icml.cc/Conferences/2018/Schedule?showEvent=2124,"Semi-implicit variational inference (SIVI) is introduced to expand the commonly used analytic variational distribution family, by mixing the variational parameter with a flexible distribution. This mixing distribution can assume any density function, explicit or not, as long as independent random samples can be generated via reparameterization. Not only does SIVI expand the variational family to incorporate highly flexible variational distributions, including implicit ones that have no analytic density functions, but also sandwiches the evidence lower bound (ELBO) between a lower bound and an upper bound, and further derives an asymptotically exact surrogate ELBO that is amenable to optimization via stochastic gradient ascent. With a substantially expanded variational family and a novel optimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the posterior in a variety of Bayesian inference tasks.
","['University of Texas at Austin', 'University of Texas at Austin']"
2018,Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization,"Hang Wu, May Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=2204,"Off-policy learning, the task of evaluating and improving policies using historic data collected from a logging policy, is important because on-policy evaluation is usually expensive and has adverse impacts. One of the major challenge of off-policy learning is to derive counterfactual estimators that also has low variance and thus low generalization error. In this work, inspired by learning bounds for importance sampling problems, we present a new counterfactual learning principle for off-policy learning with bandit feedbacks. Our method regularizes the generalization error by minimizing the distribution divergence between the logging policy and the new policy, and removes the need for iterating through all training samples to compute sample variance regularization in prior work. With neural network policies, our end-to-end training algorithms using variational divergence minimization showed significant improvement over conventional baseline algorithms and is also consistent with our theoretical results.
","['Georgia Institute of Technology', 'Georgia Institute of Technology']"
2018,Limits of Estimating Heterogeneous Treatment Effects: Guidelines for Practical Algorithm Design,"Ahmed M. Alaa, Mihaela van der Schaar",https://icml.cc/Conferences/2018/Schedule?showEvent=2226,"Estimating heterogeneous treatment effects from observational data is a central problem in many domains. Because counterfactual data is inaccessible, the problem differs fundamentally from supervised learning, and entails a more complex set of modeling choices. Despite a variety of recently proposed algorithmic solutions, a principled guideline for building estimators of treatment effects using machine learning algorithms is still lacking. In this paper, we provide such a guideline by characterizing the fundamental limits of estimating heterogeneous treatment effects, and establishing conditions under which these limits can be achieved. Our analysis reveals that the relative importance of the different aspects of observational data vary with the sample size. For instance, we show that selection bias matters only in small-sample regimes, whereas with a large sample size, the way an algorithm models the control and treated outcomes is what bottlenecks its performance. Guided by our analysis, we build a practical algorithm for estimating treatment effects using a non-stationary Gaussian processes with doubly-robust hyperparameters. Using a standard semi-synthetic simulation setup, we show that our algorithm outperforms the state-of-the-art, and that the behavior of existing algorithms conforms with our analysis.
","['UCLA', 'UCLA']"
2018,A Semantic Loss Function for Deep Learning with Symbolic Knowledge,"Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, Guy Van den Broeck",https://icml.cc/Conferences/2018/Schedule?showEvent=2431,"This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.
","['University of California, Los Angeles', 'Peking University', 'UCLA', 'UCLA', 'University of California, Los Angeles']"
2018,Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization,"Jiong Zhang, Qi Lei, Inderjit Dhillon",https://icml.cc/Conferences/2018/Schedule?showEvent=2038,"Vanishing and exploding gradients are two of the main obstacles in training  deep neural networks, especially in capturing long range dependencies in  recurrent neural networks (RNNs). In this paper, we present an efficient  parametrization of the transition matrix of an RNN that allows us to stabilize  the gradients that arise in its training. Specifically, we parameterize the  transition matrix by its singular value decomposition (SVD), which allows us  to explicitly track and control its singular values. We attain efficiency by  using tools that are common in numerical linear algebra, namely Householder  reflectors for representing the orthogonal matrices that arise in the SVD. By  explicitly controlling the singular values, our proposed Spectral-RNN method allows  us to easily solve the exploding gradient problem and we observe that it  empirically solves the vanishing gradient issue to a large extent. We note  that the SVD parameterization can be used for any rectangular weight matrix,  hence it can be easily extended to any deep neural network, such as a  multi-layer perceptron. Theoretically, we demonstrate that our  parameterization does not lose any expressive power, and show how it  potentially makes the optimization process easier. Our extensive  experimental  results also demonstrate that the proposed framework converges faster, and has  good generalization, especially in capturing long range dependencies, as shown  on the synthetic addition and copy tasks, as well as on MNIST and Penn Tree Bank data sets.
","['University of Texas at Austin', 'University of Texas at Austin', 'UT Austin & Amazon']"
2018,An Efficient Semismooth Newton based Algorithm for Convex Clustering,"Yancheng Yuan, Defeng Sun, Kim-Chuan Toh",https://icml.cc/Conferences/2018/Schedule?showEvent=2003,"Clustering is a fundamental problem in unsupervised learning. Popular methods like K-means, may suffer from instability as they are prone to get stuck in its local minima. Recently, the sumof-norms (SON) model (also known as clustering path), which is a convex relaxation of hierarchical clustering model, has been proposed in (Lindsten et al., 2011) and (Hocking et al., 2011). Although numerical algorithms like alternating direction method of multipliers (ADMM) and alternating minimization algorithm (AMA) have been proposed to solve convex clustering model (Chi & Lange, 2015), it is known to be very challenging to solve large-scale problems. In this paper, we propose a semismooth Newton based augmented Lagrangian method for large-scale convex clustering problems. Extensive numerical experiments on both simulated and real data demonstrate that our algorithm is highly efficient and robust for solving large-scale problems. Moreover, the numerical results also show the superior performance and scalability of our algorithm comparing to existing first-order methods.
","['National University of Singapore', 'The Hong Kong Polytechnic University', 'National University of Singapre']"
2018,Lightweight Stochastic Optimization for Minimizing Finite Sums with Infinite Data,"Shuai Zheng, James Kwok",https://icml.cc/Conferences/2018/Schedule?showEvent=2215,"Variance reduction has been commonly used in stochastic optimization. It relies crucially on the assumption that the data set is finite. However, when the data are imputed with random noise as in data augmentation, the perturbed data set becomes essentially infinite. Recently, the stochastic MISO (S-MISO) algorithm is introduced to address this expected risk minimization problem. Though it converges faster than SGD, a significant amount of memory is required. In this paper, we propose two SGD-like algorithms for expected risk minimization with random perturbation, namely, stochastic sample average gradient (SSAG) and stochastic SAGA (S-SAGA). The memory cost of SSAG does not depend on the sample size, while that of S-SAGA is the same as those of variance reduction methods on unperturbed data. Theoretical analysis and experimental results on logistic regression and AUC maximization show that SSAG has faster convergence rate than SGD with comparable space requirement while S-SAGA outperforms S-MISO in terms of both iteration complexity and storage.
","['Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology']"
2018,Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search,"Masanori SUGANUMA, Mete Ozay, Takayuki Okatani",https://icml.cc/Conferences/2018/Schedule?showEvent=2329,"Researchers have applied deep neural networks to image restoration tasks, in which they proposed various network architectures, loss functions, and training methods. In particular, adversarial training, which is employed in recent studies, seems to be a key ingredient to success. In this paper, we show that simple convolutional autoencoders (CAEs) built upon only standard network components, i.e., convolutional layers and skip connections, can outperform the state-of-the-art methods which employ adversarial training and sophisticated loss functions. The secret is to search for good architectures using an evolutionary algorithm. All we did was to train the optimized CAEs by minimizing the l2 loss between reconstructed images and their ground truths using the ADAM optimizer. Our experimental results show that this approach achieves 27.8 dB peak signal to noise ratio (PSNR) on the CelebA dataset and 33.3 dB on the SVHN dataset, compared to 22.8 dB and 19.0 dB provided by the former state-of-the-art methods, respectively.
","['RIKEN AIP / Tohoku University', 'Tohoku University', 'Tohoku University/RIKEN AIP']"
2018,Efficient Neural Architecture Search via Parameters Sharing,"Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, Jeff Dean",https://icml.cc/Conferences/2018/Schedule?showEvent=2247,"We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet (Zoph et al., 2018).
","['Carnegie Mellon University', 'Stanford University', 'Google', 'Google Brain', 'Google Brain']"
2018,Non-convex Conditional Gradient Sliding,"chao qu, Yan Li, Huan Xu",https://icml.cc/Conferences/2018/Schedule?showEvent=1947,"We investigate a projection free optimization method, namely non-convex conditional gradient sliding (NCGS) for non-convex optimization problems on the  batch, stochastic and finite-sum settings.  Conditional gradient sliding (CGS) method, by integrating Nesterov's accelerated gradient method with Frank-Wolfe  (FW) method in a smart way, outperforms  FW  for convex optimization, by reducing the amount of gradient computations. However, the study of CGS in the non-convex setting is limited. In this paper, we propose the non-convex conditional gradient sliding (NCGS) methods and analyze their convergence properties. We also leverage the idea of variance reduction from the recent progress in convex optimization to obtain  a new algorithm termed  {\em variance reduced NCGS} (NCGS-VR), and obtain faster convergence rate than the batch NCGS in the finite-sum setting. We show that NCGS algorithms outperform their   Frank-Wolfe counterparts both in theory and in practice, for all three settings, namely the batch, stochastic  and finite-sum setting. This significantly improves our understanding of optimizing non-convex functions with complicated feasible sets (where projection is prohibitively expensive).
","['technion', 'Georgia Institute of Technology', 'Georgia Tech']"
2018,Stochastic Variance-Reduced Cubic Regularized Newton Method,"Dongruo Zhou, Pan Xu, Quanquan Gu",https://icml.cc/Conferences/2018/Schedule?showEvent=2223,"We propose a stochastic variance-reduced cubic regularized Newton method (SVRC) for non-convex optimization. At the core of our algorithm is a novel semi-stochastic gradient along with a semi-stochastic Hessian, which are specifically designed for cubic regularization method. We show that our algorithm is guaranteed to converge to an $(\epsilon,\sqrt{\epsilon})$-approximate local minimum within $\tilde{O}(n^{4/5}/\epsilon^{3/2})$ second-order oracle calls, which outperforms the state-of-the-art cubic regularization algorithms including subsampled cubic regularization. Our work also sheds light on the application of variance reduction technique to high-order non-convex optimization methods. Thorough experiments on various non-convex optimization problems support our theory.","['University of California, Los Angeles', 'University of California, Los Angeles', 'UCLA']"
2018,On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization,"Sanjeev Arora, Nadav Cohen, Elad Hazan",https://icml.cc/Conferences/2018/Schedule?showEvent=2422,"Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization -- linear neural networks, a well-studied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with $\ell_p$ loss, $p>2$, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. We also prove that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer.","[' Princeton University and Institute for Advanced Study', 'Institute for Advanced Study', 'Google Brain and Princeton University']"
2018,The Dynamics of Learning: A Random Matrix Approach,"Zhenyu Liao, Romain Couillet",https://icml.cc/Conferences/2018/Schedule?showEvent=2023,"Understanding the learning dynamics of neural networks is one of the key issues for the improvement of optimization algorithms as well as for the theoretical comprehension of why deep neural nets work so well today. In this paper, we introduce a random matrix-based framework to analyze the learning dynamics of a single-layer linear network on a binary classification problem, for data of simultaneously large dimension and size, trained by gradient descent. Our results provide rich insights into common questions in neural nets, such as overfitting, early stopping and the initialization of training, thereby opening the door for future studies of more elaborate structures and models appearing in today's neural networks.
","['L2S, CentraleSupelec', 'CentralSupélec']"
2018,Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations,"Ting Chen, Martin Min, Yizhou Sun",https://icml.cc/Conferences/2018/Schedule?showEvent=2284,"Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying a linear transformation based on a ``one-hot'' encoding of the discrete symbols. Despite its simplicity, such approach yields the number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work, we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the ``one-hot"" encoding. In the proposed ``KD encoding'', each symbol is represented by a $D$-dimensional code with a cardinality of $K$, and the final symbol embedding vector is generated by composing the code embedding vectors. To end-to-end learn semantically meaningful codes, we derive a relaxed discrete optimization approach based on stochastic gradient descent, which can be generally applied to any differentiable computational graph with an embedding layer. In our experiments with various applications from natural language processing to graph convolutional networks, the total size of the embedding layer can be reduced up to 98% while achieving similar or better performance.","['UCLA', 'NEC Laboratories America', 'UCLA']"
2018,Discovering Interpretable Representations for Both Deep Generative and Discriminative Models,"Tameem Adel, Zoubin Ghahramani, Adrian Weller",https://icml.cc/Conferences/2018/Schedule?showEvent=1908,"Interpretability of representations in both deep generative and discriminative models is highly desirable. Current methods jointly optimize an objective combining accuracy and interpretability. However, this may reduce accuracy, and is not applicable to already trained models. We propose two interpretability frameworks. First, we provide an interpretable lens for an existing model. We use a generative model which takes as input the representation in an existing (generative or discriminative) model, weakly supervised by limited side information. Applying a flexible and invertible transformation to the input leads to an interpretable representation with no loss in accuracy. We extend the approach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what ""interpretable"" means. Our second framework relies on joint optimization for a representation which is both maximally informative about the side information and maximally compressive about the non-interpretable data factors. This leads to a novel perspective on the relationship between compression and regularization. We also propose a new interpretability evaluation metric based on our framework. Empirically, we achieve state-of-the-art results on three datasets using the two proposed algorithms.
","['University of Cambridge', 'University of Cambridge & Uber', 'University of Cambridge, Alan Turing Institute']"
2018,Continuous-Time Flows for Efficient Inference and Density Estimation,"Changyou Chen, Chunyuan Li, Liquan Chen, Wenlin Wang, Yunchen Pu, Lawrence Carin",https://icml.cc/Conferences/2018/Schedule?showEvent=2258,"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. Algorithms for the two tasks, such as normalizing flows and generative adversarial networks (GANs), are often developed independently. In this paper, we propose the concept of {\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Both tasks rely on a new technique for distribution matching within amortized learning. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.
","['SUNY at Buffalo', 'Duke University', 'Duke University', 'Duke University', 'Duke', 'Duke']"
2018,Tighter Variational Bounds are Not Necessarily Better,"Tom Rainforth, Adam Kosiorek, Tuan Anh Le, Chris Maddison, Maximilian Igl, Frank Wood, Yee-Whye Teh",https://icml.cc/Conferences/2018/Schedule?showEvent=2399,"We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.
","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'Oxford and DeepMind']"
2018,PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning,"Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, Philip Yu",https://icml.cc/Conferences/2018/Schedule?showEvent=1967,"We present PredRNN++, a recurrent network for spatiotemporal predictive learning. In pursuit of a great modeling capability for short-term video dynamics, we make our network deeper in time by leveraging a new recurrent structure named Causal LSTM with cascaded dual memories. To alleviate the gradient propagation difficulties in deep predictive models, we propose a Gradient Highway Unit, which provides alternative quick routes for the gradient flows from outputs back to long-range previous inputs. The gradient highway units work seamlessly with the causal LSTMs, enabling our model to capture the short-term and the long-term video dependencies adaptively. Our model achieves state-of-the-art prediction results on both synthetic and real video datasets, showing its power in modeling entangled motions.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'UIC']"
2018,RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks,"Jinsung Yoon, James Jordon, Mihaela van der Schaar",https://icml.cc/Conferences/2018/Schedule?showEvent=2027,"Training complex machine learning models for prediction often requires a large amount of data that is not always readily available. Leveraging these external datasets from related but different sources is therefore an important task if good predictive models are to be built for deployment in settings where data can be rare. In this paper we propose a novel approach to the problem in which we use multiple GAN architectures to learn to translate from one dataset to another, thereby allowing us to effectively enlarge the target dataset, and therefore learn better predictive models than if we simply used the target dataset. We show the utility of such an approach, demonstrating that our method improves the prediction performance on the target domain over using just the target dataset and also show that our framework outperforms several other benchmarks on a collection of real-world medical datasets.
","['University of California, Los Angeles', 'University of Oxford', 'University of Oxford']"
2018,Differentiable Compositional Kernel Learning for Gaussian Processes,"Shengyang Sun, Guodong Zhang, Chaoqi Wang, Wenyuan Zeng, Jiaman Li, Roger Grosse",https://icml.cc/Conferences/2018/Schedule?showEvent=2378,"The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The NKN’s architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient- based optimization. We show that the NKN is universal for the class of stationary kernels. Empirically we demonstrate NKN’s pattern discovery and extrapolation abilities on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.
","['University of Toronto', 'University of Toronto', 'University of Toronto', 'University of Toronto, Uber ATG', 'University of Toronto', 'University of Toronto and Vector Institute']"
2018,Markov Modulated Gaussian Cox Processes for Semi-Stationary Intensity Modeling of Events Data,Minyoung Kim,https://icml.cc/Conferences/2018/Schedule?showEvent=2047,"The Cox process is a flexible event model that can account for uncertainty of the intensity function in the Poisson process. However, previous approaches make strong assumptions in terms of time stationarity, potentially failing to generalize when the data do not conform to the assumed stationarity conditions. In this paper we bring up two most popular Cox models representing two extremes, and propose a novel semi-stationary Cox process model that can take benefits from both models. Our model has a set of Gaussian process latent functions governed by a latent stationary Markov process where we provide analytic derivations for the variational inference. Empirical evaluations on several synthetic and real-world events data including the football shot attempts and daily earthquakes, demonstrate that the proposed model is promising, can yield improved generalization performance over existing approaches.
","['SeoulTech, Rutgers University']"
2018,Improved Regret Bounds for Thompson Sampling in Linear Quadratic Control Problems,"Marc Abeille, Alessandro Lazaric",https://icml.cc/Conferences/2018/Schedule?showEvent=2353,"Thompson sampling (\ts) is an effective approach to trade off exploration and exploration in reinforcement learning. Despite its empirical success and recent advances, its theoretical analysis is often limited to the Bayesian setting, finite state-action spaces, or finite-horizon problems. In this paper, we study an instance of \ts in the challenging setting of the infinite-horizon linear quadratic (LQ) control, which models problems with continuous state-action variables, linear dynamics, and quadratic cost. In particular, we analyze the regret in the frequentist sense (i.e., for a fixed unknown environment) in one-dimensional systems. We derive the first $O(\sqrt{T})$ frequentist regret bound for this problem, thus significantly improving the $O(T^{2/3})$ bound of~\citet{abeille2017thompson} and matching the frequentist performance derived by~\citet{abbasi2011regret} for an optimistic approach and the Bayesian result of~\citet{ouyang2017learning-based}.  We obtain this result by developing a novel bound on the regret due to policy switches, which holds for LQ systems of any dimensionality and it allows updating the parameters and the policy at each step, thus overcoming previous limitations due to lazy updates. Finally, we report numerical simulations supporting the conjecture that our result extends to multi-dimensional systems.","['Criteo', 'Facebook AI Research']"
2018,Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches,"Simon Olofsson, Marc P Deisenroth, Ruth Misener",https://icml.cc/Conferences/2018/Schedule?showEvent=2236,"Healthcare companies must submit pharmaceutical drugs or medical device to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and interpretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological system and too little data to discriminate between the models. In design of experiments for model discrimination, where the goal is to design maximally informative physical experiments in order to discriminate between rival predictive models, research has focused either on analytical approaches, which cannot manage all functions, or on data-driven approaches, which may have computational difficulties or lack interpretable marginal predictive distributions. We develop a methodology for introducing Gaussian process surrogates in lieu of the original mechanistic models. This allows us to extend existing design and model discrimination methods developed for analytical models to cases of non-analytical models.
","['Imperial College London', 'Imperial College London and PROWLER.io', 'Imperial College London']"
2018,Anonymous Walk Embeddings,"Sergey Ivanov, Evgeny Burnaev",https://icml.cc/Conferences/2018/Schedule?showEvent=1875,"The task of representing entire graphs has seen a surge of prominent results, mainly due to learning convolutional neural networks (CNNs) on graph-structured data. While CNNs demonstrate state-of-the-art performance in graph classification task, such methods are supervised and therefore steer away from the original problem of network representation in task-agnostic manner. Here, we coherently propose an approach for embedding entire graphs and show that our feature representations with SVM classifier increase classification accuracy of CNN algorithms and traditional graph kernels. For this we describe a recently discovered graph object, \textit{anonymous walk}, on which we design task-independent algorithms for learning graph representations in explicit and distributed way. Overall, our work represents a new scalable unsupervised learning of state-of-the-art representations of entire graphs.
","['Criteo', 'Skoltech']"
2018,Improving Optimization in Models With Continuous Symmetry Breaking,"Robert Bamler, Stephan Mandt",https://icml.cc/Conferences/2018/Schedule?showEvent=2360,"Many loss functions in representation learning are invariant under a continuous symmetry transformation. For example, the loss function of word embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate all word and context embedding vectors. We show that representation learning models for time series possess an approximate continuous symmetry that leads to slow convergence of gradient descent. We propose a new optimization algorithm that speeds up convergence using ideas from gauge theory in physics. Our algorithm leads to orders of magnitude faster convergence and to more interpretable representations, as we show for dynamic extensions of matrix factorization and word embedding models. We further present an example application of our proposed algorithm that translates modern words into their historic equivalents.
","['UC Irvine', 'UC Irvine']"
2018,Conditional Noise-Contrastive Estimation of Unnormalised Models,"Ciwan Ceylan, Michael Gutmann",https://icml.cc/Conferences/2018/Schedule?showEvent=2158,"Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semi-automated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.
","['RWTH', 'University of Edinburgh']"
2018,Canonical Tensor Decomposition for Knowledge Base Completion,"Timothee Lacroix, Nicolas Usunier, Guillaume Obozinski",https://icml.cc/Conferences/2018/Schedule?showEvent=2246,"The problem of Knowledge Base Completion can be framed as a 3rd-order binary tensor completion problem. In this light, the Canonical Tensor Decomposition (CP) seems like a natural solution; however, current implementations of CP on standard Knowledge Base Completion benchmarks are lagging behind their competitors. In this work, we attempt to understand the limits of CP for knowledge base completion. First, we motivate and test a novel regularizer, based on tensor nuclear p-norms. Then, we present a reformulation of the problem that makes it invariant to arbitrary choices in the inclusion of predicates or their reciprocals in the dataset. These two methods combined allow us to beat the current state of the art on several datasets with a CP decomposition, and obtain even better results using the more advanced ComplEx model.
","['Facebook', 'Facebook AI Research', 'Ecole des Ponts - ParisTech']"
2018,The Power of Interpolation:  Understanding the Effectiveness of SGD in Modern Over-parametrized Learning,"Siyuan Ma, Raef Bassily, Mikhail Belkin",https://icml.cc/Conferences/2018/Schedule?showEvent=1968,"In this paper we aim to formally explain the phenomenon of fast convergence of Stochastic Gradient Descent (SGD) observed in modern machine learning. The key observation is that most modern learning architectures are over-parametrized and are trained to interpolate the data by driving the empirical loss (classification and regression) close to zero. While it is still unclear why these interpolated solutions perform well on test data, we show that these regimes allow for fast convergence of SGD, comparable in number of iterations to full gradient descent. For convex loss functions we obtain an exponential  convergence bound for {\it mini-batch} SGD  parallel to that for full gradient descent. We show that there is a critical batch size $m^*$ such that: (a) SGD iteration with mini-batch size $m\leq m^*$ is  nearly equivalent to $m$ iterations of mini-batch size $1$ (\emph{linear scaling regime}). (b) SGD iteration with mini-batch $m> m^*$ is nearly equivalent to a full gradient descent iteration (\emph{saturation regime}). Moreover, for the quadratic loss, we derive explicit expressions for the optimal mini-batch and step size and explicitly characterize the two regimes above. The critical mini-batch size can be viewed as the limit for effective mini-batch parallelization. It is also nearly independent of the data size, implying $O(n)$ acceleration over GD per unit of computation. We give experimental evidence on real data which closely follows our theoretical analyses. Finally, we show how our results fit in the recent developments in training deep neural networks and discuss connections to adaptive rates for SGD and variance reduction.","['The Ohio State University', 'Ohio State University', 'Ohio State University']"
2018,A Simple Stochastic Variance Reduced Algorithm with Fast Convergence Rates,"Kaiwen Zhou, Fanhua Shang, James Cheng",https://icml.cc/Conferences/2018/Schedule?showEvent=2016,"Recent years have witnessed exciting progress in the study of stochastic variance reduced gradient methods (e.g., SVRG, SAGA), their accelerated variants (e.g, Katyusha) and their extensions in many different settings (e.g., online, sparse, asynchronous, distributed). Among them, accelerated methods enjoy improved convergence rates but have complex coupling structures, which makes them hard to be extended to more settings (e.g., sparse and asynchronous) due to the existence of perturbation. In this paper, we introduce a simple stochastic variance reduced algorithm (MiG), which enjoys the best-known convergence rates for both strongly convex and non-strongly convex problems. Moreover, we also present its efficient sparse and asynchronous variants, and theoretically analyze its convergence rates in these settings. Finally, extensive experiments for various machine learning problems such as logistic regression are given to illustrate the practical improvement in both serial and asynchronous settings.
","['The Chinese University of Hong Kong', 'The Chinese University of Hong Kong', 'CUHK']"
2018,Escaping Saddles with Stochastic Gradients,"Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, Thomas Hofmann",https://icml.cc/Conferences/2018/Schedule?showEvent=2265,"We analyze the variance of stochastic gradients along negative curvature directions in certain non-convex machine learning models and show that stochastic gradients indeed exhibit a strong component along these directions. Furthermore, we show that - contrary to the case of isotropic noise - this variance is proportional to the magnitude of the corresponding eigenvalues and not decreasing in the dimensionality. Based upon this bservation we propose a new assumption under which we show that the injection of explicit, isotropic noise usually applied to make gradient descent escape saddle points can successfully be replaced by a simple SGD step. Additionally - and under the same condition - we derive the first convergence rate for plain SGD to a second-order stationary point in a number of iterations that is independent of the problem dimension.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2018,$D^2$: Decentralized Training over Decentralized Data,"Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, Ji Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2485,"While training a machine learning  model using multiple workers, each of     which collects data from its own data source, it would be useful     when the data collected from different workers are {\em unique} and {\em       different}. Ironically, recent analysis of decentralized parallel   stochastic gradient descent (D-PSGD) relies on the assumption that the data   hosted on different workers are {\em not too different}. In this paper, we ask   the question: {\em Can we design a decentralized parallel stochastic gradient     descent algorithm that is less sensitive to the data variance across     workers?} In this paper, we present D$^2$, a novel decentralized parallel stochastic   gradient descent algorithm designed for large data variance \xr{among workers}   (imprecisely, ``decentralized'' data). The core of D$^2$ is a variance   reduction extension of D-PSGD. It improves the   convergence rate from $O\left({\sigma \over \sqrt{nT}} +     {(n\zeta^2)^{\frac{1}{3}} \over T^{2/3}}\right)$ to $O\left({\sigma \over       \sqrt{nT}}\right)$ where $\zeta^{2}$ denotes the variance among data on     different workers. As a result, D$^2$ is robust to data variance among   workers. We empirically evaluated D$^2$ on image classification     tasks, where each worker has access to only the data of a     limited set of labels, and find that D$^2$ significantly outperforms   D-PSGD.","['University of Rochester', 'University of Rochester', 'Michigan State University', 'ETH Zurich', 'University of Rochester']"
2018,Machine Theory of Mind,"Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S. M. Ali Eslami, Matthew Botvinick",https://icml.cc/Conferences/2018/Schedule?showEvent=2159,"Theory of mind (ToM) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network – a ToMnet – which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong prior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstrap to richer predictions about agents’ characteristics and mental states. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep RL agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test of recognising that others can hold false beliefs about the world.
","['DeepMind', 'DeepMind', 'DeepMind', 'Google', 'DeepMind', 'DeepMind']"
2018,"Been There, Done That: Meta-Learning with Episodic Recall","Samuel Ritter, Jane Wang, Zeb Kurth-Nelson, Siddhant Jayakumar, Charles Blundell, Razvan Pascanu, Matthew Botvinick",https://icml.cc/Conferences/2018/Schedule?showEvent=2252,"Meta-learning agents excel at rapidly learning new tasks from open-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur – as they do in natural environments – meta-learning agents must explore again instead of immediately exploiting previously discovered solutions. We propose a formalism for generating open-ended yet repetitious environments, then develop a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural episodic memory. We explore the capabilities of agents with this episodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision problems.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2018,Faster Derivative-Free Stochastic Algorithm for Shared Memory Machines,"Bin Gu, Zhouyuan Huo, Cheng Deng, Heng Huang",https://icml.cc/Conferences/2018/Schedule?showEvent=2022,"Asynchronous parallel stochastic gradient optimization has been playing a pivotal role to solve large-scale machine learning problems in big data applications. Zeroth-order (derivative-free) methods estimate the gradient only by two function evaluations, thus have been applied to solve the problems where the explicit gradient calculations are computationally expensive or infeasible. Recently, the first asynchronous parallel stochastic zeroth-order algorithm (AsySZO) was proposed. However, its convergence rate is O(1/SQRT{T}) for the smooth, possibly non-convex learning problems, which is significantly slower than O(1/T) the best convergence rate of (asynchronous) stochastic gradient algorithm. To fill this gap, in this paper, we first point out the fundamental reason leading to the slow convergence rate of AsySZO, and then propose a new asynchronous stochastic zerothorder algorithm (AsySZO+). We provide a faster convergence rate O(1/bT) (b is the mini-batch size) for AsySZO+ by the rigorous theoretical analysis, which is a significant improvement over O(1/SQRT{T}). The experimental results on the application of ensemble learning confirm that our AsySZO+ has a faster convergence rate than the existing (asynchronous) stochastic zeroth-order algorithms.
","['University of Pittsburgh', 'University of Pittsburgh', 'Xidian University', 'University of Pittsburgh']"
2018,Coded Sparse Matrix Multiplication,"Sinong Wang, Jiashang Liu, Ness Shroff",https://icml.cc/Conferences/2018/Schedule?showEvent=1924,"In a large-scale and distributed matrix multiplication problem $C=A^{\intercal}B$, where $C\in\mathbb{R}^{r\times t}$,  the coded computation plays an important role to effectively deal with ``stragglers'' (distributed computations that may get delayed due to few slow or faulty processors). However, existing coded schemes could destroy the significant sparsity that exists in large-scale machine learning problems, and could result in much higher computation overhead, i.e., $O(rt)$ decoding time. In this paper, we develop a new coded computation strategy, we call \emph{sparse code}, which achieves near \emph{optimal recovery threshold}, \emph{low computation overhead}, and \emph{linear decoding time} $O(nnz(C))$.  We implement our scheme and demonstrate the advantage of the approach over both uncoded and current fastest coded strategies.","['The Ohio State University', 'The Ohio State University', 'The Ohio State University']"
2018,Augment and Reduce: Stochastic Inference for Large Categorical Distributions,"Francisco Ruiz, Michalis Titsias, Adji Bousso Dieng, David Blei",https://icml.cc/Conferences/2018/Schedule?showEvent=1936,"Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems. However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes. To address this problem, we propose augment and reduce (A&R), a method to alleviate the computational complexity. A&R uses two ideas: latent variable augmentation and stochastic variational inference. It maximizes a lower bound on the marginal likelihood of the data. Unlike existing methods which are specific to softmax, A&R is more general and is amenable to other categorical models, such as multinomial probit. On several large-scale classification problems, we show that A&R provides a tighter bound on the marginal likelihood and has better predictive performance than existing approaches.
","['University of Cambridge / Columbia University', 'Athens University of Economics and Business', 'Columbia University', 'Columbia University']"
2018,Efficient Gradient-Free Variational Inference using Policy Search,"Oleg Arenz, Gerhard Neumann, Mingjun Zhong",https://icml.cc/Conferences/2018/Schedule?showEvent=2482,"Inference from complex distributions is a common problem in machine learning needed for many Bayesian methods. We propose an efficient, gradient-free method for learning general GMM approximations of multimodal distributions based on recent insights from stochastic search methods. Our method establishes information-geometric trust regions to ensure efficient exploration of the sampling space and stability of the GMM updates, allowing for efficient estimation of multi-variate Gaussian variational distributions. For GMMs, we apply a variational lower bound to decompose the learning objective into sub-problems given by learning the individual mixture components and the coefficients. The number of mixture components is adapted online in order to allow for arbitrary exact approximations. We demonstrate on several domains that we can learn significantly better approximations than competing variational inference methods and that the quality of samples drawn from our approximations is on par with samples created by state-of-the-art MCMC samplers that require significantly more computational resources.
","['TU Darmstadt', 'University of Lincoln', 'University of Lincoln']"
2018,Fixing a Broken ELBO,"Alexander Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif Saurous, Kevin Murphy",https://icml.cc/Conferences/2018/Schedule?showEvent=2442,"Recent work in unsupervised representation learning has focused on learning deep directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.
","['Google', 'Stanford University', 'Google', 'Google', '', 'Google Brain']"
2018,Variational Inference and Model Selection with Generalized Evidence Bounds,"Liqun Chen, Chenyang Tao, RUIYI (ROY) ZHANG, Ricardo Henao, Lawrence Carin",https://icml.cc/Conferences/2018/Schedule?showEvent=2377,"Recent advances on the scalability and flexibility of variational inference have made it successful at unravelling hidden patterns in complex data. In this work we propose a new variational bound formulation, yielding an estimator that extends beyond the conventional variational bound. It naturally subsumes the importance-weighted and Renyi bounds as special cases, and it is provably sharper than these counterparts. We also present an improved estimator for variational learning, and advocate a novel high signal-to-variance ratio update rule for the variational parameters. We discuss model-selection issues associated with existing evidence-lower-bound-based variational inference procedures, and show how to leverage the flexibility of our new formulation to address them. Empirical evidence is provided to validate our claims.
","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke']"
2018,The Generalization Error of Dictionary Learning with Moreau Envelopes,ALEXANDROS GEORGOGIANNIS,https://icml.cc/Conferences/2018/Schedule?showEvent=1931,"This is a theoretical  study on the sample  complexity of dictionary learning with a general type of reconstruction loss. The goal is to estimate a $m \times d$ matrix $D$ of unit-norm columns when the only available information is a set of training samples. Points $x$ in $\mathbb{R}^m$ are subsequently approximated by the linear combination $Da$ after solving the problem $\min_{a \in \mathbb{R}^d}~ \Phi(x - Da) + g(a)$; function $g:\mathbb{R}^d \to [0,+\infty)$ is either an indicator function or a sparsity promoting regularizer. Here is considered the case where $ \Phi(x) = \inf_{z \in \mathbb{R}^m}~{ ||x-z||_2^2 + h(||z||_2)}$ and $h$ is an even and univariate function on the real line. Connections are drawn between $\Phi$ and the Moreau envelope of $h$. A new sample complexity result concerning the $k$-sparse dictionary problem removes the spurious condition on the coherence of $D$ appearing in previous works. Finally, comments are made on the approximation error of certain families of losses. The derived generalization bounds are of order $\mathcal{O}(\sqrt{\log n /n})$ and valid without any further restrictions on the set of dictionaries with unit-norm columns.",['TECHNICAL UNIVERSITY OF CRETE']
2018,Network Global Testing by Counting Graphlets,"Jiashun Jin, Zheng Ke, Shengming Luo",https://icml.cc/Conferences/2018/Schedule?showEvent=2338,"Consider a large social network with possibly severe degree heterogeneity and mixed-memberships. We are interested in testing whether the network has only one community or there are more than one communities. The problem is known to be non-trivial, partially due to the presence of severe degree heterogeneity.  We construct a class of test statistics using the numbers of short paths and short cycles,  and the key to our approach is a general framework for canceling the effects of degree heterogeneity. The tests compare favorably with existing methods. We support our methods with careful analysis and numerical study with simulated data and a real data example.
","['Carnegie Mellon University', 'University of Chicago', 'CMU']"
2018,Large-Scale Sparse Inverse Covariance Estimation via Thresholding and Max-Det Matrix Completion,"Richard Zhang, Salar Fattahi, Somayeh Sojoudi",https://icml.cc/Conferences/2018/Schedule?showEvent=2189,"The sparse inverse covariance estimation problem is commonly solved using an $\ell_{1}$-regularized Gaussian maximum likelihood estimator known as “graphical lasso”, but its computational cost becomes prohibitive for large data sets. A recently line of results showed–under mild assumptions–that the graphical lasso estimator can be retrieved by soft-thresholding the sample covariance matrix and solving a maximum determinant matrix completion (MDMC) problem. This paper proves an extension of this result, and describes a Newton-CG algorithm to efficiently solve the MDMC problem. Assuming that the thresholded sample covariance matrix is sparse with a sparse Cholesky factorization, we prove that the algorithm converges to an $\epsilon$-accurate solution in $O(n\log(1/\epsilon))$ time and $O(n)$ memory. The algorithm is highly efficient in practice: we solve the associated MDMC problems with as many as 200,000 variables to 7-9 digits of accuracy in less than an hour on a standard laptop computer running MATLAB.","['University of California, Berkeley', 'UC Berkeley', 'University of California, Berkeley']"
2018,Robust and Scalable Models of Microbiome Dynamics,"Travis Gibson, Georg Gerber",https://icml.cc/Conferences/2018/Schedule?showEvent=2382,"Microbes are everywhere, including in and on our bodies, and have been shown to play key roles in a variety of prevalent human diseases. Consequently, there has been intense interest in the design of bacteriotherapies or ""bugs as drugs,"" which are communities of bacteria administered to patients for specific therapeutic applications. Central to the design of such therapeutics is an understanding of the causal microbial interaction network and the population dynamics of the organisms. In this work we present a Bayesian nonparametric model and associated efficient inference algorithm that addresses the key conceptual and practical challenges of learning microbial dynamics from time series microbe abundance data. These challenges include high-dimensional (300+ strains of bacteria in the gut) but temporally sparse and non-uniformly sampled data; high measurement noise; and, nonlinear and physically non-negative dynamics. Our contributions include a new type of dynamical systems model for microbial dynamics based on what we term interaction modules, or learned clusters of latent variables with redundant interaction structure (reducing the expected number of interaction coefficients from O(n^2) to O((log n)^2)); a fully Bayesian formulation of the stochastic dynamical systems model that propagates measurement and latent state uncertainty throughout the model; and introduction of a temporally varying auxiliary variable technique to enable efficient inference by relaxing the hard non-negativity constraint on states. We apply our method to simulated and real data, and demonstrate the utility of our technique for system identification from limited data and gaining new biological insights into bacteriotherapy design.
","['Harvard Medical School', 'Harvard Medical School']"
2018,Explicit Inductive Bias for Transfer Learning with Convolutional Networks,"Xuhong LI, Yves Grandvalet, Franck Davoine",https://icml.cc/Conferences/2018/Schedule?showEvent=2225,"In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch. When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task. However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task. In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model. We show the benefit of having an explicit inductive bias towards the initial model, and we eventually recommend a simple $L^2$ penalty with the pre-trained model being a reference as the baseline of penalty for transfer learning tasks.","['CNRS/UTC Heudiasyc', 'CNRS/UTC', 'CNRS, Université de technologie de Compiègne']"
2018,GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,"Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich",https://icml.cc/Conferences/2018/Schedule?showEvent=2419,"Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.","['Magic Leap, Inc', 'Magic Leap Inc.', '', 'Magic Leap, Inc.']"
2018,Optimizing the Latent Space of Generative Networks,"Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam",https://icml.cc/Conferences/2018/Schedule?showEvent=2235,"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful applications, GAN models share two common aspects: solving a challenging saddle point optimization problem, interpreted as an adversarial game between a generator and a discriminator functions; and parameterizing the generator and the discriminator as deep convolutional neural networks. The goal of this paper is to disentangle the contribution of these two factors to the success of GANs. In particular, we introduce Generative Latent Optimization (GLO), a framework to train deep convolutional generators using simple reconstruction losses. Throughout a variety of experiments, we show that GLO enjoys many of the desirable properties of GANs: synthesizing visually-appealing samples, interpolating meaningfully between samples, and performing linear arithmetic with noise vectors; all of this without the adversarial optimization scheme.
","['Facebook', 'Facebook', 'Facebook AI Research', 'Facebook']"
2018,Theoretical Analysis of Image-to-Image Translation with Adversarial Learning,"Xudong Pan, Mi Zhang, Daizong Ding",https://icml.cc/Conferences/2018/Schedule?showEvent=1892,"Recently, a unified model for image-to-image translation tasks within adversarial learning framework has aroused widespread research interests in computer vision practitioners. Their reported empirical success however lacks solid theoretical interpretations for its inherent mechanism. In this paper, we reformulate their model from a brand-new geometrical perspective and have eventually reached a full interpretation on some interesting but unclear empirical phenomenons from their experiments. Furthermore, by extending the definition of generalization for generative adversarial nets to a broader sense, we have derived a condition to control the generalization capability of their model. According to our derived condition, several practical suggestions have also been proposed on model design and dataset construction as a guidance for further empirical researches.
","['Fudan University', 'Fudan University', 'Fudan University']"
2018,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,"Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine",https://icml.cc/Conferences/2018/Schedule?showEvent=1986,"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.
","['UC Berkeley', 'UC Berkeley', 'OpenAI / UC Berkeley', 'Berkeley']"
2018,PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos,"Paavo Parmas, Carl E Rasmussen, Jan Peters, Kenji Doya",https://icml.cc/Conferences/2018/Schedule?showEvent=1978,"Previously, the exploding gradient problem has been explained to be   central in deep learning and model-based reinforcement learning,   because it causes numerical issues and instability in   optimization. Our experiments in model-based reinforcement learning   imply that the problem is not just a numerical issue, but it may be   caused by a fundamental chaos-like nature of long chains of   nonlinear computations. Not only do the magnitudes of the gradients   become large, the direction of the gradients becomes essentially   random. We show that reparameterization gradients suffer from the   problem, while likelihood ratio gradients are robust. Using our   insights, we develop a model-based policy search framework,   Probabilistic Inference for Particle-Based Policy Search (PIPPS),   which is easily extensible, and allows for almost arbitrary models   and policies, while simultaneously matching the performance of   previous data-efficient learning algorithms. Finally, we invent   the total propagation algorithm, which efficiently   computes a union over all pathwise derivative depths during a single   backwards pass, automatically giving greater weight to estimators    with lower variance, sometimes   improving over reparameterization gradients by $10^6$ times.","['Okinawa Institute of Science and Technology Graduate University', 'Cambridge University', 'TU Darmstadt + Max Planck Institute for Intelligent Systems', 'Okinawa Institute of Science and Technology']"
2018,Probabilistic Recurrent State-Space Models,"Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan Schaal, Marc Toussaint, Sebastian Trimpe",https://icml.cc/Conferences/2018/Schedule?showEvent=2070,"State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g., LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. We propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. This combination allows efficient incorporation of latent state temporal correlations, which we found to be key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.
","['Bosch Center for Artificial Intelligence, Max Planck Institute for Intelligent Systems', 'TU Darmstadt', 'Bosch Center for AI (BCAI)', 'Bosch Center for Artificial Intelligence', 'University of Southern California', '(organization)', 'Max Planck Institute for Intelligent Systems']"
2018,Structured Variationally Auto-encoded Optimization,"Xiaoyu Lu, Javier González, Zhenwen Dai, Neil Lawrence",https://icml.cc/Conferences/2018/Schedule?showEvent=2328,"We tackle the problem of optimizing a black-box objective function defined over a highly-structured input space. This problem is ubiquitous in science and engineering. In machine learning, inferring the structure of a neural network or the Automatic Statistician (AS), where the optimal kernel combination for a Gaussian process is selected, are two important examples. We use the \as as a case study to describe our approach, that can be easily generalized to other domains. We propose an Structure Generating Variational Auto-encoder (SG-VAE) to embed the original space of kernel combinations into some low-dimensional continuous manifold where Bayesian optimization (BO) ideas are used. This is possible when structural knowledge of the problem is available, which can be given via a simulator or any other form of generating potentially good solutions. The right exploration-exploitation balance is imposed by propagating into the search the uncertainty of the latent space of the SG-VAE,  that is computed using variational inference. The key aspect of our approach is that the SG-VAE can be used to bias the search towards relevant regions, making it suitable for transfer learning tasks. Several experiments in various application domains are used to illustrate the utility and generality of the approach described in this work.
","['University of Oxford', 'Amazon', 'Amazon.com', 'Amazon']"
2018,A Robust Approach to Sequential Information Theoretic Planning,"Sue Zheng, Jason Pacheco, John Fisher",https://icml.cc/Conferences/2018/Schedule?showEvent=2372,"In many sequential planning applications a natural approach to generating high quality plans is to maximize an information reward such as mutual information (MI).  Unfortunately, MI lacks a closed form in all but trivial models, and so must be estimated.  In applications where the cost of plan execution is expensive, one desires planning estimates which admit theoretical guarantees. Through the use of robust M-estimators we obtain bounds on absolute deviation of estimated MI.  Moreover, we propose a sequential algorithm which integrates inference and planning by maximally reusing particles in each stage.  We validate the utility of using robust estimators in the sequential approach on a Gaussian Markov Random Field wherein information measures have a closed form. Lastly, we demonstrate the benefits of our integrated approach in the context of sequential experiment design for inferring causal regulatory networks from gene expression levels.  Our method shows improvements over a recent method which selects intervention experiments based on the same MI objective.
","['MIT', 'Brown University', 'MIT']"
2018,Error Estimation for Randomized Least-Squares Algorithms via the Bootstrap,"Miles Lopes, Shusen Wang, Michael Mahoney",https://icml.cc/Conferences/2018/Schedule?showEvent=2345,"Over the course of the past decade, a variety of randomized algorithms have been proposed for computing approximate least-squares (LS) solutions in large-scale settings. A longstanding practical issue is that, for any given input, the user rarely knows the actual error of an approximate solution (relative to the exact solution). Likewise, it is difficult for the user to know precisely how much computation is needed to achieve the desired error tolerance. Consequently, the user often appeals to worst-case error bounds that tend to offer only qualitative guidance. As a more practical alternative, we propose a bootstrap method to compute a posteriori error estimates for randomized LS algorithms. These estimates permit the user to numerically assess the error of a given solution, and to predict how much work is needed to improve a ""preliminary"" solution. In addition, we provide theoretical consistency results for the method, which are the first such results in this context (to the best of our knowledge). From a practical standpoint, the method also has considerable flexibility, insofar as it can be applied to several popular sketching algorithms, as well as a variety of error metrics. Moreover, the extra step of error estimation does not add much cost to an underlying sketching algorithm. Finally, we demonstrate the effectiveness of the method with empirical results.
","['University of California, Davis', 'UC Berkeley', 'UC Berkeley']"
2018,Distributed Asynchronous Optimization with Unbounded Delays: How Slow Can You Go?,"Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter Glynn, Yinyu Ye, Li-Jia Li, Li Fei-Fei",https://icml.cc/Conferences/2018/Schedule?showEvent=2293,"One of the most widely used optimization methods for large-scale machine learning problems is distributed asynchronous stochastic gradient descent (DASGD). However, a key issue that arises here is that of delayed gradients: when a “worker” node asynchronously contributes a gradient update to the “master”, the global model parameter may have changed, rendering this information stale. In massively parallel computing grids, these delays can quickly add up if the computational throughput of a node is saturated, so the convergence of DASGD is uncertain under these conditions. Nevertheless, by using a judiciously chosen quasilinear step-size sequence, we show that it is possible to amortize these delays and achieve global convergence with probability 1, even when the delays grow at a polynomial rate. In this way, our results help reaffirm the successful application of DASGD to large-scale optimization problems.
","['Stanford University', 'CNRS', 'Stanford University', 'Stanford University', '', 'Google', 'Stanford University']"
2018,Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization,"Jiaxiang Wu, Weidong Huang, Junzhou Huang, Tong Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=2148,"Large-scale distributed optimization is of great importance in various applications. For data-parallel based distributed learning, the inter-node gradient communication often becomes the performance bottleneck. In this paper, we propose the error compensated quantized stochastic gradient descent algorithm to improve the training efficiency. Local gradients are quantized to reduce the communication overhead, and accumulated quantization error is utilized to speed up the convergence. Furthermore, we present theoretical analysis on the convergence behaviour, and demonstrate its advantage over competitors. Extensive experiments indicate that our algorithm can compress gradients by a factor of up to two magnitudes without performance degradation.
","['Tencent AI Lab', 'Tencent', 'University of Texas at Arlington / Tencent AI Lab', 'Tecent AI Lab']"
2018,Low-Rank Riemannian Optimization on Positive Semidefinite Stochastic Matrices with Applications to Graph Clustering,"Ahmed Douik, Babak Hassibi",https://icml.cc/Conferences/2018/Schedule?showEvent=2061,"This paper develops a Riemannian optimization framework for solving optimization problems on the set of symmetric positive semidefinite stochastic matrices. The paper first reformulates the problem by factorizing the optimization variable as $\mathbf{X}=\mathbf{Y}\mathbf{Y}^T$ and deriving conditions on $p$, i.e., the number of columns of $\mathbf{Y}$, under which the factorization yields a satisfactory solution. The reparameterization of the problem allows its formulation as an optimization over either an embedded or quotient Riemannian manifold whose geometries are investigated. In particular, the paper explicitly derives the tangent space, Riemannian gradients and retraction operator that allow the design of efficient optimization methods on the proposed manifolds. The numerical results reveal that, when the optimal solution has a known low-rank, the resulting algorithms present a clear complexity advantage when compared with state-of-the-art Euclidean and Riemannian approaches for graph clustering applications.","['California Institute of technology', 'Caltech']"
2018,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients","Lukas Balles, Philipp Hennig",https://icml.cc/Conferences/2018/Schedule?showEvent=2002,"The ADAM optimizer is exceedingly popular in the deep learning community. Often it works very well, sometimes it doesn’t. Why? We interpret ADAM as a combination of two aspects: for each weight, the update direction is determined by the sign of stochastic gradients, whereas the update magnitude is determined by an estimate of their relative variance. We disentangle these two aspects and analyze them in isolation, gaining insight into the mechanisms underlying ADAM. This analysis also extends recent results on adverse effects of ADAM on generalization, isolating the sign aspect as the problematic one. Transferring the variance adaptation to SGD gives rise to a novel method, completing the practitioner’s toolbox for problems where ADAM fails.
","['Max Planck Institute for Intelligent Systems', 'University of Tübingen']"
2018,Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning,"Thomas Dietterich, George Trimponias, Zhitang Chen",https://icml.cc/Conferences/2018/Schedule?showEvent=2093,"Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomposed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with respect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo policy evaluation on the endogenous MDP is accelerated compared to using the full MDP. Similar speedups are likely to carry over to all RL algorithms. We develop two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are practical and can significantly speed up reinforcement learning.
","['(organization)', ""Huawei Noah's Ark Lab"", 'Huawei Noah’s Ark Lab']"
2018,Differentially Private Database Release via Kernel Mean Embeddings,"Matej Balog, Ilya Tolstikhin, Bernhard Schölkopf",https://icml.cc/Conferences/2018/Schedule?showEvent=1964,"We lay theoretical foundations for new database release mechanisms that allow third-parties to construct consistent estimators of population statistics, while ensuring that the privacy of each individual contributing to the database is protected. The proposed framework rests on two main ideas. First, releasing (an estimate of) the kernel mean embedding of the data generating random variable instead of the database itself still allows third-parties to construct consistent estimators of a wide class of population statistics. Second, the algorithm can satisfy the definition of differential privacy by basing the released kernel mean embedding on entirely synthetic data points, while controlling accuracy through the metric available in a Reproducing Kernel Hilbert Space. We describe two instantiations of the proposed framework, suitable under different scenarios, and prove theoretical results guaranteeing differential privacy of the resulting algorithms and the consistency of estimators constructed from their outputs.
","['University of Cambridge and MPI Tübingen', 'Max Planck Institute for Intelligent Systems, Tübingen', 'MPI for Intelligent Systems Tübingen, Germany']"
2018,Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples,"Gail Weiss, Yoav Goldberg, Eran Yahav",https://icml.cc/Conferences/2018/Schedule?showEvent=2276,"We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN.  We do this using Angluin's \lstar algorithm as a learner and the trained RNN as an oracle.  Our technique efficiently extracts accurate automata from trained RNNs,  even when the state vectors are large and require fine differentiation.
","['Technion', 'Bar Ilan University', 'Technion']"
2018,Neural Dynamic Programming for Musical Self Similarity,"Christian Walder, Dongwoo Kim",https://icml.cc/Conferences/2018/Schedule?showEvent=2100,"We present a neural sequence model designed specifically for symbolic music. The model is based on a learned edit distance mechanism which generalises a classic recursion from computer science, leading to a neural dynamic program. Repeated motifs are detected by learning the transformations between them. We represent the arising computational dependencies using a novel data structure, the edit tree; this perspective suggests natural approximations which afford the scaling up of our otherwise cubic time algorithm. We demonstrate our model on real and synthetic data; in all cases it out-performs a strong stacked long short-term memory benchmark.
","['Data61, the Australian National University', 'The Australian National University']"
2018,Learning long term dependencies via Fourier recurrent units,"Jiong Zhang, Yibo Lin, Zhao Song, Inderjit Dhillon",https://icml.cc/Conferences/2018/Schedule?showEvent=2006,"It is a known fact that training recurrent neural networks for tasks  that have long term dependencies is challenging. One of the main reasons is the vanishing or exploding gradient problem, which prevents gradient information from  propagating to early layers. In this paper we propose a simple recurrent architecture, the Fourier Recurrent Unit (FRU), that stabilizes the gradients that arise in its training while giving us stronger expressive power. Specifically, FRU summarizes  the hidden states $h^{(t)}$ along the temporal dimension with Fourier basis functions. This allows gradients to easily reach any layer due to FRU's residual learning structure and the global support of  trigonometric functions. We show that FRU has gradient lower and upper bounds  independent of temporal dimension. We also show the strong expressivity of  sparse Fourier basis, from which FRU obtains its strong expressive power. Our  experimental study also demonstrates that with fewer parameters the proposed architecture outperforms other recurrent architectures on many tasks.","['University of Texas at Austin', 'UT-Austin', 'UT-Austin', 'UT Austin & Amazon']"
2018,Autoregressive Convolutional Neural Networks for Asynchronous Time Series,"Mikolaj Binkowski, Gautier Marti, Philippe Donnat",https://icml.cc/Conferences/2018/Schedule?showEvent=2310,"We propose Significance-Offset Convolutional Neural Network, a deep convolutional network architecture for regression of multivariate asynchronous time series. The model is inspired by standard autoregressive (AR) models and gating mechanisms used in recurrent neural networks. It involves an AR-like weighting system, where the final predictor is obtained as a weighted sum of adjusted regressors, while the weights are data-dependent functions learnt through a convolutional network. The architecture was designed for applications on asynchronous time series and is evaluated on such datasets: a hedge fund proprietary dataset of over 2 million quotes for a credit derivative index, an artificially generated noisy autoregressive series and UCI household electricity consumption dataset. The proposed architecture achieves promising results as compared to convolutional and recurrent neural networks.
","['Imperial College London', 'Ecole Polytechnique AXA IM Chorus', 'Hellebore Capital Limited']"
2018,Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation,"Dane Corneil, Wulfram Gerstner, Johanni Brea",https://icml.cc/Conferences/2018/Schedule?showEvent=2242,"Modern reinforcement learning algorithms reach super-human performance on many board and video games, but they are sample inefficient, i.e. they typically require significantly more playing experience than humans to reach an equal performance level. To improve sample efficiency, an agent may build a model of the environment and use planning methods to update its policy. In this article we introduce Variational State Tabulation (VaST), which maps an environment with a high-dimensional state space (e.g. the space of visual inputs) to an abstract tabular model. Prioritized sweeping with small backups, a highly efficient planning method, can then be used to update state-action values. We show how VaST can rapidly learn to maximize reward in tasks like 3D navigation and efficiently adapt to sudden changes in rewards or transition probabilities.
","['EPFL', 'EPFL', 'EPFL']"
2018,Regret Minimization for Partially Observable Deep Reinforcement Learning,"Peter Jin, EECS Kurt Keutzer, Sergey Levine",https://icml.cc/Conferences/2018/Schedule?showEvent=2300,"Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial observations by using finite length observation histories or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to an advantage-like function and is robust to partially observed state. We demonstrate that this new algorithm can substantially outperform strong baseline methods on several partially observed reinforcement learning tasks: learning first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.
","['UC Berkeley', 'EECS, UC Berkeley', 'Berkeley']"
2018,Goodness-of-fit Testing for Discrete Distributions via Stein Discrepancy,"Jiasen Yang, Qiang Liu, Vinayak A Rao, Jennifer Neville",https://icml.cc/Conferences/2018/Schedule?showEvent=1894,"Recent work has combined Stein's method with reproducing kernel Hilbert space theory to develop nonparametric goodness-of-fit tests for un-normalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discrepancy measure for discrete spaces, and develop a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We apply the proposed goodness-of-fit test to three statistical models involving discrete distributions, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy.
","['Purdue University', 'UT Austin', 'Purdue University', 'Purdue University']"
2018,Unbiased Objective Estimation in Predictive Optimization,"Shinji Ito, Akihiro Yabe, Ryohei Fujimaki",https://icml.cc/Conferences/2018/Schedule?showEvent=2001,"For data-driven decision-making, one promising approach, called predictive optimization, is to solve maximization problems i n which the objective function to be maximized is estimated from data. Predictive optimization, however, suffers from the problem of a calculated optimal solution’s being evaluated too optimistically, i.e., the value of the objective function is overestimated. This paper investigates such optimistic bias and presents two methods for correcting it. The first, which is analogous to cross-validation, successfully corrects the optimistic bias but results in underestimation of the true value. Our second method employs resampling techniques to avoid both overestimation and underestimation. We show that the second method, referred to as the parameter perturbation method, achieves asymptotically unbiased estimation. Empirical results for both artificial and real-world datasets demonstrate that our proposed approach successfully corrects the optimistic bias.
","['NEC Corporation', 'NEC Corporation', '-']"
2018,Ultra Large-Scale Feature Selection using Count-Sketches,"Amirali Aghazadeh, Ryan Spring, Daniel LeJeune, Gautam Dasarathy, Anshumali Shrivastava, Richard Baraniuk",https://icml.cc/Conferences/2018/Schedule?showEvent=2249,"Feature selection is an important challenge in machine learning. It plays a crucial role in the explainability of machine-driven decisions that are rapidly permeating throughout modern society. Unfortunately, the explosion in the size and dimensionality of real-world datasets poses a severe challenge to standard feature selection algorithms. Today, it is not uncommon for datasets to have billions of dimensions. At such scale, even storing the feature vector is impossible, causing most existing feature selection methods to fail. Workarounds like feature hashing, a standard approach to large-scale machine learning, helps with the computational feasibility, but at the cost of losing the interpretability of features. In this paper, we present MISSION, a novel framework for ultra large-scale feature selection that performs stochastic gradient descent while maintaining an efficient representation of the features in memory using a Count-Sketch data structure. MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features while using only O(log^2(p)) working memory. We demonstrate that MISSION accurately and efficiently performs feature selection on real-world, large-scale datasets with billions of dimensions.
","['Stanford University', 'Rice University', 'Rice University', 'Rice University', 'Rice University', 'OpenStax / Rice University']"
2018,"Matrix Norms in Data Streams: Faster, Multi-Pass and Row-Order","Vladimir Braverman, Stephen Chestnut, Robert Krauthgamer, Yi Li, David Woodruff, Lin Yang",https://icml.cc/Conferences/2018/Schedule?showEvent=2069,"A central problem in mining massive data streams is characterizing which functions of an underlying frequency vector can be approximated efficiently.  Given the prevalence of large scale linear algebra problems in machine learning, recently there has been considerable effort in extending this data stream problem to that of estimating functions of a matrix.  This setting generalizes classical problems to the analogous ones for matrices.  For example, instead of estimating frequent-item counts,  we now wish to estimate ``frequent-direction'' counts.  A related example is to estimate norms, which now correspond to estimating a vector norm on the singular values of the matrix.  Despite recent efforts, the current understanding for such matrix problems is considerably weaker than that for vector problems.   We study a number of aspects of estimating matrix norms in a stream that have not previously been considered:  (1) multi-pass algorithms, (2) algorithms that see the underlying matrix one row at a time, and (3) time-efficient algorithms.  Our multi-pass and row-order algorithms use less memory  than what is provably required in the single-pass and entrywise-update models,  and thus give separations between these models (in terms of memory). Moreover, all of our algorithms are considerably faster than previous ones.  We also prove a number of lower bounds, and obtain for instance,  a near-complete characterization of the memory required of row-order algorithms for estimating Schatten $p$-norms of sparse matrices. We complement our results with numerical experiments.","['Johns Hopkins University', 'ETH Zurich', 'Weizmann Institute of Science', 'Nanyang Technological University', 'Carnegie Mellon University', 'Princeton']"
2018,Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?,"Maithra Raghu, Alexander Irpan, Jacob Andreas, Bobby Kleinberg, Quoc Le, Jon Kleinberg",https://icml.cc/Conferences/2018/Schedule?showEvent=2221,"Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyse multiagent play, and even develop a self play algorithm.
","['Google Brain / Cornell University', 'Google', 'UC Berkeley', 'Cornell', 'Google Brain', 'Cornell University']"
2018,The Mirage of Action-Dependent Baselines in Reinforcement Learning,"George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard E Turner, Zoubin Ghahramani, Sergey Levine",https://icml.cc/Conferences/2018/Schedule?showEvent=2355,"Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.
","['Google Brain', 'Google Brain', 'Cambridge', 'University of Cambridge', 'University of Cambridge & Uber', 'Berkeley']"
2018,Composite Marginal Likelihood Methods for Random Utility Models,"Zhibing Zhao, Lirong Xia",https://icml.cc/Conferences/2018/Schedule?showEvent=1997,"We propose a novel and flexible rank-breaking-then-composite-marginal-likelihood (RBCML) framework for learning random utility models (RUMs), which include the Plackett-Luce model. We characterize conditions for the objective function of RBCML to be strictly log-concave by proving that strict log-concavity is preserved under convolution and marginalization. We characterize necessary and sufficient conditions for RBCML to satisfy consistency and asymptotic normality. Experiments on synthetic data show that RBCML for Gaussian RUMs achieves better statistical efficiency and computation efficiency than the state-of-the-art algorithm and our RBCML for the Plackett-Luce model provides flexible tradeoffs between running time and statistical efficiency.
","['Rensselaer Polytechnic Institute', 'RPI']"
2018,Ranking Distributions based on Noisy Sorting,"Adil El Mesaoudi-Paul, Eyke Hüllermeier, Robert Busa-Fekete",https://icml.cc/Conferences/2018/Schedule?showEvent=2051,"We propose a new statistical model for ranking data, i.e., a new family of probability distributions on permutations. Our model is inspired by the idea of a data-generating process in the form of a noisy sorting procedure, in which deterministic comparisons between pairs of items are replaced by Bernoulli trials. The probability of producing a certain ranking as a result then essentially depends on the Bernoulli parameters, which can be interpreted as pairwise preferences. We show that our model can be written in closed form if insertion or quick sort are used as sorting algorithms, and propose a maximum likelihood approach for parameter estimation. We also introduce a generalization of the model, in which the constraints on pairwise preferences are relaxed, and for which maximum likelihood estimation can be carried out based on a variation of the generalized iterative scaling algorithm. Experimentally, we show that the models perform very well in terms of goodness of fit, compared to existing models for ranking data.
","['University of Paderborn', 'Paderborn University', 'Yahoo Research']"
2018,DICOD: Distributed Convolutional Coordinate Descent for Convolutional Sparse Coding,"Thomas Moreau, Laurent Oudre, Nicolas Vayatis",https://icml.cc/Conferences/2018/Schedule?showEvent=1904,"In this paper, we introduce DICOD, a convolutional sparse coding algorithm which builds shift invariant representations for long signals. This algorithm is designed to run in a distributed setting, with local message passing, making it communication efficient. It is based on coordinate descent and uses locally greedy updates which accelerate the resolution compared to greedy coordinate selection. We prove the convergence of this algorithm and highlight its computational speed-up which is super-linear in the number of cores used. We also provide empirical evidence for the acceleration properties of our algorithm compared to state-of-the-art methods.
","['CMLA, ENS Paris-Saclay', 'Universite Paris 13', 'CMLA, ENS Paris Saclay']"
2018,Exploring Hidden Dimensions in Accelerating Convolutional Neural Networks,"Zhihao Jia, Sina Lin, Charles Qi, Alex Aiken",https://icml.cc/Conferences/2018/Schedule?showEvent=1891,"The past few years have witnessed growth in the computational requirements for training deep convolutional neural networks. Current approaches parallelize training onto multiple devices by applying a single parallelization strategy (e.g., data or model parallelism) to all layers in a network. Although easy to reason about, these approaches result in suboptimal runtime performance in large-scale distributed training, since different layers in a network may prefer different parallelization strategies. In this paper, we propose layer-wise parallelism that allows each layer in a network to use an individual parallelization strategy. We jointly optimize how each layer is parallelized by solving a graph search problem. Our evaluation shows that layer-wise parallelism outperforms state-of-the-art approaches by increasing training throughput, reducing communication costs, achieving better scalability to multiple GPUs, while maintaining original network accuracy.
","['Stanford University', 'Microsoft', 'Stanford University', 'Stanford University']"
2018,Deep Models of Interactions Across Sets,"Jason Hartford, Devon Graham, Kevin Leyton-Brown, Siamak Ravanbakhsh",https://icml.cc/Conferences/2018/Schedule?showEvent=2428,"We use deep learning to model interactions across two or more sets of objects, such as user–movie ratings or protein–drug bindings. The canonical representation of such interactions is a matrix (or tensor) with an exchangeability property: the encoding’s meaning is not changed by permuting rows or columns. We argue that models should hence be Permutation Equivariant (PE): constrained to make the same predictions across such permutations. We present a parameter-sharing scheme and prove that it is maximally expressive under the PE constraint. This scheme yields three benefits. First, we demonstrate performance competitive with the state of the art on multiple matrix completion benchmarks. Second, our models require a number of parameters independent of the numbers of objects and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. We observed surprisingly good generalization performance on this matrix extrapolation task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., predicting music ratings after training on movie ratings).
","['University of British Columbia', 'University of British Columbia', 'University of British Columbia', 'University of British Columbia']"
2018,ContextNet: Deep learning for Star Galaxy Classification,"Noble Kennamer, University of California David Kirkby, Alexander Ihler, University of California Francisco Javier Sanchez-Lopez",https://icml.cc/Conferences/2018/Schedule?showEvent=2195,"We present a framework to compose artificial neural networks in cases where the data cannot be treated as independent events. Our particular motivation is star galaxy classification for ground based optical surveys. Due to a turbulent atmosphere and imperfect instruments, a single image of an astronomical object is not enough to definitively classify it as a star or galaxy. Instead the context of the surrounding objects imaged at the same time need to be considered in order to make an optimal classification. The model we present is divided into three distinct ANNs: one designed to capture local features about each object, the second to compare these features across all objects in an image, and the third to make a final prediction for each object based on the local and compared features. By exploiting the ability to replicate the weights of an ANN, the model can handle an arbitrary and variable number of individual objects embedded in a larger exposure. We train and test our model on simulations of a large up and coming ground based survey, the Large Synoptic Survey Telescope (LSST). We compare to the state of the art approach, showing improved overall performance as well as better performance for a specific class of objects that is important for the LSST.
","['University of California, Irvine', 'University of California, Irvine', 'UC Irvine', 'University of California, Irvine']"
2018,First Order Generative Adversarial Networks,"Calvin Seward, Thomas Unterthiner, Urs M Bergmann, Nikolay Jetchev, Sepp Hochreiter",https://icml.cc/Conferences/2018/Schedule?showEvent=2073,"GANs excel at learning high dimensional distributions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent examples of problematic update directions include those used in both Goodfellow's original GAN and the WGAN-GP. To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction, with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence which approximates the Wasserstein distance while regularizing the critic's first order information. Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task.
","['Zalando Research', 'Johannes Kepler University Linz', 'Zalando Research', 'Zalando Research', 'Johannes Kepler University Linz']"
2018,Max-Mahalanobis Linear Discriminant Analysis Networks,"Tianyu Pang, Chao Du, Jun Zhu",https://icml.cc/Conferences/2018/Schedule?showEvent=2079,"A deep neural network (DNN) consists of a nonlinear transformation from an input to a feature representation, followed by a common softmax linear classifier. Though many efforts have been devoted to designing a proper architecture for nonlinear transformation, little investigation has been done on the classifier part. In this paper, we show that a properly designed classifier can improve robustness to adversarial attacks and lead to better prediction results. Specifically, we define a Max-Mahalanobis distribution (MMD) and theoretically show that if the input distributes as a MMD, the linear discriminant analysis (LDA) classifier will have the best robustness to adversarial examples. We further propose a novel Max-Mahalanobis linear discriminant analysis (MM-LDA) network, which explicitly maps a complicated data distribution in the input space to a MMD in the latent feature space and then applies LDA to make predictions. Our results demonstrate that the MM-LDA networks are significantly more robust to adversarial attacks, and have better performance in class-biased classification.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2018,Learning Maximum-A-Posteriori Perturbation Models for Structured Prediction in Polynomial Time,"Asish Ghoshal, Jean Honorio",https://icml.cc/Conferences/2018/Schedule?showEvent=2436,"MAP perturbation models have emerged as a powerful framework for inference in structured prediction. Such models provide a way to efficiently sample from the Gibbs distribution and facilitate predictions that are robust to random noise. In this paper, we propose a provably polynomial time randomized algorithm for learning the parameters of perturbed MAP predictors. Our approach is based on minimizing a novel Rademacher-based generalization bound on the expected loss of a perturbed MAP predictor, which can be computed in polynomial time. We obtain conditions under which our randomized learning algorithm can guarantee generalization to unseen examples.
","['Purdue University', 'Purdue University']"
2018,Structured Output Learning with Abstention: Application to Accurate Opinion Prediction,"Alexandre Garcia, Telecom-ParisTech Chloé Clavel, Slim Essid, Florence d'Alche-Buc",https://icml.cc/Conferences/2018/Schedule?showEvent=2199,"Motivated by Supervised Opinion Analysis, we propose a novel framework devoted to Structured Output Learning with Abstention (SOLA). The structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a flexible way. For that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to structured output prediction. To compare fully labeled training data with predictions potentially containing abstentions, we define a wide class of asymmetric abstention-aware losses. Learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. Thus, SOLA extends recent ideas about Structured Output Prediction via surrogate problems and calibration theory and enjoys statistical guarantees on the resulting excess risk. Instantiated on a hierarchical abstention-aware loss, SOLA is shown to be relevant for fine-grained opinion mining and gives state-of-the-art results on this task. Moreover, the abstention-aware representations can be used to competitively predict user-review ratings  based on a sentence-level opinion predictor.
","['Telecom Paristech', 'LTCI, Telecom-ParisTech, Paris, France', 'Telecom ParisTech', 'Télécom ParisTech, Université Paris-Saclay,Paris, France']"
2018,SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation,"Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, Le Song",https://icml.cc/Conferences/2018/Schedule?showEvent=2441,"When function approximation is used, solving the Bellman optimality equation with stability guarantees has remained a major open problem in reinforcement learning for decades.  The fundamental difficulty is that the Bellman operator may become an expansion in general, resulting in oscillating and even divergent behavior of popular algorithms like Q-learning.  In this paper, we revisit the Bellman equation, and reformulate it into a novel primal-dual optimization problem using Nesterov's smoothing technique and the Legendre-Fenchel transformation.  We then develop a new algorithm, called Smoothed Bellman Error Embedding, to solve this optimization problem where any differentiable function class may be used.  We provide what we believe to be the first convergence guarantee for general nonlinear function approximation, and analyze the algorithm's sample complexity. Empirically, our algorithm compares favorably to state-of-the-art baselines in several benchmark control problems.
","['Georgia Institute of Technology', 'Georgia Tech', 'Google Inc.', 'Microsoft Research', 'UIUC', 'Georgia Tech', 'Microsoft Research', 'Georgia Institute of Technology']"
2018,Smoothed Action Value Functions for Learning Gaussian Policies,"Ofir Nachum, Mohammad Norouzi, George Tucker, Dale Schuurmans",https://icml.cc/Conferences/2018/Schedule?showEvent=2086,"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to popular algorithms such as SARSA and Q-learning. We propose a new notion of action value defined by a Gaussian smoothed version of the expected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from experience sampled from an environment. Moreover, the gradients of expected reward with respect to the mean and covariance of a parameterized Gaussian policy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationships we develop new algorithms for training a Gaussian policy directly from a learned smoothed Q-value approximator. The approach is additionally amenable to proximal optimization by augmenting the objective with a penalty on KL-divergence from a previous policy. We find that the ability to learn both a mean and covariance during training leads to significantly improved results on standard continuous control benchmarks.
","['Google Brain', 'Google Brain', 'Google Brain', 'University of Alberta']"
2018,Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron,"RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan Wang, Daisy Stanton, Joel Shor, Ron Weiss, Robert Clark, Rif Saurous",https://icml.cc/Conferences/2018/Schedule?showEvent=2281,"We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic representation containing the desired prosody. We show that conditioning Tacotron on this learned embedding space results in synthesized audio that matches the prosody of the reference signal with fine time detail even when the reference and synthesis speakers are different.  Additionally, we show that a reference prosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results with accompanying audio samples from single-speaker and 44-speaker Tacotron models on a prosody transfer task.
","['Google, Inc.', '', 'Google Inc', 'Google', '', 'Google', 'Google Brain', 'Google UK', '']"
2018,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis","Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, Rif Saurous",https://icml.cc/Conferences/2018/Schedule?showEvent=2317,"In this work, we propose global style tokens'' (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretablelabels'' they generate can be used to control synthesis in novel ways, such as varying speed and speaking style -- independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.
","['Google', '', 'Google', '', '', 'Google', 'Google Inc', 'Google', 'Google', '']"
2018,AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning,"Ahmed M. Alaa, Mihaela van der Schaar",https://icml.cc/Conferences/2018/Schedule?showEvent=2050,"Clinical prognostic models derived from largescale healthcare data can inform critical diagnostic and therapeutic decisions. To enable off-theshelf usage of machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a system for automating the design of predictive modeling pipelines tailored for clinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline configurations efficiently using a novel batched Bayesian optimization (BO) algorithm that learns a low-dimensional decomposition of the pipelines’ high-dimensional hyperparameter space in concurrence with the BO procedure. This is achieved by modeling the pipelines’ performances as a black-box function with a Gaussian process prior, and modeling the “similarities” between the pipelines’ baseline algorithms via a sparse additive kernel with a Dirichlet prior. Meta-learning is used to warmstart BO with external data from “similar” patient cohorts by calibrating the priors using an algorithm that mimics the empirical Bayes method. The system automatically explains its predictions by presenting the clinicians with logical association rules that link patients’ features to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS using 10 major patient cohorts representing various aspects of cardiovascular patient care.
","['UCLA', 'UCLA']"
2018,TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service,"Amartya Sanyal, Matt Kusner, Adria Gascon, Varun Kanade",https://icml.cc/Conferences/2018/Schedule?showEvent=2140,"Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed. Equally important is to minimize the nature of computation and amount of communication required between client and server. Fully homomorphic encryption offers a way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations. The one drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data. We combine several ideas from the machine learning literature, particularly work on quantization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.
","['University of Oxford', 'Alan Turing Institute', 'The Alan Turing Institute / Warwick University', 'University of Oxford']"
2018,End-to-End Learning for the Deep Multivariate Probit Model,"Di Chen, Yexiang Xue, Carla Gomes",https://icml.cc/Conferences/2018/Schedule?showEvent=2259,"The multivariate probit model (MVP) is a popular classic model for studying binary responses of multiple entities.  Nevertheless, the computational challenge of learning the MVP model, given that its likelihood involves integrating over a multidimensional constrained space of latent variables, significantly limits its application in practice. We propose a flexible deep generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP), which is an end-to-end learning scheme that uses an efficient parallel sampling process of the multivariate probit model to exploit  GPU-boosted deep neural networks. We present both theoretical and empirical analysis of the convergence behavior of DMVP's sampling process with respect to the resolution of the correlation structure. We provide convergence guarantees for DMVP and our empirical analysis demonstrates the advantages of DMVP's sampling compared with standard MCMC-based methods. We also show that when applied to multi-entity modelling problems, which are natural DMVP applications, DMVP trains faster than classical MVP , by at least an order of magnitude, captures rich correlations among entities, and further improves the joint likelihood of entities compared with several competitive models.
","['Cornell University', 'Purdue University', 'Cornell University']"
2018,Differentiable Dynamic Programming for Structured Prediction and Attention,"Arthur Mensch, Mathieu Blondel",https://icml.cc/Conferences/2018/Schedule?showEvent=2114,"Dynamic programming (DP) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, many DP algorithms are non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on structured prediction (audio-to-score alignment, NER) and on structured and sparse attention for translation.
","['Inria Parietal', 'NTT']"
2018,Optimal Distributed Learning with Multi-pass Stochastic Gradient Methods,"Junhong Lin, Volkan Cevher",https://icml.cc/Conferences/2018/Schedule?showEvent=2484,"We study generalization properties of distributed algorithms in the setting of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-passes over the data. We show that optimal generalization error bounds can be retained for distributed SGM  provided that the partition level is not too large.  Our results are superior to the state-of-the-art theory, covering the cases that the regression function may not be in the hypothesis spaces. Particularly, our results show that distributed SGM has a smaller theoretical computational complexity, compared with distributed kernel ridge regression (KRR) and classic SGM.
","['EPFL', 'EPFL']"
2018,Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates,"Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett",https://icml.cc/Conferences/2018/Schedule?showEvent=2405,"In this paper, we develop distributed optimization algorithms that are provably robust against Byzantine failures---arbitrary and potentially adversarial behavior, in distributed computing systems, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for all of strongly convex, non-strongly convex, and smooth non-convex population loss functions. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.
","['UC Berkeley', 'Cornell University', 'UC Berkeley', 'UC Berkeley']"
2018,SQL-Rank: A Listwise Approach to Collaborative Ranking,"LIWEI WU, Cho-Jui Hsieh, University of California James Sharpnack",https://icml.cc/Conferences/2018/Schedule?showEvent=2098,"In this paper, we propose a listwise approach for constructing user-specific rankings in recommendation systems in a collaborative fashion. We contrast the listwise approach to previous pointwise and pairwise approaches, which are based on treating either each rating or each pairwise comparison as an independent instance respectively. By extending the work of ListNet (Cao et al., 2007), we cast listwise collaborative ranking as maximum likelihood under a permutation model which applies probability mass to permutations based on a low rank latent score matrix. We present a novel algorithm called SQL-Rank, which can accommodate ties and missing data and can run in linear time. We develop a theoretical framework for analyzing listwise ranking methods based on a novel representation theory for the permutation model. Applying this framework to collaborative ranking, we derive asymptotic statistical rates as the number of users and items grow together. We conclude by demonstrating that our SQL-Rank method often outperforms current state-of-the-art algorithms for implicit feedback such as Weighted-MF and BPR and achieve favorable results when compared to explicit feedback algorithms such as matrix factorization and collaborative ranking.
","['University of California, Davis', 'University of California, Davis', 'University of California, Davis']"
2018,Extreme Learning to Rank via Low Rank Assumption,"Minhao Cheng, Ian Davidson, Cho-Jui Hsieh",https://icml.cc/Conferences/2018/Schedule?showEvent=2102,"We consider the setting where we wish to perform ranking for hundreds of thousands of users which is common in recommender systems and web search ranking. Learning a single ranking function is unlikely to capture the variability across all users while learning a ranking function for each person is time-consuming and requires large amounts of data from each user. To address this situation, we propose a Factorization RankSVM algorithm which learns a series of k basic ranking functions and then constructs for each user a local ranking function that is a combination of them. We develop a fast algorithm to reduce the time complexity of gradient descent solver by exploiting the low-rank structure, and the resulting algorithm is much faster than existing methods. Furthermore, we prove that the generalization error of the proposed method can be significantly better than training individual RankSVMs. Finally, we present some interesting patterns in the principal ranking functions learned by our algorithms.
","['UC Davis', 'UC Davis', 'University of California, Davis']"
2018,Adversarial Attack on Graph Structured Data,"Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, Le Song",https://icml.cc/Conferences/2018/Schedule?showEvent=2294,"Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool deep learning models by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. We further propose attack methods based on genetic algorithms and gradient descent in the scenario where additional prediction confidence or gradients are available.  We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.
","['Georgia Tech', 'Ant Financial Services Group', 'Tsinghua University', 'Ant Financial', '', 'Tsinghua University', 'Georgia Institute of Technology']"
2018,Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training,"Xi Wu, Wooyeong Jang, Jiefeng Chen, Lingjiao Chen, Somesh Jha",https://icml.cc/Conferences/2018/Schedule?showEvent=2298,"In this paper we study leveraging \emph{confidence information} induced by adversarial training to reinforce adversarial robustness of a given adversarially trained model. A natural measure of confidence is $\|F(\bfx)\|_\infty$ (i.e. how confident $F$ is about its prediction?). We start by analyzing an adversarial training formulation proposed by Madry et al.. We demonstrate that, under a variety of instantiations, an only somewhat good solution to their objective induces confidence to be a discriminator, which can distinguish between right and wrong model predictions in a neighborhood of a point sampled from the underlying distribution. Based on this, we propose Highly Confident Near Neighbor ($\HCNN$), a framework that combines confidence information and nearest neighbor search, to reinforce adversarial robustness of a base model. We give algorithms in this framework and perform a detailed empirical study. We report encouraging experimental results that support our analysis, and also discuss problems we observed with existing adversarial training.","['Google', 'University of Wisconsin - Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin, Madison']"
2018,Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization,"Louis Filstroff, Alberto Lumbreras, Cedric Fevotte",https://icml.cc/Conferences/2018/Schedule?showEvent=2240,"We present novel understandings of the Gamma-Poisson (GaP) model, a probabilistic matrix factorization model for count data. We show that GaP can be rewritten free of the score/activation matrix. This gives us new insights about the estimation of the topic/dictionary matrix by maximum marginal likelihood estimation. In particular, this explains the robustness of this estimator to over-specified values of the factorization rank, especially its ability to automatically prune irrelevant dictionary columns, as empirically observed in previous work. The marginalization of the activation matrix leads in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable properties.
","['CNRS, Toulouse', 'Criteo', 'CNRS']"
2018,Learning Binary Latent Variable Models: A Tensor Eigenpair Approach,"Ariel Jaffe, Roi Weiss, Boaz Nadler, Shai Carmi, Yuval Kluger",https://icml.cc/Conferences/2018/Schedule?showEvent=2053,"Latent variable models with hidden binary units appear in various applications. Learning such models, in particular in the presence of noise, is a challenging computational problem. In this paper we propose a novel spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data. We prove that under mild non-degeneracy conditions, our method consistently estimates the model parameters at the optimal parametric rate. Our tensor-based method generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be either statistically independent or mutually exclusive. We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for population mixtures in genetics.
","['Weizmann Institute of Science', 'WeizmannInstitute', 'Weizmann Institute of Science', 'The Hebrew University of Jerusalem', 'Yale School of Medicine']"
2018,Thompson Sampling for Combinatorial Semi-Bandits,"Siwei Wang, Wei Chen",https://icml.cc/Conferences/2018/Schedule?showEvent=2120,"We study the application of the Thompson sampling (TS) methodology to the stochastic combinatorial multi-armed bandit (CMAB) framework. We analyze the standard TS algorithm for the general CMAB, and obtain the first distribution-dependent regret bound of $O(m\log T / \Delta_{\min}) $ for TS under general CMAB, where $m$ is the number of arms, $T$ is the time horizon, and $\Delta_{\min}$ is the minimum gap between the expected reward of the optimal solution and any non-optimal solution. We also show that one cannot use an approximate oracle in TS algorithm for even MAB problems. Then we expand the analysis to matroid bandit, a special case of CMAB and for which we could remove the independence assumption across arms and achieve a better regret bound. Finally, we use some experiments to show the comparison of regrets of CUCB and CTS algorithms.","['Tsinghua University', 'Microsoft']"
2018,Let’s be Honest: An Optimal No-Regret Framework for Zero-Sum Games,"Ehsan Asadi Kangarshahi, Ya-Ping Hsieh, Mehmet Fatih Sahin, Volkan Cevher",https://icml.cc/Conferences/2018/Schedule?showEvent=1975,"We revisit the problem of solving two-player zero-sum games in the decentralized setting. We propose a simple algorithmic framework that simultaneously achieves the best rates for honest regret as well as adversarial regret, and in addition resolves the open problem of removing the logarithmic terms in convergence to the value of the game. We achieve this goal in three steps. First, we provide a novel analysis of the optimistic mirror descent (OMD), showing that it can be modified to guarantee fast convergence for both honest regret and value of the game, when the players are playing collaboratively. Second, we propose a new algorithm, dubbed as robust optimistic mirror descent (ROMD), which attains optimal adversarial regret without knowing the time horizon beforehand. Finally, we propose a simple signaling scheme, which enables us to bridge OMD and ROMD to achieve the best of both worlds. Numerical examples are presented to support our theoretical claims and show that our non-adaptive ROMD algorithm can be competitive to OMD with adaptive step-size selection.
","['University of Cambridge', 'École Polytechnique Fédérale d', 'Ecole Polytechnique Fédérale de Lausanne', 'EPFL']"
2018,Deep Asymmetric Multi-task Feature Learning,"Hae Beom Lee, Eunho Yang, Sung Ju Hwang",https://icml.cc/Conferences/2018/Schedule?showEvent=1960,"We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can learn deep representations shared across multiple tasks while effectively preventing negative transfer that may happen in the feature sharing process. Specifically, we introduce an asymmetric autoencoder term that allows reliable predictors for the easy tasks to have high contribution to the feature learning while suppressing the influences of unreliable predictors for more difficult tasks. This allows the learning of less noisy representations, and enables unreliable predictors to exploit knowledge from the reliable predictors via the shared latent features. Such asymmetric knowledge transfer through shared features is also more scalable and efficient than inter-task asymmetric transfer. We validate our Deep-AMTFL model on multiple benchmark datasets for multitask learning and image classification, on which it significantly outperforms existing symmetric and asymmetric multitask learning models, by effectively preventing negative transfer in deep feature learning.
","['UNIST', 'KAIST / AItrics', 'KAIST']"
2018,Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations,"Ashwin Kalyan, Stefan Lee, Anitha Kannan, Dhruv Batra",https://icml.cc/Conferences/2018/Schedule?showEvent=2325,"Many structured prediction problems (particularly in vision and language domains) are ambiguous, with multiple outputs being ‘correct’ for an input – e.g. there are many ways of describing an image, multiple ways of translating a sentence; however, exhaustively annotating the applicability of all possible outputs is intractable due to exponentially large output spaces (e.g. all English sentences). In practice, these problems are cast as multi-class prediction, with the likelihood of only a sparse set of annotations being maximized – unfortunately penalizing for placing beliefs on plausible but unannotated outputs. We make and test the following hypothesis – for a given input, the annotations of its neighbors may serve as an additional supervisory signal. Specifically, we propose an objective that transfers supervision from neighboring examples. We first study the properties of our developed method in a controlled toy setup before reporting results on multi-label classification and two image-grounded sequence modeling tasks – captioning and question generation. We evaluate using standard task-specific metrics and measures of output diversity, finding consistent improvements over standard maximum likelihood training and other baselines.
","['Georgia Tech', 'Georgia Institute of Technology', 'Curai', 'Georgia Institute of Technology / Facebook AI Research']"
2018,Stein Variational Message Passing for Continuous Graphical Models,"Dilin Wang, Zhe Zeng, Qiang Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2214,"We propose a novel distributed inference algorithm for continuous graphical models,  by extending Stein variational gradient descent (SVGD) to leverage the Markov dependency structure of the distribution of interest. Our approach combines SVGD with a set of structured local kernel functions defined on the Markov blanket of each node, which alleviates the curse of high dimensionality and simultaneously yields a distributed algorithm for decentralized inference tasks. We justify our method with theoretical analysis and show that the use of local kernels can be viewed as a new type of localized approximation that matches the target distribution on the conditional distributions of each node over its Markov blanket. Our empirical results show that our method outperforms a variety of baselines including standard MCMC and particle message passing methods.
","['UT Austin', 'Zhejiang University', 'UT Austin']"
2018,Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms,"Yi Wu, Siddharth Srivastava, Nicholas Hay, Simon Du, Stuart Russell",https://icml.cc/Conferences/2018/Schedule?showEvent=2009,"Despite the recent successes of probabilistic programming languages (PPLs) in AI applications, PPLs offer only limited support for random variables whose distributions combine discrete and continuous elements. We develop the notion of measure-theoretic Bayesian networks (MTBNs) and use it to provide more general semantics for PPLs with arbitrarily many random variables defined over arbitrary measure spaces. We develop two new general sampling algorithms that are provably correct under the MTBN framework: the lexicographic likelihood weighting (LLW) for general MTBNs and the lexicographic particle filter (LPF), a specialized algorithm for state-space models. We further integrate MTBNs into a widely used PPL system, BLOG, and verify the effectiveness of the new inference algorithms through representative examples.
","['UC Berkeley', 'Arizona State University', '', 'Carnegie Mellon University', 'UC Berkeley']"
2018,Towards Binary-Valued Gates for Robust LSTM Training,"Zhuohan Li, Di He, Fei Tian, Wei Chen, Tao Qin, Liwei Wang, Tie-Yan Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=1916,"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control information flow (e.g., whether to skip some information or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal. In this paper, we propose a new way for LSTM training, which pushes the output values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly open or closed, instead of in a middle state, which makes the results more interpretable. Empirical studies show that (1) Although it seems that we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) The outputs of gates are not sensitive to their inputs: we can easily compress the LSTM unit in multiple ways, e.g., low-rank approximation and low-precision approximation. The compressed models are even better than the baseline models without compression.
","['Peking University', 'Peking University', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research Asia', 'Peking University', 'Microsoft Research Asia']"
2018,Fitting New Speakers Based on a Short Untranscribed Sample,"Eliya Nachmani, Adam Polyak, Yaniv Taigman, Lior Wolf",https://icml.cc/Conferences/2018/Schedule?showEvent=2149,"Learning-based Text To Speech systems have the potential to generalize from one speaker to the next and thus require a relatively short sample of any new voice. However, this promise is currently largely unrealized. We present a method that is designed to capture a new speaker from a short untranscribed audio sample. This is done by employing an additional network that given an audio sample, places the speaker in the embedding space. This network is trained as part of the speech synthesis system using various consistency losses. Our results demonstrate a greatly  improved performance on both the dataset speakers, and, more importantly, when fitting new voices, even from very short samples.
","['Facebook AI Research and Tel Aviv University', 'Facebook AI Research and Tel Aviv University', 'Facebook', 'Facebook AI Research and Tel Aviv University']"
2018,Stochastic Variance-Reduced Policy Gradient,"Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, Marcello Restelli",https://icml.cc/Conferences/2018/Schedule?showEvent=2066,"In this paper, we propose a novel reinforcement-learning algorithm consisting in a stochastic variance-reduced version of policy gradient for solving Markov Decision Processes (MDPs).         Stochastic variance-reduced gradient (SVRG) methods have proven to be very successful in supervised learning.         However, their adaptation to policy gradient is not straightforward and needs to account for I) a non-concave objective function; II) approximations in the full gradient computation; and III) a non-stationary sampling process.         The result is SVRPG, a stochastic variance-reduced policy gradient algorithm that leverages on importance weights to preserve the unbiasedness of the gradient estimate.         Under standard assumptions on the MDP, we provide convergence guarantees for SVRPG with a convergence rate that is linear under increasing batch sizes.         Finally, we suggest practical variants of SVRPG, and we empirically evaluate them on continuous MDPs.
","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano', 'SequeL - Inria Lille - Nord Europe', 'Politecnico di Milano']"
2018,Convergent Tree Backup and Retrace with Function Approximation,"Ahmed Touati, Pierre-Luc Bacon, Doina Precup, Pascal Vincent",https://icml.cc/Conferences/2018/Schedule?showEvent=2470,"Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this work, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and in practice with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-point formulation. By exploiting the problem structure proper to these algorithms, we are able to provide convergence guarantees and finite-sample bounds. The applicability of our new analysis also goes beyond Tree Backup and Retrace and allows us to provide new convergence rates for the GTD and GTD2 algorithms without having recourse to projections or Polyak averaging.
","['MILA / FAIR', 'McGill University', 'McGill University / DeepMind', 'U Montreal']"
2018,Alternating Randomized Block Coordinate Descent,"Jelena Diakonikolas, Orecchia Lorenzo",https://icml.cc/Conferences/2018/Schedule?showEvent=2445,"Block-coordinate descent algorithms and alternating minimization methods are fundamental optimization algorithms and an important primitive in large-scale optimization and machine learning. While various block-coordinate-descent-type methods have been studied extensively, only alternating minimization -- which applies to the setting of only two blocks -- is known to have convergence time that scales independently of the least smooth block. A natural question is then: is the setting of two blocks special? We show that the answer is ``no'' as long as the least smooth block can be optimized exactly -- an assumption that is also needed in the setting of alternating minimization. We do so by introducing a novel algorithm AR-BCD, whose convergence time scales independently of the least smooth (possibly non-smooth) block. The basic algorithm generalizes both alternating minimization and randomized block coordinate (gradient) descent, and we also provide its accelerated version -- AAR-BCD.
","['Boston University', 'Boston']"
2018,Shampoo: Preconditioned Stochastic Tensor Optimization,"Vineet Gupta, Tomer Koren, Yoram Singer",https://icml.cc/Conferences/2018/Schedule?showEvent=2380,"Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities.  Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Surprisingly, although it involves a more complex update rule,  Shampoo's runtime per step is comparable in practice to that of simple gradient methods such as SGD, AdaGrad, and Adam.
","['Google', 'Google Brain', 'Google']"
2018,Stochastic Wasserstein Barycenters,"Sebastian Claici, Edward Chien, Justin Solomon",https://icml.cc/Conferences/2018/Schedule?showEvent=2316,"We present a stochastic algorithm to compute the barycenter of a set of probability distributions under the Wasserstein metric from optimal transport. Unlike previous approaches, our method extends to continuous input distributions and allows the support of the barycenter to be adjusted in each iteration. We tackle the problem without regularization, allowing us to recover a sharp output whose support is contained within the support of the true barycenter. We give examples where our algorithm recovers a more meaningful barycenter than previous work. Our method is versatile and can be extended to applications such as generating super samples from a given distribution and recovering blue noise approximations.
","['MIT', 'Massachusetts Institute of Technology', 'MIT']"
2018,Accelerating Natural Gradient with Higher-Order Invariance,"Yang Song, Jiaming Song, Stefano Ermon",https://icml.cc/Conferences/2018/Schedule?showEvent=2386,"An appealing property of the natural gradient is that it is invariant to arbitrary differentiable reparameterizations of the model. However, this invariance property requires infinitesimal steps and is lost in practical implementations with small but finite step sizes. In this paper, we study invariance properties from a combined perspective of Riemannian geometry and numerical differential equation solving. We define the order of invariance of a numerical method to be its convergence order to an invariant solution. We propose to use higher-order integrators and geodesic corrections to obtain more invariant optimization trajectories. We prove the numerical convergence properties of geodesic corrected updates and show that they can be as computational efficient as plain natural gradient. Experimentally, we demonstrate that invariance leads to faster optimization and our techniques improve on traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning.
","['Stanford University', 'Stanford', 'Stanford University']"
2018,Learning unknown ODE models with Gaussian processes,"Markus Heinonen, Cagatay Yildiz, Henrik Mannerström, Jukka Intosalmi, Harri Lähdesmäki",https://icml.cc/Conferences/2018/Schedule?showEvent=2277,"In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model's capabilities to infer dynamics from sparse data and to simulate the system forward into future.
","['Aalto University', 'Aalto University', 'Aalto University', 'Aalto University', 'Aalto University']"
2018,Constraining the Dynamics of Deep Probabilistic Models,"Marco Lorenzi, Maurizio Filippone",https://icml.cc/Conferences/2018/Schedule?showEvent=2209,"We introduce a novel generative formulation of deep probabilistic models implementing ""soft"" constraints on their function dynamics. In particular, we develop a flexible methodological framework where the modeled functions and derivatives of a given order are subject to inequality or equality constraints. We then characterize the posterior distribution over model and constraint parameters through stochastic variational inference. As a result, the proposed approach allows for accurate and scalable uncertainty quantification on the predictions and on all parameters. We demonstrate the application of equality constraints in the challenging problem of parameter inference in ordinary differential equation models, while we showcase the application of inequality constraints on the problem of monotonic regression of count data. The proposed approach is extensively tested in several experimental settings, leading to highly competitive results in challenging modeling applications, while offering high expressiveness, flexibility and scalability.
","['Inria Sophia Antipolis', 'Eurecom']"
2018,Fast Decoding in Sequence Models Using Discrete Latent Variables,"Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, Noam Shazeer",https://icml.cc/Conferences/2018/Schedule?showEvent=2438,"Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and Transformer are the state-of-the-art on many tasks. However, they lack parallelism and are thus slow for long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallel during training, but still lack parallelism during decoding.  We present a method to extend sequence models using  discrete latent variables that makes decoding much more parallel. The main idea behind this approach is to first autoencode the target sequence into a shorter discrete latent sequence, which is generated autoregressively, and finally decode the full sequence from this shorter latent sequence in a parallel manner. To this end, we introduce a new method for constructing discrete latent variables and compare it with previously introduced methods. Finally, we verify that our model works on the task of neural machine translation, where our models are an order of magnitude faster than comparable autoregressive models and, while lower in BLEU than purely autoregressive models, better than previously proposed non-autogregressive translation.
","['Google', 'Google Brain', 'Google Brain', 'Google Brain', 'Google', '', 'Google']"
2018,High Performance Zero-Memory Overhead Direct Convolutions,"Jiyuan Zhang, Franz Franchetti, Tze Meng Low",https://icml.cc/Conferences/2018/Schedule?showEvent=2341,"The computation of convolution layers in deep neural networks typically rely on high performance routines that  trade space for time by using additional memory (either for packing purposes or required as part of the algorithm) to improve performance.  The problems with such an approach are two-fold. First, these routines incur additional memory overhead which reduces the overall size of the network that can fit on embedded devices with limited memory capacity. Second,  these high performance routines were not optimized for performing convolution,  which means that the performance obtained is  usually less than conventionally expected. In this paper, we demonstrate that direct convolution, when implemented correctly, eliminates all memory overhead, and yields performance that is between 10% to 400% times better than existing high performance implementations of convolution layers on conventional and embedded CPU architectures. We also show that a high performance direct convolution exhibits better scaling performance, i.e. suffers less performance drop, when increasing the number of threads.
","['Cargenie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2018,Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions,"Shuaiwen Wang, Wenda Zhou, Haihao Lu, Arian Maleki, Vahab Mirrokni",https://icml.cc/Conferences/2018/Schedule?showEvent=2473,"We study the parameter tuning problem for the penalized regression model. Finding the optimal choice of the regularization parameter is a challenging problem in high-dimensional regimes where both the number of observations n and the number of parameters p are large. We propose two frameworks to obtain a computationally efficient approximation ALO of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers. Our two frameworks are based on the primal and dual formulations of the penalized regression model. We prove the equivalence of the two approaches under smoothness conditions. This equivalence enables us to justify the accuracy of both methods under such conditions. We use our approaches to obtain a risk estimate for several standard problems, including generalized LASSO, nuclear norm regularization and support vector machines. We experimentally demonstrate the effectiveness of our results for non-differentiable cases.
","['Columbia University', 'Columbia University', 'MIT', 'Columbia', 'Google Research']"
2018,Improved large-scale graph learning through ridge spectral sparsification,"Daniele Calandriello, Alessandro Lazaric, Ioannis Koutis, Michal Valko",https://icml.cc/Conferences/2018/Schedule?showEvent=2371,"The representation and learning benefits of methods based on graph Laplacians, such as Laplacian smoothing or harmonic function solution for semi-supervised learning (SSL),  are empirically and theoretically well supported. Nonetheless, the exact versions of these methods scale poorly with the number of nodes $n$ of the graph. In this paper, we combine a spectral sparsification routine with Laplacian learning. Given a graph $G$ as input, our algorithm computes a sparsifier in a distributed way in $O(n\log^3(n))$ time, $O(m\log^3(n))$ work and $O(n\log(n))$ memory, using only $\log(n)$ rounds of communication. Furthermore, motivated by the regularization often employed in learning algorithms, we show that constructing sparsifiers that preserve the spectrum of the Laplacian only up to the regularization level may drastically reduce the size of the final graph. By constructing a spectrally-similar graph, we are able to bound the error induced by the sparsification for a variety of downstream tasks (e.g., SSL). We empirically validate the theoretical guarantees on Amazon co-purchase graph and compare to the state-of-the-art heuristics.","['INRIA Lille', 'Facebook AI Research', '', 'DeepMind']"
2018,Distilling the Posterior in Bayesian Neural Networks,"Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger Grosse, Richard Zemel",https://icml.cc/Conferences/2018/Schedule?showEvent=2362,"In many applications of deep learning, it is crucial to capture model and prediction uncertainty. Unlike classic neural networks (NN), Bayesian neural networks (BNN) allow us to reason about uncertainty in a more principled way.  Stochastic Gradient Langevin Dynamics (SGLD) enables learning a BNN with only simple modifications to the standard optimization framework (SGD). Instead of obtaining a single point-estimate of the model, the result of SGLD is samples from the BNN posterior. However, SGLD and its extensions require storage of the entire history of model parameters, a potentially prohibitive cost (especially for large neural networks). We propose a framework, Adversarial Posterior Distillation, to distill the SGLD samples using Generative Adversarial Networks (GAN). At test-time, samples are generated by the GAN. We show that this distillation framework incurs no loss in performance on recent BNN applications including anomaly detection, active learning, and defense against attacks. By construction, our framework not only distills the Bayesian predictive distribution, but the posterior itself.  This allows users to compute quantity such as the approximate model variance, which is useful in the downstream tasks.
","['Univeristy of Toronto', 'University of Toronto', 'University of Toronto', 'University of Toronto', 'University of Toronto and Vector Institute', 'Vector Institute']"
2018,Scalable approximate Bayesian inference for particle tracking data,"Ruoxi Sun, Department of Statistics Liam Paninski",https://icml.cc/Conferences/2018/Schedule?showEvent=2375,"Many important datasets in physics, chemistry, and biology consist of noisy sequences of images of multiple moving overlapping particles. In many cases, the observed particles are indistinguishable, leading to unavoidable uncertainty about nearby particles’ identities. Exact Bayesian inference is intractable in this setting, and previous approximate Bayesian methods scale poorly. Non-Bayesian approaches that output a single “best” estimate of the particle tracks (thus discarding important uncertainty information) are therefore dominant in practice. Here we propose a flexible and scalable amortized approach for Bayesian inference on this task. We introduce a novel neural network method to approximate the (intractable) filter-backward-sample-forward algorithm for Bayesian inference in this setting. By varying the simulated training data for the network, we can perform inference on a wide variety of data types. This approach is therefore highly flexible and improves on the state of the art in terms of accuracy; provides uncertainty estimates about the particle locations and identities; and has a test run-time that scales linearly as a function of the data length and number of particles, thus enabling Bayesian inference in arbitrarily large particle tracking datasets.
","['Columbia University', 'Department of Statistics, Columbia University']"
2018,Weakly Consistent Optimal Pricing Algorithms in Repeated Posted-Price Auctions with Strategic Buyer,Alexey Drutsa,https://icml.cc/Conferences/2018/Schedule?showEvent=1911,We study revenue optimization learning algorithms for repeated posted-price auctions where a seller interacts with a single strategic buyer that holds a fixed private valuation for a good and seeks to maximize his cumulative discounted surplus. We propose a novel algorithm that never decreases offered prices and has  a tight strategic regret bound of $\Theta(\log\log T)$. This result closes the open research question on the existence of a no-regret horizon-independent weakly consistent pricing. We also show that the property of non-decreasing prices is nearly necessary for a weakly consistent algorithm to be a no-regret one.,['Yandex']
2018,Practical Contextual Bandits with Regression Oracles,"Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, Robert Schapire",https://icml.cc/Conferences/2018/Schedule?showEvent=2475,"A major challenge in contextual bandits is to design general-purpose algorithms that are both practically useful and theoretically well-founded. We present a new technique that has the empirical and computational advantages of realizability-based approaches combined with the flexibility of agnostic methods. Our algorithms leverage the availability of a regression oracle for the value-function class, a more realistic and reasonable oracle than the classification oracles over policies typically assumed by agnostic methods. Our approach generalizes both UCB and LinUCB to far more expressive possible model classes and achieves low regret under certain distributional assumptions. In an extensive empirical evaluation, we find that our approach typically matches or outperforms both realizability-based and agnostic baselines.
","['Cornell University', 'Microsoft Research', 'Microsoft Research', 'University of Southern California', 'Microsoft Research']"
2018,Stochastic Variance-Reduced Hamilton Monte Carlo Methods,"Difan Zou, Pan Xu, Quanquan Gu",https://icml.cc/Conferences/2018/Schedule?showEvent=2347,"We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling from a smooth and strongly log-concave distribution. At the core of our proposed method is a variance reduction technique inspired by the recent advance in stochastic optimization. We show that, to achieve $\epsilon$ accuracy in 2-Wasserstein distance, our algorithm achieves $\tilde O\big(n+\kappa^{2}d^{1/2}/\epsilon+\kappa^{4/3}d^{1/3}n^{2/3}/\epsilon^{2/3}\big)$ gradient complexity (i.e., number of component gradient evaluations), which outperforms the state-of-the-art HMC and stochastic gradient HMC methods in a wide regime. We also extend our algorithm for sampling from smooth and general log-concave distributions, and prove the corresponding gradient complexity as well. Experiments on both synthetic and real data demonstrate the superior performance of our algorithm.","['University of Virginia', 'University of California, Los Angeles', 'UCLA']"
2018,Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization,"Umut Simsekli, Cagatay Yildiz, Thanh Huy Nguyen, Ali Taylan Cemgil, Gaël RICHARD",https://icml.cc/Conferences/2018/Schedule?showEvent=1877,"Recent studies have illustrated that stochastic gradient Markov Chain Monte Carlo techniques have a strong potential in non-convex optimization, where local and global convergence guarantees can be shown under certain conditions. By building up on this recent theory, in this study, we develop an asynchronous-parallel stochastic L-BFGS algorithm for non-convex optimization. The proposed algorithm is suitable for both distributed and shared-memory settings. We provide formal theoretical analysis and show that the proposed method achieves an ergodic convergence rate of ${\cal O}(1/\sqrt{N})$ ($N$ being the total number of iterations) and it can achieve a linear speedup under certain conditions. We perform several experiments on both synthetic and real datasets. The results support our theory and show that the proposed algorithm provides a significant speedup over the recently proposed synchronous distributed L-BFGS algorithm.","['Telecom ParisTech', 'Aalto University', 'Telecom ParisTech', 'DeepMind', 'Télécom ParisTech']"
2018,GAIN: Missing Data Imputation using Generative Adversarial Nets,"Jinsung Yoon, James Jordon, Mihaela van der Schaar",https://icml.cc/Conferences/2018/Schedule?showEvent=2025,"We propose a novel method for imputing missing data by adapting the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imputation Nets (GAIN). The generator (G) observes some components of a real data vector, imputes the missing components conditioned on what is actually observed, and outputs a completed vector. The discriminator (D) then takes a completed vector and attempts to determine which components were actually observed and which were imputed. To ensure that D forces G to learn the desired distribution, we provide D with some additional information in the form of a hint vector. The hint reveals to D partial information about the missingness of the original sample, which is used by D to focus its attention on the imputation quality of particular components. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outperforms state-of-the-art imputation methods.
","['University of California, Los Angeles', 'University of Oxford', 'University of Oxford']"
2018,Synthesizing Programs for Images using Reinforced Adversarial Learning,"Iaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, Oriol Vinyals",https://icml.cc/Conferences/2018/Schedule?showEvent=1972,"Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.
","['Montreal Institute for Learning Algorithms', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2018,Geometry Score: A Method For Comparing Generative Adversarial Networks,"Valentin Khrulkov, Ivan Oseledets",https://icml.cc/Conferences/2018/Schedule?showEvent=2153,"One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.
","['Skolkovo Institute Of Science And Technology', 'Skoltech']"
2018,Addressing Function Approximation Error in Actor-Critic Methods,"Scott Fujimoto, Herke van Hoof, David Meger",https://icml.cc/Conferences/2018/Schedule?showEvent=2227,"In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead  to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.
","['McGill University', 'McGill University', 'McGill University']"
2018,Fast Bellman Updates for Robust MDPs,"Chin Pang Ho, Marek Petrik, Wolfram Wiesemann",https://icml.cc/Conferences/2018/Schedule?showEvent=2318,"We describe two efficient, and exact, algorithms for computing Bellman updates in robust Markov decision processes (MDPs). The first algorithm uses a homotopy continuation method to compute updates for L1-constrained s,a-rectangular ambiguity sets. It runs in quasi-linear time for plain L1-norms and also generalizes to weighted L1-norms. The second algorithm uses bisection to compute updates for robust MDPs with s-rectangular ambiguity sets. This algorithm, when combined with the homotopy method, also has a quasi-linear runtime. Unlike previous methods, our algorithms compute the primal solution in addition to the optimal objective value, which makes them useful in policy iteration methods. Our experimental results indicate that the proposed methods are over 1,000 times faster than Gurobi, a state-of-the-art commercial optimization package, for small instances, and the performance gap grows considerably with problem size.
","['Imperial College London', 'University of New Hampshire', 'Imperial College']"
2018,Configurable Markov Decision Processes,"Alberto Maria Metelli, Mirco Mutti, Marcello Restelli",https://icml.cc/Conferences/2018/Schedule?showEvent=2137,"In many real-world problems, there is the possibility to configure, to a limited extent, some environmental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new type of interaction with the environment. Furthermore, we provide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy and the environment configuration. After having introduced our approach and derived some theoretical results, we present the experimental evaluation in two explicative problems to show the benefits of the environment configurability on the performance of the learned policy.
","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano']"
2018,Prediction Rule Reshaping,"Matt Bonakdarpour, Sabyasachi Chatterjee, Rina Barber, John Lafferty",https://icml.cc/Conferences/2018/Schedule?showEvent=2222,"Two methods are proposed for high-dimensional shape-constrained regression and classification. These methods reshape pre-trained prediction rules to satisfy shape constraints like monotonicity and convexity. The first method can be applied to any pre-trained prediction rule, while the second method deals specifically with random forests. In both cases, efficient algorithms are developed for computing the estimators, and experiments are performed to demonstrate their performance on four datasets. We find that reshaping methods enforce shape constraints without compromising predictive accuracy.
","['University of Chicago', 'Sabyasachi Chatterjee', '', 'Yale University']"
2018,Dimensionality-Driven Learning with Noisy Labels,"Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah Erfani, Shutao Xia, Sudanthi Wijewickrema, James Bailey",https://icml.cc/Conferences/2018/Schedule?showEvent=1970,"Datasets with significant proportions of noisy (incorrect) class labels present challenges for training accurate Deep Neural Networks (DNNs). We propose a new perspective for understanding DNN generalization for such datasets, by investigating the dimensionality of the deep representation subspace of training samples. We show that from a dimensionality perspective, DNNs exhibit quite distinctive learning styles when trained with clean labels versus when trained with a proportion of noisy labels. Based on this finding, we develop a new dimensionality-driven learning strategy, which monitors the dimensionality of subspaces during training and adapts the loss function accordingly. We empirically demonstrate that our approach is highly tolerant to significant proportions of noisy labels, and can effectively learn low-dimensional local subspaces that capture the data distribution.
","['The University of Melbourne', 'Tsinghua University', 'National Institute of Informatics', 'The University of Melbourne', 'University of Melbourne', 'Tsinghua University', 'University of Melbourne', 'The University of Melbourne']"
2018,Learning Memory Access Patterns,"Milad Hashemi, Kevin Swersky, Jamie Smith, Grant Ayers, Heiner Litz, Jichuan Chang, Christos Kozyrakis, Parthasarathy Ranganathan",https://icml.cc/Conferences/2018/Schedule?showEvent=2243,"The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations; augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.
","['Google', 'Google Brain', 'Google', 'Stanford', 'UC Santa Cruz', 'Google', 'Stanford University', 'Google, USA']"
2018,Geodesic Convolutional Shape Optimization,"Pierre Baque, Edoardo Remelli, Francois Fleuret, EPFL Pascal Fua",https://icml.cc/Conferences/2018/Schedule?showEvent=1944,"Aerodynamic shape optimization has many industrial applications. Existing methods, however, are so computationally demanding that typical engineering practices are to either simply try a limited number of hand-designed shapes or restrict oneself to shapes that can be parameterized using only few degrees of freedom. In this work, we introduce a new way to optimize complex shapes fast and accurately. To this end, we train Geodesic Convolutional Neural Networks to emulate a fluidynamics simulator. The key to making this approach practical is remeshing the original shape using a poly-cube map, which makes it possible to perform the computations on GPUs instead of CPUs.  The neural net is then used to formulate an objective function that is differentiable with respect to the shape parameters, which can then be optimized using a gradient-based technique.  This outperforms state-of-the-art methods by 5 to 20\% for standard problems and, even more importantly, our approach applies to cases that previous methods cannot handle.
","['EPFL', 'epfl', 'Idiap research institute', 'EPFL, Switzerland']"
2018,Visualizing and Understanding Atari Agents,"Samuel Greydanus, Anurag Koul, Jonathan Dodge, Alan Fern",https://icml.cc/Conferences/2018/Schedule?showEvent=1987,"While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how agents evolve during learning. We also test our method on non-expert human subjects and find that it improves their ability to reason about these agents. Overall, our results show that saliency information can provide significant insight into an RL agent's decisions and learning behavior.
","['Oregon State University', 'Oregon State University', 'Oregon State University', 'Oregon State University']"
2018,"An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning","Dhruv Malik, Malayandi Palaniappan, Jaime Fisac, Dylan Hadfield-Menell, Stuart Russell, Anca Dragan",https://icml.cc/Conferences/2018/Schedule?showEvent=1969,"Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL: the human is a full information agent. This enables us to derive an optimality-preserving modification to the standard Bellman update, which reduces the complexity of the problem by an exponential factor. Additionally, we show that our modified Bellman update allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers, including exact methods, point-based methods, and Monte Carlo Tree Search methods. We find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogical (teaching) behavior, while the robot interprets it as such and attains higher value for the human.
","['UC Berkeley', 'UC Berkeley/ Stanford', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'EECS Department, University of California, Berkeley']"
2018,Is Generator Conditioning Causally Related to GAN Performance?,"Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B Brown, Christopher Olah, Colin Raffel, Ian Goodfellow",https://icml.cc/Conferences/2018/Schedule?showEvent=2439,"Recent work suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks. We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (across the latent space) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the “quality” of trained GANs: the Inception Score and the Frechet Inception Distance. We then test the hypothesis that this relationship is causal by proposing a “regularization” technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean score for nearly all datasets on which we tested it. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.
","['Google Brain', 'Google', 'Google Brain', 'Google Brain', 'Google Brain', 'Google', 'Google Brain']"
2018,K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning,"Jihun Hamm, Yung-Kyun Noh",https://icml.cc/Conferences/2018/Schedule?showEvent=1962,"Minimax optimization plays a key role in adversarial training of machine learning algorithms, such as learning generative models, domain adaptation, privacy preservation, and robust learning. In this paper, we demonstrate the failure of alternating gradient descent in minimax optimization problems due to the discontinuity of solutions of the inner maximization.  To address this, we propose a new $\epsilon$-subgradient descent algorithm that addresses this problem by simultaneously tracking $K$ candidate solutions. Practically, the algorithm can find solutions that previous saddle-point algorithms cannot find, with only a sublinear increase of complexity in $K$.  We analyze the conditions under which the algorithm converges to the true solution in detail.  A significant improvement in stability and convergence speed of the algorithm is observed in simple representative problems, GAN training, and domain-adaptation problems.","['The Ohio State University', 'Seoul National University']"
2018,Inductive Two-Layer Modeling with Parametric Bregman Transfer,"Vignesh Ganapathiraman, Zhan Shi, Xinhua Zhang, Yaoliang Yu",https://icml.cc/Conferences/2018/Schedule?showEvent=2104,"Latent prediction models, exemplified by multi-layer networks, employ hidden variables that automate abstract feature discovery. They typically pose nonconvex optimization problems and effective semi-definite programming (SDP) relaxations have been developed to enable global solutions (Aslan et al., 2014).However, these models rely on nonparametric training of layer-wise kernel representations, and are therefore restricted to transductive learning which slows down test prediction. In this paper, we develop a new inductive learning framework for parametric transfer functions using matching losses. The result for ReLU utilizes completely positive matrices, and the inductive learner not only delivers superior accuracy but also offers an order of magnitude speedup over SDP with constant approximation guarantees.
","['University of Illinois at Chicago', 'University of Illinois at Chicago', 'University of Illinois at Chicago', 'University of Waterloo']"
2018,Does Distributionally Robust Supervised Learning Give Robust Classifiers?,"Weihua Hu, Gang Niu, Issei Sato, Masashi Sugiyama",https://icml.cc/Conferences/2018/Schedule?showEvent=2181,"Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.
","['The University of Tokyo', 'RIKEN', 'University of Tokyo / RIKEN', 'RIKEN / The University of Tokyo']"
2018,Understanding Generalization and Optimization Performance of Deep CNNs,"Pan Zhou, Jiashi Feng",https://icml.cc/Conferences/2018/Schedule?showEvent=1932,"This work aims to provide understandings on the remarkable success of deep convolutional neural networks (CNNs) by theoretically analyzing their generalization performance and establishing optimization guarantees for gradient descent based training algorithms. Specifically, for a CNN model consisting of $l$ convolutional layers and one fully connected layer, we prove that its generalization error is bounded by $\mathcal{O}(\sqrt{\dt\widetilde{\varrho}/n})$ where $\theta$ denotes  freedom degree of the network parameters and $\widetilde{\varrho}=\mathcal{O}(\log(\prod_{i=1}^{l}\rwi{i} (\ki{i}-\si{i}+1)/p)+\log(\rf))$ encapsulates architecture parameters including the  kernel size $\ki{i}$, stride $\si{i}$, pooling size $p$ and parameter magnitude $\rwi{i}$. To our best knowledge, this is the first generalization bound that only depends on $\mathcal{O}(\log(\prod_{i=1}^{l+1}\rwi{i}))$, tighter than existing ones that all involve an exponential term like $\mathcal{O}(\prod_{i=1}^{l+1}\rwi{i})$. Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk. This well explains why gradient descent training algorithms usually perform sufficiently well in practice. Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks. It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk,  thus ensuring that the optimized CNN model well generalizes to new data.","['National University of Singapore', 'National University of Singapore']"
2018,The Multilinear Structure of ReLU Networks,"Thomas Laurent, James von Brecht",https://icml.cc/Conferences/2018/Schedule?showEvent=2005,"We study the loss surface of neural networks equipped with a hinge loss criterion and ReLU or leaky ReLU nonlinearities. Any such network defines a piecewise multilinear form in parameter space.  By appealing to harmonic analysis we show that all local minima of such network are non-differentiable, except for those minima that occur in a region of parameter space where the loss surface is perfectly flat. Non-differentiable minima are therefore not technicalities or pathologies; they are heart of the problem when investigating the loss of ReLU networks. As a consequence, we must employ techniques from nonsmooth analysis to study these loss surfaces. We show how to apply these techniques in some illustrative cases.
","['Loyola Marymount University', 'CSULB']"
2018,Parallel and Streaming Algorithms for K-Core Decomposition,"Hossein Esfandiari, Silvio Lattanzi, Vahab Mirrokni",https://icml.cc/Conferences/2018/Schedule?showEvent=2186,"The k-core decomposition is a fundamental primitive in many machine learning and data mining applications. We present the first distributed and the first streaming algorithms to compute and maintain an approximate k-core decomposition with provable guarantees. Our algorithms achieve rigorous bounds on space complexity while bounding the number of passes or number of rounds of computation. We do so by presenting a new powerful sketching technique for k-core decomposition, and then by showing it can be computed efficiently in both streaming and MapReduce models. Finally, we confirm the effectiveness of our sketching technique empirically on a number of publicly available graphs.
","['Harvard University', 'Google Zurich', 'Google Research']"
2018,Fast Approximate Spectral Clustering for Dynamic Networks,"Lionel Martin, Andreas Loukas, Pierre Vandergheynst",https://icml.cc/Conferences/2018/Schedule?showEvent=2028,"Spectral clustering is a widely studied problem, yet its complexity is prohibitive for dynamic graphs of even modest size. We claim that it is possible to reuse information of past cluster assignments to expedite computation. Our approach builds on a recent idea of sidestepping the main bottleneck of spectral clustering, i.e., computing the graph eigenvectors, by a polynomial-based randomized sketching technique. We show that the proposed algorithm achieves clustering assignments with quality approximating that of spectral clustering and that it can yield significant complexity benefits when the graph dynamics are appropriately bounded. In our experiments, our method clusters 30k node graphs 3.9$\times$ faster in average and deviates from the correct assignment by less than 0.1\%.","['EPFL', 'EPFL', 'École polytechnique fédérale de Lausanne']"
2018,Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima,"Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, Barnabás Póczos",https://icml.cc/Conferences/2018/Schedule?showEvent=1922,"We consider the problem of learning an one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation function, i.e., $f(Z; w, a) = \sum_j a_j\sigma(w^\top Z_j)$, in which both the convolutional weights $w$ and the output weights $a$ are parameters to be learned.  We prove that with Gaussian input $\vZ$, there is a spurious local minimizer. Surprisingly, in the presence of the spurious local minimizer, starting from randomly initialized weights, gradient descent with weight normalization can still be proven to recover the true parameters with constant probability (which can be boosted to probability $1$ with multiple restarts). We also show that with constant probability, the same procedure could also converge to the spurious local minimum, showing that the local minimum plays a non-trivial role in the dynamics of gradient descent.  Furthermore, a quantitative analysis shows that the gradient descent dynamics has two phases: it starts off slow, but converges much faster after several iterations.","['Carnegie Mellon University', 'University of Southern California', 'Facebook AI Research', 'Carnegie Mellon University', 'CMU']"
2018,Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions,"Quynh Nguyen, Mahesh Mukkamala, Matthias Hein",https://icml.cc/Conferences/2018/Schedule?showEvent=2272,"In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no layer has more hidden units than the input dimension, produce necessarily connected decision regions. This implies that a sufficiently wide hidden layer is necessary to guarantee that the network can produce disconnected decision regions. We discuss the implications of this result for the construction of neural networks, in particular the relation to the problem of adversarial manipulation of classifiers.
","['Saarland University', 'Saarland University', 'University of Tuebingen']"
2018,Greed is Still Good: Maximizing Monotone Submodular+Supermodular (BP) Functions,"Wenruo Bai, Jeff Bilmes",https://icml.cc/Conferences/2018/Schedule?showEvent=2350,"We analyze the performance of the greedy algorithm, and also a   discrete semi-gradient based algorithm, for maximizing the sum of a   suBmodular and suPermodular (BP) function (both of which are   non-negative monotone non-decreasing) under two types of   constraints, either a cardinality constraint or $p\geq 1$ matroid   independence constraints.  These problems occur naturally in several   real-world applications in data science, machine learning, and   artificial intelligence.  The problems are ordinarily inapproximable   to any factor.  Using the curvature $\curv_f$ of the   submodular term, and introducing $\curv^g$ for the supermodular term   (a natural dual curvature for supermodular functions), however, both   of which are computable in linear time, we show that BP maximization   can be efficiently approximated by both the greedy and the   semi-gradient based algorithm.  The algorithms yield multiplicative   guarantees of   $\frac{1}{\curv_f}\left[1-e^{-(1-\curv^g)\curv_f}\right]$ and   $\frac{1-\curv^g}{(1-\curv^g)\curv_f + p}$ for the two types of   constraints respectively. For pure monotone supermodular constrained   maximization, these yield $1-\curvg$ and $(1-\curvg)/p$ for the two   types of constraints respectively.  We also analyze the hardness of   BP maximization and show that our guarantees match hardness by a   constant factor and by $O(\ln(p))$ respectively. Computational   experiments are also provided supporting our analysis.","['University of Washington', 'UW']"
2018,Black-box Adversarial Attacks with Limited Queries and Information,"Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin",https://icml.cc/Conferences/2018/Schedule?showEvent=2426,"Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partial-information setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.
","['Massachusetts Institute of Technology', 'MIT', 'MIT CSAIL', 'MIT']"
2018,Using Inherent Structures to design Lean 2-layer RBMs,"Abhishek Bansal, Abhinav Anand, Chiranjib Bhattacharyya",https://icml.cc/Conferences/2018/Schedule?showEvent=2376,"Understanding the representational power of  Restricted Boltzmann Machines (RBMs) with multiple layers is  an ill-understood problem and is an area of active research. Motivated from the approach of \emph{Inherent Structure formalism} (Stillinger & Weber, 1982), extensively used in analysing Spin Glasses, we propose a novel measure called \emph{Inherent Structure Capacity} (ISC), which characterizes the representation capacity of a fixed architecture RBM by the expected number of modes of distributions emanating from the RBM with parameters drawn from a prior distribution. Though ISC is intractable, we show that for a single layer RBM architecture ISC approaches a finite constant as number of hidden units are increased and to further improve the ISC, one needs to add a second layer. Furthermore, we introduce \emph{Lean} RBMs, which are multi-layer RBMs where each layer can have at-most O(n) units with the number of visible units being n.  We show that for every single layer RBM with Omega(n^{2+r}), r >= 0,   hidden units there exists  a two-layered \emph{lean} RBM with Theta(n^2) parameters with the same ISC, establishing  that 2 layer RBMs can achieve the same representational power as single-layer RBMs but using far fewer number of parameters.  To the best of our knowledge, this is the first result which quantitatively establishes the need for layering.
","['IBM Research', 'Indian Institute of Science', 'Indian Institute of Science']"
2018,Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care,"Patrick Schwab, Emanuela Keller, Carl Muroi, David J. Mack, Christian Strässle, Walter Karlen",https://icml.cc/Conferences/2018/Schedule?showEvent=2119,"Patients in the intensive care unit (ICU) require constant and close supervision. To assist clinical staff in this task, hospitals use monitoring systems that trigger audiovisual alarms if their algorithms indicate that a patient's condition may be worsening. However, current monitoring systems are extremely sensitive to movement artefacts and technical errors. As a result, they typically trigger hundreds to thousands of false alarms per patient per day - drowning the important alarms in noise and adding to the exhaustion of clinical staff. In this setting, data is abundantly available, but obtaining trustworthy annotations by experts is laborious and expensive. We frame the problem of false alarm reduction from multivariate time series as a machine-learning task and address it with a novel multitask network architecture that utilises distant supervision through multiple related auxiliary tasks in order to reduce the number of expensive labels required for training. We show that our approach leads to significant improvements over several state-of-the-art baselines on real-world ICU data and provide new insights on the importance of task selection and architectural choices in distantly supervised multitask learning.
","['ETH Zurich', 'University Hospital Zurich', 'University Hospital Zurich', 'University Hospital Zurich', 'University Hospital Zurich', 'ETH Zurich']"
2018,Composable Planning with Attributes,"Amy Zhang, Sainbayar Sukhbaatar, Adam Lerer, Arthur Szlam, Facebook Rob Fergus",https://icml.cc/Conferences/2018/Schedule?showEvent=2348,"The tasks that an agent will need to solve often are not known during training. However, if the agent knows which properties of the environment are important then, after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without training specifically for them. Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest. We propose a method that learns a policy for transitioning between ``nearby'' sets of attributes, and maintains a graph of possible transitions. Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan.  We show in 3D block stacking, grid-world games, and StarCraft that our model is able to generalize to longer, more complex tasks at test time by composing simpler learned policies.
","['Facebook AI Research', 'NYU', 'Facebook AI Research', 'Facebook', 'Facebook AI Research, NYU']"
2018,Measuring abstract reasoning in neural networks,"Adam Santoro, Feilx Hill, David GT Barrett, Ari S Morcos, Timothy Lillicrap",https://icml.cc/Conferences/2018/Schedule?showEvent=2194,"Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation 'regimes' in which the training data and test questions differ in clearly-defined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.
","['DeepMind', 'Deepmind', 'DeepMind', 'DeepMind', 'Google DeepMind']"
2018,Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity,"Lin Chen, Christopher Harshaw, Hamed Hassani, Amin Karbasi",https://icml.cc/Conferences/2018/Schedule?showEvent=2385,"Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of non-convex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projection-free algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal $O( \sqrt{T})$ adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an $O(T^{2/3})$ stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel ""lifting"" framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments.","['Yale University', 'Yale University', 'University of Pennsylvania', 'Yale']"
2018,Self-Bounded Prediction Suffix Tree via Approximate String Matching,"Dongwoo Kim, Christian Walder",https://icml.cc/Conferences/2018/Schedule?showEvent=2112,"Prediction suffix trees (PST) provide an effective tool for sequence modelling and prediction. Current prediction techniques for PSTs rely on exact matching between the suffix of the current sequence and the previously observed sequence. We present a provably correct algorithm for learning a PST with approximate suffix matching by relaxing the exact matching condition. We then present a self-bounded enhancement of our algorithm where the depth of suffix tree grows automatically in response to the model performance on a training sequence. Through experiments on synthetic datasets as well as three real-world datasets, we show that the approximate matching PST results in better predictive performance than the other variants of PST.
","['The Australian National University', 'Data61, the Australian National University']"
2018,MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels,"Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, Li Fei-Fei",https://icml.cc/Conferences/2018/Schedule?showEvent=1952,"Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels.
","['Google', 'Stanford University', 'Google Inc', 'Google', 'Stanford University & Google']"
2018,Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks,"Daphna Weinshall, Gad A Cohen, Dan Amir",https://icml.cc/Conferences/2018/Schedule?showEvent=2021,"We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss. We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples. Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task. While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training. When the task is made more difficult, improvement in generalization performance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.
","['Hebrew University of Jerusalem, Israel', 'Hebrew University', 'Hebrew University of Jerusalem']"
2018,Composite Functional Gradient Learning of Generative  Adversarial Models,"Rie Johnson, Tong Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=2012,"This paper first presents a theory for generative adversarial methods that does not rely on the traditional minimax formulation. It shows that with a strong discriminator, a good generator can be learned so that the KL divergence between the distributions of real data and generated data improves after each functional gradient step until it converges to zero. Based on the theory, we propose a new stable generative adversarial method. A theoretical insight into the original GAN from this new viewpoint is also provided. The experiments on image generation show the effectiveness of our new method.
","['RJ Research Consulting', 'Tecent AI Lab']"
2018,LaVAN: Localized and Visible Adversarial Noise,"Danny Karmon, Daniel Zoran, Yoav Goldberg",https://icml.cc/Conferences/2018/Schedule?showEvent=2154,"Most works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object(s) in the image. We show that it  is possible to generate localized adversarial noises that cover only 2% of the pixels in the  image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates.
","['Bar Ilan University', 'DeepMind', 'Bar Ilan University']"
2018,Approximation Guarantees for Adaptive Sampling,"Eric Balkanski, Yaron Singer",https://icml.cc/Conferences/2018/Schedule?showEvent=1882,"In this paper we analyze an adaptive sampling approach for submodular maximization. Adaptive sampling is a technique that has recently been shown to achieve a constant factor approximation guarantee for submodular maximization under a cardinality constraint with exponentially fewer adaptive rounds than any previously studied constant factor approximation algorithm for this problem. Adaptivity quantifies the number of sequential rounds that an algorithm makes when function evaluations can be executed in parallel and is the parallel running time of an algorithm, up to low order terms. Adaptive sampling achieves its exponential speedup at the expense of approximation. In theory, it is guaranteed to produce a solution that is a 1/3 approximation to the optimum. Nevertheless, experiments show that adaptive sampling techniques achieve far better values in practice.  In this paper we provide theoretical justification for this phenomenon. In particular, we show that under very mild conditions of curvature of a function, adaptive sampling techniques achieve an approximation arbitrarily close to 1/2 while maintaining their low adaptivity. Furthermore, we show that the approximation ratio approaches 1 in direct relationship to a homogeneity property of the submodular function. In addition, we conduct experiments on real data sets in which the curvature and homogeneity properties can be easily manipulated and demonstrate the relationship between approximation and curvature, as well as the effectiveness of adaptive sampling in practice.
","['Harvard', 'Harvard']"
2018,Constrained Interacting Submodular Groupings,"Andrew Cotter, Mahdi Milani Fard, Seungil You, Maya Gupta, Jeff Bilmes",https://icml.cc/Conferences/2018/Schedule?showEvent=2129,"We introduce the problem of grouping a finite ground set into blocks   where each block is a subset of the ground set and where: (i) the   blocks are individually highly valued by a submodular function (both   robustly and in the average case) while satisfying block-specific   matroid constraints; and (ii) block scores interact where blocks are   jointly scored highly, thus making the blocks mutually   non-redundant.  Submodular functions are good models of information   and diversity; thus, the above can be seen as grouping the ground   set into matroid constrained blocks that are both intra- and   inter-diverse. Potential applications include forming ensembles of   classification/regression models, partitioning data for parallel   processing, and summarization.  In the non-robust case, we reduce   the problem to non-monotone submodular maximization subject to   multiple matroid constraints. In the mixed robust/average case, we   offer a bi-criterion guarantee for a polynomial time deterministic   algorithm and a probabilistic guarantee for randomized algorithm, as   long as the involved submodular functions (including the inter-block   interaction terms) are monotone. We close with a case study in which   we use these algorithms to find high quality diverse ensembles of   classifiers, showing good results.
","['Google AI', 'Google', 'Google', 'Google', 'UW']"
2018,Residual Unfairness in Fair Machine Learning from Prejudiced Data,"Nathan Kallus, Angela Zhou",https://icml.cc/Conferences/2018/Schedule?showEvent=2314,"Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a ""bias in, bias out"" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.
","['Cornell University', 'Cornell University']"
2018,Adversarial Regression with Multiple Learners,"Liang Tong, Sixie Yu, Scott Alfeld, Yevgeniy Vorobeychik",https://icml.cc/Conferences/2018/Schedule?showEvent=2481,"Despite the considerable success enjoyed by machine learning techniques in practice, numerous studies demonstrated that many approaches are vulnerable to attacks. An important class of such attacks involves adversaries changing features at test time to cause incorrect predictions. Previous investigations of this problem pit a single learner against an adversary. However, in many situations an adversary’s decision is aimed at a collection of learners, rather than specifically targeted at each independently. We study the problem of adversarial linear regression with multiple learners. We approximate the resulting game by exhibiting an upper bound on learner loss functions, and show that the resulting game has a unique symmetric equilibrium. We present an algorithm for computing this equilibrium, and show through extensive experiments that equilibrium models are significantly more robust than conventional regularized linear regression.
","['Vanderbilt University', 'Vanderbilt University', 'Amherst College', 'Vanderbilt University']"
2018,Representation Tradeoffs for Hyperbolic Embeddings,"Frederic Sala, Christopher De Sa, Albert Gu, Christopher Re",https://icml.cc/Conferences/2018/Schedule?showEvent=2289,"Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures. We give a combinatorial construction that embeds trees into hyperbolic space with arbitrarily low distortion without optimization. On WordNet, this algorithm obtains a mean-average-precision of 0.989 with only two dimensions, outperforming existing work by 0.11 points. We provide bounds characterizing the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that enables us to reduce dimensionality. Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based implementation that can handle incomplete information.
","['Stanford', 'Cornell', 'Stanford University', 'Stanford']"
2018,Improving Sign Random Projections With Additional Information,"Keegan Kang, Wei Pin Wong",https://icml.cc/Conferences/2018/Schedule?showEvent=1896,"Sign random projections (SRP) is a technique which allows the user to quickly estimate the angular similarity and inner products between data. We propose using additional information to improve these estimates which is easy to implement and cost efficient. We prove that the variance of our estimator is lower than the variance of SRP. Our proposed method can also be used together with other modifications of SRP, such as Super-Bit LSH (SBLSH). We demonstrate the effectiveness of our method on the MNIST test dataset and the Gisette dataset. We discuss how our proposed method can be extended to random projections or even other hashing algorithms.
","['Singapore University Of Technology And Design', 'Singapore University of Technology and Design']"
2018,"Bandits with Delayed, Aggregated Anonymous Feedback","Ciara Pike-Burke, Shipra Agrawal, Csaba Szepesvari, Steffen Grünewälder",https://icml.cc/Conferences/2018/Schedule?showEvent=2212,"We study a variant of the stochastic $K$-armed bandit problem, which we call ""bandits with delayed, aggregated anonymous feedback''. In this problem, when the player pulls an arm, a reward is generated, however it is not immediately observed. Instead, at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round. The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a particular reward is lost. The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback? Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay. In this paper, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the expected delay (or a bound on it) is known. We provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded, and up to logarithmic factors or an additive variance term for unbounded delays.","['Lancaster University', 'Columbia University', 'DeepMind/University of Alberta', 'Lancaster University']"
2018,Make the Minority Great Again: First-Order Regret Bound for Contextual Bandits,"Zeyuan Allen-Zhu, Sebastien Bubeck, Yuanzhi Li",https://icml.cc/Conferences/2018/Schedule?showEvent=2111,"Regret bounds in online learning compare the player's performance to $L*$, the optimal performance in hindsight with a fixed strategy. Typically such bounds scale with the square root of the time horizon $T$. The more refined concept of first-order regret bound replaces this with a scaling $\sqrt{L*}$, which may be much smaller than $\sqrt{T}$.  It is well known that minor variants of standard algorithms satisfy first-order regret bounds in the full information and multi-armed bandit settings. In a COLT 2017 open problem, Agarwal, Krishnamurthy, Langford, Luo, and Schapire raised the issue that existing techniques do not seem sufficient to obtain first-order regret bounds for the contextual bandit problem. In the present paper, we resolve this open problem by presenting a new strategy based on augmenting the policy space.","['Microsoft Research AI', 'Microsoft Research', 'Princeton University']"
2018,Learning Policy Representations in Multiagent Systems,"Aditya Grover, Maruan Al-Shedivat, Jayesh K. Gupta, Yura Burda, Harrison Edwards",https://icml.cc/Conferences/2018/Schedule?showEvent=2435,"Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.
","['Stanford University', 'Carnegie Mellon University', 'Stanford University', 'OpenAI', 'OpenAI / University of Edinburgh']"
2018,Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems,"Eugenio Bargiacchi, Timothy Verstraeten, Diederik Roijers, Ann Nowé, Hado van Hasselt",https://icml.cc/Conferences/2018/Schedule?showEvent=2177,"Learning to coordinate between multiple agents is an important problem in many reinforcement learning problems. Key to learning to coordinate is exploiting loose couplings, i.e., conditional independences between agents. In this paper we study learning in repeated fully cooperative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph. We propose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that exploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents. We empirically compare MAUCE to sparse cooperative Q-learning, and a state-of-the-art combinatorial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.
","['Vrije Universiteit Brussel', 'Vrije Universiteit Brussel', 'VUB', 'Vrije Universiteit Brussel', 'DeepMind']"
2018,Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations,"Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong",https://icml.cc/Conferences/2018/Schedule?showEvent=1935,"Deep neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge deep neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interpreted as different numerical discretizations of differential equations. This finding brings us a brand new perspective on the design of effective deep architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and potentially more effective deep networks. As an example, we propose a linear multi-step architecture (LM-architecture) which is inspired by the linear multi-step method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by applying the LM-architecture on ResNet and ResNeXt respectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comparable numbers of trainable parameters. In particular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly compress (>50%) the original networks while maintaining a similar performance. This can be explained mathematically using the concept of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training process which helps to improve generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily apply stochastic training to the networks with the LM-architecture. As an example, we introduced stochastic depth to LM-ResNet and achieve significant improvement over the original LM-ResNet on CIFAR10.
","['Peking University', 'Zhejiang University', 'Mass General Hospital, Harvard Medical School', 'Peking University']"
2018,Compressing Neural Networks using the Variational Information Bottelneck,"Bin Dai, Chen Zhu, Baining Guo, David Wipf",https://icml.cc/Conferences/2018/Schedule?showEvent=2411,"Neural networks can be compressed to reduce memory and computational requirements, or to increase accuracy by facilitating the use of a larger base architecture.  In this paper we focus on pruning individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory.  To improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound.  Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved.  In contrast, the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework, providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape.  We demonstrate state-of-the-art compression rates across an array of datasets and network architectures.
","['Tsinghua University', 'University of Maryland', 'MSR Asia', 'Microsoft Research']"
2018,Scalable Bilinear Pi Learning Using State and Action Features,"Yichen Chen, Lihong Li, Mengdi Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=2330,"Approximate linear programming (ALP) represents one of the major algorithmic families to solve large-scale Markov decision processes (MDP).  In this work, we study a primal-dual formulation of the ALP, and develop a scalable, model-free algorithm called bilinear $\pi$ learning for reinforcement learning when a sampling oracle is provided.  This algorithm enjoys a number of advantages.  First, it adopts linear and bilinear models to represent the high-dimensional value function and state-action distributions, respectively, using given state and action features.  Its run-time complexity depends on the number of features, not the size of the underlying MDPs.  Second, it operates in a fully online fashion without having to store any sample, thus having minimal memory footprint.  Third, we prove that it is sample-efficient, solving for the optimal policy to high precision with a sample complexity linear in the dimension of the parameter space.","['Princeton University', 'Google Inc.', 'Princeton University']"
2018,Time Limits in Reinforcement Learning,"Fabio Pardo, Arash Tavakoli, Vitaly Levdik, Petar Kormushev",https://icml.cc/Conferences/2018/Schedule?showEvent=2463,"In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.
","['Imperial College London', 'Imperial College London', 'Imperial College London', 'Imperial College London']"
2018,Semi-Supervised Learning on Data Streams via Temporal Label Propagation,"Tal Wagner, Sudipto Guha, Shiva Kasiviswanathan, Nina Mishra",https://icml.cc/Conferences/2018/Schedule?showEvent=1879,"We consider the problem of labeling points on a fast-moving data stream when only a small number of labeled examples are available. In our setting, incoming points must be processed efficiently and the stream is too large to store in its entirety. We present a semi-supervised learning algorithm for this task. The algorithm maintains a small synopsis of the stream which can be quickly updated as new points arrive, and labels every incoming point by provably learning from the full history of the stream. Experiments on real datasets validate that the algorithm can quickly and accurately classify points on a stream with a small quantity of labeled examples.
","['MIT', 'Amazon', 'Amazon', 'Amazon']"
2018,Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval and Matrix Completion,"Cong Ma, Kaizheng Wang, Yuejie Chi, Yuxin Chen",https://icml.cc/Conferences/2018/Schedule?showEvent=1893,"Recent years have seen a flurry of activities in designing provably efficient nonconvex optimization procedures for solving statistical estimation problems. For various problems like phase retrieval or low-rank matrix completion, state-of-the-art nonconvex procedures  require proper regularization (e.g.~trimming, regularized cost, projection) in order to guarantee fast convergence. When it comes to vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in several nonconvex problems: even in the absence of explicit regularization, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This ``implicit regularization'' feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on two statistical estimation problems, i.e.~solving random quadratic systems of equations and low-rank matrix completion, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent enables optimal control of both entrywise and spectral-norm errors.
","['Princeton University', 'Princeton University', 'CMU', 'Princeton University']"
2018,A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models,"Beilun Wang, Arshdeep Sekhon, Yanjun Qi",https://icml.cc/Conferences/2018/Schedule?showEvent=2327,"We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models (sGGMs) from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks (large $K$) under a high-dimensional (large $p$) situation.  In this paper, we propose a  novel \underline{J}oint \underline{E}lementary \underline{E}stimator incorporating additional \underline{K}nowledge (JEEK) to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledge-specific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art  $O(p^5K^4)$ to $O(p^2K^4)$. We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate $O(\log(Kp)/n_{tot})$ as the state-of-the-art estimators that are much harder to compute. Empirically, on multiple synthetic datasets and one real-world data from neuroscience, JEEP outperforms the speed of the state-of-arts significantly while achieving the same level of prediction accuracy.","['University of Virginia', 'University of Virginia', 'University of Virginia']"
2018,Bucket Renormalization for Approximate Inference,"Sungsoo Ahn, Michael Chertkov, Adrian Weller, Jinwoo Shin",https://icml.cc/Conferences/2018/Schedule?showEvent=2157,"Probabilistic graphical models are a key tool in machine learning applications.  Computing the partition function, i.e., normalizing constant, is a fundamental task of statistical inference but is generally computationally intractable, leading to extensive study of approximation methods. Iterative variational methods are a popular and successful family of approaches. However, even state of the art variational methods can return poor results or fail to converge on difficult instances. In this paper, we instead consider computing the partition function via sequential summation over variables. We develop robust approximate algorithms by combining ideas from mini-bucket elimination with tensor network and renormalization group methods from statistical physics. The resulting “convergence-free” methods show good empirical performance on both synthetic and real-world benchmark models, even for difficult instances.
","['KAIST', 'Los Alamos National Laboratory', 'University of Cambridge, Alan Turing Institute', 'KAIST']"
2018,Kernel Recursive ABC: Point Estimation with Intractable Likelihood,"Takafumi Kajihara, Motonobu Kanagawa, Keisuke Yamazaki, Kenji Fukumizu",https://icml.cc/Conferences/2018/Schedule?showEvent=2239,"We propose a novel approach to parameter estimation for simulator-based statistical models with intractable likelihood. Our proposed method involves recursive application of kernel ABC and kernel herding to the same observed data. We provide a theoretical explanation regarding why the approach works, showing (for the population setting) that, under a certain assumption, point estimates obtained with this method converge to the true parameter, as recursion proceeds. We have conducted a variety of numerical experiments, including parameter estimation for a real-world pedestrian flow simulator, and show that in most cases our method outperforms existing approaches.
","['NEC', 'Max Planck Institute for Intelligent Systems', 'National Institute of Advanced Industrial Science and Technology', 'Institute of Statistical Mathematics']"
2018,Modeling Others using Oneself in Multi-Agent Reinforcement Learning,"Roberta Raileanu, Emily Denton, Arthur Szlam, Facebook Rob Fergus",https://icml.cc/Conferences/2018/Schedule?showEvent=2319,"We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.
","['NYU', 'New York University', 'Facebook AI Research', 'Facebook AI Research, NYU']"
2018,Tropical Geometry of Deep Neural Networks,"Liwen Zhang, Gregory Naisat, Lek-Heng Lim",https://icml.cc/Conferences/2018/Schedule?showEvent=2048,"We establish, for the first time, explicit connections between feedforward neural networks  with ReLU activation and tropical geometry --- we show that the family of such neural networks is equivalent to the family of tropical rational maps.  Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions.  An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.
","['University of Chicago', 'The University of Chicago', 'University of Chicago']"
2018,Learning Dynamics of Linear Denoising Autoencoders,"Arnu Pretorius, Steve Kroon, Herman Kamper",https://icml.cc/Conferences/2018/Schedule?showEvent=2013,"Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning. Here we develop theory for how noise influences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics. We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them. Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics. We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs.
","['Stellenbosch University', 'Stellenbosch University', 'Stellenbosch University']"
2018,Nonparametric variable importance using an augmented neural network with multi-task learning,"Jean Feng, Brian Williamson, Noah Simon, Marco Carone",https://icml.cc/Conferences/2018/Schedule?showEvent=2042,"In predictive modeling applications, it is often of interest to determine the relative contribution of subsets of features in explaining the variability of an outcome. It is useful to consider this variable importance as a function of the unknown, underlying data-generating mechanism rather than the specific predictive algorithm used to fit the data. In this paper, we connect these ideas in nonparametric variable importance to machine learning, and provide a method for efficient estimation of variable importance when building a predictive model using a neural network. We show how a single augmented neural network with multi-task learning simultaneously estimates the importance of many feature subsets, improving on previous procedures for estimating importance. We demonstrate on simulated data that our method is both accurate and computationally efficient, and apply our method to both a study of heart disease and for predicting mortality in ICU patients.
","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']"
2018,Training Neural Machines with Trace-Based Supervision,"Matthew Mirman, Dimitar Dimitrov, Pavle Djordjevic, Timon Gehr, Martin Vechev",https://icml.cc/Conferences/2018/Schedule?showEvent=2452,"We investigate the effectiveness of trace-based supervision methods for training existing neural abstract machines. To define the class of neural machines amenable to trace-based supervision, we introduce the concept of a differential neural computational machine (dNCM) and show that several existing architectures (NTMs, NRAMs) can be described as dNCMs. We performed a detailed experimental evaluation with NTM and NRAM machines, showing that additional supervision on the interpretable portions of these architectures leads to better convergence and generalization capabilities of the learning phase than standard training, in both noise-free and noisy scenarios.
","['ETH Zürich', 'ETH Zurich', 'ETH', 'ETH Zurich', 'ETH Zurich']"
2018,Open Category Detection with PAC Guarantees,"Si Liu, Risheek Garrepalli, Thomas Dietterich, Alan Fern, Dan Hendrycks",https://icml.cc/Conferences/2018/Schedule?showEvent=2115,"Open category detection is the problem of detecting ""alien"" test instances that belong to categories or classes that were not present in the training data. In many applications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set predictions. Unfortunately, there are no algorithms that provide theoretical guarantees on their ability to detect aliens under general assumptions. Further, while there are algorithms for open category detection, there are few empirical results that directly report alien detection rates. Thus, there are significant theoretical and empirical gaps in our understanding of open category detection. In this paper, we take a step toward addressing this gap by studying a simple, but practically-relevant variant of open category detection. In our setting, we are provided with a ""clean"" training set that contains only the target categories of interest and an unlabeled ""contaminated'' training set that contains a fraction alpha of alien examples. Under the assumption that we know an upper bound on alpha we develop an algorithm with PAC-style guarantees on the alien detection rate, while aiming to minimize false alarms. Empirical results on synthetic and standard benchmark datasets demonstrate the regimes in which the algorithm can be effective and provide a baseline for further advancements.
","['Oregon State University', 'Oregon State University', '(organization)', 'Oregon State University', 'UC Berkeley']"
2018,SAFFRON: an Adaptive Algorithm for Online Control of the False Discovery Rate,"Aaditya Ramdas, Tijana Zrnic, Martin Wainwright, Michael Jordan",https://icml.cc/Conferences/2018/Schedule?showEvent=2207,"In the online false discovery rate (FDR) problem, one observes a possibly infinite sequence of $p$-values $P_1,P_2,\dots$, each testing a different null hypothesis, and an algorithm must pick a sequence of rejection thresholds $\alpha_1,\alpha_2,\dots$ in an online fashion, effectively rejecting the $k$-th null hypothesis whenever $P_k \leq \alpha_k$. Importantly, $\alpha_k$ must be a function of the past, and cannot depend on $P_k$ or any of the later unseen $p$-values, and must be chosen to guarantee that for any time $t$, the FDR up to time $t$ is less than some pre-determined quantity $\alpha \in (0,1)$. In this work, we present a powerful new framework for online FDR control that we refer to as ``SAFFRON''. Like older alpha-investing algorithms, SAFFRON starts off with an error budget (called alpha-wealth) that it intelligently allocates to different tests over time, earning back some alpha-wealth whenever it makes a new discovery. However, unlike older methods, SAFFRON's threshold sequence is based on a novel estimate of the alpha fraction that it allocates to true null hypotheses. In the offline setting, algorithms that employ an estimate of the proportion of true nulls are called ``adaptive'', hence SAFFRON can be seen as an online analogue of the offline Storey-BH adaptive procedure. Just as Storey-BH is typically more powerful than the Benjamini-Hochberg (BH) procedure under independence, we demonstrate that SAFFRON is also more powerful than its non-adaptive counterparts such as LORD.","['UC Berkeley', 'University of California, Berkeley', 'University of California at Berkeley', 'UC Berkeley']"
2018,Learning Localized Spatio-Temporal Models From Streaming Data,"Muhammad Osama, Dave Zachariah, Thomas Schön",https://icml.cc/Conferences/2018/Schedule?showEvent=2015,"We address the problem of predicting spatio-temporal processes with temporal patterns that vary across spatial regions, when data is obtained as a stream. That is, when the training dataset is augmented sequentially. Specifically, we develop a localized spatio-temporal covariance model of the process that can capture spatially varying temporal periodicities in the data. We then apply a covariance-fitting methodology to learn the model parameters which yields a predictor that can be updated sequentially with each new data point. The proposed method is evaluated using both synthetic and real climate data which demonstrate its ability to accurately predict data missing in spatial regions over time.
","['Uppsala University', 'Uppsala University', 'Uppsala University']"
2018,Feasible Arm Identification,"Julian Katz-Samuels, Clay Scott",https://icml.cc/Conferences/2018/Schedule?showEvent=2064,"We introduce the feasible arm identification problem, a pure exploration multi-armed bandit problem where the agent is given a set of $D$-dimensional arms and a polyhedron $P = \{x : A x \leq b \} \subset R^D$. Pulling an arm gives a random vector and the goal is to determine, using a fixed budget of $T$ pulls, which of the arms have means belonging to $P$. We propose three algorithms MD-UCBE, MD-SAR, and MD-APT and provide a unified analysis establishing upper bounds for each of them. We also establish a lower bound that matches up to constants the upper bounds of MD-UCBE and MD-APT. Finally, we demonstrate the effectiveness of our algorithms on synthetic and real-world datasets.","['University of Michigan', 'University of Michigan']"
2018,"Fast Maximization of Non-Submodular, Monotonic Functions on the Integer Lattice","Alan Kuhnle, J. Smith, Victoria Crawford, My T. Thai",https://icml.cc/Conferences/2018/Schedule?showEvent=1912,"The optimization of submodular functions on the integer lattice has received much attention recently, but the objective functions of many applications are non-submodular. We provide two approximation algorithms for maximizing a non-submodular function on the integer lattice subject to a cardinality constraint; these are the first algorithms for this purpose that have polynomial query complexity. We propose a general framework for influence maximization on the integer lattice that generalizes prior works on this topic, and we demonstrate the efficiency of our algorithms in this context.
","['University of Florida', 'University of Florida', 'University of Florida', 'University of Florida']"
2018,Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings,"Aryan Mokhtari, Hamed Hassani, Amin Karbasi",https://icml.cc/Conferences/2018/Schedule?showEvent=2358,"In this paper, we showcase the interplay between discrete and continuous optimization in network-structured settings. We propose the first  fully decentralized optimization method for a wide class of non-convex objective functions that possess a diminishing returns property. More specifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develop Decentralized Continuous Greedy (DCG), a message passing algorithm that converges to the tight $(1-1/e)$ approximation factor of the optimum global solution using only local computation and communication. We also provide strong convergence bounds as a function of network size and spectral characteristics of the underlying topology. Interestingly, DCG readily provides a simple recipe for decentralized discrete submodular maximization through the means of continuous relaxations. Formally, we demonstrate that by lifting the local discrete functions to continuous domains and using DCG as an interface we can develop a consensus algorithm that also achieves the tight $(1-1/e)$ approximation guarantee of the global discrete solution once a proper rounding scheme is applied.","['MIT', 'University of Pennsylvania', 'Yale']"
2018,Towards Fast Computation of Certified Robustness for ReLU Networks,"Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, Inderjit Dhillon",https://icml.cc/Conferences/2018/Schedule?showEvent=2052,"Verifying the robustness property of a general Rectified Linear Unit (ReLU) network is an NP-complete problem. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is possible. Current available methods of computing such a bound are either time-consuming or deliver low quality bounds that are too loose to be useful. In this paper, we exploit the special structure of ReLU networks and provide two computationally efficient algorithms (Fast-Lin, Fast-Lip) that are able to certify non-trivial lower bounds of minimum adversarial distortions. Experiments show that (1) our methods deliver bounds close to (the gap is 2-3X) exact minimum distortions found by Reluplex in small networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the gap is within 35\% and usually around 10\%; sometimes our bounds are even better) for larger networks compared to the methods based on solving linear programming problems but our algorithms are 33-14,000 times faster; (3) our method is capable of solving large MNIST and CIFAR networks up to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core. In addition, we show that there is no polynomial time algorithm that can approximately find the minimum $\ell_1$ adversarial distortion of a ReLU network with a $0.99\ln n$ approximation ratio unless NP=P, where $n$ is the number of neurons in the network.","['MIT', 'UC Davis', 'MIT', 'UT-Austin', 'University of California, Davis', 'MIT', 'MIT', 'UT Austin & Amazon']"
2018,A Two-Step Computation of the Exact GAN Wasserstein Distance,"Huidong Liu, Xianfeng GU, Samaras Dimitris",https://icml.cc/Conferences/2018/Schedule?showEvent=2363,"In this paper, we propose a two-step method to compute the Wasserstein distance in Wasserstein Generative Adversarial Networks (WGANs): 1) The convex part of our objective can be solved by linear programming; 2) The non-convex residual can be approximated by a deep neural network. We theoretically prove that the proposed formulation is equivalent to the discrete Monge-Kantorovich dual formulation. Furthermore, we give the approximation error bound of the Wasserstein distance and the error bound of generalizing the Wasserstein distance from discrete to continuous distributions. Our approach optimizes the exact Wasserstein distance, obviating the need for weight clipping previously used in WGANs. Results on synthetic data show that the our method computes the Wasserstein distance more accurately. Qualitative and quantitative results on MNIST, LSUN and CIFAR-10 datasets show that the proposed method is more efficient than state-of-the-art WGAN methods, and still produces images of comparable quality.
","['Stony Brook University', 'Stony Brook University', 'Stony Brook University']"
2018,Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection,"Jeremias Knoblauch, Theodoros Damoulas",https://icml.cc/Conferences/2018/Schedule?showEvent=1887,"Bayesian On-line Changepoint Detection is extended to on-line model selection and non-stationary spatio-temporal processes. We propose spatially structured Vector Autoregressions (VARs) for modelling the process between changepoints (CPs) and give an upper bound on the approximation error of such models. The resulting algorithm performs prediction, model selection and CP detection on-line. Its time complexity is linear and its space complexity constant, and thus it is two orders of magnitudes faster than its closest competitor.  In addition, it outperforms the state of the art for multivariate data.
","['Warwick University', 'University of Warwick']"
2018,Fast Stochastic AUC Maximization with $O(1/n)$-Convergence Rate,"Mingrui Liu, Xiaoxuan Zhang, Zaiyi Chen, Xiaoyu Wang, Tianbao Yang",https://icml.cc/Conferences/2018/Schedule?showEvent=1940,"In this paper, we consider statistical learning with AUC (area under ROC curve) maximization in the classical stochastic setting where one random data drawn from an  unknown distribution is revealed at each iteration for updating the model. Although consistent convex surrogate losses for AUC maximization have been proposed to make the problem tractable, it remains an challenging problem to design fast optimization algorithms in the classical stochastic setting due to that the convex surrogate loss depends on random pairs of examples from positive and negative classes. Building on a saddle point formulation for a consistent square loss,  this paper proposes a novel stochastic algorithm to improve the standard $O(1/\sqrt{n})$ convergence rate to $\widetilde O(1/n)$ convergence rate without strong convexity assumption or any favorable statistical assumptions (e.g., low noise), where $n$ is the number of random samples. To the best of our knowledge, this is the first stochastic algorithm for AUC maximization with a statistical convergence rate as fast as $O(1/n)$ up to a logarithmic factor. Extensive experiments on eight large-scale benchmark data sets demonstrate the superior performance of the proposed algorithm comparing with existing stochastic or online algorithms for AUC maximization.","['The University of Iowa', 'University of Iowa', 'University of Science and Technology of China', '-', 'The University of Iowa']"
2018,Accurate Uncertainties for Deep Learning Using Calibrated Regression,"Volodymyr Kuleshov, Nathan Fenner, Stefano Ermon",https://icml.cc/Conferences/2018/Schedule?showEvent=2486,"Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate — for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.
","['Stanford University', 'Afresh Technologies', 'Stanford University']"
2018,Neural Autoregressive Flows,"Chin-Wei Huang, David Krueger, Alexandre Lacoste, Aaron Courville",https://icml.cc/Conferences/2018/Schedule?showEvent=2394,"Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.
","['MILA', 'Universit? de Montr?al', 'Element AI', 'University of Montreal']"
2018,Probabilistic Boolean Tensor Decomposition,"Tammo Rukat, Christopher Holmes, Christopher Yau",https://icml.cc/Conferences/2018/Schedule?showEvent=1988,"Boolean tensor decomposition approximates data of multi-way binary relationships as product of interpretable low-rank binary factors, following the rules Boolean algebra. Here, we present its first probabilistic treatment. We facilitate scalable sampling-based posterior inference by exploitation of the combinatorial structure of the factor conditionals. Maximum a posteriori estimates consistently outperform existing non-probabilistic approaches. We show that our performance gains can partially be explained by convergence to solutions that occupy relatively large regions of the parameter space, as well as by implicit model averaging. Moreover, the Bayesian treatment facilitates model selection with much greater accuracy than the previously suggested minimum description length based approach. We investigate three real-world data sets. First, temporal interaction networks and behavioural data of university students demonstrate the inference of instructive latent patterns. Next, we decompose a tensor with more than 10 Billion data points, indicating relations of gene expression in cancer patients. Not only does this demonstrate scalability, it also provides an entirely novel perspective on relational properties of continuous data and, in the present example, on the molecular heterogeneity of cancer. Our implementation is available on GitHub: https://github.com/TammoR/LogicalFactorisationMachines
","['University of Oxford', 'University of Oxford', 'University of Birmingham']"
2018,A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank Matrix Recovery,"Xiao Zhang, Lingxiao Wang, Yaodong Yu, Quanquan Gu",https://icml.cc/Conferences/2018/Schedule?showEvent=2096,"We propose a primal-dual based framework for analyzing the  global optimality of nonconvex low-rank matrix recovery. Our analysis are based on the restricted strongly convex and smooth conditions, which can be verified for a broad family of loss functions. In addition, our analytic framework can directly handle the widely-used incoherence constraints through the lens of duality. We illustrate the applicability of the proposed framework to matrix completion and one-bit matrix completion, and prove that all these problems have no spurious local minima. Our results not only improve the sample complexity required for characterizing the global optimality of matrix completion, but also resolve an open problem in \citet{ge2017no} regarding one-bit matrix completion. Numerical experiments show that primal-dual based algorithm can successfully recover the global optimum for various low-rank problems.
","['University of Virginia', 'UCLA', 'University of Virginia', 'UCLA']"
2018,A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning,"Konstantin Mishchenko, Franck Iutzeler, Jérôme Malick, Massih-Reza Amini",https://icml.cc/Conferences/2018/Schedule?showEvent=2143,"Distributed learning aims at computing high-quality models by training over scattered data. This covers a diversity of scenarios, including computer clusters or mobile agents. One of the main challenges is then to deal with heterogeneous machines and unreliable communications. In this setting, we propose and analyze a flexible asynchronous optimization algorithm for solving nonsmooth learning problems. Unlike most existing methods, our algorithm is adjustable to various levels of communication costs, machines computational powers, and data distribution evenness. We prove that the algorithm converges linearly with a fixed learning rate that does not depend on communication delays nor on the number of machines. Although long delays in communication may slow down performance, no delay can break convergence.
","['King Abdullah University of Science & Technology (KAUST)', 'Univ. Grenoble Alpes', 'CNRS', 'Univ. Grenoble Alpes']"
2018,Randomized Block Cubic Newton Method,"Nikita Doikov, Peter Richtarik",https://icml.cc/Conferences/2018/Schedule?showEvent=2322,"We study the problem of minimizing the sum of three convex functions: a differentiable, twice-differentiable and a non-smooth term in a high dimensional setting. To this effect we propose and analyze a randomized block cubic Newton (RBCN) method, which in each iteration builds a model of the objective function formed as the sum of the natural models of its three components: a linear model with a quadratic regularizer for the differentiable term, a quadratic model with a cubic regularizer for the twice differentiable term, and perfect (proximal)  model for the nonsmooth term. Our method in each iteration minimizes the model over a random subset of blocks of the search variable. RBCN is the first algorithm with these properties, generalizing several existing methods, matching the best known bounds in all special cases. We establish ${\cal O}(1/\epsilon)$, ${\cal O}(1/\sqrt{\epsilon})$ and ${\cal O}(\log (1/\epsilon))$ rates under different assumptions on the component functions. Lastly, we show numerically that our method outperforms the state-of-the-art on a variety of machine learning problems, including cubically regularized least-squares, logistic regression with constraints, and Poisson regression.","['National Research University Higher School of Economics', 'King Abdullah University of Science and Technology (KAUST) - University of Edinburgh, Scotland']"
2018,Massively Parallel Algorithms and Hardness for Single-Linkage Clustering under $\ell_p$ Distances,"Grigory Yaroslavtsev, Adithya Vadapalli",https://icml.cc/Conferences/2018/Schedule?showEvent=2366,"We present first massively parallel (MPC) algorithms and hardness of approximation results for computing  Single-Linkage Clustering of n input d-dimensional vectors under Hamming, $\ell_1, \ell_2$ and $\ell_\infty$ distances. All our algorithms run in O(log n) rounds of MPC for any fixed d and achieve (1+\epsilon)-approximation for all distances (except Hamming for which we show an exact algorithm). We also show constant-factor inapproximability results for o(\log n)-round algorithms under standard MPC hardness assumptions (for sufficiently large dimension depending on the distance used). Efficiency of implementation of our algorithms in Apache Spark is demonstrated through experiments on the largest available vector datasets from the UCI machine learning repository exhibiting speedups of several orders of magnitude.","['Indiana University', 'INDIANA UNIVERSITY']"
2018,Local Density Estimation in High Dimensions,"Xian Wu, Moses Charikar, Vishnu Natchu",https://icml.cc/Conferences/2018/Schedule?showEvent=2460,"An important question that arises in the study of high dimensional vector representations learned from data is: given a set D of vectors and a query q, estimate the number of points within a specified distance threshold of q. Our algorithm uses locality sensitive hashing to preprocess the data to accurately and efficiently estimate the answers to such questions via an unbiased estimator that uses importance sampling. A key innovation is the ability to maintain a small number of hash tables via preprocessing data structures and algorithms that sample from multiple buckets in each hash table. We give bounds on the space requirements and query complexity of our scheme, and demonstrate the effectiveness of our algorithm by experiments on a standard word embedding dataset.
","['Stanford University', 'Stanford University', '']"
2018,To Understand Deep Learning We Need to Understand Kernel Learning,"Mikhail Belkin, Siyuan Ma, Soumik Mandal",https://icml.cc/Conferences/2018/Schedule?showEvent=2026,"Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly.  Despite this overfitting"", they perform well on test data, a  phenomenon  not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world  and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification error  or  near zero regression error (interpolation) perform very well on test data. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with  data size.  None of the existing bounds produce non-trivial results for interpolating solutions. We also show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results recently reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar  performance of overfitted Laplacian and Gaussian classifiers on test, suggests that  generalization is tied to the properties of the kernel function  rather than the  optimization process. Some  key phenomena of  deep learning are manifested similarly in  kernel methods in the modernoverfitted"" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for  new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding  deep learning will be difficult until more  tractable ``shallow'' kernel methods are better understood.
","['Ohio State University', 'The Ohio State University', '']"
2018,Learning in Reproducing Kernel Kreı̆n Spaces,"Dino Oglic, Thomas Gaertner",https://icml.cc/Conferences/2018/Schedule?showEvent=2200,"We formulate a novel regularized risk minimization problem for learning in reproducing kernel Kreı̆n spaces and show that the strong representer theorem applies to it. As a result of the latter, the learning problem can be expressed as the minimization of a quadratic form over a hypersphere of constant radius. We present an algorithm that can find a globally optimal solution to this non-convex optimization problem in time cubic in the number of instances. Moreover, we derive the gradient of the solution with respect to its hyperparameters and, in this way, provide means for efficient hyperparameter tuning. The approach comes with a generalization bound expressed in terms of the Rademacher complexity of the corresponding hypothesis space. The major advantage over standard kernel methods is the ability to learn with various domain specific similarity measures for which positive definiteness does not hold or is difficult to establish. The approach is evaluated empirically using indefinite kernels defined on structured as well as vectorial data. The empirical results demonstrate a superior performance of our approach over the state-of-the-art baselines.
","[""King's College London"", 'The University of Nottingham']"
2018,Functional Gradient Boosting based on Residual Network Perception,"Atsushi Nitanda, Taiji Suzuki",https://icml.cc/Conferences/2018/Schedule?showEvent=2184,"Residual Networks (ResNets) have become state-of-the-art models in deep learning and several theoretical studies have been devoted to understanding why ResNet works so well. One attractive viewpoint on ResNet is that it is optimizing the risk in a functional space by consisting of an ensemble of effective features.  In this paper, we adopt this viewpoint to construct a new gradient boosting method, which is known to be very powerful in data analysis. To do so, we formalize the boosting perspective of ResNet mathematically using the notion of functional gradients and propose a new method called ResFGB for classification tasks by leveraging ResNet perception. Two types of generalization guarantees are provided from the optimization perspective: one is the margin bound and the other is the expected risk bound by the sample-splitting technique. Experimental results show superior performance of the proposed method over state-of-the-art methods such as LightGBM.
","['The University of Tokyo / RIKEN', 'The University of Tokyo / RIKEN']"
2018,"Binary Classification with Karmic, Threshold-Quasi-Concave Metrics","Bowei Yan, Sanmi Koyejo, Kai Zhong, Pradeep Ravikumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2232,"Complex performance measures, beyond the popular measure of accuracy, are increasingly being used in the context of binary classification. These complex performance measures are typically not even decomposable, that is, the loss evaluated on a batch of samples cannot typically be expressed as a sum or average of losses evaluated at individual samples, which in turn requires new theoretical and methodological developments beyond standard treatments of supervised learning. In this paper, we advance this understanding of binary classification for complex performance measures by identifying two key properties: a so-called Karmic property, and a more technical threshold-quasi-concavity property, which we show is milder than existing structural assumptions imposed on performance measures. Under these properties, we show that the Bayes optimal classifier is a threshold function of the conditional probability of positive class. We then leverage this result to come up with a computationally practical plug-in classifier, via a novel threshold estimator, and further, provide a novel statistical analysis of classification error with respect to complex performance measures.
","['University of Texas at Austin', 'University of Illinois at Urbana-Champaign', 'University of Texas at Austin', 'Carnegie Mellon University']"
2018,Characterizing Implicit Bias in Terms of Optimization Geometry,"Suriya Gunasekar, Jason Lee, Daniel Soudry, Nati Srebro",https://icml.cc/Conferences/2018/Schedule?showEvent=2406,"We study the bias of generic optimization methods, including Mirror Descent, Natural Gradient Descent and Steepest Descent with respect to different potentials and norms, when optimizing underdetermined linear models or separable linear classification problems.  We ask the question of whether the global minimum (among the many possible global minima) reached by optimization can be characterized in terms of the potential or norm, and indecently of hyper-parameter choices, such as stepsize and momentum.
","['Toyota Technological Institute at Chicago', 'University of Southern California', 'Technion', 'Toyota Technological Institute at Chicago']"
2018,prDeep: Robust Phase Retrieval with a Flexible Deep Network,"Christopher Metzler, Phillip Schniter, Ashok Veeraraghavan, Richard Baraniuk",https://icml.cc/Conferences/2018/Schedule?showEvent=2480,"Phase retrieval algorithms have become an important component in many modern computational imaging systems.  For instance, in the context of ptychography and speckle correlation imaging, they enable imaging past the diffraction limit and through scattering media, respectively. Unfortunately, traditional phase retrieval algorithms struggle in the presence of noise. Progress has been made recently on more robust algorithms using signal priors, but at the expense of limiting the range of supported measurement models (e.g., to Gaussian or coded diffraction patterns).  In this work we leverage the regularization-by-denoising framework and a convolutional neural network denoiser to create {\em prDeep}, a new phase retrieval algorithm that is both robust and broadly applicable.   We test and validate prDeep in simulation to demonstrate that it is robust to noise and can handle a variety of system models.
","['Rice University', 'Ohio State', 'Rice University', 'OpenStax / Rice University']"
2018,Adversarial Time-to-Event Modeling,"Paidamoyo Chapfuwa, Chenyang Tao, Chunyuan Li, Courtney Page, Benjamin Goldstein, Lawrence Carin, Ricardo Henao",https://icml.cc/Conferences/2018/Schedule?showEvent=2332,"Modern health data science applications leverage abundant molecular and electronic health data, providing opportunities for machine learning to build statistical models to support clinical practice. Time-to-event analysis, also called survival analysis, stands as one of the most representative examples of such statistical models. We present a deep-network-based approach that leverages adversarial learning to address a key challenge in modern time-to-event modeling: nonparametric estimation of event-time distributions. We also introduce a principled cost function to exploit information from censored events (events that occur subsequent to the observation window). Unlike most time-to-event models, we focus on the estimation of time-to-event distributions, rather than time ordering. We validate our model on both benchmark and real datasets, demonstrating that the proposed formulation yields significant performance gains relative to a parametric alternative, which we also propose.
","['Duke University', 'Duke University', 'Duke University', 'Duke University', '', 'Duke', 'Duke University']"
2018,MAGAN: Aligning Biological Manifolds,"Matt Amodio, Smita Krishnaswamy",https://icml.cc/Conferences/2018/Schedule?showEvent=2455,"It is increasingly common in many types of natural and physical systems (especially biological systems) to have different types of measurements performed on the same underlying system. In such settings, it is important to align the manifolds arising from each measurement in order to integrate such data and gain an improved picture of the system; we tackle this problem using generative adversarial networks (GANs). Recent attempts to use GANs to find correspondences between sets of samples do not explicitly perform proper alignment of manifolds. We present the new Manifold Aligning GAN (MAGAN) that aligns two manifolds such that related points in each measurement space are aligned. We demonstrate applications of MAGAN in single-cell biology in integrating two different measurement types together: cells from the same tissue are measured with both genomic (single-cell RNA-sequencing) and proteomic (mass cytometry) technologies. We show that MAGAN successfully aligns manifolds such that known correlations between measured markers are improved compared to other recently proposed models.
","['Yale University', 'Yale University']"
2018,Multicalibration: Calibration for the (Computationally-Identifiable) Masses,"Ursula Hebert-Johnson, Michael Kim, Omer Reingold, Guy Rothblum",https://icml.cc/Conferences/2018/Schedule?showEvent=2448,"We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data).  Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations.  The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group.  We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.
","['Stanford University', 'Stanford University', 'Stanford University', 'Weizmann Institute of Science']"
2018,Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms,"Xueru Zhang, Mohammad Mahdi Khalili, Mingyan Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2075,"Alternating direction method of multiplier (ADMM) is a popular method used to design distributed versions of a machine learning algorithm, whereby local computations are performed on local data with the output exchanged among neighbors in an iterative fashion. During this iterative process the leakage of data privacy arises. A differentially private ADMM was proposed in prior work (Zhang & Zhu, 2017) where only the privacy loss of a single node during one iteration was bounded, a method that makes it difficult to balance the tradeoff between the utility attained through distributed computation and privacy guarantees when considering the total privacy loss of all nodes over the entire iterative process. We propose a perturbation method for ADMM where the perturbed term is correlated with the penalty parameters; this is shown to improve the utility and privacy simultaneously. The method is based on a modified ADMM where each node independently determines its own penalty parameter in every iteration and decouples it from the dual updating step size. The condition for convergence of the modified ADMM and the lower bound on the convergence rate are also derived.
","['University of Michigan', 'University of Michigan', 'University of Michigan, Ann Arbor']"
2018,PixelSNAIL: An Improved Autoregressive Generative Model,"Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, Pieter Abbeel",https://icml.cc/Conferences/2018/Schedule?showEvent=2296,"Autoregressive generative models achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this paper, we describe the resulting model and present state-of-the-art log-likelihood results on heavily benchmarked datasets: CIFAR-10, $32 \times 32$ ImageNet and $64 \times 64$ ImageNet. Our implementation will be made available at \url{https://github.com/neocxi/pixelsnail-public}.","['covariant.ai', '', '', 'OpenAI / UC Berkeley']"
2018,Focused Hierarchical RNNs for Conditional Sequence Processing,"Rosemary Nan Ke, Konrad Zolna, Alessandro Sordoni, Zhouhan Lin, Adam Trischler, Yoshua Bengio, Joelle Pineau, Laurent Charlin, Christopher Pal",https://icml.cc/Conferences/2018/Schedule?showEvent=2312,"Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional %hierarchical  sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question  Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.
","['MILA, University of Montreal', 'Universite de Montreal, Jagiellonian University', 'Microsoft Research', 'MILA, University of Montreal', 'Microsoft Research', 'Mila / U. Montreal', 'McGill University / Facebook', 'McGill University', 'École Polytechnique de Montréal']"
2018,Noise2Noise: Learning Image Restoration without Clean Data,"Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, Timo Aila",https://icml.cc/Conferences/2018/Schedule?showEvent=2014,"We apply basic statistical reasoning to signal reconstruction by machine learning - learning to map corrupted observations to clean signals - with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans - all corrupted by different processes - based on noisy data only.
","['Aalto University & NVIDIA', 'NVIDIA', 'NVIDIA', 'NVIDIA Research', 'NVIDIA', 'MIT', 'NVIDIA']"
2018,Learning to Reweight Examples for Robust Deep Learning,"Mengye Ren, Wenyuan Zeng, Bin Yang, Raquel Urtasun",https://icml.cc/Conferences/2018/Schedule?showEvent=1991,"Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns.  However, they can also easily overfit to training set biases and label noises.  In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they  require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.
","['Uber ATG / University of Toronto', 'University of Toronto, Uber ATG', 'Uber ATG / University of Toronto', 'University of Toronto']"
2018,Policy and Value Transfer in Lifelong Reinforcement Learning,"David Abel, Yuu Jinnai, Sophie Guo, George Konidaris, Michael L. Littman",https://icml.cc/Conferences/2018/Schedule?showEvent=2271,"We consider the problem of how best to use prior experience to bootstrap lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial policy that optimizes expected performance over the distribution of tasks for increasingly complex classes of policy and task distributions. We empirically demonstrate the relative performance of each policy class’ optimal element in a variety of simple task distributions. We then consider value-function initialization methods that preserve PAC guarantees while simultaneously minimizing the learning required in two learning algorithms, yielding MaxQInit, a practical new method for value-function-based transfer. We show that MaxQInit performs well in simple lifelong RL experiments.
","['Brown University', 'Brown University', 'Guo', 'Brown', 'Brown University']"
2018,GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms,"Cédric Colas, Olivier Sigaud, Pierre-Yves Oudeyer",https://icml.cc/Conferences/2018/Schedule?showEvent=2151,"In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, Quality-Diversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient-descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG . We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments.
","['Inria', 'Sorbonne University', 'Inria']"
2018,A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music,"Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck",https://icml.cc/Conferences/2018/Schedule?showEvent=2420,"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the ""posterior collapse"" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a ""flat"" baseline model. An implementation of our ""MusicVAE"" is available online at https://goo.gl/magenta/musicvae-code.
","['Google Brain', 'Google Brain', 'Google', 'Google Brain', 'Google Brain']"
2018,Understanding the Loss Surface of Neural Networks for Binary Classification,"SHIYU LIANG, Ruoyu Sun, Yixuan Li, R Srikant",https://icml.cc/Conferences/2018/Schedule?showEvent=2237,"It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see (LeCun et al., 2015; Choromanska et al., 2015; Dauphin et al., 2014). Performance is typically measured in terms of two metrics: training performance and generalization performance. Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection, and the surrogate loss function should be a smooth version of hinge loss. We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.
","['UIUC', 'University of Illinois at Urbana-Champaign', 'Facebook Inc', 'UIUC']"
2018,Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks,"Minmin Chen, Jeffrey Pennington, Samuel Schoenholz",https://icml.cc/Conferences/2018/Schedule?showEvent=2464,"Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improvement in training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.
","['Google research', 'Google Brain', 'Google Brain']"
2018,Reviving and Improving Recurrent Back-Propagation,"Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Zachary S Pitkow, Raquel Urtasun, Richard Zemel",https://icml.cc/Conferences/2018/Schedule?showEvent=1998,"In this paper, we revisit the recurrent back-propagation (RBP) algorithm, discuss the conditions under which it applies as well as how to satisfy them in deep neural networks.  We show that RBP can be unstable and propose two variants based on conjugate gradient on the normal equations (CG-RBP) and Neumann series (Neumann-RBP). We further investigate the relationship between Neumann-RBP and back propagation through time (BPTT) and its truncated version (TBPTT). Our Neumann-RBP has the same time complexity as TBPTT but only requires constant memory, whereas TBPTT's memory cost scales linearly with the number of truncation steps. We examine all RBP variants along with BPTT and TBPTT in three different application domains: associative memory with continuous Hopfield networks, document classification in citation networks using graph neural networks and hyperparameter optimization for fully connected networks. All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are efficient and effective for optimizing convergent recurrent neural networks.
","['University of Toronto', 'Uber ATG / University of Toronto', 'University of Toronto', 'University of Toronto', 'Rice University', 'Baylor College of Medicine / Rice University', 'University of Toronto', 'Vector Institute']"
2018,Riemannian Stochastic Recursive Gradient Algorithm with Retraction and Vector Transport and Its Convergence Analysis,"Hiroyuki Kasai, Hiroyuki Sato, Bamdev Mishra",https://icml.cc/Conferences/2018/Schedule?showEvent=2109,"Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite number of loss functions on a Riemannian manifold. The present paper proposes a Riemannian stochastic recursive gradient algorithm (R-SRG), which does not require the inverse of retraction between two distant iterates on the manifold. Convergence analyses of R-SRG are performed on both retraction-convex and non-convex functions under computationally efficient retraction and vector transport operations. The key challenge is analysis of the influence of vector transport along the retraction curve. Numerical evaluations reveal that R-SRG competes well with state-of-the-art Riemannian batch and stochastic gradient algorithms.
","['The University of Electro-Communications', 'Kyoto University', 'Microsoft']"
2018,Learning Compact Neural Networks with Regularization,Samet Oymak,https://icml.cc/Conferences/2018/Schedule?showEvent=1933,"Proper regularization is critical for speeding up training, improving generalization performance, and learning compact models that are cost efficient. We propose and analyze regularized gradient descent algorithms for learning shallow neural networks. Our framework is general and covers weight-sharing (convolutional networks), sparsity (network pruning), and low-rank constraints among others. We first introduce covering dimension to quantify the complexity of the constraint set and provide insights on the generalization properties. Then, we show that proposed algorithms become well-behaved and local linear convergence occurs once the amount of data exceeds the covering dimension. Overall, our results demonstrate that near-optimal sample complexity is sufficient for efficient learning and illustrate how regularization can be beneficial to learn over-parameterized networks.
","['University of California, Riverside']"
2018,Investigating Human Priors for Playing Video Games,"Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Tom Griffiths, Alexei Efros",https://icml.cc/Conferences/2018/Schedule?showEvent=2133,"What makes humans so good at solving seemingly complex video games?  Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors on human performance. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play. Videos and the game manipulations are available at https://rach0012.github.io/humanRL_website/
","['University of California, Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2018,Decoupling Gradient-Like Learning Rules from Representations,"Philip Thomas, Christoph Dann, Emma Brunskill",https://icml.cc/Conferences/2018/Schedule?showEvent=2343,"In machine learning, learning often corresponds to changing the parameters of a parameterized function. A learning rule is an algorithm or mathematical expression that specifies precisely how the parameters should be changed. When creating a machine learning system, we must make two decisions: what representation should be used (i.e., what parameterized function should be used) and what learning rule should be used to search through the resulting set of representable functions. In this paper we focus on gradient-like learning rules, wherein these two decisions are coupled in a subtle (and often unintentional) way. Using most learning rules, these two decisions are coupled in a subtle (and often unintentional) way. That is, using the same learning rule with two different representations that can represent the same sets of functions can result in two different outcomes. After arguing that this coupling is undesirable, particularly when using neural networks, we present a method for partially decoupling these two decisions for a broad class of gradient-like learning rules that span unsupervised learning, reinforcement learning, and supervised learning.
","['University of Massachusetts Amherst', 'Carnegie Mellon University', 'Stanford University']"
2018,Invariance of Weight Distributions in Rectified MLPs,"Susumu Tsuchida, Fred Roosta, Marcus Gallagher",https://icml.cc/Conferences/2018/Schedule?showEvent=1925,"An interesting approach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a mapping into a Hilbert space. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a previous result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having $0$ mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights. In deep networks, as depth increases the equivalent kernel approaches a pathological fixed point, which can be used to argue why training randomly initialized networks can be difficult. Our results also have implications for weight initialization.","['The University of Queensland', 'University of Queensland', 'University of Queensland']"
2018,Stronger Generalization Bounds for Deep Nets via a Compression Approach,"Sanjeev Arora, Rong Ge, Behnam Neyshabur, Yi Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=2412,"Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that are orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net --- a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified noise stability properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded  earlier attempts on proving generalization.
","[' Princeton University and Institute for Advanced Study', 'Duke University', 'New York University', 'Princeton University']"
2018,Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices,Zengfeng Huang,https://icml.cc/Conferences/2018/Schedule?showEvent=2125,"Given a large matrix $A\in\real^{n\times d}$, we consider the problem of computing a sketch matrix $B\in\real^{\ell\times d}$ which is significantly smaller than but still well approximates $A$. We are interested in minimizing the \emph{covariance error} $\norm{A^TA-B^TB}_2.$ We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space. The popular Frequent Directions algorithm of~\cite{liberty2013simple} and its variants achieve optimal space-error tradeoff. However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the time complexity of this problem. In particular, we provide new space-optimal algorithms with faster running times.  Moreover, we also show that the running times of our algorithms are near-optimal unless the state-of-the-art running time of matrix multiplication can be improved significantly.",['Fudan University']
2018,Loss Decomposition for Fast Learning in Large Output Spaces,"En-Hsu Yen, Satyen Kale, Felix Xinnan Yu, Daniel Holtmann-Rice, Sanjiv Kumar, Pradeep Ravikumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2391,"For problems with large output spaces, evaluation of the loss function and its gradient are expensive, typically taking linear time in the size of the output space. Recently, methods have been developed to speed up learning via efficient data structures for Nearest-Neighbor Search (NNS) or Maximum Inner-Product Search (MIPS). However, the performance of such data structures typically degrades in high dimensions. In this work, we propose a novel technique to reduce the intractable high dimensional search problem to several much more tractable lower dimensional ones via dual decomposition of the loss function. At the same time, we demonstrate guaranteed convergence to the original loss via a greedy message passing procedure. In our experiments on multiclass and multilabel classification with hundreds of thousands of classes, as well as training skip-gram word embeddings with a vocabulary size of half a million, our technique consistently improves the accuracy of search-based gradient approximation methods and outperforms sampling-based gradient approximation methods by a large margin.
","['Carnegie Mellon University', 'Google Research', 'Google AI', 'Google Inc', 'Google Research, NY', 'Carnegie Mellon University']"
2018,Stochastic Proximal Algorithms for AUC Maximization,"Michael Natole Jr, Yiming Ying, Siwei Lyu",https://icml.cc/Conferences/2018/Schedule?showEvent=1921,"Stochastic optimization algorithms such as SGDs update the model sequentially with cheap per-iteration costs, making them amenable for large-scale data analysis. However, most of the existing studies focus on the classification accuracy which can not be directly applied to the important problems of maximizing the Area under the ROC curve (AUC) in  imbalanced classification and bipartite ranking.  In this paper, we develop a novel stochastic proximal algorithm for AUC maximization which is referred to as SPAM.   Compared with the previous literature, our algorithm SPAM applies to a non-smooth penalty function, and achieves a convergence rate of O(log t/t) for strongly convex functions while both space and per-iteration costs are of one datum.
","['University at Albany', 'SUNY Albany', 'University at Albany, State University of New York']"
2018,Accelerated Spectral Ranking,"Arpit Agarwal, Prathamesh Patil, Shivani Agarwal",https://icml.cc/Conferences/2018/Schedule?showEvent=1999,"The problem of rank aggregation from pairwise and multiway comparisons has a wide range of implications, ranging from recommendation systems to sports rankings to social choice. Some of the most popular algorithms for this problem come from the class of spectral ranking algorithms; these include the rank centrality (RC) algorithm for pairwise comparisons, which returns consistent estimates under the Bradley-Terry-Luce (BTL) model for pairwise comparisons (Negahban et al., 2017), and its generalization, the Luce spectral ranking (LSR) algorithm, which returns consistent estimates under the more general multinomial logit (MNL) model for multiway comparisons (Maystre & Grossglauser, 2015). In this paper, we design a provably faster spectral ranking algorithm, which we call accelerated spectral ranking (ASR), that is also consistent under the MNL/BTL models.  Our accelerated algorithm is achieved by designing a random walk that has a faster mixing time than the random walks associated with previous algorithms. In addition to a faster algorithm, our results yield improved sample complexity bounds for recovery of the MNL/BTL parameters: to the best of our knowledge, we give the first general sample complexity bounds for recovering the parameters of the MNL model from multiway comparisons under any (connected) comparison graph (and improve significantly over previous bounds for the BTL model for pairwise comparisons). We also give a message-passing interpretation of our algorithm, which suggests a decentralized distributed implementation. Our experiments on several real-world and synthetic datasets confirm that our new ASR algorithm is indeed orders of magnitude faster than existing algorithms.
","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']"
2018,Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning,"Stefan Depeweg, Jose Miguel Hernandez-Lobato, Finale Doshi-Velez, Steffen Udluft",https://icml.cc/Conferences/2018/Schedule?showEvent=2262,"Bayesian neural networks with latent variables are scalable and flexible probabilistic models: they account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. Using these models we show how to perform and utilize a decomposition of uncertainty in aleatoric and epistemic components for decision making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.
","['TU Munich', 'University of Cambridge', 'Harvard University', 'Siemens AG']"
2018,Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,"Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, Akash Srivastava",https://icml.cc/Conferences/2018/Schedule?showEvent=2261,"Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.
","['RIKEN', 'RIKEN', 'RIKEN AIP', 'University of British Columbia', 'University of OXford', 'MIT, IBM']"
2018,Learning One Convolutional Layer with Overlapping Patches,"Surbhi Goel, Adam Klivans, Raghu Meka",https://icml.cc/Conferences/2018/Schedule?showEvent=1985,"We give the first provably efficient algorithm for learning a one hidden layer convolutional network with respect to a general class of (potentially overlapping) patches under mild conditions on the underlying distribution.  We prove that our framework captures commonly used schemes from computer vision, including one-dimensional and two-dimensional ``patch and stride'' convolutions. Our algorithm-- {\em Convotron}-- is inspired by recent work applying isotonic regression to learning neural networks.  Convotron uses a simple, iterative update rule that is stochastic in nature and tolerant to noise (requires only that the conditional mean function is a one layer convolutional network, as opposed to the realizable setting).  In contrast to gradient descent, Convotron requires no special initialization or learning-rate tuning to converge to the global optimum. We also point out that learning one hidden convolutional layer with respect to a Gaussian distribution and just {\em one} disjoint patch $P$ (the other patches may be arbitrary) is {\em easy} in the following sense: Convotron can efficiently recover the hidden weight vector by updating {\em only} in the direction of $P$.","['University of Texas at Austin', 'University of Texas at Austin', 'UCLA']"
2018,A Spline Theory of Deep Learning,"Randall Balestriero, Richard Baraniuk",https://icml.cc/Conferences/2018/Schedule?showEvent=2302,"We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of {\em max-affine spline operators} (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture.  The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.
","['Rice University', 'OpenStax / Rice University']"
2018,Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors,"Soumya Ghosh, Jiayu Yao, Finale Doshi-Velez",https://icml.cc/Conferences/2018/Schedule?showEvent=2321,"Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to provide well-calibrated posterior uncertainties.  However, model selection---even choosing the number of nodes---remains an open question.  Recent work has proposed the use of a horseshoe prior over node pre-activations of a Bayesian neural network, which effectively turns off nodes that do not help explain the data.  In this work, we propose several modeling and inference advances that consistently improve the compactness of the model learned while maintaining predictive performance, especially in smaller-sample settings including reinforcement learning.
","['IBM Research', 'Harvard University', 'Harvard University']"
2018,Variational Bayesian dropout: pitfalls and fixes,"Jiri Hron, Alexander Matthews, Zoubin Ghahramani",https://icml.cc/Conferences/2018/Schedule?showEvent=2336,"Dropout, a~stochastic regularisation technique for training of neural networks, has recently been reinterpreted as a~specific type of approximate inference algorithm for Bayesian neural networks. The~main contribution of the~reinterpretation is in providing a~theoretical framework useful for analysing and extending the~algorithm. We show that the~proposed framework suffers from several issues; from undefined or pathological behaviour of the~true posterior related to use of improper priors, to an ill-defined variational objective due to singularity of the~approximating distribution relative to the~true posterior. Our analysis of the~improper log uniform prior used in variational Gaussian dropout suggests the~pathologies are generally irredeemable, and that the~algorithm still works only because the~variational formulation annuls some of the~pathologies. To address the~singularity issue, we proffer Quasi-KL (QKL) divergence, a~new approximate inference objective for approximation of high-dimensional distributions. We show that motivations for variational Bernoulli dropout based on discretisation and noise have QKL as a limit. Properties of QKL are studied both theoretically and on a~simple practical example which shows that the~QKL-optimal approximation of a~full rank Gaussian with a~degenerate one naturally leads to the~Principal Component Analysis solution.
","['University of Cambridge', 'University of Cambridge', 'University of Cambridge & Uber']"
2018,Adversarial Learning with Local Coordinate Coding,"Jiezhang Cao, Yong Guo, Qingyao Wu, Chunhua Shen, Junzhou Huang, Mingkui Tan",https://icml.cc/Conferences/2018/Schedule?showEvent=1902,"Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.
","['South China University of Technology', 'South China University of Technology', 'South China University of Technology', 'University of Adelaide', 'University of Texas at Arlington / Tencent AI Lab', 'South China University of Technology']"
2018,Learning Representations and Generative Models for 3D Point Clouds,"Panagiotis Achlioptas, Olga Diamanti, Ioannis Mitliagkas, Leonidas Guibas",https://icml.cc/Conferences/2018/Schedule?showEvent=1917,"Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.
","['Stanford', 'Autodesk', 'MILA, UdeM', 'Stanford University']"
2018,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"Mattias Teye, Hossein Azizpour, Kevin Smith",https://icml.cc/Conferences/2018/Schedule?showEvent=2039,"We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.
","['KTH / EA SEED', 'KTH', 'KTH Royal Institute of Technology']"
2018,Noisy Natural Gradient as Variational Inference,"Guodong Zhang, Shengyang Sun, David Duvenaud, Roger Grosse",https://icml.cc/Conferences/2018/Schedule?showEvent=2250,"Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.~fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.
","['University of Toronto', 'University of Toronto', 'University of Toronto', 'University of Toronto and Vector Institute']"
2018,Deep Variational Reinforcement Learning for POMDPs,"Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, Shimon Whiteson",https://icml.cc/Conferences/2018/Schedule?showEvent=2456,"Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of rewards and incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.
","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of British Columbia', 'University of Oxford']"
2018,Recurrent Predictive State Policy Networks,"Ahmed Hefny, Zita Marinho, Wen Sun, Siddhartha Srinivasa, Geoff Gordon",https://icml.cc/Conferences/2018/Schedule?showEvent=2062,"We introduce Recurrent Predictive State Policy(RPSP)  networks,  a recurrent architecture that brings insights from predictive state representations to reinforcement learning in partially ob-servable environments.   Predictive state policy networks consist of a recursive filter, which keeps track of a belief about the state of the environment, and a reactive policy that directly maps beliefs to actions, to maximize the cumulative reward. The recursive filter leverages predictive state representations (PSRs) (Rosencrantz & Gordon, 2004; Sun et al., 2016) by modeling predictive state—a prediction of the distribution of future observations conditioned on history and future actions.This representation gives rise to a rich class of statistically consistent algorithms (Hefny et al.,2017) to initialize the recursive filter. Predictive stats serves as an equivalent representation of a belief state. Therefore, the policy component of the RPSP-network can be purely reactive, simplifying training while still allowing optimal behavior. Moreover, we use the PSR interpretation during training as well, by incorporating prediction error in the loss function. The entire network (recursive filter and reactive policy) is still differentiable and can be trained using gradient-based methods. We optimize our policy using a combination of policy gradient based on rewards (Williams, 1992)and gradient descent based on prediction error.We show the efficacy of RPSP-networks on a set of robotic control tasks from OpenAI Gym. We empirically show that RPSP-networks perform well compared with memory-preserving networks such as GRUs, as well as finite memory models, being the overall best performing method.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'University of Washington', 'Carnegie Mellon University']"
2018,The Mechanics of n-Player Differentiable Games,"David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, Thore Graepel",https://icml.cc/Conferences/2018/Schedule?showEvent=2030,"The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding local Nash equilibria in GANs -- whilst at the same time being applicable to -- and having guarantees in -- much more general games.
","['DeepMind', 'DeepMind', 'DeepMind', 'Facebook AI Research', 'DeepMind', 'DeepMind']"
2018,Improved Training of Generative Adversarial Networks Using Representative Features,"Duhyeon Bang, Hyunjung Shim",https://icml.cc/Conferences/2018/Schedule?showEvent=1888,"Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.
","['Yonsei univ.', 'Yonsei University']"
2018,Hierarchical Multi-Label Classification Networks,"Jonatas Wehrmann, Ricardo Cerri, Rodrigo Barros",https://icml.cc/Conferences/2018/Schedule?showEvent=2306,"One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC.
","['Pontifícia Universidade Catolica do Rio Grande do Sul - PUCRS', 'UFSCAR', 'PUCRS']"
2018,Knowledge Transfer with Jacobian Matching,"Suraj Srinivas, Francois Fleuret",https://icml.cc/Conferences/2018/Schedule?showEvent=2171,"Classical distillation methods transfer representations from a teacher'' neural network to astudent'' network by matching their output activations. Recent methods also match the Jacobians, or the gradient of output activations with the input. However, this involves making some ad hoc decisions, in particular, the choice of the loss function. In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching. We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation. We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning.
","['Idiap', 'Idiap research institute']"
2018,Towards Black-box Iterative Machine Teaching,"Weiyang Liu, Bo Dai, Xingguo Li, Zhen Liu, James Rehg, Le Song",https://icml.cc/Conferences/2018/Schedule?showEvent=2398,"In this paper, we make an important step towards the black-box machine teaching by considering the cross-space machine teaching, where the teacher and the learner use different feature representations and the teacher can not fully observe the learner's model. In such scenario, we study how the teacher is still able to teach the learner to achieve faster convergence rate than the traditional passive learning. We propose an active teacher model that can actively query the learner (i.e., make the learner take exams) for estimating the learner's status and provably guide the learner to achieve faster convergence. The sample complexities for both teaching and query are provided. In the experiments, we compare the proposed active teacher with the omniscient teacher and verify the effectiveness of the active teacher model.
","['Georgia Tech', 'Georgia Institute of Technology', 'Princeton University', 'Georgia Tech', 'Georgia Tech', 'Georgia Institute of Technology']"
2018,Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising,"Borja de Balle Pigem, Yu-Xiang Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=2245,"The Gaussian mechanism is an essential building block used in multitude of differentially private data analysis algorithms. In this paper we revisit the Gaussian mechanism and show that the original analysis has several important limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high privacy regime ($\varepsilon \to 0$) and it cannot be extended to the low privacy regime ($\varepsilon \to \infty$). We address these limitations by developing an optimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound approximation. We also propose to equip the Gaussian mechanism with a post-processing step based on adaptive estimation techniques by leveraging that the distribution of the perturbation is known. Our experiments show that analytical calibration removes at least a third of the variance of the noise compared to the classical Gaussian mechanism, and that denoising dramatically improves the accuracy of the Gaussian mechanism in the high-dimensional regime.","['Amazon Research', 'UC Santa Barbara']"
2018,Importance Weighted Transfer of Samples in Reinforcement Learning,"Andrea Tirinzoni, Andrea Sessa, Matteo Pirotta, Marcello Restelli",https://icml.cc/Conferences/2018/Schedule?showEvent=2008,"We consider the transfer of experience samples (i.e., tuples < s, a, s', r >) in reinforcement learning (RL), collected from a set of source tasks to improve the learning process in a given target task. Most of the related approaches focus on selecting the most relevant source samples for solving the target task, but then all the transferred samples are used without considering anymore the discrepancies between the task models. In this paper, we propose a model-based technique that automatically estimates the relevance (importance weight) of each source sample for solving the target task. In the proposed approach, all the samples are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning process is proportional to their importance weight. By extending the results for importance weighting provided in supervised learning literature, we develop a finite-sample analysis of the proposed batch RL algorithm. Furthermore, we empirically compare the proposed algorithm to state-of-the-art approaches, showing that it achieves better learning performance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.
","['Politecnico di Milano', 'Politecnico di Milano', 'SequeL - Inria Lille - Nord Europe', 'Politecnico di Milano']"
2018,Beyond the One-Step Greedy Approach in Reinforcement Learning,"Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor",https://icml.cc/Conferences/2018/Schedule?showEvent=2126,"The famous Policy Iteration algorithm alternates between policy improvement and policy evaluation. Implementations of this algorithm with several variants of the latter evaluation stage, e.g, n-step and trace-based returns, have been analyzed in previous works. However, the case of multiple-step lookahead policy improvement, despite the recent increase in empirical evidence of its strength, has to our knowledge not been carefully analyzed yet. In this work, we introduce the first such analysis. Namely, we formulate variants of multiple-step policy improvement, derive new algorithms using these definitions and prove their convergence. Moreover, we show that recent prominent Reinforcement Learning algorithms are, in fact, instances of our framework. We thus shed light on their empirical success and give a recipe for deriving new algorithms for future study.
","['Technion', '', 'INRIA', 'Technion']"
2018,"Optimization, fast and slow: optimally switching between local and Bayesian optimization","Mark McLeod, Stephen Roberts, Michael A Osborne",https://icml.cc/Conferences/2018/Schedule?showEvent=2224,"We develop the first Bayesian Optimization algorithm, BLOSSOM, which selects between multiple alternative acquisition functions and traditional local optimization at each step. This is combined with a novel stopping condition based on expected regret. This pairing allows us to obtain the best characteristics of both local and Bayesian optimization, making efficient use of function evaluations while yielding superior convergence to the global minimum on a selection of optimization problems, and also halting optimization once a principled and intuitive stopping condition has been fulfilled.
","['University of Oxford', 'University of Oxford', 'U Oxford']"
2018,Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design,"Wenlong Lyu, Fan Yang, Changhao Yan, Dian Zhou, Xuan Zeng",https://icml.cc/Conferences/2018/Schedule?showEvent=1919,"Bayesian optimization methods are promising for the optimization of black-box functions that are expensive to evaluate. In this paper, a novel batch Bayesian optimization approach is proposed. The parallelization is realized via a multi-objective ensemble of multiple acquisition functions. In each iteration, the multi-objective optimization of the multiple acquisition functions is performed to search for the Pareto front of the acquisition functions. The batch of inputs are then selected from the Pareto front. The Pareto front represents the best trade-off between the multiple acquisition functions. Such a policy for batch Bayesian optimization can significantly improve the efficiency of optimization. The proposed method is compared with several state-of-the-art batch Bayesian optimization algorithms using analytical benchmark functions and real-world analog integrated circuits. The experimental results show that the proposed method is competitive compared with the state-of-the-art algorithms.
","['Fudan University', 'Fudan University', '', 'Department of Electrical Engineering The University of Texas at Dallas Richardso', 'Fudan University']"
2018,Graphical Nonconvex Optimization via an Adaptive Convex Relaxation,"Qiang Sun, Kean Ming Tan, Han Liu, Tong Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=1955,"We consider the problem of learning high-dimensional Gaussian graphical models.  The graphical lasso is one of the most popular methods for estimating Gaussian graphical models. However, it does not achieve the oracle rate of convergence. In this paper, we propose the graphical nonconvex optimization for optimal estimation in Gaussian graphical models, which is then approximated by a sequence of convex programs. Our proposal is computationally tractable and produces an estimator that achieves the oracle rate of convergence. The statistical error introduced by the sequential approximation using a sequence of convex programs is clearly demonstrated via a contraction property.  The proposed methodology is then extended to modeling semiparametric graphical models. We show via numerical studies that the proposed estimator outperforms other popular methods for estimating Gaussian graphical models.
","['University of Toronto', 'University of Minnesota Twin Cities', 'Princeton University', 'Tecent AI Lab']"
2018,Approximate message passing for amplitude based optimization,"Junjie Ma, Ji Xu, Arian Maleki",https://icml.cc/Conferences/2018/Schedule?showEvent=2297,"We consider an $\ell_2$-regularized non-convex optimization problem for recovering signals from their noisy phaseless observations. We design and study the performance of a message passing algorithm that aims to solve this optimization problem. We consider the asymptotic setting $m,n \rightarrow \infty$, $m/n \rightarrow \delta$ and obtain sharp performance bounds, where $m$ is the number of measurements and $n$ is the signal dimension. We show that for complex signals the algorithm can perform accurate recovery with only $m=\left ( \frac{64}{\pi^2}-4\right)n\approx 2.5n$ measurements. Also, we provide sharp analysis on the sensitivity of the algorithm to noise. We highlight the following facts about our message passing algorithm: (i) Adding $\ell_2$ regularization to the non-convex loss function can be beneficial even in the noiseless setting; (ii) spectral initialization has marginal impact on the performance of the algorithm.","['Columbia University', 'Columbia University', 'Columbia']"
2018,Tempered Adversarial Networks,"Mehdi S. M. Sajjadi, Giambattista Parascandolo, Arash Mehrjou, Bernhard Schölkopf",https://icml.cc/Conferences/2018/Schedule?showEvent=1870,"Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).
","['Max Planck Institute for Intelligent Systems', 'Max Planck Institute for Intelligent Systems and ETH Zurich', 'Max Planck Institute for Intelligent Systems', 'MPI for Intelligent Systems Tübingen, Germany']"
2018,Delayed Impact of Fair Machine Learning,"Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, Moritz Hardt",https://icml.cc/Conferences/2018/Schedule?showEvent=2403,"Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.
","['University of California Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'University of California, Berkeley']"
2018,Fast Information-theoretic Bayesian Optimisation,"Binxin Ru, Michael A Osborne, Mark Mcleod, Diego Granziol",https://icml.cc/Conferences/2018/Schedule?showEvent=1973,"Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with information-theoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.
","['University of Oxford', 'U Oxford', 'University of Oxford', 'Oxford']"
2018,Tight Regret Bounds for Bayesian Optimization in One Dimension,Jonathan Scarlett,https://icml.cc/Conferences/2018/Schedule?showEvent=1945,"We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise.  We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time $T$ behaves as $\Omega(\sqrt{T})$ and $O(\sqrt{T\log T})$. This gives a tight characterization up to a $\sqrt{\log T}$ factor, and includes the first non-trivial lower bound for noisy BO.  Our assumptions are satisfied, for example, by the squared exponential and Mat\'ern-$\nu$ kernels, with the latter requiring $\nu > 2$.  Our results certify the near-optimality of existing bounds (Srinivas {\em et al.}, 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat\'ern kernel with $\nu > 2$.",['National University of Singapore']
2018,Image Transformer,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran",https://icml.cc/Conferences/2018/Schedule?showEvent=2430,"Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.
","['Google', 'Google Brain', '', 'Google', 'Google', 'UC Berkeley', 'Google']"
2018,Kernelized Synaptic Weight Matrices,"Lorenz Müller, Julien Martel, Giacomo Indiveri",https://icml.cc/Conferences/2018/Schedule?showEvent=2141,"In this paper we introduce a novel neural network architecture, in which weight matrices are re-parametrized in terms of low-dimensional vectors, interacting through kernel functions. A layer of our network can be interpreted as introducing a (potentially infinitely wide) linear layer between input and output. We describe the theory underpinning this model and validate it with concrete examples, exploring how it can be used to impose structure on neural networks in diverse applications ranging from data visualization to recommender systems. We achieve state-of-the-art performance in a collaborative filtering task (MovieLens).
","['ETH Zurich and University of Zurich', 'ETH Zurich', 'University of Zurich']"
2018,A Distributed Second-Order Algorithm You Can Trust,"Celestine Mendler-Dünner, Aurelien Lucchi, Matilde Gargiani, Yatao Bian, Thomas Hofmann, Martin Jaggi",https://icml.cc/Conferences/2018/Schedule?showEvent=2457,"Due to the rapid growth of data and computational resources, distributed optimization has become an active research area in recent years. While first-order methods seem to dominate the field, second-order methods are nevertheless attractive as they potentially require fewer communication rounds to converge. However, there are significant drawbacks that impede their wide adoption, such as the computation and the communication of a large Hessian matrix. In this paper we present a new algorithm for distributed training of generalized linear models that only requires the computation of diagonal blocks of the Hessian matrix on the individual workers. To deal with this approximate information we propose an adaptive approach that - akin to trust-region methods - dynamically adapts the auxiliary model to compensate for modeling errors. We provide theoretical rates of convergence for a wide class of problems including $L_1$-regularized objectives. We also demonstrate that our approach achieves state-of-the-art results on multiple large benchmark datasets.","['IBM Research', 'ETH Zurich', 'University of Freiburg', 'ETH Zürich', 'ETH Zurich', 'EPFL']"
2018,On Acceleration with Noise-Corrupted Gradients,"Michael Cohen, Jelena Diakonikolas, Orecchia Lorenzo",https://icml.cc/Conferences/2018/Schedule?showEvent=2173,"Accelerated algorithms have broad applications in large-scale optimization, due to their generality and fast convergence. However, their stability in the practical setting of noise-corrupted gradient oracles is not well-understood. This paper provides two main technical contributions: (i) a new accelerated method AGDP that generalizes Nesterov's AGD and improves on the recent method AXGD (Diakonikolas & Orecchia, 2018), and (ii) a theoretical study of accelerated algorithms under noisy and inexact gradient oracles, which is supported by numerical experiments.  This study leverages the simplicity of AGDP and its analysis to clarify the interaction between noise and acceleration and to suggest modifications to the algorithm that reduce the mean and variance of the error incurred due to the gradient noise.
","['', 'Boston University', 'Boston']"
2018,Gradient Coding from Cyclic MDS Codes and Expander Graphs,"Netanel Raviv, Rashish Tandon, Alexandros Dimakis, Itzhak Tamo",https://icml.cc/Conferences/2018/Schedule?showEvent=2063,"Gradient coding is a technique for straggler mitigation in distributed learning. In this paper we design novel gradient codes using tools from classical coding theory, namely, cyclic MDS codes, which compare favourably with existing solutions, both in the applicable range of parameters and in the complexity of the involved algorithms. Second, we introduce an approximate variant of the gradient coding problem, in which we settle for approximate gradient computation instead of the exact one. This approach enables graceful degradation, i.e., the $\ell_2$ error of the approximate gradient is a decreasing function of the number of stragglers. Our main result is that the normalized adjacency matrix of an expander graph can yield excellent approximate gradient codes, and that this approach allows us to perform significantly less computation compared to exact gradient coding. We experimentally test our approach on Amazon EC2, and show that the generalization error of approximate gradient coding is very close to the full gradient while requiring significantly less computation from the workers.","['California Institute of Technology', 'Apple', 'UT Austin', 'Tel-Aviv University']"
2018,Accelerating Greedy Coordinate Descent Methods,"Haihao Lu, Robert Freund, Vahab Mirrokni",https://icml.cc/Conferences/2018/Schedule?showEvent=2201,"We introduce and study two algorithms to accelerate greedy coordinate descent in theory and in practice: Accelerated Semi-Greedy Coordinate Descent (ASCD) and Accelerated Greedy Coordinate Descent (AGCD). On the theory side, our main results are for ASCD: we show that ASCD achieves $O(1/k^2)$ convergence, and it also achieves accelerated linear convergence for strongly convex functions. On the empirical side, while both AGCD and ASCD outperform Accelerated Randomized Coordinate Descent on most instances in our numerical experiments, we note that AGCD significantly outperforms the other two methods in our experiments, in spite of a lack of theoretical guarantees for this method.  To complement this empirical finding for AGCD, we present an explanation why standard proof techniques for acceleration cannot work for AGCD, and we further introduce a technical condition under which AGCD is guaranteed to have accelerated convergence. Finally, we confirm that this technical condition holds in our numerical experiments.","['MIT', 'MIT', 'Google Research']"
2018,Finding Influential Training Samples for Gradient Boosted Decision Trees,"Boris Sharchilev, Yury Ustinovskiy, Pavel Serdyukov, Maarten de Rijke",https://icml.cc/Conferences/2018/Schedule?showEvent=2395,"We address the problem of finding influential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model's predictions change upon leave-one-out retraining, leaving out each individual training sample. Recent work has shown that, for parametric models, this analysis can be conducted in a computationally efficient way. We propose several ways of extending this framework to non-parametric GBDT ensembles under the assumption that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further approximations to our method that balance the trade-off between performance and computational complexity. We evaluate our approaches on various experimental setups and use-case scenarios and demonstrate both the quality of our approach to finding influential training samples in comparison to the baselines and its computational efficiency.
","['Yandex', 'Princeton University', 'Yandex', 'University of Amsterdam']"
2018,Improving Regression Performance with Distributional Losses,"Ehsan Imani, Martha White",https://icml.cc/Conferences/2018/Schedule?showEvent=2092,"There is growing evidence that converting targets to soft targets in supervised learning can provide considerable gains in performance. Much of this work has considered classification, converting hard zero-one values to soft labels---such as by adding label noise, incorporating label ambiguity or using distillation. In parallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can improve performance. In this work, we investigate the reasons for this improvement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly improves prediction accuracy. We investigate several common hypotheses, around reducing overfitting and improved representations. We instead find evidence for an alternative hypothesis: this loss is easier to optimize, with better behaved gradients, resulting in improved generalization. We provide theoretical support for this alternative hypothesis, by characterizing the norm of the gradients of this loss.
","['University of Alberta', 'University of Alberta']"
2018,QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,"Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, Shimon Whiteson",https://icml.cc/Conferences/2018/Schedule?showEvent=2389,"In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.
","['University of Oxford', 'Russian-Armenian University', 'University of Oxford', 'University of Oxford', 'Facebook AI Research', 'University of Oxford']"
2018,Learning to Act in Decentralized Partially Observable MDPs,"Jilles Dibangoye, Olivier Buffet",https://icml.cc/Conferences/2018/Schedule?showEvent=2278,"We address a long-standing open problem of reinforcement learning in decentralized partially observable Markov decision processes. Previous attempts focussed on different forms of generalized policy iteration, which at best led to local optima. In this paper, we restrict attention to plans, which are simpler to store and update than policies. We derive, under certain conditions, the first near-optimal cooperative multi-agent reinforcement learning algorithm. To achieve significant scalability gains,  we replace the greedy maximization by mixed-integer linear programming. Experiments show our approach can learn to act near-optimally in many finite domains from the literature.
","['INRIA', 'INRIA']"
2018,Local Convergence Properties of SAGA/Prox-SVRG and Acceleration,"Clarice Poon, Jingwei Liang, Carola-Bibiane Schönlieb",https://icml.cc/Conferences/2018/Schedule?showEvent=1974,"In this paper, we present a local convergence anal- ysis for a class of stochastic optimisation meth- ods: the proximal variance reduced stochastic gradient methods, and mainly focus on SAGA (Defazio et al., 2014) and Prox-SVRG (Xiao & Zhang, 2014). Under the assumption that the non-smooth component of the optimisation prob- lem is partly smooth relative to a smooth mani- fold, we present a unified framework for the local convergence analysis of SAGA/Prox-SVRG: (i) the sequences generated by the methods are able to identify the smooth manifold in a finite num- ber of iterations; (ii) then the sequence enters a local linear convergence regime. Furthermore, we discuss various possibilities for accelerating these algorithms, including adapting to better lo- cal parameters, and applying higher-order deter- ministic/stochastic optimisation methods which can achieve super-linear convergence. Several concrete examples arising from machine learning are considered to demonstrate the obtained result.
","['University of Cambridge', 'University of Cambridge', 'University of Cambridge']"
2018,Stein Points,"Wilson Ye Chen, Lester Mackey, Jackson Gorham, Francois-Xavier Briol, Chris J Oates",https://icml.cc/Conferences/2018/Schedule?showEvent=2275,"An important task in computational statistics and machine learning is to approximate a posterior distribution p(x) with an empirical measure supported on a set of representative points {x_i\}_{i=1}^n. This paper focuses on methods where the selection of points is essentially deterministic, with an emphasis on achieving accurate approximation when $n$ is small. To this end, we present Stein Points. The idea is to exploit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discrepancy between the empirical measure and p(x). Our empirical results demonstrate that Stein Points enable accurate approximation of the posterior at modest computational cost. In addition, theoretical results are provided to establish convergence of the method.","['University of Technology Sydney', 'Microsoft Research', 'STANFORD', 'University of Warwick', 'Newcastle University']"
2018,Large-Scale Cox Process Inference using Variational Fourier Features,"ST John, James Hensman",https://icml.cc/Conferences/2018/Schedule?showEvent=1982,"Gaussian process modulated Poisson processes provide a flexible framework for modeling spatiotemporal point patterns. So far this had been restricted to one dimension, binning to a pre-determined grid, or small data sets of up to a few thousand data points. Here we introduce Cox process inference based on Fourier features. This sparse representation induces global rather than local constraints on the function space and is computationally efficient. This allows us to formulate a grid-free approximation that scales well with the number of data points and the size of the domain. We demonstrate that this allows MCMC approximations to the non-Gaussian posterior. In practice, we find that Fourier features have more consistent optimization behavior than previous approaches. Our approximate Bayesian method can fit over 100 000 events with complex spatiotemporal patterns in three dimensions on a single GPU.
","['PROWLER.io', 'PROWLER.io']"
2018,SADAGRAD: Strongly Adaptive Stochastic Gradient Methods,"Zaiyi Chen, Yi Xu, Enhong Chen, Tianbao Yang",https://icml.cc/Conferences/2018/Schedule?showEvent=2010,"Although the convergence rates of existing variants of ADAGRAD have a better dependence on the number of iterations under the strong convexity condition, their iteration complexities have a explicitly linear dependence on the dimensionality of the problem. To alleviate this bad dependence, we propose a simple yet novel variant of ADAGRAD for stochastic (weakly) strongly convex optimization. Different from existing variants, the proposed variant (referred to as SADAGRAD) uses an adaptive restarting scheme in which (i) ADAGRAD serves as a sub-routine and is restarted periodically; (ii) the number of iterations for restarting ADAGRAD depends on the history of learning that incorporates knowledge of the geometry of the data. In addition to the adaptive proximal functions and adaptive number of iterations for restarting, we also develop a variant that is adaptive to the (implicit) strong convexity from the data, which together makes the proposed algorithm strongly adaptive. In terms of iteration complexity, in the worst case SADAGRAD has an O(1/\epsilon) for finding an \epsilon-optimal solution similar to other variants. However, it could enjoy faster convergence and much better dependence on the problem’s dimensionality when stochastic gradients are sparse. Extensive experiments on large-scale data sets demonstrate the efficiency of the proposed algorithms in comparison with several variants of ADAGRAD and stochastic gradient method.
","['University of Science and Technology of China', 'The University of Iowa', 'University of Science and Technology of China', 'The University of Iowa']"
2018,Gradient Primal-Dual Algorithm Converges to Second-Order Stationary Solution for  Nonconvex Distributed Optimization Over Networks,"Mingyi Hong, Meisam Razaviyayn, Jason Lee",https://icml.cc/Conferences/2018/Schedule?showEvent=2156,"In this work, we study two first-order primal-dual based algorithms, the Gradient Primal-Dual Algorithm (GPDA) and the Gradient Alternating Direction Method of Multipliers (GADMM),  for  solving a class of linearly constrained non-convex optimization problems. We show that with random initialization of  the primal and dual variables,  both algorithms are able to compute second-order stationary solutions (ss2) with probability one. This is the first result showing that primal-dual algorithm is capable of finding ss2 when only using first-order information;  it also extends the existing results for first-order, but {primal-only} algorithms.       An important implication of our result is that it also gives rise to the first global convergence result to the ss2, for two classes of  unconstrained distributed non-convex learning problems over multi-agent networks.
","['University of Minnesota', 'University of southern California', 'University of Southern California']"
2018,A Progressive Batching L-BFGS Method for Machine Learning,"Vijaya Raghavendra Bollapragada, Jorge Nocedal, Dheevatsa Mudigere, Hao-Jun M Shi, Peter Tang",https://icml.cc/Conferences/2018/Schedule?showEvent=2059,"The standard L-BFGS method relies on gradient approximations that are not dominated by noise, so that search directions are descent directions, the line search is reliable, and quasi-Newton updating yields useful quadratic models of the objective function. All of this appears to call for a full batch approach, but since small batch sizes give rise to faster algorithms with better generalization properties, L-BFGS is currently not considered an algorithm of choice for large-scale machine learning applications. One need not, however, choose between the two extremes represented by the full batch or highly stochastic regimes, and may instead follow a progressive batching approach in which the sample size increases during the course of the optimization. In this paper, we present a new version of the L-BFGS algorithm that combines three basic components - progressive batching, a stochastic line search, and stable quasi-Newton updating - and that performs well on training logistic regression and deep neural networks. We provide supporting convergence theory for the method.
","['Northwestern University', 'Northwestern University', 'Intel Labs', 'Northwestern University', 'Intel Corporation']"
2018,WSNet: Compact and Efficient Networks Through Weight Sampling,"Xiaojie Jin, Yingzhen Yang, Ning Xu, Jianchao Yang, Nebojsa Jojic, Jiashi Feng, Shuicheng Yan",https://icml.cc/Conferences/2018/Schedule?showEvent=2402,"We present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via ad hoc processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces parameter sharing throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to 180x smaller and theoretically up to 16x faster than the well-established baselines, without noticeable performance drop.
","['National University of Singapore', 'University of Illinois at Urbana-Champaign', 'Snap', 'Bytedance Inc.', 'Microsoft Research', 'National University of Singapore', 'Qihoo/360']"
2018,Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors,"Gintare Karolina Dziugaite, Daniel Roy",https://icml.cc/Conferences/2018/Schedule?showEvent=2274,"We show that Entropy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, optimizes a PAC-Bayes bound on the risk of a Gibbs (posterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive perturbation of the weights of a learned classifier. Entropy-SGD works by optimizing the bound’s prior, violating the hypothesis of the PAC-Bayes theorem that the prior is chosen independently of the data. Indeed, available implementations of Entropy-SGD rapidly obtain zero training error on random labels and the same holds of the Gibbs posterior. In order to obtain a valid generalization bound, we rely on a result showing that data-dependent priors obtained by stochastic gradient Langevin dynamics (SGLD) yield valid PAC-Bayes bounds provided the target distribution of SGLD is eps-differentially private. We observe that test error on MNIST and CIFAR10 falls within the (empirically nonvacuous) risk bounds computed under the assumption that SGLD reaches stationarity. In particular, Entropy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art performance.
","['University of Cambridge', 'Univ of Toronto | Toronto']"
2018,"High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach","Tim Pearce, Alexandra Brintrup, Mohamed Zaki, Andy Neely",https://icml.cc/Conferences/2018/Schedule?showEvent=2188,"This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a specified portion of data. We derive a loss function directly from this axiom that requires no distributional assumption. We show how its form derives from a likelihood principle, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form. Benchmark experiments show the method outperforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10%.
","['University of Cambridge / The Alan Turing Institute', '', 'University of Cambridge', '']"
2018,Competitive Caching with Machine Learned Advice,"Thodoris Lykouris, Sergei Vassilvitskii",https://icml.cc/Conferences/2018/Schedule?showEvent=2367,"We develop a framework for augmenting online algorithms with a machine learned oracle to achieve competitive ratios that provably improve upon unconditional worst case lower bounds when the oracle has low error. Our approach treats the oracle as a complete black box, and is not dependent on its inner workings, or the exact distribution of its errors. We apply this framework to the traditional caching problem — creating an eviction strategy for a cache of size k. We demonstrate that naively following the oracle’s recommendations may lead to very poor performance, even when the average error is quite low. Instead we show how to modify the Marker algorithm to take into account the oracle’s predictions, and prove that this combined approach achieves a competitive ratio that both (i) decreases as the oracle’s error decreases, and (ii) is always capped by O(log k), which can be achieved without any oracle input. We complement our results with an empirical evaluation of our algorithm on real world datasets, and show that it performs well empirically even using simple off the shelf predictions.
","['Cornell University', 'Google']"
2018,Approximation Algorithms for Cascading Prediction Models,Matthew Streeter,https://icml.cc/Conferences/2018/Schedule?showEvent=2035,"We present an approximation algorithm that takes a pool of pre-trained models as input and produces from it a cascaded model with similar accuracy but lower average-case cost.  Applied to state-of-the-art ImageNet classification models, this yields up to a 2x reduction in floating point multiplications, and up to a 6x reduction in average-case memory I/O.  The auto-generated cascades exhibit intuitive properties, such as using lower-resolution input for easier images and requiring higher prediction confidence when using a computationally cheaper model.
",['Google']
2018,Orthogonal Machine Learning: Power and Limitations,"Ilias Zadik, Lester Mackey, Vasilis Syrgkanis",https://icml.cc/Conferences/2018/Schedule?showEvent=2409,"Double machine learning provides n^{1/2}-consistent estimates of parameters of interest even when high-dimensional or nonparametric nuisance parameters are estimated at an n^{-1/4} rate. The key is to employ Neyman-orthogonal moment equations which are first-order insensitive to perturbations in the nuisance parameters. We show that the n^{-1/4} requirement can be improved to n^{-1/(2k+2)} by employing a k-th order notion of orthogonality that grants robustness to more complex or higher-dimensional nuisance parameters. In the partially linear regression setting popular in causal inference, we show that we can construct second-order orthogonal moments if and only if the treatment residual is not normally distributed.  Our proof relies on Stein's lemma and may be of independent interest.  We conclude by demonstrating the robustness benefits of an explicit doubly-orthogonal estimation procedure for treatment effect.
","['MIT', 'Microsoft Research', 'Microsoft Research']"
2018,Causal Bandits with Propagating Inference,"Akihiro Yabe, Daisuke Hatano, Hanna Sumita, Shinji Ito, Naonori Kakimura, Takuro Fukunaga, Ken-ichi Kawarabayashi",https://icml.cc/Conferences/2018/Schedule?showEvent=2074,"Bandit is a framework for designing sequential experiments, where a learner selects an arm $A \in \mathcal{A}$ and obtains an observation corresponding to $A$ in each experiment. Theoretically, the tight regret lower-bound for the general bandit is polynomial with respect to the number of arms $|\mathcal{A}|$, and thus, to overcome this bound, the bandit problem with side-information is often considered. Recently, a bandit framework over a causal graph was introduced, where the structure of the causal graph is available as side-information and the arms are identified with interventions on the causal graph. Existing algorithms for causal bandit overcame the $\Omega(\sqrt{|\mathcal{A}|/T})$ simple-regret lower-bound; however, their algorithms work only when the interventions $\mathcal{A}$ are localized around a single node (i.e., an intervention propagates only to its neighbors). We then propose a novel causal bandit algorithm for an arbitrary set of interventions, which can propagate throughout the causal graph. We also show that it achieves $O(\sqrt{ \gamma^*\log(|\mathcal{A}|T) / T})$ regret bound, where $\gamma^*$ is determined by using a causal graph structure. In particular, if the maximum in-degree of the causal graph is a constant, then $\gamma^* = O(N^2)$, where $N$ is the number of nodes.","['NEC Corporation', 'RIKEN AIP', 'National Institute of Informatics', 'NEC Corporation', 'Keio University', 'RIKEN AIP', 'National Institute of Informatics']"
2018,Mix & Match - Agent Curricula for Reinforcement Learning,"Wojciech Czarnecki, Siddhant Jayakumar, Max Jaderberg, Leonard Hasenclever, Yee Teh, Nicolas Heess, Simon Osindero, Razvan Pascanu",https://icml.cc/Conferences/2018/Schedule?showEvent=2187,"We introduce Mix and match (M&M) -- a training framework designed to facilitate rapid and effective learning in RL agents that would be too slow or too challenging to train otherwise.The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents.In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally.We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods.(2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2018,The Uncertainty Bellman Equation and Exploration,"Brendan O'Donoghue, Ian Osband, Remi Munos, Vlad Mnih",https://icml.cc/Conferences/2018/Schedule?showEvent=1959,"We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for $\epsilon$-greedy improves DQN performance on 51 out of 57 games in the Atari suite.","['DeepMind', 'Google DeepMind', 'DeepMind', 'Google Deepmind']"
2018,Hierarchical Imitation and Reinforcement Learning,"Hoang Le, Nan Jiang, Alekh Agarwal, Miroslav Dudik, Yisong Yue, Hal Daume",https://icml.cc/Conferences/2018/Schedule?showEvent=2290,"We study how to effectively leverage expert feedback to learn sequential decision-making policies. We focus on problems with sparse rewards and long time horizons, which typically pose significant challenges in reinforcement learning. We propose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying problem to integrate different modes of expert interaction. Our framework can incorporate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both expert effort and cost of exploration. Using long-horizon benchmarks, including Montezuma's Revenge, we  demonstrate that our approach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.
","['Caltech', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Caltech', 'Microsoft Research']"
2018,Policy Optimization with Demonstrations,"Bingyi Kang, Zequn Jie, Jiashi Feng",https://icml.cc/Conferences/2018/Schedule?showEvent=1956,"Exploration remains a significant challenge to reinforcement learning methods, especially in environments where reward signals are sparse. Recent methods of learning from demonstrations have shown to be promising in overcoming exploration difficulties but typically require considerable high-quality demonstrations that are difficult to collect. We propose to effectively leverage available demonstrations to guide exploration through enforcing occupancy measure matching between the learned policy and current demonstrations, and develop a novel Policy Optimization from Demonstration (POfD) method. We show that POfD induces implicit dynamic reward shaping and brings provable benefits for policy improvement. Furthermore, it can be combined with policy gradient methods to produce state-of-the-art results, as demonstrated experimentally on a range of popular benchmark sparse-reward tasks, even when the demonstrations are few and imperfect.
","['National University of Singapore', 'Tencent AI Lab', 'National University of Singapore']"
2018,Fast Gradient-Based Methods with Exponential Rate: A Hybrid Control Framework,"Arman Sharifi Kolarijani, Peyman Mohajerin Esfahani, Tamas Keviczky",https://icml.cc/Conferences/2018/Schedule?showEvent=2320,"Ordinary differential equations, and in general a dynamical system viewpoint, have seen a resurgence of interest in developing fast optimization methods, mainly thanks to the availability of well-established analysis tools. In this study, we pursue a similar objective and propose a class of hybrid control systems that adopts a 2nd-order differential equation as its continuous flow. A distinctive feature of the proposed differential equation in comparison with the existing literature is a state-dependent, time-invariant damping term that acts as a feedback control input. Given a user-defined scalar $\alpha$, it is shown that the proposed control input steers the state trajectories to the global optimizer of a desired objective function with a guaranteed rate of  convergence $\mathcal{O}(e^{-\alpha t})$. Our framework requires that the objective function satisfies the so called Polyak--{\L}ojasiewicz inequality. Furthermore, a discretization method is introduced such that the resulting discrete dynamical system possesses an exponential rate of convergence.","['Delft University of Technology', 'Delft University of Technology', 'Delft University of Technology']"
2018,Level-Set Methods for Finite-Sum Constrained Convex Optimization,"Qihang Lin, Runchao Ma, Tianbao Yang",https://icml.cc/Conferences/2018/Schedule?showEvent=2122,"We consider the constrained optimization where the objective function and the constraints are defined as summation of finitely many loss functions. This model has applications in machine learning such as Neyman-Pearson classification. We consider two level-set methods to solve this class of problems, an existing inexact Newton method and a new feasible level-set method.  To update the level parameter towards the optimality, both methods require an oracle that generates upper and lower bounds as well as an affine-minorant of the level function. To construct the desired oracle, we reformulate the level function as the value of a saddle-point problem using the conjugate and perspective of the loss functions. Then a stochastic variance-reduced gradient method with a special Bregman divergence is proposed as the oracle for solving that saddle-point problem. The special divergence ensures the proximal mapping in each iteration can be solved in a closed form. The total complexity of both level-set methods using the proposed oracle are analyzed.
","['Univ Iowa', 'University of Iowa', 'The University of Iowa']"
2018,A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations,"Weili Nie, Yang Zhang, Ankit Patel",https://icml.cc/Conferences/2018/Schedule?showEvent=2279,"Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less class-sensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.
","['Rice University', 'Rice University', 'Rice University, Baylor College of Medicine']"
2018,A Boo(n) for Evaluating Architecture Performance,"Ondrej Bajgar, Rudolf Kadlec, Jan Kleindienst",https://icml.cc/Conferences/2018/Schedule?showEvent=2260,"We point out important problems with the common practice of using the best single model performance for comparing deep learning architectures, and we propose a method that corrects these flaws. Each time a model is trained, one gets a different result due to random factors in the training process, which include random parameter initialization and random data shuffling. Reporting the best single model performance does not appropriately address this stochasticity. We propose a normalized expected best-out-of-$n$ performance ($\text{Boo}_n$) as a way to correct these problems.","['IBM Watson', 'IBM Watson', 'IBM Watson']"
2018,RLlib: Abstractions for Distributed Reinforcement Learning,"Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph E Gonzalez, Michael Jordan, Ion Stoica",https://icml.cc/Conferences/2018/Schedule?showEvent=2116,"Reinforcement learning (RL) algorithms involve the deep nesting of highly irregular computation patterns, each of which typically exhibits opportunities for distributed computation. We argue for distributing RL components in a composable way by adapting algorithms for top-down hierarchical control, thereby encapsulating parallelism and resource requirements within short-running compute tasks. We demonstrate the benefits of this principle through RLlib: a library that provides scalable software primitives for RL. These primitives enable a broad range of algorithms to be implemented with high performance, scalability, and substantial code reuse. RLlib is available as part of the open source Ray project at http://rllib.io/.
","['University of California, Berkeley', 'UC Berkeley', 'Unknown', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2018,Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator,"Maryam Fazel, Rong Ge, Sham Kakade, Mehran Mesbahi",https://icml.cc/Conferences/2018/Schedule?showEvent=2091,"Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model, 2) they are an ``end-to-end'' approach, directly optimizing the performance metric of interest, 3) they inherently allow for richly parameterized policies. A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties.  This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities.
","['University of Washington', 'Duke University', 'University of Washington', '']"
2018,The Edge Density Barrier: Computational-Statistical Tradeoffs in Combinatorial Inference,"Hao Lu, Yuan Cao, Junwei Lu, Han Liu, Zhaoran Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=2088,"We study the hypothesis testing problem of inferring the existence of combinatorial structures in undirected graphical models. Although there exist extensive studies on the information-theoretic limits of this problem, it remains largely unexplored whether such limits can be attained by efficient algorithms. In this paper, we quantify the minimum computational complexity required to attain the information-theoretic limits based on an oracle computational model. We prove that, for testing common combinatorial structures, such as clique, nearest neighbor graph and perfect matching, against an empty graph, or large clique against small clique, the information-theoretic limits are provably unachievable by tractable algorithms in general. More importantly, we define structural quantities called the weak and strong edge densities, which offer deep insight into the existence of such computational-statistical tradeoffs. To the best of our knowledge, our characterization is the first to identify and explain the fundamental tradeoffs between statistics and computation for combinatorial inference problems in undirected graphical models.
","['Princeton University', 'Princeton University', '', 'Princeton University', 'Northwestern U']"
2018,Sound Abstraction and Decomposition of Probabilistic Programs,"Steven Holtzen, Guy Van den Broeck, Todd Millstein",https://icml.cc/Conferences/2018/Schedule?showEvent=2418,"Probabilistic programming languages are a flexible tool for specifying statistical models, but this flexibility comes at the cost of efficient analysis. It is currently difficult to compactly represent the subtle independence properties of a probabilistic program, and exploit independence properties to decompose inference. Classical graphical model abstractions do capture some properties of the underlying distribution, enabling inference algorithms to operate at the level of the graph topology. However, we observe that graph-based abstractions are often too coarse to capture interesting properties of programs. We propose a form of sound abstraction for probabilistic programs wherein the abstractions are themselves simplified programs. We provide a theoretical foundation for these abstractions, as well as an algorithm to generate them. Experimentally, we also illustrate the practical benefits of our framework as a tool to decompose probabilistic program inference.
","['University of California, Los Angeles', 'University of California, Los Angeles', 'University of California, Los Angeles']"
2018,Parallel WaveNet: Fast High-Fidelity Speech Synthesis,"Aäron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, koray kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov, Demis Hassabis",https://icml.cc/Conferences/2018/Schedule?showEvent=2198,"The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.
","['Google Deepmind', 'Deepmind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', '', 'DeepMind', '', 'DeepMind', '', 'DeepMind', 'DeepMind', '', 'Google Brain Amsterdam', '', 'DeepMind', 'DeepMind', 'DeepMind', 'Google', 'Deepmind']"
2018,Modeling Sparse Deviations for Compressed Sensing using Generative Models,"Manik Dhar, Aditya Grover, Stefano Ermon",https://icml.cc/Conferences/2018/Schedule?showEvent=2331,"In compressed sensing,  a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the  restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.
","['Stanford University', 'Stanford University', 'Stanford University']"
2018,Revealing Common Statistical Behaviors in Heterogeneous Populations,"Andrey Zhitnikov, Rotem Mulayoff, Tomer Michaeli",https://icml.cc/Conferences/2018/Schedule?showEvent=2019,"In many areas of neuroscience and biological data analysis, it is desired to reveal common patterns among a group of subjects. Such analyses play important roles e.g., in detecting functional brain networks from fMRI scans and in identifying brain regions which show increased activity in response to certain stimuli. Group level techniques usually assume that all subjects in the group behave according to a single statistical model, or that deviations from the common model have simple parametric forms. Therefore, complex subject-specific deviations from the common model severely impair the performance of such methods. In this paper, we propose nonparametric algorithms for estimating the common covariance matrix and the common density function of several variables in a heterogeneous group of subjects. Our estimates converge to the true model as the number of subjects tends to infinity, under very mild conditions. We illustrate the effectiveness of our methods through extensive simulations as well as on real-data from fMRI scans and from arterial blood pressure and photoplethysmogram measurements.
","['Technion', 'Technion', 'Technion']"
2018,Improved nearest neighbor search using auxiliary information and priority functions,"Omid Keivani, Kaushik Sinha",https://icml.cc/Conferences/2018/Schedule?showEvent=2041,"Nearest neighbor search using random projection trees has recently been shown to achieve superior performance,  in terms of better accuracy while retrieving less number of data points, compared to locality sensitive hashing based methods. However, to achieve acceptable nearest neighbor search accuracy for large scale applications, where number of data points and/or number of features can be very large, it requires users to maintain, store and search through large number of such independent random projection trees, which may be undesirable for many practical applications. To address this issue, in this paper we present different search strategies to improve nearest neighbor search performance of a single random projection tree. Our approach exploits properties of single and multiple random projections, which allows us  to store meaningful auxiliary information at internal nodes of a random projection tree as well as to design priority functions to guide the search process that results  in improved nearest neighbor search performance. Empirical results on multiple real world datasets show that our proposed method improves the search accuracy of a single tree compared to baseline methods.
","['Wichita State University', 'Wichita State University']"
2018,Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings,"Aviral Kumar, Sunita Sarawagi, Ujjwal Jain",https://icml.cc/Conferences/2018/Schedule?showEvent=2326,"Modern neural networks have recently been found to be poorly calibrated, primarily in the direction of over-confidence. Methods like entropy penalty and temperature smoothing improve calibration by clamping confidence, but in doing so compromise the many legitimately confident predictions. We propose a more principled fix that minimizes an explicit calibration error during training. We present MMCE, a RKHS kernel based measure of calibration that is efficiently trainable alongside the negative likelihood loss without careful hyper-parameter tuning. Theoretically too, MMCE is a sound measure of calibration that is minimized at perfect calibration, and whose finite sample estimates are consistent and enjoy fast convergence rates. Extensive experiments on several network architectures demonstrate that MMCE is a fast, stable, and accurate method to minimize calibration error while maximally preserving the number of high confidence predictions.
","['IIT Bombay', 'IIT Bombay', 'IIT Bombay']"
2018,QuantTree: Histograms for Change Detection in Multivariate Data Streams,"Giacomo Boracchi, Diego Carrera, Cristiano Cervellera, Danilo Macciò",https://icml.cc/Conferences/2018/Schedule?showEvent=2268,"We address the problem of detecting distribution changes in multivariate data streams by means of histograms. Histograms are very general and flexible models, which have been relatively ignored in the change-detection literature as they often require a number of bins that grows unfeasibly with the data dimension. We present \QuantTree, a recursive binary splitting scheme that adaptively defines the histogram bins to ease the detection of any distribution change. Our design scheme implies that i) we can easily control the overall number of bins and ii) the bin probabilities do not depend on the distribution of stationary data. This latter is a very relevant aspect in change detection, since thresholds of tests statistics based on these histograms (e.g., the Pearson statistic or the total variation) can be numerically computed from univariate and synthetically generated data, yet guaranteeing a controlled false positive rate. Our experiments show that the proposed histograms are very effective in detecting changes in high dimensional data streams, and that the resulting thresholds can effectively control the false positive rate, even when the number of training samples is relatively small.
","['Politecnico di Milano', 'Politecnico di Milano', 'National Research Council', '']"
2018,"An Iterative, Sketching-based Framework for Ridge Regression","Agniva Chowdhury, Jiasen Yang, Petros Drineas",https://icml.cc/Conferences/2018/Schedule?showEvent=1895,"Ridge regression is a variant of regularized least squares regression that is particularly suitable in settings where the number of predictor variables greatly exceeds the number of observations. We present a simple, iterative, sketching-based algorithm for ridge regression that  guarantees high-quality approximations to the optimal solution vector. Our analysis builds upon two simple structural results that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized linear algebra. An important contribution of our work is the analysis of the behavior of subsampled ridge regression problems when the ridge leverage scores are used: we prove that accurate approximations can be achieved by a sample whose size depends on the degrees of freedom of the ridge-regression problem rather than the dimensions of the design matrix. Our experimental evaluations verify our theoretical results on both real and synthetic data.
","['Purdue University', 'Purdue University', 'Purdue University']"
2018,Learning Low-Dimensional Temporal Representations,"Bing Su, Ying Wu",https://icml.cc/Conferences/2018/Schedule?showEvent=1910,"Low-dimensional discriminative representations enhance machine learning methods in both performance and complexity, motivating supervised dimensionality reduction (DR) that transforms high-dimensional data to a discriminative subspace. Most DR methods require data to be i.i.d., however, in some domains, data naturally come in sequences, where the observations are temporally correlated. We propose a DR method called LT-LDA to learn low-dimensional temporal representations. We construct the separability among sequence classes by lifting the holistic temporal structures, which are established based on temporal alignments and may change in different subspaces. We jointly learn the subspace and the associated alignments by optimizing an objective which favors easily-separable temporal structures, and show that this objective is connected to the inference of alignments, thus allows an iterative solution. We provide both theoretical insight and empirical evaluation on real-world sequence datasets to show the interest of our method.
","['Institute of Software, Chinese Academy of Sciences', 'Northwestern University']"
2018,Rapid Adaptation with Conditionally Shifted Neurons,"Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, Adam Trischler",https://icml.cc/Conferences/2018/Schedule?showEvent=1966,"We describe a mechanism by which artificial neural networks can learn rapid adaptation - the ability to adapt on the fly, with little data, to new tasks - that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.
","['Microsoft Research', 'Microsoft Maluuba', 'Microsoft Research', 'Microsoft Research']"
2018,PDE-Net: Learning PDEs from Data,"Zichao Long, Yiping Lu, Xianzhong Ma, Bin Dong",https://icml.cc/Conferences/2018/Schedule?showEvent=1934,"Partial differential equations (PDEs) play a prominent role in many disciplines of science and engineering. PDEs are commonly derived based on empirical observations. However, with the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. Comparing with existing approaches, our approach has the most flexibility by learning both differential operators and the nonlinear response function of the underlying PDE model. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.
","['Peking University', 'Peking University', 'Peking University', 'Peking University']"
2018,Theoretical Analysis of Sparse Subspace Clustering with Missing Entries,"Manolis Tsakiris, Rene Vidal",https://icml.cc/Conferences/2018/Schedule?showEvent=1928,"Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning method for clustering data lying close to an unknown union of low-dimensional linear subspaces; a problem with numerous applications in pattern recognition and computer vision. Even though the behavior of SSC for complete data is by now well-understood, little is known about its theoretical properties when applied to data with missing entries. In this paper we give theoretical guarantees for SSC with incomplete data, and provide theoretical evidence that projecting the zero-filled data onto the observation pattern of the point being expressed can lead to substantial improvement in performance; a phenomenon already known experimentally. The main insight of our analysis is that even though this projection induces additional missing entries, this is counterbalanced by the fact that the projected and zero-filled data are in effect incomplete points associated with the union of the corresponding projected subspaces, with respect to which the point being expressed is complete. The significance of this phenomenon potentially extends to the entire class of self-expressive methods.
","['Johns Hopkins University', 'Johns Hopkins University']"
2018,Topological mixture estimation,Steve Huntsman,https://icml.cc/Conferences/2018/Schedule?showEvent=1873,"We introduce topological mixture estimation, a completely nonparametric and computationally efficient solution to the problem of estimating a one-dimensional mixture with generic unimodal components.  We repeatedly perturb the unimodal decomposition of Baryshnikov and Ghrist to produce a topologically and information-theoretically optimal unimodal mixture.  We also detail a smoothing process that optimally exploits topological persistence of the unimodal category in a natural way when working directly with sample data.  Finally, we illustrate these techniques through examples.
",['BAE Systems FAST Labs']
2018,On Matching Pursuit and Coordinate Descent,"Francesco Locatello, Anant Raj, Sai Praneeth Reddy Karimireddy, Gunnar Ratsch, Bernhard Schölkopf, Sebastian Stich, Martin Jaggi",https://icml.cc/Conferences/2018/Schedule?showEvent=2228,"Two popular examples of first-order optimization methods over linear spaces are coordinate descent and matching pursuit algorithms, with their randomized variants. While the former targets the optimization by moving along coordinates, the latter considers a generalized notion of directions. Exploiting the connection between the two algorithms, we present a unified analysis of both, providing affine invariant sublinear $O(1/t)$ rates on smooth objectives and linear convergence on strongly convex objectives. As a byproduct of our affine invariant analysis of matching pursuit, our rates for steepest coordinate descent are the tightest known. Furthermore, we show the first accelerated convergence rate $O(1/t^2)$ for matching pursuit and steepest coordinate descent on convex objectives.","['MPI - ETH', 'Max-Planck Institute for Intelligent Systems', 'EPFL', 'ETH Zurich', 'MPI for Intelligent Systems Tübingen, Germany', 'EPFL', 'EPFL']"
2018,Frank-Wolfe with Subsampling Oracle,"Thomas Kerdreux, Fabian Pedregosa, Alexandre d'Aspremont",https://icml.cc/Conferences/2018/Schedule?showEvent=2057,"We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization problem over a small \emph{subset} of the original domain. The first algorithm that we propose is a randomized variant of the original FW algorithm and achieves a $\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic counterpart. The second algorithm is a randomized variant of the Away-step FW algorithm, and again as its deterministic counterpart, reaches linear (i.e., exponential) convergence rate making it the first provably convergent randomized variant of Away-step FW. In both cases, while subsampling reduces the convergence rate by a constant factor, the linear minimization step can be a fraction of the cost of that of the deterministic versions, especially when the data is streamed. We illustrate computational gains of both algorithms on regression problems, involving both $\ell_1$ and latent group lasso penalties.","['INRIA', 'UC Berkeley', 'CNRS, Ecole Normale Superieure']"
2018,Reinforcement Learning with Function-Valued Action Spaces for Partial Differential Equation Control,"Yangchen Pan, Amir-massoud Farahmand, Martha White, Saleh Nabi, Piyush Grover, Daniel Nikovski",https://icml.cc/Conferences/2018/Schedule?showEvent=2182,"Recent work has shown that reinforcement learning (RL) is a promising approach to control dynamical systems described by partial differential equations (PDE). This paper shows how to use RL to tackle more general PDE control problems that have continuous high-dimensional action spaces with spatial relationship among action dimensions. In particular, we propose the concept of action descriptors, which encode regularities among spatially-extended action dimensions and enable the agent to control high-dimensional action PDEs. We provide theoretical evidence suggesting that this approach can be more sample efficient compared to a conventional approach that treats each action dimension separately and does not explicitly exploit the spatial regularity of the action space. The action descriptor approach is then used within the deep deterministic policy gradient algorithm. Experiments on two PDE control problems, with up to 256-dimensional continuous actions, show the advantage of the proposed approach over the conventional one.
","['University of Alberta', 'Vector Institute', 'University of Alberta', '', 'Mitsubishi Electric Research Labs', 'Mitsubishi Electric Research Labs']"
2018,Fourier Policy Gradients,"Matthew Fellows, Kamil Ciosek, Shimon Whiteson",https://icml.cc/Conferences/2018/Schedule?showEvent=2414,"We propose a new way of deriving policy gradient updates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with expected policy gradients as convolutions and turns them into multiplications. The obtained analytical solutions allow us to capture the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal approximation property. The choice of policy can be almost arbitrary, including mixtures or hybrid continuous-discrete probability distributions. Moreover, we derive a general family of sample-based estimators for stochastic policy gradients, which unifies existing results on sample-based approximation. We believe that this technique has the potential to shape the next generation of policy gradient approaches, powered by analytical results.
","['University of Oxford', 'Oxford', 'University of Oxford']"
2018,Adaptive Three Operator Splitting,"Fabian Pedregosa, Gauthier Gidel",https://icml.cc/Conferences/2018/Schedule?showEvent=1871,"We propose and analyze a novel adaptive step size variant of the Davis-Yin three operator splitting, a method that can solve optimization problems composed of a sum of a smooth term for which we have access to its gradient and an arbitrary number of potentially non-smooth terms for which we have access to their proximal operator. The proposed method leverages local information of the objective function, allowing for larger step sizes while preserving the convergence properties of the original method. It only requires two extra function evaluations per iteration and does not depend on any step size hyperparameter besides an initial estimate. We provide a convergence rate analysis of this method, showing sublinear convergence rate for general convex functions and linear convergence under stronger assumptions, matching the best known rates of its non adaptive variant. Finally, an empirical comparison with related methods on 6 different problems illustrates the computational advantage of the adaptive step size strategy.
","['UC Berkeley', 'MILA']"
2018,A Conditional Gradient Framework for Composite Convex Minimization with Applications to Semidefinite Programming,"Alp Yurtsever, Olivier Fercoq, Francesco Locatello, Volkan Cevher",https://icml.cc/Conferences/2018/Schedule?showEvent=2152,"We propose a conditional gradient framework for a composite convex minimization template with broad applications. Our approach combines smoothing and homotopy techniques under the CGM framework, and provably achieves the optimal convergence rate. We demonstrate that the same rate holds if the linear subproblems are solved approximately with additive or multiplicative error. In contrast with the relevant work, we are able to characterize the convergence when the non-smooth term is an indicator function. Specific applications of our framework include the non-smooth minimization, semidefinite programming, and minimization with linear inclusion constraints over a compact domain. Numerical evidence demonstrates the benefits of our framework.
","['EPFL', 'Télécom Paris, IP Paris', 'ETH Zurich - Max Planck Institute', 'EPFL']"
2018,Learning Semantic Representations for Unsupervised Domain Adaptation,"Shaoan Xie, Zibin Zheng, Liang Chen, Chuan Chen",https://icml.cc/Conferences/2018/Schedule?showEvent=1961,"It is important to transfer the knowledge from label-rich source domain to unlabeled target domain due to the expensive cost of manual labeling efforts. Prior domain adaptation methods address this problem through aligning the global distribution statistics between source domain and target domain, but a drawback of prior methods is that they ignore the semantic information contained in samples, e.g., features of backpacks in target domain might be mapped near features of cars in source domain. In this paper, we present moving semantic transfer network, which learn semantic representations for unlabeled target samples by aligning labeled source centroid and pseudo-labeled target centroid. Features in same class but different domains are expected to be mapped nearby, resulting in an improved target classification accuracy. Moving average centroid alignment is cautiously designed to compensate the insufficient categorical information within each mini batch. Experiments testify that our model yields state of the art results on standard datasets.
","['Sun Yat-sen University', '', 'Sun Yat-sen University', 'Sun Yat-sen University']"
2018,Learning Adversarially Fair and Transferable Representations,"David Madras, Elliot Creager, Toniann Pitassi, Richard Zemel",https://icml.cc/Conferences/2018/Schedule?showEvent=2393,"In this paper, we advocate for representation learning as the key to mitigating unfair prediction outcomes downstream. Motivated by a scenario where learned representations are used by third parties with unknown objectives, we propose and explore adversarial representation learning as a natural method of ensuring those parties act fairly. We connect group fairness (demographic parity, equalized odds, and equal opportunity) to different adversarial objectives. Through worst-case theoretical guarantees and experimental validation, we show that the choice of this objective is crucial to fair prediction. Furthermore, we present the first in-depth experimental demonstration of fair transfer learning and demonstrate empirically that our learned representations admit fair predictions on new tasks while maintaining utility, an essential goal of fair representation learning.
","['University of Toronto', 'University of Toronto', 'University of Toronto', 'Vector Institute']"
2018,Spurious Local Minima are Common in Two-Layer ReLU Neural Networks,"Itay Safran, Ohad Shamir",https://icml.cc/Conferences/2018/Schedule?showEvent=2033,"We consider the optimization problem associated with training simple ReLU neural networks of the form $\mathbf{x}\mapsto \sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\top \mathbf{x}\}$ with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once $6\le k\le 20$. By a concentration of measure argument, this implies that in high input dimensions, \emph{nearly all} target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.","['Weizmann Institute of Science', 'Weizmann Institute of Science']"
2018,Efficient end-to-end learning for quantizable representations,"Yeonwoo Jeong, Hyun Oh Song",https://icml.cc/Conferences/2018/Schedule?showEvent=2123,"Embedding representation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization. To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods. We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98X and 478X search speedup respectively over exhaustive linear search. The source code is available at https://github.com/maestrojeong/Deep-Hash-Table-ICML18.
","['Seoul National University', 'Seoul National University']"
2018,Solving Partial Assignment Problems using Random Clique Complexes,"Charu Sharma, Deepak Nathani, Manu Kaul",https://icml.cc/Conferences/2018/Schedule?showEvent=2170,"We present an alternate formulation of the partial assignment problem as matching random clique complexes, that are higher-order analogues of random graphs, designed to provide a set of invariants that better detect higher-order structure. The proposed method creates random clique adjacency matrices for each k-skeleton of the random clique complexes and matches them, taking into account each point as the affine combination of its geometric neighborhood. We justify our solution theoretically, by analyzing the runtime and storage complexity of our algorithm along with the asymptotic behavior of the quadratic assignment problem (QAP) that is associated with the underlying random clique adjacency matrices. Experiments on both synthetic and real-world datasets, containing severe occlusions and distortions, provide insight into the accuracy, efficiency, and robustness of our approach. We outperform diverse matching algorithms by a significant margin.
","['Indian Institute of Technology Hyderabad', 'IIT Hyderabad', 'IIT Hyderabad']"
2018,Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction,"Siyuan Qi, Baoxiong Jia, Song-Chun Zhu",https://icml.cc/Conferences/2018/Schedule?showEvent=1920,"Future predictions on sequence data (e.g., videos or audios) require the algorithms to capture non-Markovian and compositional properties of high-level semantics. Context-free grammars are natural choices to capture such properties, but traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as inputs. In this paper, we generalize the Earley parser to parse sequence data which is neither segmented nor labeled. This generalized Earley parser integrates a grammar parser with a classifier to find the optimal segmentation and labels, and makes top-down future predictions. Experiments show that our method significantly outperforms other approaches for future human activity prediction.
","['UCLA', 'Peking University', 'UCLA']"
2018,Convergence guarantees for a class of non-convex and   non-smooth optimization problems,"Koulik Khamaru, Martin Wainwright",https://icml.cc/Conferences/2018/Schedule?showEvent=2283,"Non-convex optimization problems arise frequently in machine learning, including feature selection, structured matrix learning, mixture modeling, and neural network training.  We consider the problem of finding critical points of a broad class of non-convex problems with non-smooth components. We analyze the behavior of two gradient-based methods---namely a sub-gradient method, and a proximal method.  Our main results are to establish rates of convergence for general problems, and also exhibit faster rates for sub-analytic functions.  As an application of our theory, we obtain a simplification of the popular CCCP algorithm, which retains all the desirable convergence properties of the original method, along with a significantly lower cost per iteration.  We illustrate our methods and theory via application to the problems of best subset selection, robust estimation, and shape from shading reconstruction.
","['University Of California Berkeley', 'University of California at Berkeley']"
2018,Estimation of Markov Chain via Rank-constrained Likelihood,"XUDONG LI, Mengdi Wang, Anru Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=2304,This paper studies the estimation of low-rank Markov chains from empirical trajectories. We propose a non-convex estimator based on rank-constrained likelihood maximization. Statistical upper bounds are provided for the Kullback-Leiber divergence and the $\ell_2$ risk between the estimator and the true transition matrix. The estimator reveals a compressed state space of the Markov chain. We also develop a novel DC (difference of convex function) programming algorithm to tackle the rank-constrained non-smooth optimization problem. Convergence results are established.  Experiments show that the proposed estimator achieves better empirical performance than other popular approaches.,"['Princeton Univerisity', 'Princeton University', 'University of Wisconsin-Madison']"
2018,Efficient First-Order Algorithms for Adaptive Signal Denoising,"Dmitrii Ostrovskii, Zaid Harchaoui",https://icml.cc/Conferences/2018/Schedule?showEvent=2359,"We consider the problem of discrete-time signal denoising, focusing on a specific family of non-linear convolution-type estimators. Each such estimator is associated with a time-invariant filter which is obtained adaptively, by solving a certain convex optimization problem. Adaptive convolution-type estimators were demonstrated to have favorable statistical properties, see (Juditsky & Nemirovski, 2009; 2010; Harchaoui et al., 2015b; Ostrovsky et al., 2016). Our first contribution is an efficient implementation of these estimators via the known first-order proximal algorithms. Our second contribution is a computational complexity analysis of the proposed procedures, which takes into account their statistical nature and the related notion of statistical accuracy. The proposed procedures and their analysis are illustrated on a simulated data benchmark.
","['INRIA', 'University of Washington']"
2018,Continuous and Discrete-time Accelerated Stochastic Mirror Descent for Strongly Convex Functions,"Pan Xu, Tianhao Wang, Quanquan Gu",https://icml.cc/Conferences/2018/Schedule?showEvent=2166,"We provide a second-order stochastic differential equation (SDE), which characterizes the continuous-time dynamics of accelerated stochastic mirror descent (ASMD) for strongly convex functions. This SDE plays a central role in designing new discrete-time ASMD algorithms via numerical discretization, and providing neat analyses of their convergence rates based on Lyapunov functions. Our results suggest that the only existing ASMD algorithm, namely, AC-SA proposed in \citet{ghadimi2012optimal} is one instance of its kind, and we can actually derive new instances of ASMD with fewer tuning parameters. This sheds light on revisiting accelerated stochastic optimization through the lens of SDEs, which can lead to a better understanding of acceleration in stochastic optimization, as well as new simpler algorithms. Numerical experiments on both synthetic and real data support our theory.
","['University of California, Los Angeles', 'University of Science and Technology of China', 'UCLA']"
2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,"Adji Bousso Dieng, Rajesh Ranganath, Jaan Altosaar, David Blei",https://icml.cc/Conferences/2018/Schedule?showEvent=2407,"Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.
","['Columbia University', 'New York University', 'Princeton University', 'Columbia University']"
2018,Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series,"Zhengping Che, Sanjay Purushotham, Max Guangyu Li, Bo Jiang, Yan Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2108,"Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and encode multiple temporal dependencies. State-space models such as Kalman filters and deep learning models such as deep Markov models are mainly designed for time series data with the same sampling rate and cannot capture all the dependencies present in the MR-MTS data. To address this challenge, we propose the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), a novel deep generative model which uses the latent hierarchical structure with a learnable switch mechanism to capture the temporal dependencies of MR-MTS. Experimental results on two real-world datasets demonstrate that our MR-HDMM model outperforms the existing state-of-the-art deep learning and state-space models on forecasting and interpolation tasks. In addition, the latent hierarchies in our model provide a way to show and interpret the multiple temporal dependencies.
","['University of Southern California', 'University of Southern California', 'University of Southern California', 'University of Southern California', 'University of Southern California']"
2018,Disentangled Sequential Autoencoder,"Yingzhen Li, Stephan Mandt",https://icml.cc/Conferences/2018/Schedule?showEvent=2147,"We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics.  Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.
","['Microsoft Research Cambridge', 'UC Irvine']"
2018,Stochastic Video Generation with a Learned Prior,"Emily Denton, Rob Fergus",https://icml.cc/Conferences/2018/Schedule?showEvent=2191,"Generating video frames that accurately predict future world states is challenging. Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both. In this paper we introduce a video generation model with a learned prior over stochastic latent variables at each time step. Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame. The approach is simple and easily trained end-to-end on a variety of datasets. Sample generations are both varied and sharp, even many frames into the future, and compare favorably to those from existing approaches.
","['New York University', 'Facebook / NYU']"
2018,Mutual Information Neural Estimation,"Mohamed Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R Devon Hjelm, Aaron Courville",https://icml.cc/Conferences/2018/Schedule?showEvent=2440,"We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks.   We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.
","['MILA', 'University of Montreal', 'University of Montreal', 'University of Montreal', 'Mila / U. Montreal', 'Microsoft Research / Mila', 'University of Montreal']"
2018,Adversarially Regularized Autoencoders,"Jake Zhao, Yoon Kim, Kelly Zhang, Alexander Rush, Yann LeCun",https://icml.cc/Conferences/2018/Schedule?showEvent=2267,"Deep latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for representation learning of continuous structures. However, applying similar methods to discrete structures, such as text sequences or discretized images, has proven to be more challenging. In this work, we propose a more flexible method for training deep latent variable models of discrete structures. Our approach is based on the recently proposed Wasserstein Autoencoder (WAE) which formalizes adversarial autoencoders as an optimal transport problem. We first extend this framework to model discrete sequences, and then further explore different learned priors targeting a controllable representation. Unlike many other latent variable generative models for text, this adversarially regularized autoencoder (ARAE) allows us to generate fluent textual outputs as well as perform manipulations in the latent space to induce change in the output space. Finally we show that the latent representation can be trained to perform unaligned textual style transfer, giving improvements both in automatic measures and human evaluation.
","['NYU / Facebook AI Research', 'Harvard University', 'New York University', 'Harvard University', 'New York University']"
2018,Policy Optimization as Wasserstein Gradient Flows,"RUIYI (ROY) ZHANG, Changyou Chen, Chunyuan Li, Lawrence Carin",https://icml.cc/Conferences/2018/Schedule?showEvent=2121,"Policy optimization is a core component of reinforcement learning (RL), and most existing RL methods directly optimize parameters of a policy based on maximizing the expected total reward, or its surrogate. Though often achieving encouraging empirical success, its correspondence to policy-distribution optimization has been unclear mathematically. We place policy optimization into the space of probability measures, and interpret it as Wasserstein gradient flows. On the probability-measure space, under specified circumstances, policy optimization becomes convex in terms of distribution optimization. To make optimization feasible, we develop efficient algorithms by numerically solving the corresponding discrete gradient flows. Our technique is applicable to several RL settings, and is related to many state-of-the-art policy-optimization algorithms. Specifically, we define gradient flows on both the parameter-distribution space and policy-distribution space, leading to what we term indirect-policy and direct-policy learning frameworks, respectively. Extensive experiments verify the effectiveness of our framework, often obtaining better performance compared to related algorithms.
","['Duke University', 'SUNY at Buffalo', 'Duke University', 'Duke']"
2018,Self-Imitation Learning,"Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee",https://icml.cc/Conferences/2018/Schedule?showEvent=2101,"This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.
","['University of Michigan', 'University of Michigan', 'University of Michigan', 'Google / U. Michigan']"
2018,Spectrally Approximating Large Graphs with Smaller Graphs,"Andreas Loukas, Pierre Vandergheynst",https://icml.cc/Conferences/2018/Schedule?showEvent=1948,"How does coarsening affect the spectrum of a general graph? We provide conditions such that the principal eigenvalues and eigenspaces of a coarsened and original graph Laplacian matrices are close. The achieved approximation is shown to depend on standard graph-theoretic properties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual graph sizes. Our results carry implications for learning methods that utilize coarsening. For the particular case of spectral clustering, they imply that coarse eigenvectors can be used to derive good quality assignments even without refinement—this phenomenon was previously observed, but lacked formal justification.
","['EPFL', 'École polytechnique fédérale de Lausanne']"
2018,On the Spectrum of Random Features Maps of High Dimensional Data,"Zhenyu Liao, Romain Couillet",https://icml.cc/Conferences/2018/Schedule?showEvent=1898,"Random feature maps are ubiquitous in modern statistical machine learning, where they generalize random projections by means of powerful, yet often difficult to analyze nonlinear operators. In this paper we leverage the ""concentration"" phenomenon induced by random matrix theory to perform a spectral analysis on the Gram matrix of these random feature maps, here for Gaussian mixture models of simultaneously large dimension and size. Our results are instrumental to a deeper understanding on the interplay of the nonlinearity and the statistics of the data, thereby allowing for a better tuning of random feature-based techniques.
","['L2S, CentraleSupelec', 'CentralSupélec']"
2018,Learning Registered Point Processes from Idiosyncratic Observations,"Hongteng Xu, Lawrence Carin, Hongyuan Zha",https://icml.cc/Conferences/2018/Schedule?showEvent=2045,"A parametric point process model is developed, with modeling based on the assumption that sequential observations often share latent phenomena, while also possessing idiosyncratic effects.  An alternating optimization method is proposed to learn a registered'' point process that accounts for shared structure, as well aswarping'' functions that characterize idiosyncratic aspects of each observed sequence.  Under reasonable constraints, in each iteration we update the sample-specific warping functions by solving a set of constrained nonlinear programming problems in parallel, and update the model by maximum likelihood estimation.  The justifiability, complexity and robustness of the proposed method are investigated in detail, and the influence of sequence stitching on the learning results is examined empirically. Experiments on both synthetic and real-world data demonstrate that the method yields explainable point process models, achieving encouraging results compared to state-of-the-art methods.
","['InfiniaML, Inc.', 'Duke', 'Georgia Institute of Technology']"
2018,Deep Bayesian Nonparametric Tracking,"Aonan Zhang, John Paisley",https://icml.cc/Conferences/2018/Schedule?showEvent=2090,"Time-series data often exhibit irregular behavior, making them hard to analyze and explain with a simple dynamic model. For example, information in social networks may show change-point-like bursts that then diffuse  with smooth dynamics. Powerful models such as deep neural networks learn smooth functions from data, but are not as well-suited (in off-the-shelf form) for discovering and explaining sparse, discrete and bursty dynamic patterns. Bayesian models can do this well by encoding the appropriate probabilistic assumptions in the model prior. We propose an integration of Bayesian nonparametric methods within deep neural networks for modeling irregular patterns in time-series data. We use a Bayesian nonparametrics to model change-point behavior in time, and a deep neural network to model nonlinear latent space dynamics. We compare with a non-deep linear version of the model also proposed here. Empirical evaluations demonstrates improved performance and interpretable results when tracking stock prices and Twitter trends.
","['Columbia University', 'Columbia University']"
2018,Learning and Memorization,Sat Chatterjee,https://icml.cc/Conferences/2018/Schedule?showEvent=2080,"In the machine learning research community, it is generally believed that there is a tension between memorization and generalization. In this work we examine to what extent this tension exists by exploring if it is possible to generalize by memorizing alone. Although direct memorization with a lookup table obviously does not generalize, we find that introducing depth in the form of a network of support-limited lookup tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10. Furthermore, we demonstrate through a series of empirical results that our approach allows for a smooth tradeoff between memorization and generalization and exhibits some of the most salient characteristics of neural networks: depth improves performance; random data can be memorized and yet there is generalization on real data; and memorizing random data is harder in a certain sense than memorizing real data. The extreme simplicity of the algorithm and potential connections with generalization theory point to several interesting directions for future research.
",['Two Sigma Investments']
2018,Attention-based Deep Multiple Instance Learning,"Maximilian Ilse, Jakub Tomczak, Max Welling",https://icml.cc/Conferences/2018/Schedule?showEvent=2029,"Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.
","['University of Amsterdam', 'Qualcomm AI Research', 'University of Amsterdam']"
2018,Classification from Pairwise Similarity and Unlabeled Data,"Han Bao, Gang Niu, Masashi Sugiyama",https://icml.cc/Conferences/2018/Schedule?showEvent=2134,"Supervised learning needs a huge amount of labeled data, which can be a big bottleneck under the situation where there is a privacy concern or labeling cost is high. To overcome this problem, we propose a new weakly-supervised learning setting where only similar (S) data pairs (two examples belong to the same class) and unlabeled (U) data points are needed instead of fully labeled data, which is called SU classification. We show that an unbiased estimator of the classification risk can be obtained only from SU data, and the estimation error of its empirical risk minimizer achieves the optimal parametric convergence rate. Finally, we demonstrate the effectiveness of the proposed method through experiments.
","['The University of Tokyo / RIKEN', 'RIKEN', 'RIKEN / The University of Tokyo']"
2018,Analyzing the Robustness of Nearest Neighbors to Adversarial Examples,"Yizhen Wang, Somesh Jha, Kamalika Chaudhuri",https://icml.cc/Conferences/2018/Schedule?showEvent=2076,"Motivated by safety-critical applications, test-time attacks on classifiers via adversarial examples has recently received a great deal of attention. However, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood. In this work, we introduce a theoretical framework analogous to bias-variance theory for understanding these effects. We use our framework to analyze the robustness of a canonical non-parametric classifier – the k-nearest neighbors. Our analysis shows that its robustness properties depend critically on the value of k – the classifier may be inherently non-robust for small k, but its robustness approaches that of the Bayes Optimal classifier for fast-growing k. We propose a novel modified 1-nearest neighbor classifier, and guarantee its robustness in the large sample limit. Our experiments suggest that this classifier may have good robustness properties even for reasonable data set sizes.
","['UCSD', 'University of Wisconsin, Madison', 'University of California at San Diego']"
2018,On the Implicit Bias of Dropout,"Poorya Mianjy, Raman Arora, Rene Vidal",https://icml.cc/Conferences/2018/Schedule?showEvent=2354,"Algorithmic approaches endow deep learning systems with implicit bias that helps them generalize even in over-parametrized settings. In this paper, we focus on understanding such a bias induced in learning through dropout, a popular technique to avoid overfitting in deep learning. For single hidden-layer linear neural networks, we show that dropout tends to make the norm of incoming/outgoing weight vectors of all the hidden nodes equal. In addition, we provide a complete characterization of the optimization landscape induced by dropout.
","['Johns Hopkins University', 'Johns Hopkins University', 'Johns Hopkins University']"
2018,Convolutional Imputation of Matrix Networks,"Qingyun Sun, Mengyuan Yan, David Donoho, stephen boyd",https://icml.cc/Conferences/2018/Schedule?showEvent=2135,"A matrix network is a family of matrices, with their relations modeled as a weighted graph. We consider the task of completing a partially observed matrix network. The observation comes from a novel sampling scheme where a fraction of matrices might be completely unobserved. How can we recover the entire matrix network from incomplete observations? This mathematical problem arises in many applications including medical imaging and social networks.  To recover the matrix network, we propose a structural assumption that the matrices are low-rank after the graph Fourier transform on the network. We formulate a convex optimization problem and prove an exact recovery guarantee for the optimization problem. Furthermore, we numerically characterize the exact recovery regime for varying rank and sampling rate and discover a new phase transition phenomenon. Then we give an iterative imputation algorithm to efficiently solve optimization problem and complete large scale matrix networks.  We demonstrate the algorithm with a variety of applications such as MRI and Facebook user network.
","['Stanford University', 'Stanford University', 'Stanford University', 'stanford university']"
2018,Detecting and Correcting for Label Shift with Black Box Predictors,"Zachary Lipton, Yu-Xiang Wang, Alexander Smola",https://icml.cc/Conferences/2018/Schedule?showEvent=2117,"Faced with  distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal p(y) changes but the conditional p(x| y) does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution p(y). BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.
","['Carnegie Mellon University', 'UC Santa Barbara', 'Amazon']"
2018,Orthogonality-Promoting Distance Metric Learning: Convex Relaxation and Theoretical Analysis,"Pengtao Xie, Wei Wu, Yichen Zhu, Eric Xing",https://icml.cc/Conferences/2018/Schedule?showEvent=1884,"Distance metric learning (DML), which learns a distance metric from labeled ""similar"" and ""dissimilar"" data pairs, is widely utilized. Recently, several works investigate orthogonality-promoting regularization (OPR), which encourages the projection vectors in DML to be close to being orthogonal, to achieve three effects: (1) high balancedness -- achieving comparable performance on both frequent and infrequent classes; (2) high compactness -- using a small number of projection vectors to achieve a ""good"" metric; (3) good generalizability -- alleviating overfitting to training data. While showing promising results, these approaches suffer three problems. First, they involve solving non-convex optimization problems where achieving the global optimal is NP-hard. Second, it lacks a theoretical understanding why OPR can lead to balancedness. Third, the current generalization error analysis of OPR is not directly on the regularizer. In this paper, we address these three issues by (1) seeking convex relaxations of the original nonconvex problems so that the global optimal is guaranteed to be achievable; (2) providing a formal analysis on OPR's capability of promoting balancedness; (3)  providing a theoretical analysis that directly reveals the relationship between OPR and generalization performance. Experiments on various datasets demonstrate that our convex methods are more effective in promoting balancedness, compactness, and generalization, and are computationally more efficient, compared with the nonconvex methods.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Peking University', 'Petuum Inc. and CMU']"
2018,Comparison-Based Random Forests,"Siavash Haghiri, Damien Garreau, Ulrike von Luxburg",https://icml.cc/Conferences/2018/Schedule?showEvent=1979,"Assume we are given a set of items from a general metric space, but we neither have access to the representation of the data nor to the distances between data points. Instead, suppose that we can actively choose a triplet of items (A, B, C) and ask an oracle whether item A is closer to item B or to item C. In this paper, we propose a novel random forest algorithm for regression and classification that relies only on such triplet comparisons. In the theory part of this paper, we establish sufficient conditions for the consistency of such a forest. In a set of comprehensive experiments, we then demonstrate that the proposed random forest is efficient both for classification and regression. In particular, it is even competitive with other methods that have direct access to the metric representation of the data.
","['University of Tübingen', 'Max Planck Institute', 'University of Tübingen']"
2018,A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization,"Robin Vogel, Aurélien Bellet, Stéphan Clémençon",https://icml.cc/Conferences/2018/Schedule?showEvent=2118,"The performance of many machine learning techniques depends on the choice of an appropriate similarity or distance measure on the input space. Similarity learning (or metric learning) aims at building such a measure from training data so that observations with the same (resp. different) label are as close (resp. far) as possible. In this paper, similarity learning is investigated from the perspective of pairwise bipartite ranking, where the goal is to rank the elements of a database by decreasing order of the probability that they share the same label with some query data point, based on the similarity scores. A natural performance criterion in this setting is pointwise ROC optimization: maximize the true positive rate under a fixed false positive rate. We study this novel perspective on similarity learning through a rigorous probabilistic framework. The empirical version of the problem gives rise to a constrained optimization formulation involving U-statistics, for which we derive universal learning rates as well as faster rates under a noise assumption on the data distribution. We also address the large-scale setting by analyzing the effect of sampling-based approximations. Our theoretical results are supported by illustrative numerical experiments.
","['Télécom ParisTech', 'INRIA', 'Télécom ParisTech']"
2018,Provable Variable Selection for Streaming Features,"Jing Wang, Jie Shen, Ping Li",https://icml.cc/Conferences/2018/Schedule?showEvent=2352,"In large-scale machine learning applications and high-dimensional statistics, it is ubiquitous to address a considerable number of features among which many are redundant. As a remedy, online feature selection has attracted increasing attention in recent years. It sequentially reveals features and evaluates the importance of them. Though online feature selection has proven an elegant methodology, it is usually challenging to carry out a rigorous theoretical characterization. In this work, we propose a provable online feature selection algorithm that utilizes the online leverage score. The selected features are then fed to $k$-means clustering, making the clustering step memory and computationally efficient. We prove that with high probability, performing $k$-means clustering based on the selected feature space does not deviate far from the optimal clustering using the original data. The empirical results on real-world data sets demonstrate the effectiveness of our algorithm.","['Cornell University', 'Stevens Institute of Technology', 'Rugters University']"
2018,Out-of-sample extension of graph adjacency spectral embedding,"Keith Levin, Fred Roosta, Michael Mahoney, Carey Priebe",https://icml.cc/Conferences/2018/Schedule?showEvent=1937,"Many popular dimensionality reduction procedures have out-of-sample extensions, which allow a practitioner to apply a learned embedding to observations not seen in the initial training sample. In this work, we consider the problem of obtaining an out-of-sample extension for the adjacency spectral embedding, a procedure for embedding the vertices of a graph into Euclidean space. We present two different approaches to this problem, one based on a least-squares objective and the other based on a maximum-likelihood formulation. We show that if the graph of interest is drawn according to a certain latent position model called a random dot product graph, then both of these out-of-sample extensions estimate the true latent position of the out-of-sample vertex with the same error rate. Further, we prove a central limit theorem for the least-squares-based extension, showing that the estimate is asymptotically normal about the truth in the large-graph limit.
","['University of Michigan', 'University of Queensland', 'UC Berkeley', 'Johns Hopkins University']"
2018,Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers,"Yao Ma, Alex Olshevsky, Csaba Szepesvari, Venkatesh Saligrama",https://icml.cc/Conferences/2018/Schedule?showEvent=2196,"We consider worker skill estimation for the single coin Dawid-Skene crowdsourcing model. In practice skill-estimation is challenging because worker assignments are sparse and irregular due to the arbitrary, and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix completion problem, where the observed components correspond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills identifiable if and only if the sampling matrix (observed components) is irreducible and aperiodic. We then propose an efficient gradient descent scheme and show that skill estimates converges to the desired global optima for such sampling matrices. Our proof is original and the results are surprising in light of the fact that even the weighted rank-one matrix factorization problem is NP hard in general. Next we derive sample complexity bounds for the noisy case in terms of spectral properties of the signless Laplacian of the sampling matrix. Our proposed scheme achieves state-of-art performance on a number of real-world datasets.
","['Boston University', 'Boston University', 'Deepmind', 'Boston University']"
2018,Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow,"Xiao Zhang, Simon Du, Quanquan Gu",https://icml.cc/Conferences/2018/Schedule?showEvent=1951,"We revisit the inductive matrix completion problem that aims to recover a rank-$r$ matrix with ambient dimension $d$ given $n$ features as the side prior information. The goal is to make use of the known $n$ features to reduce sample and computational complexities. We present and analyze a new gradient-based non-convex optimization algorithm that converges to the true underlying matrix at a linear rate with sample complexity only linearly depending on $n$ and logarithmically depending on $d$. To the best of our knowledge, all previous algorithms either have a quadratic dependency on the number of features in sample complexity or a sub-linear computational convergence rate. In addition, we provide experiments on both synthetic and real world data to demonstrate the effectiveness of our proposed algorithm.","['University of Virginia', 'Carnegie Mellon University', 'UCLA']"
2018,DCFNet: Deep Neural Network with Decomposed Convolutional Filters,"Qiang Qiu, Xiuyuan Cheng, robert Calderbank, Guillermo Sapiro",https://icml.cc/Conferences/2018/Schedule?showEvent=2056,"Filters in a Convolutional Neural Network (CNN) contain model parameters learned from enormous amounts of data. In this paper,  we suggest to decompose convolutional filters in CNN as a truncated expansion with pre-fixed bases, namely the Decomposed Convolutional Filters network (DCFNet), where the expansion coefficients remain learned from data. Such a structure not only reduces the number of trainable parameters and computation, but also imposes filter regularity by bases truncation. Through extensive experiments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model parameters, particularly with Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we analyze the representation stability of DCFNet with respect to input variations, and prove representation stability under generic assumptions on the expansion coefficients. The analysis is consistent with the empirical observations.
","['Duke University', 'Duke University', 'Duke University', 'Duke University']"
2018,Optimization Landscape and Expressivity of Deep CNNs,"Quynh Nguyen, Matthias Hein",https://icml.cc/Conferences/2018/Schedule?showEvent=2011,"We analyze the loss landscape and expressiveness of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a ``wide'' layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we show that almost every critical point of the empirical loss is a global minimum with zero training error. Our analysis suggests that both depth and width are very important in deep learning. While depth brings more representational power and allows the network to learn high level features, width smoothes the optimization landscape of the loss function in the sense that a sufficiently wide network  has a well-behaved loss surface with almost no bad local minima.
","['Saarland University', 'University of Tuebingen']"
2018,Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF),"Trefor Evans, Prasanth B Nair",https://icml.cc/Conferences/2018/Schedule?showEvent=2139,"We introduce a kernel approximation strategy that enables computation of the Gaussian process log marginal likelihood and all hyperparameter derivatives in O(p) time. Our GRIEF kernel consists of p eigenfunctions found using a Nyström approximation from a dense Cartesian product grid of inducing points. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor products, computational complexity of the training procedure can be practically independent of the number of inducing points. This allows us to use arbitrarily many inducing points to achieve a globally accurate kernel approximation, even in high-dimensional problems. The fast likelihood evaluation enables type-I or II Bayesian inference on large-scale datasets. We benchmark our algorithms on real-world problems with up to two-million training points and 10^33 inducing points.
","['University of Toronto', 'University of Toronto']"
2018,Learning in Integer Latent Variable Models with Nested Automatic Differentiation,"Daniel Sheldon, Kevin Winner, Debora Sujono",https://icml.cc/Conferences/2018/Schedule?showEvent=2357,"We develop nested automatic differentiation (AD) algorithms for exact inference and learning in integer latent variable models. Recently, Winner, Sujono, and Sheldon showed how to reduce marginalization in a class of integer latent variable models to evaluating a probability generating function which contains many levels of nested high-order derivatives. We contribute faster and more stable AD algorithms for this challenging problem and a novel algorithm to compute exact gradients for learning. These contributions lead to significantly faster and more accurate learning algorithms, and are the first AD algorithms whose running time is polynomial in the number of levels of nesting.
","['University of Massachusetts Amherst', 'University of Massachusetts, Amherst', 'University of Massachusetts Amherst']"
2018,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,"Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Philip Isola, Kate Saenko, Alexei Efros, Trevor Darrell",https://icml.cc/Conferences/2018/Schedule?showEvent=2286,"Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains.  While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment.  Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and  avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.
","['UC Berkeley and Georgia Tech', 'UC Berkeley', 'UC Berkeley', 'MIT', 'UC Berkeley', 'Boston University', 'UC Berkeley', 'University of California at Berkeley']"
2018,Rectify Heterogeneous Models with Semantic Mapping,"Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, Zhi-Hua Zhou",https://icml.cc/Conferences/2018/Schedule?showEvent=1971,"On the way to the robust learner for real-world applications, there are still great challenges, including considering unknown environments with limited data. Learnware (Zhou; 2016) describes a novel perspective, and claims that learning models should have reusable and evolvable properties. We propose to Encode Meta InformaTion of features (EMIT), as the model specification for characterizing the changes, which grants the model evolvability to bridge heterogeneous feature spaces. Then, pre-trained models from related tasks can be Reused by our REctiFy via heterOgeneous pRedictor Mapping (REFORM}) framework. In summary, the pre-trained model is adapted to a new environment with different features, through model refining on only a small amount of training data in the current task. Experimental results over both synthetic and real-world tasks with diverse feature configurations validate the effectiveness and practical utility of the proposed framework.
","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University']"
2018,DVAE++: Discrete Variational Autoencoders with Overlapping Transformations,"Arash Vahdat, William Macready, Zhengbing Bian, Amir Khoshaman, Evgeny Andriyash",https://icml.cc/Conferences/2018/Schedule?showEvent=2273,"Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult. We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors. We derive a new variational bound to efficiently train with Boltzmann machine priors. Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables. Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and discrete variational autoencoders (Rolfe 2016).
","['Quadrant.ai, D-Wave', 'D-Wave', 'Quadrant.ai, D-Wave Systems Inc.', 'D-Wave systems Inc', 'D-Wave']"
2018,Iterative Amortized Inference,"Joe Marino, Yisong Yue, Stephan Mandt",https://icml.cc/Conferences/2018/Schedule?showEvent=2110,"Inference models are a key component in scaling variational inference to deep latent variable models, most notably as encoder networks in variational auto-encoders (VAEs). By replacing conventional optimization-based inference with a learned model, inference is amortized over data examples and therefore more computationally efficient. However, standard inference models are restricted to direct mappings from data to approximate posterior estimates. The failure of these models to reach fully optimized approximate posterior estimates results in an amortization gap. We aim toward closing this gap by proposing iterative inference models, which learn to perform inference optimization through repeatedly encoding gradients. Our approach generalizes standard inference models in VAEs and provides insight into several empirical findings, including top-down inference techniques. We demonstrate the inference optimization capabilities of iterative inference models and show that they outperform standard inference models on several benchmark data sets of images and text.
","['Caltech', 'Caltech', 'UC Irvine']"
2018,Blind Justice: Fairness with Encrypted Sensitive Attributes,"Niki Kilbertus, Adria Gascon, Matt Kusner, Michael Veale, Krishna Gummadi, Adrian Weller",https://icml.cc/Conferences/2018/Schedule?showEvent=1906,"Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes.
","['MPI Tübingen & Cambridge', 'The Alan Turing Institute / Warwick University', 'Alan Turing Institute', 'UCL', 'MPI-SWS', 'University of Cambridge, Alan Turing Institute']"
2018,Active Learning with Logged Data,"Songbai Yan, Kamalika Chaudhuri, Tara Javidi",https://icml.cc/Conferences/2018/Schedule?showEvent=1990,"We consider active learning with logged data, where labeled examples are drawn conditioned on a predetermined logging policy, and the goal is to learn a classifier on the entire population, not just conditioned on the logging policy. Prior work addresses this problem either when only logged data is available, or purely in a controlled random experimentation setting where the logged data is ignored. In this work, we combine both approaches to provide an algorithm that uses logged data to bootstrap and inform experimentation, thus achieving the best of both worlds. Our work is inspired by a connection between controlled random experimentation and active learning, and modifies existing disagreement-based active learning algorithms to exploit logged data.
","['University of California San Diego', 'University of California at San Diego', 'University of California San Diego']"
2018,A Reductions Approach to Fair Classification,"Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, Hanna Wallach",https://icml.cc/Conferences/2018/Schedule?showEvent=2361,"We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.
","['Microsoft Research', 'Yahoo Research', 'Microsoft Research', 'MSR', 'Microsoft Research']"
2018,Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness,"Michael Kearns, Seth Neel, Aaron Roth, Steven Wu",https://icml.cc/Conferences/2018/Schedule?showEvent=2241,"The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups.  Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values.  We thus consider  fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning --- which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice.  We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.
","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'Microsoft Research']"
2018,Bayesian Model Selection for Change Point Detection and Clustering,"othmane mazhar, Cristian R. Rojas, Inst. of Technology Carlo Fischione, Mohammad Reza Hesamzadeh",https://icml.cc/Conferences/2018/Schedule?showEvent=2308,"We address a generalization of change point detection with the purpose of detecting the change locations and the levels of clusters of a piecewise constant signal. Our approach is to model it as a nonparametric penalized least square model selection on a family of models indexed over the collection of partitions of the design points and propose a computationally efficient algorithm to approximately solve it. Statistically, minimizing such a penalized criterion yields an approximation to the maximum a-posteriori probability (MAP) estimator. The criterion is then analyzed and an oracle inequality is derived using a Gaussian concentration inequality. The oracle inequality is used to derive on one hand conditions for consistency and on the other hand an adaptive upper bound on the expected square risk of the estimator, which statistically motivates our approximation. Finally, we apply our algorithm to simulated data to experimentally validate the statistical guarantees and illustrate its behavior.
","['KTH Royal Institute of Technology', 'KTH Royal Institute of Technology', 'Royal Inst. of Technology, KTH', 'KTH Royal Institute of Technology']"
2018,A Unified Framework for Structured Low-rank Matrix Learning,"Pratik Kumar Jawanpuria, Bamdev Mishra",https://icml.cc/Conferences/2018/Schedule?showEvent=1930,"We consider the problem of learning a low-rank matrix, constrained to lie in a linear subspace, and introduce a novel factorization for modeling such matrices. A salient feature of the proposed factorization scheme is it decouples the low-rank and the structural constraints onto separate factors. We formulate the optimization problem on the  Riemannian spectrahedron manifold, where the Riemannian framework allows to develop computationally efficient conjugate gradient and trust-region algorithms. Experiments on problems such as standard/robust/non-negative matrix completion, Hankel matrix learning and multi-task learning demonstrate the efficacy of our approach.
","['Microsoft', 'Microsoft']"
2018,Firing Bandits: Optimizing Crowdfunding,"Lalit Jain, Kevin Jamieson",https://icml.cc/Conferences/2018/Schedule?showEvent=2462,"In this paper, we model the problem of optimizing crowdfunding platforms, such as the non-profit Kiva or for-profit KickStarter, as a variant of the multi-armed bandit problem. In our setting, Bernoulli arms emit no rewards until their cumulative number of successes over any number of trials exceeds a fixed threshold and then provides no additional reward for any additional trials - a process reminiscent to that of a neuron firing once it reaches the action potential and then saturates. In the spirit of an infinite armed bandit problem, the player can add new arms whose expected probability of success is drawn iid from an unknown distribution -- this endless supply of projects models the harsh reality that the number of projects seeking funding greatly exceeds the total capital available by lenders. Crowdfunding platforms naturally fall under this setting where the arms are potential projects, and their probability of success is the probability that a potential funder decides to fund it after reviewing it. The goal is to play arms (prioritize the display of projects on a webpage) to maximize the number of arms that reach the firing threshold (meet their goal amount) using as few total trials (number of impressions) as possible over all the played arms. We provide an algorithm for this setting and prove sublinear regret bounds.
","['University of Washington', 'University of Washington']"
2018,Multi-Fidelity Black-Box Optimization with Hierarchical Partitions,"Rajat Sen, kirthevasan kandasamy, Sanjay Shakkottai",https://icml.cc/Conferences/2018/Schedule?showEvent=2264,"Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function. Multi-fidelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost. A canonical example is that of hyper-parameter selection in a learning algorithm. The learning algorithm can be trained for fewer iterations -- this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till completion. We incorporate the multi-fidelity setup into the powerful framework of black-box optimization through hierarchical partitioning. We develop tree-search based multi-fidelity algorithms with theoretical guarantees on simple regret. We finally demonstrate the performance gains of our algorithms on both real and synthetic datasets.
","['University of Texas at Austin', 'CMU', 'University of Texas at Austin']"
2018,Compiling Combinatorial Prediction Games,Frederic Koriche,https://icml.cc/Conferences/2018/Schedule?showEvent=2374,"In online optimization, the goal is to iteratively choose solutions from a decision space, so as to minimize the average cost over time. As long as this decision space is described by combinatorial constraints, the problem is generally intractable. In this paper, we consider the paradigm of compiling the set of combinatorial constraints into a deterministic and Decomposable Negation Normal Form (dDNNF) circuit, for which the tasks of linear optimization and solution sampling take linear time. Based on this framework, we provide efficient characterizations of existing combinatorial prediction strategies, with a particular attention to mirror descent techniques. These strategies are compared on several real-world benchmarks for which the set of Boolean constraints is preliminarily compiled into a dDNNF circuit.
","['CRIL UMR CNRS 8188, Univ. Artois']"
2018,Rates of Convergence of Spectral Methods for Graphon Estimation,Jiaming Xu,https://icml.cc/Conferences/2018/Schedule?showEvent=2335,"This paper studies the problem of estimating the graphon function -- a generative mechanism for a class of random graphs that are useful approximations to real networks. Specifically, a graph of  $n$ vertices is generated such that each pair of two vertices  $i$ and $j$ are connected independently with probability $\rho_n \times f(x_i,x_j)$, where $x_i$ is the unknown $d$-dimensional  label of vertex $i$, $f$ is an unknown symmetric function, and $\rho_n$, assumed to be $\Omega(\log n/n)$, is a scaling parameter characterizing the graph sparsity. The task is to estimate graphon $f$ given the graph. Recent studies have identified the  minimax optimal estimation error rate for $d=1$. However, there exists a wide gap between the known error rates of polynomial-time estimators  and the minimax optimal error rate.  We improve on the previously known error rates of polynomial-time estimators,    by analyzing a spectral method, namely universal singular value thresholding (USVT) algorithm. When $f$ belongs to either H\""{o}lder or Sobolev space with smoothness  index $\alpha$, we show the error rates of USVT are at most $(n\rho)^{ -2 \alpha / (2\alpha+d)}$. These error rates approach the  minimax optimal error rate $\log (n\rho)/(n\rho)$ proved in prior work  for $d=1$, as  $\alpha$ increases, i.e., $f$ becomes smoother. Furthermore, when $f$ is analytic with infinitely many times differentiability, we show the error rate of USVT is  at most $\log^d (n\rho)/(n\rho)$.  When $f$ is a step function which corresponds to the stochastic block model with $k$ blocks for some $k$, the error rate of USVT is at most $k/(n\rho)$, which is larger than the  minimax optimal error rate by at most a multiplicative factor $k/\log k$. This coincides with the computational gap observed in community detection. A key ingredient of our analysis is  to derive the eigenvalue decaying rate of the edge probability matrix using piecewise polynomial approximations of the graphon function $f$.",['Duke University']
2018,Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions,"Karren Yang, Abigail Katoff, Caroline Uhler",https://icml.cc/Conferences/2018/Schedule?showEvent=2097,"We consider the problem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser & Buhlmann (2012) previously characterized the identifiability of causal DAGs under perfect interventions, which eliminate dependencies between targeted variables and their direct causes. In this paper, we extend these identifiability results to general interventions, which may modify the dependencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily perfect) intervention experiments. We also propose the first provably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.
","['Massachusetts Institute of Technology', 'MIT', 'Massachusetts Institute of Technology']"
2018,Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models,"Raj Agrawal, Caroline Uhler, Tamara Broderick",https://icml.cc/Conferences/2018/Schedule?showEvent=2472,"Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationships. However, traditional methods often fail in modern applications, which exhibit a larger number of observed variables than data points. The resulting uncertainty about the underlying network as well as the desire to incorporate prior information recommend a Bayesian approach to learning the BN, but the highly combinatorial structure of BNs poses a striking challenge for inference. The current state-of-the-art methods such as order MCMC are faster than previous methods but prevent the use of many natural structural priors and still have running time exponential in the maximum indegree of the true directed acyclic graph (DAG) of the BN. We here propose an alternative posterior approximation based on the observation that, if we incorporate empirical conditional independence tests, we can focus on a high-probability DAG associated with each order of the vertices. We show that our method allows the desired flexibility in prior specification, removes timing dependence on the maximum indegree, and yields provably good posterior approximations; in addition, we show that it achieves superior accuracy, scalability, and sampler mixing on several datasets.
","['MIT', 'Massachusetts Institute of Technology', 'MIT']"
2018,StrassenNets: Deep Learning with a Multiplication Budget,"Michael Tschannen, Aran Khanna, Animashree Anandkumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2167,"A large fraction of the arithmetic operations required to evaluate deep neural networks (DNNs) consists of matrix multiplications, in both convolution and fully connected layers. We perform end-to-end learning of low-cost approximations of matrix multiplications in DNN layers by casting matrix multiplications as 2-layer sum-product networks (SPNs) (arithmetic circuits) and learning their (ternary) edge weights from data. The SPNs disentangle multiplication and addition operations and enable us to impose a budget on the number of multiplication operations. Combining our method with knowledge distillation and applying it to image classification DNNs (trained on ImageNet) and language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction in number of multiplications (over 99.5%) while maintaining the predictive performance of the full-precision models. Finally, we demonstrate that the proposed framework is able to rediscover Strassen's matrix multiplication algorithm, learning to multiply $2 \times 2$ matrices using only 7 multiplications instead of 8.","['ETH Zurich', 'Dolores Technologies', 'Caltech']"
2018,Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace,"Yoonho Lee, Seungjin Choi",https://icml.cc/Conferences/2018/Schedule?showEvent=2202,"Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While previous such methods have been successful in meta-learning tasks, they resort to simple gradient descent during meta-testing. Our primary contribution is the {\em MT-net}, which enables the meta-learner to learn on each layer's activation space a subspace that the task-specific learner performs gradient descent on. Additionally, a task-specific learner of an {\em MT-net} performs gradient descent with respect to a meta-learned distance metric, which warps the activation space to be more sensitive to task identity. We demonstrate that the dimension of this learned subspace reflects the complexity of the task-specific learner's adaptation task, and also that our model is less sensitive to the choice of initial learning rates than previous gradient-based meta-learning methods. Our method achieves state-of-the-art or comparable performance on few-shot classification and regression tasks.
","['Pohang University of Science and Techonology', 'POSTECH']"
2018,Candidates vs. Noises Estimation for Large Multi-Class Classification Problem,"Lei Han, Yiheng Huang, Tong Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=2130,"This paper proposes a method for multi-class classification problems, where the number of classes K is large. The method, referred to as Candidates vs. Noises Estimation (CANE), selects a small subset of candidate classes and samples the remaining classes. We show that CANE is always consistent and computationally efficient. Moreover, the resulting estimator has low statistical variance approaching that of the maximum likelihood estimator, when the observed label belongs to the selected candidates with high probability. In practice, we use a tree structure with leaves as classes to promote fast beam search for candidate selection. We further apply the CANE method to estimate word probabilities in learning large neural language models. Extensive experimental results show that CANE achieves better prediction accuracy over the Noise-Contrastive Estimation (NCE), its variants and a number of the state-of-the-art tree classifiers, while it gains significant speedup compared to standard O(K) methods.
","['Tencent AI Lab', 'Tencent AI Lab', 'Tecent AI Lab']"
2018,"CRAFTML, an Efficient Clustering-based Random Forest for Extreme Multi-label Learning","Wissam Siblini, Frank Meyer, Pascale Kuntz",https://icml.cc/Conferences/2018/Schedule?showEvent=2216,"Extreme Multi-label Learning (XML) considers large sets of items described by a number of labels that can exceed one million. Tree-based methods, which hierarchically partition the problem into small scale sub-problems, are particularly promising in this context to reduce the learning/prediction complexity and to open the way to parallelization. However, the current best approaches do not exploit tree randomization which has shown its efficiency in random forests and they resort to complex partitioning strategies. To overcome these limits, we here introduce a new random forest based algorithm with a very fast partitioning approach called CRAFTML. Experimental comparisons on nine datasets from the XML literature show that it outperforms the other tree-based approaches. Moreover with a parallelized implementation reduced to five cores, it is competitive with the best state-of-the-art methods which run on one hundred-core machines.
","['Orange Labs', 'Orange Labs Lannion', 'LS2N']"
2018,Overcoming Catastrophic Forgetting with Hard Attention to the Task,"Joan Serrà, Didac Suris, Marius Miron, Alexandros Karatzoglou",https://icml.cc/Conferences/2018/Schedule?showEvent=2155,"Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.
","['Telefónica Research, Barcelona', 'Universitat Politecnica de Catalunya', 'Joint Research Centre of the European Commission', 'Telefonica']"
2018,Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions,"Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, Yingyan Lin",https://icml.cc/Conferences/2018/Schedule?showEvent=2219,"Many existing compression approaches have been focused and evaluated on convolutional neural networks (CNNs) where fully-connected layers contain the most parameters (e.g., LeNet and AlexNet). However, the current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the parameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, convolutional layers always account for most energy consumption in run time. To this end, this paper investigates the relatively less-explored direction of compressing convolutional layers in deep CNNs. We introduce a novel spectrally relaxed k -means regularization, that tends to approximately make hard assignments of convolutional layer weights to K learned cluster centers during re-training. Compression is then achieved through weight-sharing, by only recording K cluster centers and weight assignment indexes. Our proposed pipeline, termed Deep k -Means, has well-aligned goals between re-training and compression stages. We further propose an improved set of metrics to estimate energy consumption of CNN hardware implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual hardware measurements. We have evaluated Deep k -Means in compressing several CNN models in terms of both compression ratio and energy consumption reduction, observing promising results without incurring accuracy loss.
","['Texas A&M University', 'Rice University', 'Texas A&M University', 'Texas A&M University', 'Rice University', 'Rice University']"
2018,Efficient Neural Audio Synthesis,"Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aäron van den Oord, Sander Dieleman, koray kavukcuoglu",https://icml.cc/Conferences/2018/Schedule?showEvent=2333,"Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating desired samples. Efficient sampling for this class of models at the cost of little to no loss in quality has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a  dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24 kHz 16-bit audio 4 times faster than real time on a GPU.  Secondly, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds past sparsity levels of more than 96%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile phone CPU in real time. Finally, we describe a new dependency scheme for sampling that lets us trade a constant number of non-local, distant dependencies for the ability to generate samples in batches. The Batch WaveRNN produces 8 samples per step without loss of quality and offers orthogonal ways of further increasing sampling efficiency.
","['Google Brain Amsterdam', 'Google', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', '', 'Google Deepmind', 'DeepMind', 'DeepMind']"
2018,Born Again Neural Networks,"Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, Anima Anandkumar",https://icml.cc/Conferences/2018/Schedule?showEvent=2404,"Knowledge Distillation (KD) consists of  transferring ``knowledge'' from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components  of KD, demonstrating the effect of the teacher outputs on both predicted and non-predicted classes.
","['University of Southern California', 'Carnegie Mellon University', 'ETH Zurich', 'University of Southern California', 'Amazon']"
2018,Adaptive Sampled Softmax with Kernel Based Sampling,"Guy Blanc, Steffen Rendle",https://icml.cc/Conferences/2018/Schedule?showEvent=1953,"Softmax is the most commonly used output function for multiclass problems and is widely used in areas such as vision, natural language processing, and recommendation. A softmax model has linear costs in the number of classes which makes it too expensive for many real-world problems. A common approach to speed up training involves sampling only some of the classes at each training step. It is known that this method is biased and that the bias increases the more the sampling distribution deviates from the output distribution. Nevertheless, almost all recent work uses simple sampling distributions that require a large sample size to mitigate the bias. In this work, we propose a new class of kernel based sampling methods and develop an efficient sampling algorithm. Kernel based sampling adapts to the model as it is trained, thus resulting in low bias. It can also be easily applied to many models because it relies only on the model's last hidden layer. We empirically study the trade-off of bias, sampling distribution and sample size and show that kernel based sampling results in low bias with few samples.
","['Stanford University', 'Google']"
2018,JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets,"Yunchen Pu, Shuyang Dai, Zhe Gan, Weiyao Wang, Guoyin Wang, Yizhe Zhang, Ricardo Henao, Lawrence Carin",https://icml.cc/Conferences/2018/Schedule?showEvent=2161,"A new generative adversarial network is developed for joint distribution matching.Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain.The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning.From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented.
","['Duke', 'Duke University', 'Duke University', 'Duke', 'Duke University', 'Duke University', 'Duke University', 'Duke']"
2018,Autoregressive Quantile Networks for Generative Modeling,"Georg Ostrovski, Will Dabney, Remi Munos",https://icml.cc/Conferences/2018/Schedule?showEvent=2416,"We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception scores, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.
","['DeepMind', 'DeepMind', 'DeepMind']"
2018,On the Power of Over-parametrization in Neural Networks with Quadratic Activation,"Simon Du, Jason Lee",https://icml.cc/Conferences/2018/Schedule?showEvent=1923,"We provide new theoretical insights on why over-parametrization is effective in learning neural networks. For a $k$ hidden node shallow network with quadratic activation and $n$ training  data points, we show as long as $ k \ge \sqrt{2n}$, over-parametrization enables local search algorithms to find a  \emph{globally} optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when $k\ge \sqrt{2n}$, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks.","['Carnegie Mellon University', 'University of Southern California']"
2018,On the Limitations of First-Order Approximation in GAN Dynamics,"Jerry Li, Aleksander Madry, John Peebles, Ludwig Schmidt",https://icml.cc/Conferences/2018/Schedule?showEvent=2342,"While Generative Adversarial Networks (GANs) have demonstrated promising performance on multiple vision tasks, their learning dynamics are not yet well understood, both in theory and in practice. To address this issue, we study GAN dynamics in a simple yet rich parametric model that exhibits several of the common problematic convergence behaviors such as vanishing gradients, mode collapse, and diverging or oscillatory behavior. In spite of the non-convex nature of our model, we are able to perform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an optimal discriminator provably converges, while first order approximations of the discriminator steps lead to unstable GAN dynamics and mode collapse. Our result suggests that using first order discriminator steps (the de-facto standard in most existing GAN setups) might be one of the factors that makes GAN training challenging in practice.
","['MIT', 'MIT', 'MIT', 'UC Berkeley']"
2018,Learning to Explore via Meta-Policy Gradient,"Tianbing Xu, Qiang Liu, Liang Zhao, Jian Peng",https://icml.cc/Conferences/2018/Schedule?showEvent=1939,"The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration methods are mostly based on adding noise to the on-going actor policy and can only explore \emph{local} regions close to what the actor policy dictates.  In this work, we develop a simple meta-policy gradient algorithm that allows us to adaptively learn the exploration policy in DDPG. Our algorithm allows us to train flexible exploration behaviors that are independent of the actor policy, yielding a \emph{global exploration} that significantly speeds up the learning process. With an extensive study, we show that our method significantly improves the sample-efficiency of DDPG on a variety of reinforcement learning continuous control tasks.
","['Baidu Research, USA', 'UT Austin', 'Baidu Research USA', 'UIUC']"
2018,Mean Field Multi-Agent Reinforcement Learning,"Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, Jun Wang",https://icml.cc/Conferences/2018/Schedule?showEvent=2458,"Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the  agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.
","['University College London', 'UCL', 'University College London', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'UCL']"
2018,Online Linear Quadratic Control,"Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, Kunal Talwar",https://icml.cc/Conferences/2018/Schedule?showEvent=2142,"We study the problem of controlling linear time-invariant systems with known noisy dynamics and adversarially chosen quadratic losses.  We present the first efficient online learning algorithms in this setting that guarantee $O(\sqrt{T})$ regret under mild assumptions, where $T$ is the time horizon.  Our algorithms rely on a novel SDP relaxation for the steady-state distribution of the system.  Crucially, and in contrast to previously proposed relaxations, the feasible solutions of our SDP all correspond to ``strongly stable'' policies that mix exponentially fast to a steady state.","['Google Inc.', 'Google', 'Google Brain', 'Google', 'Google', 'Google']"
2018,Online Learning with Abstention,"Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, Scott Yang",https://icml.cc/Conferences/2018/Schedule?showEvent=2113,"We present an extensive study of a key problem in online learning where the learner can opt to abstain from making a prediction, at a certain cost. In the adversarial setting, we show how existing online algorithms and guarantees can be adapted to this problem. In the stochastic setting, we first point out a bias problem that limits the straightforward extension of algorithms such as UCB-N to this context. Next, we give a new algorithm, UCB-GT, that exploits historical data and time-varying feedback graphs. We show that this algorithm benefits from more favorable regret guarantees than a natural extension of UCB-N . We further report the results of a series of experiments demonstrating that UCB-GT largely outperforms that extension of UCB-N, as well as other standard baselines.
","['Google Research', 'Google Research', 'INRIA', 'Courant Institute and Google Research', 'D. E. Shaw & Co.']"
2018,Celer: a Fast Solver for the Lasso with Dual Extrapolation,"Mathurin MASSIAS, Joseph Salmon, Alexandre Gramfort",https://icml.cc/Conferences/2018/Schedule?showEvent=2132,"Convex sparsity-inducing regularizations are ubiquitous in high-dimensional machine learning, but solving the resulting optimization problems can be slow. To accelerate solvers, state-of-the-art approaches consist in reducing the size of the optimization problem at hand. In the context of regression, this can be achieved either by discarding irrelevant features (screening techniques) or by prioritizing features likely to be included in the support of the solution (working set techniques). Duality comes into play at several steps in these techniques. Here, we propose an extrapolation technique starting from a sequence of iterates in the dual that leads to the construction of improved dual points. This enables a tighter control of optimality as used in stopping criterion, as well as better screening performance of Gap Safe rules. Finally, we propose a working set strategy based on an aggressive use of Gap Safe screening rules. Thanks to our new dual point construction, we show significant computational speedups on multiple real-world problems.
","['INRIA', 'Telecom ParisTech', 'Inria']"
2018,Cut-Pursuit Algorithm for Regularizing Nonsmooth Functionals with Graph Total Variation,"Hugo Raguet, loic landrieu",https://icml.cc/Conferences/2018/Schedule?showEvent=2392,"We present an extension of the cut-pursuit algorithm, introduced by Landrieu and Obozinski (2017), to the graph total-variation regularization of functions with a separable nondifferentiable part. We propose a modified algorithmic scheme as well as adapted proofs of convergence. We also present a heuristic approach for handling the cases in which the values associated to each vertex of the graph are multidimensional. The performance of our algorithm, which we demonstrate on difficult, ill-conditioned large-scale inverse and learning problems, is such that it may in practice extend the scope of application of the total-variation regularization.
","['LIVE (CNRS)', 'IGN']"
2018,Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data,"Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip Bachman, Aaron Courville",https://icml.cc/Conferences/2018/Schedule?showEvent=2408,"Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.
","['Element AI', 'University of Montreal', 'Microsoft Research', 'Microsoft Research', 'University of Montreal']"
2018,Mixed batches and symmetric discriminators for GAN training,"Thomas LUCAS, Corentin Tallec, Yann Ollivier, Jakob Verbeek",https://icml.cc/Conferences/2018/Schedule?showEvent=2234,"Generative adversarial networks (GANs) are pow- erful generative models based on providing feed- back to a generative network via a discriminator network. However, the discriminator usually as- sesses individual samples. This prevents the dis- criminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution. We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch. The latter score does not depend on the order of samples in a batch. Rather than learning this invariance, we introduce a generic permutation-invariant discriminator ar- chitecture. This architecture is provably a uni- versal approximator of all symmetric functions. Experimentally, our approach reduces mode col- lapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.
","['Inria', 'INRIA', 'Facebook Artificial Intelligence Research', 'INRIA']"
2018,An Algorithmic Framework of Variable Metric Over-Relaxed Hybrid Proximal Extra-Gradient Method,"Li Shen, Peng Sun, Yitong Wang, Wei Liu, Tong Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=1885,"We propose a novel algorithmic framework of Variable Metric Over-Relaxed  Hybrid Proximal Extra-gradient (VMOR-HPE) method with a global convergence guarantee for the maximal monotone operator inclusion problem. Its iteration complexities and local linear convergence rate are provided, which theoretically demonstrate that a large over-relaxed step-size contributes to accelerating the proposed VMOR-HPE as a byproduct. Specifically, we find that a large class of primal and primal-dual operator splitting algorithms are all special cases of VMOR-HPE. Hence, the proposed framework offers a new insight into these operator splitting algorithms. In addition, we apply VMOR-HPE to the Karush-Kuhn-Tucker (KKT) generalized equation of linear equality constrained multi-block composite convex optimization, yielding a new algorithm, namely nonsymmetric Proximal Alternating Direction Method of Multipliers with a preconditioned Extra-gradient step in which the preconditioned metric is generated by a blockwise Barzilai-Borwein line search technique (PADMM-EBB). We also establish iteration complexities of PADMM-EBB in terms of the KKT residual. Finally, we apply PADMM-EBB to handle the nonnegative dual graph regularized low-rank representation problem. Promising results on synthetic and real datasets corroborate the efficacy of PADMM-EBB.
","['Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab', 'Tecent AI Lab']"
2018,Learning Hidden Markov Models from Pairwise Co-occurrences with Application to Topic Modeling,"Kejun Huang, Xiao Fu, Nicholas Sidiropoulos",https://icml.cc/Conferences/2018/Schedule?showEvent=2231,"We present a new algorithm for identifying the transition and emission probabilities of a hidden Markov model (HMM) from the emitted data. Expectation-maximization becomes computationally prohibitive for long observation records, which are often required for identification. The new algorithm is particularly suitable for cases where the available sample size is large enough to accurately estimate second-order output probabilities, but not higher-order ones. We show that if one is only able to obtain a reliable estimate of the pairwise co-occurrence probabilities of the emissions, it is still possible to uniquely identify the HMM if the emission probability is \emph{sufficiently scattered}. We apply our method to hidden topic Markov modeling, and demonstrate that we can learn topics with higher quality if documents are modeled as observations of HMMs sharing the same emission (topic) probability, compared to the simple but widely used bag-of-words model.
","['University of Minnesota', 'Oregon State University', 'University of Virginia']"
2018,DRACO: Byzantine-resilient Distributed Training via Redundant Gradients,"Lingjiao Chen, Hongyi Wang, Zachary Charles, Dimitris Papailiopoulos",https://icml.cc/Conferences/2018/Schedule?showEvent=2474,"Distributed model training is vulnerable to byzantine system failures and adversarial compute nodes, i.e., nodes that use malicious updates to corrupt the global model stored at a parameter server (PS). To guarantee some form of robustness, recent work suggests using variants of the geometric median as an aggregation rule, in place of gradient averaging. Unfortunately, median-based rules can incur a prohibitive computational overhead in large-scale settings, and their convergence guarantees often require strong assumptions. In this work, we present DRACO, a scalable framework for robust distributed training that uses ideas from coding theory. In DRACO, each compute node evaluates redundant gradients that are used by the parameter server to eliminate the effects of adversarial updates. DRACO comes with problem-independent robustness guarantees, and the model that it trains is identical to the one trained in the adversary-free setup. We provide extensive experiments on real datasets and distributed setups across a variety of large-scale models, where we show that DRACO is several times, to orders of magnitude faster than median-based approaches.
","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'ECE at University of Wisconsin-Madison']"
2018,Communication-Computation Efficient Gradient Coding,"Min Ye, Emmanuel Abbe",https://icml.cc/Conferences/2018/Schedule?showEvent=1994,"This paper develops coding techniques to reduce the running time of distributed learning tasks. It characterizes the fundamental tradeoff to compute gradients in terms of three parameters: computation load, straggler tolerance and communication cost. It further gives an explicit coding scheme that achieves the optimal tradeoff based on recursive polynomial constructions, coding both across data subsets and vector components. As a result, the proposed scheme allows to minimize the running time for gradient computations. Implementations are made on Amazon EC2 clusters using Python with mpi4py package. Results show that the proposed scheme maintains the same generalization error while reducing the running time by $32\%$ compared to uncoded schemes and $23\%$ compared to prior coded schemes focusing only on stragglers (Tandon et al., ICML 2017).","['Princeton University', '']"
2018,"Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities and Spectral Clustering","Pan Li, Olgica Milenkovic",https://icml.cc/Conferences/2018/Schedule?showEvent=2044,"We introduce submodular hypergraphs, a family of hypergraphs that have different submodular weights associated with different cuts of hyperedges. Submodular hypergraphs arise in cluster- ing applications in which higher-order structures carry relevant information. For such hypergraphs, we define the notion of p-Laplacians and derive corresponding nodal domain theorems and k-way Cheeger inequalities. We conclude with the description of algorithms for computing the spectra of 1- and 2-Laplacians that constitute the basis of new spectral hypergraph clustering methods.
","['University of Illinois Urbana-Champaign', 'University of Illinois UC']"
2018,SMAC: Simultaneous Mapping and Clustering Using Spectral Decompositions,"chandrajit bajaj, Tingran Gao, Zihang He, Qixing Huang, Zhenxiao Liang",https://icml.cc/Conferences/2018/Schedule?showEvent=1899,"We introduce a principled approach for \emph{simultaneous mapping and clustering} (SMAC) for establishing consistent maps across heterogeneous object collections (e.g., 2D images or 3D shapes). Our approach takes as input a heterogeneous object collection and a set of maps computed between some pairs of objects, and outputs a homogeneous object clustering together with a new set of maps possessing optimal intra- and inter-cluster consistency.  Our approach is based on the spectral decomposition of a data matrix storing all pairwise maps in its blocks. We additionally provide tight theoretical guarantees on the exactness of SMAC under established noise models. We also demonstrate the usefulness of the approach on synthetic and real datasets.
","['University of Texas at Austin', 'University of Chicago', 'Tsinghua University', 'The University of Texas at Austin', 'Tsinghua University']"
2018,On Nesting Monte Carlo Estimators,"Tom Rainforth, Rob Cornish, Hongseok Yang, andrew warrington, Frank Wood",https://icml.cc/Conferences/2018/Schedule?showEvent=1874,"Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a separate, nested, estimation. We investigate the statistical implications of nesting MC estimators, including cases of multiple levels of nesting, and establish the conditions under which they converge. We derive corresponding rates of convergence and provide empirical evidence that these rates are observed in practice. We further establish a number of pitfalls that can arise from naive nesting of MC estimators, provide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested expectation problems into single expectations, leading to improved convergence rates. We demonstrate the applicability of our work by using our results to develop a new estimator for discrete Bayesian experimental design problems and derive error bounds for a class of variational objectives.
","['University of Oxford', 'Oxford', 'KAIST', 'University of Oxford', 'University of Oxford']"
2018,Stein Variational Gradient Descent Without Gradient,"Jun Han, Qiang Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=2105,"Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the introduced bias by re-weighting the gradients in a proper form.  We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits all the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and further, propose an annealed GF-SVGD that consistently outperforms a number of recent advanced gradient-free MCMC methods in our empirical studies.
","['Dartmouth College', 'UT Austin']"
2018,Detecting non-causal artifacts in multivariate linear regression models,"Dominik Janzing, Bernhard Schölkopf",https://icml.cc/Conferences/2018/Schedule?showEvent=2007,"We consider linear models where d potential causes X1,...,Xd are correlated with one target quantity Y and propose a method to infer whether the association is causal or whether  it is an artifact caused by overfitting or hidden common causes. We employ the idea that in the former case the vector of regression coefficients has `generic' orientation relative to the covariance matrix Sigma{XX}  of X. Using an ICA based model for confounding, we show that both confounding and overfitting yield regression vectors  that concentrate mainly in the space of low eigenvalues of Sigma{XX}.
","['Amazon Research Tübingen', 'MPI for Intelligent Systems Tübingen, Germany']"
2018,The Hierarchical Adaptive Forgetting Variational Filter,Vincent Moens,https://icml.cc/Conferences/2018/Schedule?showEvent=2185,"A common problem in Machine Learning and statistics consists in detecting whether the current sample in a stream of data belongs to the same distribution as previous ones, is an isolated outlier or inaugurates a new distribution of data. We present a hierarchical Bayesian algorithm that aims at learning a time-specific approximate posterior distribution of the parameters describing the distribution of the data observed. We derive the update equations of the variational parameters of the approximate posterior at each time step for models from the exponential family, and show that these updates find interesting correspondents in Reinforcement Learning (RL). In this perspective, our model can be seen as a hierarchical RL algorithm that learns a posterior distribution according to a certain stability confidence that is, in turn, learned according to its own stability confidence. Finally, we show some applications of our generic model, first in a RL context, next with an adaptive Bayesian Autoregressive model, and finally in the context of Stochastic Gradient Descent optimization.
",['Université catholique de Louvain']
2018,Junction Tree Variational Autoencoder for Molecular Graph Generation,"Wengong Jin, Regina Barzilay, Tommi Jaakkola",https://icml.cc/Conferences/2018/Schedule?showEvent=1903,"We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.
","['MIT Computer Science and Artificial Intelligence Laboratory', 'MIT CSAIL', 'MIT']"
2018,Semi-Amortized Variational Autoencoders,"Yoon Kim, Sam Wiseman, Andrew Miller, David Sontag, Alexander Rush",https://icml.cc/Conferences/2018/Schedule?showEvent=1981,"Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.
","['Harvard University', 'Harvard University', 'Harvard', 'Massachusetts Institute of Technology', 'Harvard University']"
2018,Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits,"Huasen Wu, Xueying Guo, Xin Liu",https://icml.cc/Conferences/2018/Schedule?showEvent=1996,"In this paper, we propose and study opportunistic bandits - a new variant of bandits where the regret of pulling a suboptimal arm varies under different environmental conditions, such as network load or produce price. When the load/price is low, so is the cost/regret of pulling a suboptimal arm (e.g., trying a suboptimal network configuration). Therefore, intuitively, we could explore more when the load/price is low and exploit more when the load/price is high. Inspired by this intuition, we propose  an Adaptive Upper-Confidence-Bound (AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff for opportunistic bandits. We prove that AdaUCB achieves O(log T) regret with a smaller coefficient than the traditional UCB algorithm. Furthermore, AdaUCB achieves O(1) regret with respect to T if the exploration cost is zero when the load level is below a certain threshold. Last, based on both synthetic data and real-world traces, experimental results show that AdaUCB significantly outperforms other bandit algorithms, such as UCB and TS (Thompson Sampling), under large load/price fluctuations.
","['Twitter', 'University of California Davis', 'University of California, Davis']"
2018,Semiparametric Contextual Bandits,"Akshay Krishnamurthy, Steven Wu, Vasilis Syrgkanis",https://icml.cc/Conferences/2018/Schedule?showEvent=2270,"This paper studies semiparametric contextual bandits, a generalization of the linear stochastic bandit problem where the reward for a chosen action is modeled as a linear function of known action features confounded by a non-linear action-independent term. We design new algorithms that achieve $\tilde{O}(d\sqrt{T})$ regret over $T$ rounds, when the linear function is $d$-dimensional, which matches the best known bounds for the simpler unconfounded case and improves on a recent result of Greenwald et al. (2017). Via an empirical evaluation, we show that our algorithms outperform prior approaches when there are non-linear confounding effects on the rewards. Technically, our algorithms use a new reward estimator inspired by doubly-robust approaches and our proofs require new concentration inequalities for self-normalized martingales.","['Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2018,Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV),"Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viégas, Rory sayres",https://icml.cc/Conferences/2018/Schedule?showEvent=2089,"The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.
","['Google', 'Google', 'Google Brain', '', 'Google', 'Google', 'Google']"
2018,Weightless: Lossy weight encoding for deep neural network compression,"Brandon Reagen, Udit Gupta, Bob Adolf, Michael Mitzenmacher, Alexander Rush, Gu-Yeon Wei, David Brooks",https://icml.cc/Conferences/2018/Schedule?showEvent=2218,"The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, named Weightless, can compress weights by up to 496x without loss of model accuracy. This results in up to a 1.51x improvement over the state-of-the-art.
","['Harvard University', 'Harvard University', 'Harvard University', 'Harvard University', 'Harvard University', '', 'Harvard University']"
2018,Parallel Bayesian Network Structure Learning,"Tian Gao, Dennis Wei",https://icml.cc/Conferences/2018/Schedule?showEvent=1918,"Recent advances in Bayesian Network (BN) structure learning have focused on local-to-global learning, where the graph structure is learned via one local subgraph at a time. As a natural progression, we investigate parallel learning of BN structures via multiple learning agents simultaneously, where each agent learns one local subgraph at a time. We find that parallel learning can reduce the number of subgraphs requiring structure learning by storing previously queried results and communicating (even partial) results among agents. More specifically, by using novel rules on query subset and superset inference, many subgraph structures can be inferred without learning. We provide a sound and complete parallel structure learning (PSL) algorithm, and demonstrate its improved efficiency over state-of-the-art single-thread learning algorithms.
","['IBM Research', 'IBM Research']"
2018,Temporal Poisson Square Root Graphical Models,"Sinong Geng, Zhaobin Kuang, Peggy Peissig, University of Wisconsin David Page",https://icml.cc/Conferences/2018/Schedule?showEvent=2301,"We propose temporal Poisson square root graphical models (TPSQRs), a generalization of Poisson square root graphical models (PSQRs) specifically designed for modeling longitudinal event data. By estimating the temporal relationships for all possible pairs of event types, TPSQRs can offer a holistic perspective about whether the occurrences of any given event type could excite or inhibit any other type. A TPSQR is learned by estimating a collection of interrelated PSQRs that share the same template parameterization. These PSQRs are estimated jointly in a pseudo-likelihood fashion, where Poisson pseudo-likelihood is used to approximate the original more computationally intensive pseudo-likelihood problem stemming from PSQRs. Theoretically, we demonstrate that under mild assumptions, the Poisson pseudolikelihood approximation is sparsistent for recovering the underlying PSQR. Empirically, we learn TPSQRs from a real-world large-scale electronic health record (EHR) with millions of drug prescription and condition diagnosis events, for adverse drug reaction (ADR) detection. Experimental results demonstrate that the learned TPSQRs can recover ADR signals from the EHR effectively and efficiently.
","['UW-Madison; Princeton University', 'UW-Madison/Stanford', 'Marshfield Clinic Research Foundation', 'University of Wisconsin, Madison']"
2018,Minimax Concave Penalized Multi-Armed Bandit Model with High-Dimensional Covariates,"xue wang, Mingcheng Wei, Tao Yao",https://icml.cc/Conferences/2018/Schedule?showEvent=2469,"In this paper, we propose a Minimax Concave Penalized Multi-Armed Bandit (MCP-Bandit) algorithm for a decision-maker facing high-dimensional data with latent sparse structure in an online learning and decision-making process. We demonstrate that the MCP-Bandit algorithm asymptotically achieves the optimal cumulative regret in sample size T, O(log T), and further attains a tighter bound in both covariates dimension d and the number of significant covariates s, O(s^2 (s + log d).  In addition, we develop a linear approximation method, the 2-step Weighted Lasso procedure, to identify the MCP estimator for the MCP-Bandit algorithm under non-i.i.d. samples. Using this procedure, the MCP estimator matches the oracle estimator with high probability. Finally, we present two experiments to benchmark our proposed the MCP-Bandit algorithm to other bandit algorithms. Both experiments demonstrate that the MCP-Bandit algorithm performs favorably over other benchmark algorithms, especially when there is a high level of data sparsity or when the sample size is not too small.
","['THE PENNSYLVANIA STATE UNIVERSITY', 'University at Buffalo', 'penn state university']"
2018,Dynamic Regret of Strongly Adaptive Methods,"Lijun Zhang, Tianbao Yang, rong jin, Zhi-Hua Zhou",https://icml.cc/Conferences/2018/Schedule?showEvent=1989,"To cope with changing environments, recent developments in online learning have introduced the concepts of adaptive regret and dynamic regret independently. In this paper, we illustrate an intrinsic connection between these two concepts by showing that the dynamic regret can be expressed in terms of the adaptive regret and the functional variation. This observation implies that strongly adaptive algorithms can be directly leveraged to minimize the dynamic regret. As a result, we present a series of strongly adaptive algorithms that have small dynamic regrets for convex functions, exponentially concave functions, and strongly convex functions, respectively. To the best of our knowledge, this is the first time that exponential concavity is utilized to upper bound the dynamic regret. Moreover, all of those adaptive algorithms do not need any prior knowledge of the functional variation, which is a significant advantage over previous specialized methods for minimizing dynamic regret.
","['Nanjing University', 'The University of Iowa', 'alibaba group', 'Nanjing University']"
2018,Distributed Clustering via LSH Based Data Partitioning,"Aditya Bhaskara, Pruthuvi Maheshakya Wijewardena",https://icml.cc/Conferences/2018/Schedule?showEvent=2443,"Given the importance of clustering in the analysisof large scale data, distributed algorithms for formulations such as k-means, k-median, etc. have been extensively studied. A successful approach here has been the “reduce and merge” paradigm, in which each machine reduces its input size to Õ(k), and this data reduction continues (possibly iteratively) until all the data fits on one machine, at which point the problem is solved locally. This approach has the intrinsic bottleneck that each machine must solve a problem of size ≥ k, and needs to communicate at least Ω(k) points to the other machines. We propose a novel data partitioning idea to overcome this bottleneck, and in effect, have different machines focus on “finding different clusters”. Under the assumption that we know the optimum value of the objective up to a poly(n) factor (arbitrary polynomial), we establish worst-case approximation guarantees for our method. We see that our algorithm results in lower communication as well as a near-optimal number of ‘rounds’ of computation (in the popular MapReduce framework).
","['University of Utah', 'University of Utah']"
2018,Learning to Branch,"Nina Balcan, Travis Dick, Tuomas Sandholm, Ellen Vitercik",https://icml.cc/Conferences/2018/Schedule?showEvent=2244,"Tree search algorithms, such as branch-and-bound, are the most widely used tools for solving combinatorial problems. These algorithms recursively partition the search space to find an optimal solution. To keep the tree small, it is crucial to carefully decide, when expanding a tree node, which variable to branch on at that node to partition the remaining space. Many partitioning techniques have been proposed, but no theory describes which is optimal. We show how to use machine learning to determine an optimal weighting of any set of partitioning procedures for the instance distribution at hand using samples. Via theory and experiments, we show that learning to branch is both practical and hugely beneficial.
","['Carnegie Mellon University', 'CMU', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2018,Minibatch Gibbs Sampling on Large Graphical Models,"Christopher De Sa, Vincent Chen,  Wong",https://icml.cc/Conferences/2018/Schedule?showEvent=2383,"Gibbs sampling is the de facto Markov chain Monte Carlo method used for inference and learning on large scale graphical models. For complicated factor graphs with lots of factors, the performance of Gibbs sampling can be limited by the computational cost of executing a single update step of the Markov chain. This cost is proportional to the degree of the graph, the number of factors adjacent to each variable. In this paper, we show how this cost can be reduced by using minibatching: subsampling the factors to form an estimate of their sum. We introduce several minibatched variants of Gibbs, show that they can be made unbiased, prove bounds on their convergence rates, and show that under some conditions they can result in asymptotic single-update-run-time speedups over plain Gibbs sampling.
","['Cornell', 'Cornell University', 'Stanford university']"
2018,On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo,"Niladri Chatterji, Nicolas Flammarion, Yian Ma, Peter Bartlett, Michael Jordan",https://icml.cc/Conferences/2018/Schedule?showEvent=2094,"We provide convergence guarantees in Wasserstein distance for a variety of variance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion and control-variate underdamped Langevin diffusion. We analyze these methods under a uniform set of assumptions on the log-posterior distribution, assuming it to be smooth, strongly convex and Hessian Lipschitz. This is achieved by a new proof technique combining ideas from finite-sum optimization and the analysis of sampling methods. Our sharp theoretical bounds allow us to identify regimes of interest where each method performs better than the others. Our theory is verified with experiments on real-world and synthetic datasets.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2018,Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning,"Rodrigo A Toro Icarte, Toryn Q Klassen, Richard Valenzano, Sheila McIlraith",https://icml.cc/Conferences/2018/Schedule?showEvent=2454,"In this paper we propose Reward Machines — a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.
","['University of Toronto', 'University of Toronto', 'Element AI', 'University of Toronto']"
2018,Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks,"Brenden Lake, Marco Baroni",https://icml.cc/Conferences/2018/Schedule?showEvent=1938,"Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax."" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.
","['New York University', 'Facebook Artificial Intelligence Research']"
2018,Pathwise Derivatives Beyond the Reparameterization Trick,"Martin Jankowiak, Fritz Obermeyer",https://icml.cc/Conferences/2018/Schedule?showEvent=2213,"We observe that gradients computed via the reparameterization trick are in direct correspondence with solutions of the transport equation in the formalism of optimal transport. We use this perspective to compute (approximate) pathwise gradients for probability distributions not directly amenable to the reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that when the reparameterization trick is applied to the Cholesky-factorized multivariate Normal distribution, the resulting gradients are suboptimal in the sense of optimal transport. We derive the optimal gradients and show that they have reduced variance in a Gaussian Process regression task. We demonstrate with a variety of synthetic experiments and stochastic variational inference tasks that our pathwise gradients are competitive with other methods.
","['Uber AI Labs', 'Uber AI Labs']"
2018,Message Passing Stein Variational Gradient Descent,"Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, Bo Zhang",https://icml.cc/Conferences/2018/Schedule?showEvent=1915,"Stein variational gradient descent (SVGD) is a recently proposed particle-based Bayesian inference method, which has attracted a lot of interest due to its remarkable approximation ability and particle efficiency compared to traditional variational inference and Markov Chain Monte Carlo methods. However, we observed that particles of SVGD tend to collapse to modes of the target distribution, and this particle degeneracy phenomenon becomes more severe with higher dimensions. Our theoretical analysis finds out that there exists a negative correlation between the dimensionality and the repulsive force of SVGD which should be blamed for this phenomenon. We propose Message Passing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional independence structure of probabilistic graphical models (PGMs), MP-SVGD converts the original high-dimensional global inference problem into a set of local ones over the Markov blanket with lower dimensions. Experimental results show its advantages of preventing vanishing repulsive force in high-dimensional space over SVGD, and its particle efficiency and approximation flexibility over other inference methods on graphical models.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University', '', 'Tsinghua University']"
2018,State Space Gaussian Processes with Non-Gaussian Likelihood,"Hannes Nickisch, Arno Solin, Alexander Grigorevskiy",https://icml.cc/Conferences/2018/Schedule?showEvent=2065,"We provide a comprehensive overview and tooling for GP modelling with non-Gaussian likelihoods using state space methods. The state space formulation allows for solving one-dimensonal GP models in O(n) time and memory complexity. While existing literature has focused on the connection between GP regression and state space methods, the computational primitives allowing for inference using general likelihoods in combination with the Laplace approximation (LA), variational Bayes (VB), and assumed density filtering (ADF) / expectation propagation (EP) schemes has been largely overlooked. We present means of combining the efficient O(n) state space methodology with existing inference methods. We also furher extend existing methods, and provide unifying code implementing all approaches.
","['Philips Research', 'Aalto University', 'Aalto University']"
2018,Constant-Time Predictive Distributions for Gaussian Processes,"Geoff Pleiss, Jacob Gardner, Kilian Weinberger, Andrew Wilson",https://icml.cc/Conferences/2018/Schedule?showEvent=2067,"One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent advances in inducing point methods have sped up GP marginal likelihood and posterior mean computations, leaving posterior covariance estimation and sampling as the remaining computational bottlenecks. In this paper we address these shortcomings by using the Lanczos algorithm to rapidly approximate the predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs Variance Estimates), substantially improves time and space complexity. In our experiments, LOVE computes covariances up to 2,000 times faster and draws samples 18,000 times faster than existing methods, all without sacrificing accuracy.
","['Cornell University', 'Uber AI Labs', 'Cornell University', 'Cornell University']"
2018,Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks,"Peter Bartlett, Dave Helmbold, Phil Long",https://icml.cc/Conferences/2018/Schedule?showEvent=1980,"We analyze algorithms for approximating a function $f(x) = \Phi x$ mapping $\Re^d$ to $\Re^d$ using deep linear neural networks, i.e.\ that learn a function $h$ parameterized by matrices $\Theta_1,...,\Theta_L$ and defined by $h(x) = \Theta_L \Theta_{L-1} ... \Theta_1 x$.  We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic.   We provide polynomial bounds on the number of iterations for gradient descent to approximate the least squares matrix $\Phi$, in the case where the initial hypothesis $\Theta_1 = ... = \Theta_L = I$  has excess loss bounded by a small enough constant.  On the other hand, we show that gradient descent fails to converge for $\Phi$ whose distance from the identity is a larger constant, and  we show that some forms of regularization toward the identity in each layer do not help.   If $\Phi$ is symmetric positive definite, we show that an algorithm that initializes $\Theta_i = I$ learns an $\epsilon$-approximation of $f$  using a number of updates polynomial in $L$, the condition number of $\Phi$, and $\log(d/\epsilon)$.  In contrast, we show that if the least squares matrix $\Phi$ is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge.   We analyze an algorithm for the case that $\Phi$ satisfies $u^{\top} \Phi u > 0$ for all $u$, but may not be symmetric.  This algorithm uses two regularizers: one that maintains the invariant $u^{\top} \Theta_L \Theta_{L-1} ... \Theta_1 u > 0$ for all $u$, and another that ``balances'' $\Theta_1, ..., \Theta_L$  so that they have the same singular values.","['UC Berkeley', '', 'Google']"
2018,On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups,"Risi Kondor, Shubhendu Trivedi",https://icml.cc/Conferences/2018/Schedule?showEvent=2476,"Convolutional neural networks have been extremely successful in the image recognition domain because they ensure equivariance with respect to translations. There have been many recent attempts to generalize this framework to other domains, including graphs and data lying on manifolds. In this paper we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with respect to not just translations, but the action of any compact group. Our main result is to prove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a compact group. Our exposition makes use of concepts from representation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.
","['The University of Chicago', 'Toyota Technological Institute']"
2018,Racing Thompson: an Efficient Algorithm for Thompson Sampling with Non-conjugate Priors,"Yichi Zhou, Jun Zhu, Jingwei Zhuo",https://icml.cc/Conferences/2018/Schedule?showEvent=2160,"Thompson sampling has impressive empirical performance for many multi-armed bandit problems. But current algorithms for Thompson sampling only work for the case of conjugate priors since they require to perform online Bayesian posterior inference, which is a difficult task when the prior is not conjugate. In this paper, we propose a novel algorithm for Thompson sampling which only requires to draw samples from a tractable proposal distribution. So our algorithm is efficient even when the prior is non-conjugate. To do this, we reformulate Thompson sampling as an optimization proplem via the Gumbel-Max trick. After that we construct a set of random variables and our goal is to identify the one with highest mean which is an instance of best arm identification problems. Finally, we solve it with techniques in best arm identification. Experiments show that our algorithm works well in practice.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2018,Probably Approximately Metric-Fair Learning,"Gal Yona, Guy Rothblum",https://icml.cc/Conferences/2018/Schedule?showEvent=2365,"The seminal work of Dwork {\em et al.} [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-specific similarity metric, their notion required that every pair of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying population. We show that this can lead to computational intractability even for simple fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of {\em approximate metric-fairness}: for a random pair of individuals sampled from the population, with all but a small probability of error, if they are similar then they should be treated similarly.  We formalize the goal of achieving approximate metric-fairness simultaneously with best-possible accuracy as Probably Approximately Correct and Fair (PACF) Learning. We show that approximate metric-fairness {\em does} generalize, and leverage these generalization guarantees to construct polynomial-time PACF learning algorithms for the classes of linear and logistic predictors.
","['Weizmann Institute of Science', 'Weizmann Institute of Science']"
2018,Neural Program Synthesis from Diverse Demonstration Videos,"Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasundaram, Joseph Lim",https://icml.cc/Conferences/2018/Schedule?showEvent=1909,"Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a neural program synthesizer that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module as part of our model to improve the network’s ability to integrate multiple demonstrations varying in behavior. We also employ a multi-task objective to encourage the model to learn meaningful intermediate representations for end-to-end training. We show that our model is able to reliably synthesize underlying programs as well as capture diverse behaviors exhibited in demonstrations. The code is available at https://shaohua0116.github.io/demo2program.
","['University of Southern California', 'POSTECH', 'University of Southern California', 'Univ. of Southern California']"
2018,Video Prediction with Appearance and Motion Conditions,"Yunseok Jang, Gunhee Kim, Yale Song",https://icml.cc/Conferences/2018/Schedule?showEvent=1926,"Video prediction aims to generate realistic future frames by learning dynamic visual patterns. One fundamental challenge is to deal with future uncertainty: How should a model behave when there are multiple correct, equally probable future? We propose an Appearance-Motion Conditional GAN to address this challenge. We provide appearance and motion information as conditions that specify how the future may look like, reducing the level of uncertainty. Our model consists of a generator, two discriminators taking charge of appearance and motion pathways, and a perceptual ranking module that encourages videos of similar conditions to look similar. To train our model, we develop a novel conditioning scheme that consists of different combinations of appearance and motion conditions. We evaluate our model using facial expression and human action datasets and report favorable results compared to existing methods.
","['Seoul National University', 'Seoul National University', 'Microsoft AI & Research']"
2018,CRVI: Convex Relaxation for Variational Inference,"Ghazal Fazelnia, John Paisley",https://icml.cc/Conferences/2018/Schedule?showEvent=2175,"We present a new technique for solving non-convex variational inference optimization problems. Variational inference is a widely used method for posterior approximation in which the inference problem is transformed into an optimization problem. For most models, this optimization is highly non-convex and so hard to solve. In this paper, we introduce a new approach to solving the variational inference optimization based on convex relaxation and semidefinite programming. Our theoretical results guarantee very tight relaxation bounds that get nearer to the global optimal solution than traditional coordinate ascent. We evaluate the performance of our approach on regression and sparse coding.
","['Columbia University', 'Columbia University']"
2018,Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent,"Trevor Campbell, Tamara Broderick",https://icml.cc/Conferences/2018/Schedule?showEvent=1950,"Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability.  This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty.  To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally.  GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors.  The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.
","['MIT', 'MIT']"
2018,Transformation Autoregressive Networks,"Junier Oliva, Kumar Avinava Dubey, Manzil Zaheer, Barnabás Póczos, Ruslan Salakhutdinov, Eric Xing, Jeff Schneider",https://icml.cc/Conferences/2018/Schedule?showEvent=2324,"The fundamental task of general density estimation $p(x)$ has been of keen interest to machine learning. In this work, we attempt to systematically characterize methods for density estimation.  Broadly speaking, most of the existing methods can be categorized into either using: a) autoregressive models to estimate the conditional factors of the chain rule, $p(x_{i}\, |\, x_{i-1}, \ldots)$; or b) non-linear transformations of variables of a simple base distribution. To better study the characteristics of these categories we propose multiple methods for each category. For example we propose RNN based transformations to model non-Markovian transformation of variables. Further, through a comprehensive study over both real world and synthetic data, we show for that jointly leveraging transformations of variables and autoregressive conditional models, results in a considerable improvement in performance. We illustrate the use of our models in outlier detection and image modeling. Finally we introduce a novel data driven framework for learning a family of distributions.","['Carnegie Mellon University', 'Carnegie Mellon University', 'Google AI', 'CMU', 'Carnegie Mellen University', 'Petuum Inc. and CMU', 'Uber/CMU']"
2018,Learning equations for extrapolation and control,"Subham S Sahoo, Christoph H. Lampert, Georg Martius",https://icml.cc/Conferences/2018/Schedule?showEvent=2364,"We present an approach to identify concise equations from data using a shallow neural network approach. In contrast to ordinary black-box regression, this approach allows understanding functional relations and generalizing them from observed data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation learning network to include divisions, and we improve the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical expressions, our method can in many cases identify the true underlying equation and extrapolate to unseen domains. We demonstrate its effectiveness by experiments on a cart-pendulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-up task.
","['Indian Institute of Technology', 'IST Austria', 'Max Planck Institute for Intelligent Systems']"
2018,Analyzing Uncertainty in Neural Machine Translation,"Myle Ott, Michael Auli, David Grangier, Marc'Aurelio Ranzato",https://icml.cc/Conferences/2018/Schedule?showEvent=2292,"Machine translation is a popular test bed for research in neural sequence-to-sequence models but despite much recent research, there is still a lack of understanding of these models. Practitioners report performance degradation with large beams, the under-estimation of rare words and a lack of diversity in the final translations. Our study relates some of these issues to the inherent uncertainty of the task, due to the existence of multiple valid translations for a single source sentence, and to the extrinsic uncertainty caused by noisy training data. We propose tools and metrics to assess how uncertainty in the data is captured by the model distribution and how it affects search strategies that generate translations. Our results show that search works remarkably well but that the models tend to spread too much probability mass over the hypothesis space. Next, we propose tools to assess model calibration and show how to easily fix some shortcomings of current models. We release both code and multiple human reference translations for two popular benchmarks.
","['Facebook', 'Facebook', 'Google Brain', 'Facebook AI Research']"
2018,Hierarchical Text Generation and Planning for Strategic Dialogue,"Denis Yarats, Mike Lewis",https://icml.cc/Conferences/2018/Schedule?showEvent=2257,"End-to-end models for goal-orientated dialogue are challenging to train, because linguistic and strategic aspects are entangled in latent state vectors.  We introduce an approach to learning representations of messages in dialogues by maximizing the likelihood of subsequent sentences and actions, which decouples the semantics of the dialogue utterance from its linguistic realization. We then use these latent sentence representations for hierarchical language generation, planning and reinforcement learning. Experiments show that our approach  increases the end-task reward achieved by the model, improves the effectiveness of long-term planning using rollouts, and allows self-play reinforcement learning to improve decision making without diverging from human language. Our hierarchical latent-variable model outperforms previous work both linguistically and strategically.
","['Facebook AI Research', 'Facebook']"
2018,Budgeted Experiment Design for Causal Structure Learning,"AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Elias Bareinboim",https://icml.cc/Conferences/2018/Schedule?showEvent=2085,"We study the problem of causal structure learning when the experimenter is limited to perform at most $k$ non-adaptive experiments of size $1$. We formulate the problem of finding the best intervention target set as an optimization problem, which aims to maximize the average number of edges whose directions are resolved. We prove that the corresponding objective function is submodular and a greedy algorithm suffices to achieve $(1-\frac{1}{e})$-approximation of the optimal value. We further present an accelerated variant of the greedy algorithm, which can lead to orders of magnitude performance speedup. We validate our proposed approach on synthetic and real graphs. The results show that compared to the purely observational setting, our algorithm orients the majority of the edges through a considerably small number of interventions.","['UIUC', 'Sharif University of Technology', 'University of Illinois at Urbana-Champaign', 'Purdue']"
2018,Accurate Inference for Adaptive Linear Models,"Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, Matt Taddy",https://icml.cc/Conferences/2018/Schedule?showEvent=2060,"Estimators computed from adaptively collected data do not behave like their non-adaptive brethren. Rather, the sequential dependence of the collection policy can lead to severe distributional biases that persist even in the infinite data limit. We develop a general method  -- \emph{$\vect{W}$-decorrelation} -- for transforming the bias of adaptive linear regression estimators into variance. The method uses only coarse-grained information about the data collection policy and does not need access to propensity scores or exact knowledge of the policy. We bound the finite-sample bias and variance of the $\vect{W}$-estimator and develop asymptotically correct confidence intervals based on a novel martingale central limit theorem.  We then demonstrate the empirical benefits of the generic $\vect{W}$-decorrelation procedure in two different adaptive data settings: the multi-armed bandit and the autoregressive time series.","['Massachusetts Institute of Technology', 'Microsoft Research', 'Microsoft Research', 'MICROSOFT']"
2018,Path-Level Network Transformation for Efficient Architecture Search,"Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, Yong Yu",https://icml.cc/Conferences/2018/Schedule?showEvent=2208,"We introduce a new function-preserving transformation for efficient neural architecture search. This network transformation allows reusing previously trained networks and existing successful architectures that improves sample efficiency. We aim to address the limitation of current network transformation operations that can only perform layer-level architecture modifications, such as adding (pruning) filters or inserting (removing) a layer, which fails to change the topology of connection paths. Our proposed path-level transformation operations enable the meta-controller to modify the path topology of the given network while keeping the merits of reusing weights, and thus allow efficiently designing effective structures with complex path topologies like Inception models. We further propose a bidirectional tree-structured reinforcement learning meta-controller to explore a simple yet highly expressive tree-structured architecture space that can be viewed as a generalization of multi-branch architectures. We experimented on the image classification datasets with limited computational resources (about 200 GPU-hours), where we observed improved parameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.
","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'MIT', 'Shanghai Jiao Tong University']"
2018,Progress & Compress: A scalable framework for continual learning,"Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Teh, Razvan Pascanu, Raia Hadsell",https://icml.cc/Conferences/2018/Schedule?showEvent=2131,"We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress & compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.
","['DeepMind', 'DeepMind', 'The University of Oxford', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2018,Learning Longer-term Dependencies in RNNs with Auxiliary Losses,"Trieu H Trinh, Andrew Dai, Thang Luong, Quoc Le",https://icml.cc/Conferences/2018/Schedule?showEvent=2127,"Despite recent advances in training recurrent neural networks (RNNs), capturing long-term dependencies in sequences remains a fundamental challenge. Most approaches use backpropagation through time (BPTT), which is difficult to scale to very long sequences. This paper proposes a simple method that improves the ability to capture long term dependencies in RNNs by adding an unsupervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct previous events or predict next events in a sequence, making truncated backpropagation feasible for long sequences and also improving full BPTT. We evaluate our method on a variety of settings, including pixel-by-pixel image classification with sequence lengths up to 16000, and a real document classification benchmark. Our results highlight good performance and resource efficiency of this approach over competitive baselines, including other recurrent models and a comparable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on optimization and regularization, as well as extreme cases where there is little to no backpropagation.
","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']"
2018,Understanding and Simplifying One-Shot Architecture Search,"Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le",https://icml.cc/Conferences/2018/Schedule?showEvent=2077,"There is growing interest in automating neural network architecture design. Existing architecture search methods can be computationally expensive, requiring thousands of different architectures to be trained from scratch. Recent work has explored \emph{weight sharing} across  models to amortize the cost of training. Although previous methods reduced the cost of architecture search by orders of magnitude, they remain complex, requiring hypernetworks or reinforcement learning controllers. We aim to understand weight sharing for one-shot architecture search. With careful experimental analysis, we show that it is possible to efficiently identify promising architectures from a complex search space without either hypernetworks or RL.
","['Google', 'Google', 'Google', 'Google', 'Google Brain']"
2018,Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents,"Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, Tamer Basar",https://icml.cc/Conferences/2018/Schedule?showEvent=2269,"We consider the fully decentralized multi-agent reinforcement learning (MARL) problem, where the agents are connected via a time-varying and possibly sparse communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. To maximize the globally averaged return over the network, we propose two fully decentralized actor-critic algorithms, which are applicable to large-scale MARL problems in an online fashion. Convergence guarantees are provided when the value functions are approximated within the class of linear functions. Our work appears to be the first theoretical study of fully decentralized MARL algorithms for networked agents that use function approximation.
","['University of Illinois at Urbana-Champaign (UIUC)', 'Princeton University', 'Northwestern', 'Tecent AI Lab', 'University of Illinois at Urbana-Champaign']"
2018,State Abstractions for Lifelong Reinforcement Learning,"David Abel, Dilip S. Arumugam, Lucas Lehnert, Michael L. Littman",https://icml.cc/Conferences/2018/Schedule?showEvent=2087,"In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.
","['Brown University', 'Stanford University', 'Brown University', 'Brown University']"
2018,Bounding and Counting Linear Regions of Deep Neural Networks,"Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam",https://icml.cc/Conferences/2018/Schedule?showEvent=2449,"We investigate the complexity of deep neural networks (DNN) that represent piecewise linear (PWL) functions. In particular, we study the number of linear regions, i.e. pieces, that a PWL function represented by a DNN can attain, both theoretically and empirically. We present (i) tighter upper and lower bounds for the maximum number of linear regions on rectifier networks, which are exact for inputs of dimension one; (ii) a first upper bound for multi-layer maxout networks; and (iii) a first method to perform exact enumeration or counting of the number of regions by modeling the DNN with a mixed-integer linear formulation.  These bounds come from leveraging the dimension of the space defining each linear region. The results also indicate that a deep rectifier network can only have more linear regions than every shallow counterpart with same number of neurons if that number exceeds the dimension of the input.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'University of Utah']"
2018,Clipped Action Policy Gradient,"Yasuhiro Fujita, Shin-ichi Maeda",https://icml.cc/Conferences/2018/Schedule?showEvent=2018,"Many continuous control tasks have bounded action spaces. When policy gradient methods are applied to such tasks, out-of-bound actions need to be clipped before execution, while policies are usually optimized as if the actions are not clipped. We propose a policy gradient estimator that exploits the knowledge of actions being clipped to reduce the variance in estimation. We prove that our estimator, named clipped action policy gradient (CAPG), is unbiased and achieves lower variance than the conventional estimator that ignores action bounds. Experimental results demonstrate that CAPG generally outperforms the conventional estimator, indicating that it is a better policy gradient estimator for continuous control tasks. The source code is available at https://github.com/pfnet-research/capg.
","['Preferred Networks, Inc.', 'Preferred Networks, Inc.']"
2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, koray kavukcuoglu",https://icml.cc/Conferences/2018/Schedule?showEvent=2174,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.
","['Google Brain Amsterdam', 'DeepMind', 'DeepMind', 'DeepMind', 'Google Deepmind', 'DeepMind', 'DeepMind', '', 'DeepMind', '', 'DeepMind', 'DeepMind']"
2018,Inter and Intra Topic Structure Learning with Word Embeddings,"He Zhao, Lan Du, Wray Buntine, Mingyuan Zhou",https://icml.cc/Conferences/2018/Schedule?showEvent=2183,"One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.
","['FIT, Monash University', 'Faculty of Information Technology, Monash University', 'Monash University', 'University of Texas at Austin']"
2018,oi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis,"Samuel Ainsworth, Nicholas J Foti, Adrian KC Lee, Emily Fox",https://icml.cc/Conferences/2018/Schedule?showEvent=2410,"Deep generative models have recently yielded encouraging results in producing subjectively realistic samples of complex data. Far less attention has been paid to making these generative models interpretable. In many scenarios, ranging from scientific applications to finance, the observed variables have a natural grouping.  It is often of interest to understand systems of interaction amongst these groups, and latent factor models (LFMs) are an attractive approach.  However, traditional LFMs are limited by assuming a linear correlation structure.  We present an output interpretable VAE (oi-VAE) for grouped data that models complex, nonlinear latent-to-observed relationships.  We combine a structured VAE comprised of group-specific generators with a sparsity-inducing prior.  We demonstrate that oi-VAE yields meaningful notions of interpretability in the analysis of motion capture and MEG data.  We further show that in these situations, the regularization inherent to oi-VAE can actually lead to improved generalization and learned generative processes.
","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']"
2018,The Hidden Vulnerability of Distributed Learning in Byzantium,"El Mahdi El Mhamdi, Rachid Guerraoui, Sébastien Rouault",https://icml.cc/Conferences/2018/Schedule?showEvent=2351,"While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent approaches have been proposed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending \emph{poisoned} gradients during the training phase. Some of these approaches have been proven \emph{Byzantine--resilient}: they ensure the \emph{convergence} of SGD despite the presence of a minority of adversarial workers. We show in this paper that \emph{convergence is not enough}. In high dimension $d \gg 1$, an adver\-sary can build on the loss function's non--convexity to make SGD converge to \emph{ineffective} models. More precisely, we bring to light that existing Byzantine--resilient schemes leave a \emph{margin of poisoning} of $\bigOmega\left(f(d)\right)$, where $f(d)$ increases at least like $\sqrt[p]{d~}$. Based on this \emph{leeway}, we build a simple attack, and experimentally show its strong to utmost effectivity on CIFAR--10 and MNIST. We introduce \emph{Bulyan}, and prove it significantly reduces the attackers leeway to a narrow $\bigO\,( \sfrac{1}{\sqrt{d~}})$ bound. We empirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence \emph{as if} only non--Byzantine gradients had been used to update the model.","['EPFL', 'EPFL', 'EPFL']"
2018,Asynchronous Byzantine Machine Learning (the case of SGD),"Georgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Rhicheek Patra, Mahsa Taziki",https://icml.cc/Conferences/2018/Schedule?showEvent=1963,"Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce Kardam, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against 1/3 Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than f/n, where f is the number of Byzantine failures tolerated and n the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.
","['EPFL', 'EPFL', 'EPFL', 'EPFL', 'EPFL']"
2019,SelectiveNet: A Deep Neural Network with an Integrated Reject Option,"Yonatan Geifman, Ran El-Yaniv",https://icml.cc/Conferences/2019/Schedule?showEvent=4020,"We consider the problem of selective prediction (also known as reject option) in deep neural networks, and introduce SelectiveNet, a deep neural architecture with an integrated reject option. Existing rejection mechanisms are based mostly on a threshold over the prediction confidence of a pre-trained network. In contrast, SelectiveNet is trained to optimize both classification (or regression) and rejection simultaneously, end-to-end.  The result is a deep neural network that is optimized over the covered domain. In our experiments, we show a consistently improved risk-coverage trade-off over several well-known classification and regression datasets, thus reaching new state-of-the-art results for deep selective classification.
","['Technion', 'Technion']"
2019,Manifold Mixup: Better Representations by Interpolating Hidden States,"Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, Yoshua Bengio",https://icml.cc/Conferences/2019/Schedule?showEvent=3776,"Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples.  This includes distribution shifts, outliers, and adversarial examples.  To address these issues, we propose \manifoldmixup{}, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations.  \manifoldmixup{} leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation.  As a result, neural networks trained with \manifoldmixup{} learn flatter class-representations, that is, with fewer directions of variance.  We prove theory on why this flattening happens under ideal conditions, validate it empirically on practical situations, and connect it to the previous works on information theory and generalization.  In spite of incurring no significant computation and being implemented in a few lines of code, \manifoldmixup{} improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.
","['Aalto University', 'Universite de Montreal', 'Ecole Polytechnique de Montreal', 'Sharif University of Technology', 'University of Montreal', 'Facebook AI Research', 'Mila / U. Montreal']"
2019,Processing Megapixel Images with Deep Attention-Sampling Models,"Angelos Katharopoulos, Francois Fleuret",https://icml.cc/Conferences/2019/Schedule?showEvent=4076,"Existing deep architectures cannot operate on very large signals such
as megapixel images due to computational and memory constraints. To
tackle this limitation, we propose a fully differentiable end-to-end
trainable model that samples and processes only a fraction of the full
resolution input image.
The locations to process are sampled from an attention distribution
computed from a low resolution view of the input. We refer to our
method as attention sampling and it can process images of
several megapixels with a standard single GPU setup.
We show that sampling from the attention distribution results in an
unbiased estimator of the full model with minimal variance, and we
derive an unbiased estimator of the gradient that we use to train our
model end-to-end with a normal SGD procedure.
This new method is evaluated on three classification tasks, where we
show that it allows to reduce computation and memory footprint by
an order of magnitude for the same accuracy as classical
architectures. We also show the consistency of the sampling that
indeed focuses on informative parts of the input images.
","['Idiap & EPFL', 'Idiap research institute']"
2019,TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning,"Sung Whan Yoon, Jun Seo, Jaekyun Moon",https://icml.cc/Conferences/2019/Schedule?showEvent=4024,"Handling previously unseen tasks after given only a few training examples continues to be a tough challenge in machine learning. We propose TapNets, neural networks augmented with task-adaptive projection for improved few-shot learning. Here, employing a meta-learning strategy with episode-based training, a network and a set of per-class reference vectors are learned across widely varying tasks. At the same time, for every episode, features in the embedding space are linearly projected into a new space as a form of quick task-specific conditioning. The training loss is obtained based on a distance metric between the query and the reference vectors in the projection space. Excellent generalization results in this way. When tested on the Omniglot, miniImageNet and tieredImageNet datasets, we obtain state of the art classification accuracies under various few-shot scenarios.
","['Korea Advanced Institute of Science and Technology (KAIST)', 'Korea Advanced Institute of Science and Technology(KAIST)', 'KAIST']"
2019,Online Meta-Learning,"Chelsea Finn, Aravind Rajeswaran, Sham Kakade, Sergey Levine",https://icml.cc/Conferences/2019/Schedule?showEvent=4195,"A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-specific adaptation.
This work introduces an online meta-learning setting, which merges ideas from both paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale problems suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.
","['Stanford, Google, UC Berkeley', 'University of Washington', 'University of Washington', 'UC Berkeley']"
2019,Training Neural Networks with Local Error Signals,"Arild Nøkland, Lars Hiller Eidnes",https://icml.cc/Conferences/2019/Schedule?showEvent=3575,"Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility.
","['Kongsberg Seatex', 'None']"
2019,GMNN: Graph Markov Neural Networks,"Meng Qu, Yoshua Bengio, Jian Tang",https://icml.cc/Conferences/2019/Schedule?showEvent=4121,"This paper studies semi-supervised object classification in relational data, which is a fundamental problem in relational data modeling. The problem has been extensively studied in the literature of both statistical relational learning (e.g. relational Markov networks) and graph neural networks (e.g. graph convolutional networks). Statistical relational learning methods can effectively model the dependency of object labels through conditional random fields for collective classification, whereas graph neural networks learn effective object representations for classification through end-to-end training. In this paper, we propose the Graph Markov Neural Network (GMNN) that combines the advantages of both worlds. A GMNN models the joint distribution of object labels with a conditional random field, which can be effectively trained with the variational EM algorithm. In the E-step, one graph neural network learns effective object representations for approximating the posterior distributions of object labels. In the M-step, another graph neural network is used to model the local label dependency. Experiments on object classification, link classification, and unsupervised node representation learning show that GMNN achieves state-of-the-art results. 
","['MILA', 'Mila / U. Montreal', 'HEC Montreal & MILA']"
2019,Self-Attention Graph Pooling,"Junhyun Lee, Inyeop Lee, Jaewoo Kang",https://icml.cc/Conferences/2019/Schedule?showEvent=3965,"Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.
","['Korea University', 'Korea University', 'Korea University']"
2019,Combating Label Noise in Deep Learning using Abstention,"Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, Jamal Mohd-Yusof",https://icml.cc/Conferences/2019/Schedule?showEvent=4141,"We introduce a novel method to combat label noise when training deep neural networks for classification. We propose a loss function that permits abstention during training thereby allowing the DNN to abstain on confusing samples while continuing to learn and improve classification performance on the non-abstained samples. We show how such a deep abstaining classifier (DAC) can be used for robust learning in the presence of different types of label noise. In the case of structured or systematic label noise – where noisy training labels or confusing examples are correlated with underlying features of the data– training with abstention enables representation learning for features that are associated with unreliable labels. In the case of unstructured (arbitrary) label noise, abstention during training enables the DAC to be used as an effective data cleaner by identifying samples that are likely to have label noise. We provide analytical results on the loss function behavior that enable dynamic adaption of abstention rates based on learning progress during training. We demonstrate the utility of the deep abstaining classifier for various image classification tasks under different types of label noise; in the case of arbitrary label noise, we show significant im- provements over previously published results on multiple image benchmarks.
","['Los Alamos National Laboratory & University of Washington', 'Los Alamos National Laboratory', 'UW', 'Los Alamos National Laboratory', 'Los Alamos National Laboratory']"
2019,LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning,"Huaiyu Li, Weiming Dong, Xing Mei, Chongyang Ma, Feiyue Huang, Bao-Gang Hu",https://icml.cc/Conferences/2019/Schedule?showEvent=3808,"In this work, we propose a novel meta-learning approach for few-shot classification, which learns transferable prior knowledge across tasks and directly produces network parameters for similar unseen tasks with training samples. Our approach, called LGM-Net, includes two key modules, namely, TargetNet and MetaNet. The TargetNet module is a neural network for solving a specific task and the MetaNet module aims at learning to generate functional weights for TargetNet by observing training samples. We also present an intertask normalization strategy for the training process to leverage common information shared across different tasks. The experimental results on Omniglot and miniImageNet datasets demonstrate that LGM-Net can effectively adapt to similar unseen tasks and achieve competitive performance, and the results on synthetic datasets show that transferable prior knowledge is learned by the MetaNet module via mapping training data to functional weights. LGM-Net enables fast learning and adaptation since no further tuning steps are required compared to other meta-learning approaches
","['Institute of Automation, Chinese Academy of Sciences', 'NLPR, Institute of Automation, Chinese Academy of Sciences', 'Snap Inc.', 'Kwai Inc.', 'Tencent', 'Institute of Automation, Chinese Academy of Sciences']"
2019,Self-Attention Generative Adversarial Networks,"Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena",https://icml.cc/Conferences/2019/Schedule?showEvent=3774,"In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other.
Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN performs better than prior work, boosting the best published Inception score from 36.8 to 52.52 and reducing Fr\'echet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.
","['Google', 'Google Brain', 'Rutgers', 'Google Brain']"
2019,Multivariate-Information Adversarial Ensemble for Scalable Joint Distribution Matching,"Ziliang Chen, ZHANFU YANG, Xiaoxi Wang, Xiaodan Liang, xiaopeng yan, Guanbin Li, Liang Lin",https://icml.cc/Conferences/2019/Schedule?showEvent=3858,"A broad range of cross-$m$-domain generation researches boil down to matching a joint distribution by deep generative models (DGMs). Hitherto algorithms excel in pairwise domains while as $m$ increases, remain struggling to scale themselves to ﬁt a joint distribution. In this paper, we propose a domain-scalable DGM, i.e., MMI-ALI for $m$-domain joint distribution matching. As an $m$-domain ensemble model of ALIs (Dumoulin et al., 2016), MMI-ALI is adversarially trained with maximizing Multivariate Mutual Information (MMI) w.r.t. joint variables of each pair of domains and their shared feature. The negative MMIs are upper bounded by a series of feasible losses provably leading to matching $m$-domain joint distributions. MMI-ALI linearly scales as $m$ increases and thus, strikes a right balance between efﬁcacy and scalability. We evaluate MMI-ALI in diverse challenging $m$-domain scenarios and verify its superiority.","['Sun Yat-sen University', 'Purdue University', 'Sun Yat-sen University', 'Sun Yat-sen University', 'SYSU', 'Sun Yat-sen University', 'Sun Yat-sen University']"
2019,High-Fidelity Image Generation With Fewer Labels,"Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, Sylvain Gelly",https://icml.cc/Conferences/2019/Schedule?showEvent=4273,"Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outperform it using 20% of the labels.
","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']"
2019,Revisiting precision recall definition for generative modeling,"Loic Simon, Ryan Webster, Julien Rabin",https://icml.cc/Conferences/2019/Schedule?showEvent=3584,"In this article we revisit the definition of Precision-Recall (PR) curves for generative models proposed by (Sajjadi et al., 2018). Rather than providing a scalar for generative quality, PR curves distinguish mode-collapse (poor recall) and bad quality (poor precision). We first generalize their formulation to arbitrary measures hence removing any restriction to finite support. We also expose a bridge between PR curves and type I and type II error (a.k.a. false detection and rejection) rates of likelihood ratio classifiers on the task of discriminating between samples of the two distributions. Building upon this new perspective, we propose a novel algorithm to approximate precision-recall curves, that shares some interesting methodological properties with the hypothesis testing technique from (Lopez-Paz  &  Oquab,  2017). We demonstrate the interest of the proposed formulation over the original approach on controlled multi-modal datasets.
","['CNRS GREYC UMR 6072', 'CNRS GREYC UMR 6072', 'Unicaen']"
2019,Wasserstein of Wasserstein Loss for Learning Generative Models,"Yonatan Dukler, Wuchen Li, Alex Lin, Guido Montufar",https://icml.cc/Conferences/2019/Schedule?showEvent=4124,"The Wasserstein distance serves as a loss function for unsupervised learning which depends on the choice of a ground metric on sample space. We propose to use the Wasserstein distance itself as the ground metric on the sample space of images. This ground metric is known as an effective distance for image retrieval, that correlates with human perception. We derive the Wasserstein ground metric on pixel space and define a Riemannian Wasserstein gradient penalty to be used in the Wasserstein Generative Adversarial Network (WGAN) framework. The new gradient penalty is computed efficiently via convolutions on the $L^2$ gradients with negligible additional computational cost. The new formulation is more robust to the natural variability of the data and provides for a more continuous discriminator in sample space.
","['UCLA', 'UCLA', 'University of California, Los Angeles', 'UCLA']"
2019,Flat Metric Minimization with Applications in Generative Modeling,"Thomas Möllenhoff, Daniel Cremers",https://icml.cc/Conferences/2019/Schedule?showEvent=3598,"We take the novel perspective to view data not as a probability distribution but rather as a current. Primarily studied in the field of geometric measure theory, k-currents are continuous linear functionals acting on compactly supported smooth differential forms and can be understood as a generalized notion of oriented k-dimensional manifold. By moving from distributions (which are 0-currents) to k-currents, we can explicitly orient the data by attaching a k-dimensional tangent plane to each sample point. Based on the flat metric which is a fundamental distance between currents, we derive FlatGAN, a formulation in the spirit of generative adversarial networks but generalized to k-currents. In our theoretical contribution we prove that the flat metric between a parametrized current and a reference current is Lipschitz continuous in the parameters. In experiments, we show that the proposed shift to k>0 leads to interpretable and disentangled latent representations which behave equivariantly to the specified oriented tangent planes.
","['TU Munich', 'TU Munich']"
2019,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,"Yogesh Balaji, Hamed Hassani, Rama Chellappa, Soheil Feizi",https://icml.cc/Conferences/2019/Schedule?showEvent=3909,"Building on the success of deep learning, two modern approaches to learn a probability model from the data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data. The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems. In this work, we resolve this issue by constructing an explicit probability model that can be used to compute sample likelihood statistics in GANs. In particular, we prove that under this probability model, a family of Wasserstein GANs with an entropy regularization can be viewed as a generative model that maximizes a variational lower-bound on average sample log likelihoods, an approach that VAEs are based on. This result makes a principled connection between two modern generative models, namely GANs and VAEs. In addition to the aforementioned theoretical results, we compute likelihood statistics for GANs trained on Gaussian, MNIST, SVHN, CIFAR-10 and LSUN datasets. Our numerical results validate the proposed theory.
","['University of Maryland', 'University of Pennsylvania', 'University of Maryland', 'University of Maryland']"
2019,Non-Parametric Priors For Generative Adversarial Networks,"Rajhans Singh, Pavan Turaga, Suren Jayasuriya, Ravi Garg, Martin  Braun",https://icml.cc/Conferences/2019/Schedule?showEvent=3685,"The advent of generative adversarial networks (GAN) has enabled new capabilities in synthesis, interpolation, and data augmentation heretofore considered very challenging. However, one of the common assumptions in most GAN architectures is the assumption of simple parametric latent-space distributions. While easy to implement, a simple latent-space distribution can be problematic for uses such as interpolation. This is due to distributional mismatches when samples are interpolated in the latent space. We present a straightforward formalization of this problem; using basic results from probability theory and off-the-shelf-optimization tools, we develop ways to arrive at appropriate non-parametric priors. The obtained prior exhibits unusual qualitative properties in terms of its shape, and quantitative benefits in terms of lower divergence with its mid-point distribution. We demonstrate that our designed prior helps improve image generation along any Euclidean straight line during interpolation, both qualitatively and quantitatively, without any additional training or architectural modifications. The proposed formulation is quite flexible, paving the way to impose newer constraints on the latent-space statistics.
","['Arizona State University', 'Arizona State University', 'Arizona State University', 'Intel Corporation', 'Intel Corporation']"
2019,Lipschitz Generative Adversarial Nets,"Zhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang, Weinan Zhang, Yong Yu, Zhihua Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3698,"In this paper we show that generative adversarial networks (GANs) without restriction on the discriminative function space commonly suffer from the problem that the gradient produced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lipschitz, does not suffer from such a gradient uninformativeness problem. We further show in the paper that the model with a compact dual form of Wasserstein distance, where the Lipschitz condition is relaxed, may also theoretically suffer from this issue. This implies the importance of Lipschitz condition and motivates us to study the general formulation of GANs with Lipschitz constraint, which leads to a new family of GANs that we call Lipschitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the optimal discriminative function as well as the existence of a unique Nash equilibrium. We prove that LGANs are generally capable of eliminating the gradient uninformativeness problem. According to our empirical analysis, LGANs are more stable and generate consistently higher quality samples compared with WGAN.
","['Shanghai Jiao Tong University', 'Peking University', 'Shanghai Jiao Tong Univesity', 'Stanford University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Peking University']"
2019,HexaGAN: Generative Adversarial Nets for Real World Classification,"Uiwon Hwang, Dahuin Jung, Sungroh Yoon",https://icml.cc/Conferences/2019/Schedule?showEvent=3594,"Most deep learning classification studies assume clean data. However, when dealing with the real world data, we encounter three problems such as 1) missing data, 2) class imbalance, and 3) missing label problems. These problems undermine the performance of a classifier. Various preprocessing techniques have been proposed to mitigate one of these problems, but an algorithm that assumes and resolves all three problems together has not been proposed yet. In this paper, we propose HexaGAN, a generative adversarial network framework that shows promising classification performance for all three problems. We interpret the three problems from a single perspective to solve them jointly. To enable this, the framework consists of six components, which interact with each other. We also devise novel loss functions corresponding to the architecture. The designed loss functions allow us to achieve state-of-the-art imputation performance, with up to a 14% improvement, and to generate high-quality class-conditional data. We evaluate the classification performance (F1-score) of the proposed method with 20% missingness and confirm up to a 5% improvement in comparison with the performance of combinations of state-of-the-art methods.
","['Seoul National University', 'Seoul National University', 'Seoul National University']"
2019,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,"Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, Pushmeet Kohli",https://icml.cc/Conferences/2019/Schedule?showEvent=4018,"This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain specific baseline systems that have been carefully hand-engineered for these problems.
","['DeepMind', 'DeepMind', 'Google', 'DeepMind', 'DeepMind']"
2019,BayesNAS: A Bayesian Approach for Neural Architecture Search,"Hongpeng Zhou, Minghao Yang, Jun Wang, Wei Pan",https://icml.cc/Conferences/2019/Schedule?showEvent=4231,"One-Shot Neural Architecture Search (NAS) is a promising method to significantly reduce search time without any separate training. It can be treated as a Network Compression problem on the architecture parameters from an over-parameterized network. However, there are two issues associated with most one-shot NAS methods.  First, dependencies between a node and its predecessors and successors are often disregarded which result in improper treatment over zero operations.  Second, architecture parameters pruning based on their magnitude is questionable. In this paper, we employ the classic Bayesian learning approach to alleviate these two issues by modeling architecture parameters using hierarchical automatic relevance determination (HARD) priors. Unlike other NAS methods, we train the over-parameterized network for only one epoch then update the architecture. Impressively, this enabled us to find the architecture in both proxy and proxyless tasks on CIFAR-10 within only 0.2 GPU days using a single GPU. As a byproduct, our approach can be transferred directly to compress convolutional neural networks by enforcing structural sparsity which achieves extremely sparse networks without accuracy deterioration.
","['Delft University of Technology', 'TUDelft', 'UCL', 'TUDelft']"
2019,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,"Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, Yee-Whye Teh",https://icml.cc/Conferences/2019/Schedule?showEvent=3716,"Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.
","['AITRICS', 'Kakao Corporation', 'POSTECH', 'University of Oxford', 'POSTECH', 'Oxford and DeepMind']"
2019,Shallow-Deep Networks: Understanding and Mitigating Network Overthinking,"Yigitcan Kaya, Sanghyun Hong, Tudor Dumitras",https://icml.cc/Conferences/2019/Schedule?showEvent=3840,"We characterize a prevalent weakness of deep neural networks (DNNs), 'overthinking', which occurs when a DNN can reach correct predictions before its final layer. Overthinking is computationally wasteful, and it can also be destructive when, by the final layer, a correct prediction changes into a misclassification. Understanding overthinking requires studying how each prediction evolves during a DNN's forward pass, which conventionally is opaque. For prediction transparency, we propose the Shallow-Deep Network (SDN), a generic modification to off-the-shelf DNNs that introduces internal classifiers. We apply SDN to four modern architectures, trained on three image classification tasks, to characterize the overthinking problem. We show that SDNs can mitigate the wasteful effect of overthinking with confidence-based early exits, which reduce the average inference cost by more than 50% and preserve the accuracy. We also find that the destructive effect occurs for 50% of misclassifications on natural inputs and that it can be induced, adversarially, with a recent backdooring attack. To mitigate this effect, we propose a new confusion metric to quantify the internal disagreements that will likely to lead to misclassifications.
","['University of Maryland, College Park', 'University of Maryland College Park; shhong@cs.umd.edu', 'University of Maryland']"
2019,Graph U-Nets,"Hongyang Gao, Shuiwang Ji",https://icml.cc/Conferences/2019/Schedule?showEvent=3676,"We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Nets. Our experimental results on node classification and graph classification tasks demonstrate that our methods achieve consistently better performance than previous models.
","['Texas A&M University', 'Texas A&M University']"
2019,SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver,"Po-Wei Wang, Priya Donti, Bryan Wilder, Zico Kolter",https://icml.cc/Conferences/2019/Schedule?showEvent=3947,"Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a ``visual Sudoku'' problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'University of Southern California', 'Carnegie Mellon University / Bosch Center for AI']"
2019,Area Attention,"Yang Li, Lukasz Kaiser, Samy Bengio, Si Si",https://icml.cc/Conferences/2019/Schedule?showEvent=3744,"Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a predefined, fixed granularity, e.g., a word token or an image grid. We propose area attention: a way to attend to areas in the memory, where each area contains a group of items that are structurally adjacent, e.g., spatially for a 2D memory such as images, or temporally for a 1D memory such as natural language sentences. Importantly, the shape and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multiple areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image captioning, and improve upon strong (state-of-the-art) baselines in all the cases. These improvements are obtainable with a basic form of area attention that is parameter free.
","['Google Research', 'Google', 'Google Research    Brain Team', 'Google Research']"
2019,The Evolved Transformer,"David So, Quoc Le, Chen Liang",https://icml.cc/Conferences/2019/Schedule?showEvent=4285,"Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original ""big"" Transformer with 37.6% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.
","['Google Brain', 'Google Brain', 'Google Brain']"
2019,Jumpout : Improved Dropout for Deep Neural Networks with ReLUs,"Shengjie Wang, Tianyi Zhou, Jeff Bilmes",https://icml.cc/Conferences/2019/Schedule?showEvent=3904,"We discuss three novel insights about dropout for DNNs with ReLUs: 1) dropout encourages each local linear piece of a DNN to be trained on data points from nearby regions; 2) the same dropout rate results in different (effective) deactivation rates for layers with different portions of ReLU-deactivated neurons; and 3) the rescaling factor of dropout causes a normalization inconsistency between training and test when used together with batch normalization. The above leads to three simple but nontrivial modifications resulting in our method ``jumpout.'' Jumpout samples the dropout rate from a monotone decreasing distribution (e.g., the right half of a Gaussian), so each local linear piece is trained, with high probability, to work better for data points from nearby than more distant regions. Jumpout moreover adaptively normalizes the dropout rate at each layer and every training batch, so the effective deactivation rate on the activated neurons is kept the same. Furthermore, it rescales the outputs for a better trade-off that keeps both the variance and mean of neurons more consistent between training and test phases, thereby mitigating the incompatibility between dropout and batch normalization. Jumpout significantly improves the performance of different neural nets on CIFAR10, CIFAR100, Fashion-MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and computation costs.
","['""University of Washington, Seattle""', 'University of Washington', 'UW']"
2019,Stochastic Deep Networks,"Gwendoline De Bie, Gabriel Peyré, Marco Cuturi",https://icml.cc/Conferences/2019/Schedule?showEvent=3748,"Machine learning is increasingly targeting areas where input data cannot be accurately described by a single vector, but can be modeled instead using the more flexible concept of random vectors, namely probability measures or more simply point clouds of varying cardinality. Using deep architectures on measures poses, however, many challenging issues. Indeed, deep architectures are originally designed to handle fixed-length vectors, or, using recursive mechanisms, ordered sequences thereof. In sharp contrast, measures describe a varying number of weighted observations with no particular order. We propose in this work a deep framework designed to handle crucial aspects of measures, namely permutation invariances, variations in weights and cardinality. Architectures derived from this pipeline can (i) map measures to measures - using the concept of push-forward operators; (ii) bridge the gap between measures and Euclidean spaces - through integration steps. This allows to design discriminative networks (to classify or reduce the dimensionality of input measures), generative architectures (to synthesize measures) and recurrent pipelines (to predict measure dynamics). We provide a theoretical analysis of these building blocks, review our architectures' approximation abilities and robustness w.r.t. perturbation, and try them on various discriminative and generative tasks.
","['Ecole normale supérieure', 'CNRS and ENS', 'ENSAE / CREST']"
2019,ELF OpenGo: an analysis and open reimplementation of AlphaZero,"Yuandong Tian, Jerry Ma, Qucheng Gong, Shubho Sengupta, Zhuoyuan Chen, James Pinkerton, Larry Zitnick",https://icml.cc/Conferences/2019/Schedule?showEvent=3883,"The AlphaGo, AlphaGo Zero, and AlphaZero series of algorithms are remarkable demonstrations of deep reinforcement learning's capabilities, achieving superhuman performance in the complex game of Go with progressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these promising approaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, we propose ELF OpenGo, an open-source reimplementation of the AlphaZero algorithm. ELF OpenGo is the first open-source Go AI to convincingly demonstrate superhuman performance with a perfect (20:0) record against global top professionals. We apply ELF OpenGo to conduct extensive ablation studies, and to identify and analyze numerous interesting phenomena in both the model training and in the gameplay inference procedures. Our code, models, selfplay datasets, and auxiliary data are publicly available.
","['Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research', 'Facebook', 'Facebook AI Research', 'Facebook AI Research']"
2019,Making Deep Q-learning methods robust to time discretization,"Corentin Tallec, Leonard Blier, Yann Ollivier",https://icml.cc/Conferences/2019/Schedule?showEvent=4112,"Despite remarkable successes, Deep Reinforce-
ment Learning (DRL) is not robust to hyperparam-
eterization, implementation details, or small envi-
ronment changes (Henderson et al. 2017, Zhang
et al. 2018). Overcoming such sensitivity is key
to making DRL applicable to real world problems.
In this paper, we identify sensitivity to time dis-
cretization in near continuous-time environments
as a critical factor; this covers, e.g., changing
the number of frames per second, or the action
frequency of the controller. Empirically, we find
that Q-learning-based approaches such as Deep Q-
learning (Mnih et al., 2015) and Deep Determinis-
tic Policy Gradient (Lillicrap et al., 2015) collapse
with small time steps. Formally, we prove that
Q-learning does not exist in continuous time. We
detail a principled way to build an off-policy RL
algorithm that yields similar performances over
a wide range of time discretizations, and confirm
this robustness empirically.
","['Univ. Paris-Sud', 'Université Paris Sud and Facebook', 'Facebook Artificial Intelligence Research']"
2019,Nonlinear Distributional Gradient Temporal-Difference Learning,"chao qu, Shie Mannor, Huan Xu",https://icml.cc/Conferences/2019/Schedule?showEvent=3628,"We devise a distributional  variant of gradient temporal-difference (TD) learning.   Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study \citep{bellemare2017distributional}. In the policy evaluation setting, we  design two new algorithms called  distributional GTD2  and distributional TDC  using the Cram{\'e}r distance on the distributional version of the Bellman error objective function, which inherits  advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using  similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for   general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexity of above three algorithms is linear w.r.t.\ the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.
","['Ant Financial Service Group', 'Technion', 'Georgia Tech']"
2019,Composing Entropic Policies using Divergence Correction,"Jonathan Hunt, Andre Barreto, Timothy Lillicrap, Nicolas Heess",https://icml.cc/Conferences/2019/Schedule?showEvent=3678,"Composing skills mastered in one task to solve novel tasks promises dramatic improvements in the data efficiency of reinforcement learning. Here, we analyze two recent works composing behaviors represented in the form of action-value functions and show that they perform poorly in some situations. As part of this analysis, we extend an important generalization of policy improvement to the maximum entropy framework and introduce an algorithm for the practical implementation of successor features in continuous action spaces. Then we propose a novel approach which addresses the failure cases of prior work and, in principle, recovers the optimal policy during transfer. This method works by explicitly learning the (discounted, future) divergence between base policies. We study this approach in the  tabular case and on non-trivial continuous control problems with compositional structure and show that it outperforms or matches existing methods across all tasks considered.
","['DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind']"
2019,TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning,"Tameem Adel, Adrian Weller",https://icml.cc/Conferences/2019/Schedule?showEvent=3581,"One of the challenges to reinforcement learning (RL) is scalable transferability among complex tasks. Incorporating a graphical model (GM), along with the rich family of related methods, as a basis for RL frameworks provides potential to address issues such as transferability, generalisation and exploration. Here we propose a flexible GM-based RL framework which leverages efficient inference procedures to enhance generalisation and transfer power. In our proposed transferable and information-based graphical model framework ‘TibGM’, we show the equivalence between our mutual information-based objective in the GM, and an RL consolidated objective consisting of a standard reward maximisation target and a generalisation/transfer objective. In settings where there is a sparse or deceptive reward signal, our TibGM framework is flexible enough to incorporate exploration bonuses depicting intrinsic rewards. We empirically verify improved performance and exploration power. 
","['University of Cambridge', 'University of Cambridge, Alan Turing Institute']"
2019,Multi-Agent Adversarial Inverse Reinforcement Learning,"Lantao Yu, Jiaming Song, Stefano Ermon",https://icml.cc/Conferences/2019/Schedule?showEvent=3622,"Reinforcement learning agents are prone to undesired behaviors due to reward mis-specification. Finding a set of reward functions to properly guide agent behaviors is particularly challenging in multi-agent scenarios. Inverse reinforcement learning provides a framework to automatically acquire suitable reward functions from expert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more complex notions of rational behaviors. In this paper, we propose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action space and unknown dynamics. We derive our algorithm based on a new solution concept and maximum pseudolikelihood estimation within an adversarial reward learning framework. In the experiments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with the ground truth rewards, while significantly outperforms prior methods in terms of policy imitation.
","['Stanford University', 'Stanford', 'Stanford University']"
2019,Policy Consolidation for Continual Reinforcement Learning,"Christos Kaplanis, Murray Shanahan, Claudia Clopath",https://icml.cc/Conferences/2019/Schedule?showEvent=4148,"We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is \textit{agnostic} to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries and can adapt in \textit{continuously} changing environments. In our \textit{policy consolidation} model, the policy network interacts with a cascade of hidden networks that simultaneously remember the agent's policy at a range of timescales and regularise the current policy by its own history, thereby improving its ability to learn without forgetting. We find that the model improves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent competitive self-play settings.
","['Imperial College London', 'DeepMind / Imperial College London', 'Imperial College London']"
2019,Off-Policy Deep Reinforcement Learning without Exploration,"Scott Fujimoto, David Meger, Doina Precup",https://icml.cc/Conferences/2019/Schedule?showEvent=3655,"Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.
","['McGill University', 'McGill University', 'McGill University / DeepMind']"
2019,Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation,"Ruohan Wang, Carlo Ciliberto, Pierluigi Vito Amadori, Yiannis Demiris",https://icml.cc/Conferences/2019/Schedule?showEvent=4038,"We consider the problem of imitation learning from a finite set of expert trajectories, without access to reinforcement signals. The classical approach of extracting the expert's reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be computationally expensive. Recent generative adversarial methods based on matching the policy distribution between the expert and the agent could be unstable during training. We propose a new framework for imitation learning by estimating the support of the expert policy to compute a fixed reward function, which allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains, achieving comparable or better performance than the state of the art under different reinforcement learning algorithms.
","['Imperial College London', 'Imperial College London', 'Imperial College London', 'Imperial College London']"
2019,Revisiting the Softmax Bellman Operator: New Benefits and New Perspective,"Zhao Song, Ron Parr, Lawrence Carin",https://icml.cc/Conferences/2019/Schedule?showEvent=4170,"The impact of softmax on the value function itself in reinforcement learning (RL) is often viewed as problematic because it leads to sub-optimal value (or Q) functions and interferes with the contraction properties of the Bellman operator. Surprisingly, despite these concerns, and independent of its effect on exploration, the softmax Bellman operator when combined with Deep Q-learning, leads to Q-functions with superior policies in practice, even outperforming its double Q-learning counterpart. To better understand how and why this occurs, we revisit theoretical properties of the softmax Bellman operator, and prove that (i) it converges to the standard Bellman operator exponentially fast in the inverse temperature parameter, and (ii) the distance of its Q function from the optimal one can be bounded. These alone do not explain its superior performance, so we also show that the softmax operator can reduce the overestimation error, which may give some insight into why a sub-optimal operator leads to better performance in the presence of value function approximation. A comparison among different Bellman operators is then presented, showing the trade-offs when selecting them.
","['Baidu Research', 'Duke University', 'Duke University']"
2019,An Investigation of Model-Free Planning,"Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sebastien Racaniere, Theophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver, Timothy Lillicrap",https://icml.cc/Conferences/2019/Schedule?showEvent=3809,"The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent's effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.
","['Google DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'Google DeepMind']"
2019,CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning,"Cédric Colas, Pierre-Yves Oudeyer, Olivier Sigaud, Pierre Fournier, Mohamed Chetouani",https://icml.cc/Conferences/2019/Schedule?showEvent=3722,"In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS , an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.
","['Inria', 'Inria', 'Sorbonne University', 'UPMC', 'UPMC']"
2019,Task-Agnostic Dynamics Priors for Deep Reinforcement Learning,"Yilun Du, Karthik Narasimhan",https://icml.cc/Conferences/2019/Schedule?showEvent=4136,"While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exploited by existing algorithms. In fact, humans continuously acquire and use such dynamics priors to easily adapt to operating in new environments. In this work, we propose an approach to learn task-agnostic dynamics priors from videos and incorporate them into an RL agent. Our method involves pre-training a frame predictor on task-agnostic physics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame prediction architecture, SpatialNet, is designed specifically to capture localized physical phenomena and interactions. Our approach allows for both faster policy learning and convergence to better policies, outperforming competitive approaches on several different environments. We also demonstrate that incorporating this prior allows for more effective transfer between environments.
","['MIT', 'Princeton']"
2019,Diagnosing Bottlenecks in Deep Q-learning Algorithms,"Justin Fu, Aviral Kumar, Matthew Soh, Sergey Levine",https://icml.cc/Conferences/2019/Schedule?showEvent=4253,"Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function approximation, especially with neural networks, is poorly understood theoretically and empirically. In this work, we aim to experimentally investigate potential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. 
Specifically, we investigate questions related to function approximation, sampling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with deep RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several practical compensations for overfitting; and develop a novel sampling method based on explicitly compensating for function approximation error that yields fair improvement on high-dimensional continuous control domains. 
","['University of California, Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2019,Collaborative Evolutionary Reinforcement Learning,"Shauharda Khadka, Somdeb Majumdar, Tarek Nassar, Zach Dwiel, Evren Tumer, Santiago Miret, Yinyin Liu, Kagan Tumer",https://icml.cc/Conferences/2019/Schedule?showEvent=3992,"Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyperparameters. One reason is that most approaches use a noisy version of their operating policy to explore - thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners - typically proven algorithms like TD3 - optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient - notably solving the Mujoco Humanoid benchmark where all of its composite learners (TD3) fail entirely in isolation.
","['Intel AI', 'Intel AI Lab', 'Intel AI Lab', 'Intel AI Lab', 'Intel Corporation', 'Intel AI Products Group', 'Intel AI Lab', 'Oregon State University US']"
2019,EMI: Exploration with Mutual Information,"Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, Hyun Oh Song",https://icml.cc/Conferences/2019/Schedule?showEvent=4026,"Reinforcement learning algorithms struggle when the reward signal is very sparse. In these cases, naive random exploration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or discriminative modeling of novelty. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show competitive results on challenging locomotion tasks with continuous control and on image-based exploration tasks with discrete actions on Atari. The source code is available at https://github.com/snu-mllab/EMI.
","['Seoul National University', 'Seoul National University', 'Seoul National University', 'UC Berkeley', 'Seoul National University']"
2019,Imitation Learning from Imperfect Demonstration,"Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, Masashi Sugiyama",https://icml.cc/Conferences/2019/Schedule?showEvent=3632,"Imitation learning (IL) aims to learn an optimal policy from demonstrations. However, such demonstrations are often imperfect since collecting optimal ones is costly. To effectively learn from imperfect demonstrations, we propose a novel approach that utilizes confidence scores, which describe the quality of demonstrations. More specifically, we propose two confidence-based IL methods, namely two-step importance weighting IL (2IWIL) and generative adversarial IL with imperfect demonstration and confidence (IC-GAIL). We show that confidence scores given only to a small portion of sub-optimal demonstrations significantly improve the performance of IL both theoretically and empirically.
","['National Taiwan University / RIKEN', 'The University of Tokyo / RIKEN', 'The University of Tokyo / RIKEN', 'RIKEN AIP', 'RIKEN / The University of Tokyo']"
2019,Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty ,"Youngjin Kim, Daniel Nam, Hyunwoo Kim, Ji-Hoon Kim, Gunhee Kim",https://icml.cc/Conferences/2019/Schedule?showEvent=3829,"Exploration based on state novelty has brought great success in challenging reinforcement learning problems with sparse rewards. However, existing novelty-based strategies become inefficient in real-world problems where observation contains not only task-dependent state novelty of our interest but also task-irrelevant information that should be ignored. We introduce an information- theoretic exploration strategy named Curiosity-Bottleneck that distills task-relevant information from observation. Based on the information bottleneck principle, our exploration bonus is quantified as the compressiveness of observation with respect to the learned representation of a compressive value network. With extensive experiments on static image classification, grid-world and three hard-exploration Atari games, we show that Curiosity-Bottleneck learns an effective exploration strategy by robustly measuring the state novelty in distractive environments where state-of-the-art exploration methods often degenerate.
","['NALBI Inc.', 'KC Machine Learning Lab', 'Seoul National University', 'Naver Corp.', 'Seoul National University']"
2019,Dynamic Weights in Multi-Objective Deep Reinforcement Learning,"Axel Abels, Diederik Roijers, Tom Lenaerts, Ann Nowé, Denis Steckelmacher",https://icml.cc/Conferences/2019/Schedule?showEvent=3636,"Many real-world decision problems are characterized by multiple conflicting objectives which must be balanced based on their relative importance. In the dynamic weights setting the relative importance changes over time and specialized algorithms that deal with such change, such as a tabular Reinforcement Learning (RL) algorithm by Natarajan and Tadepalli (2005), are required. However, this earlier work is not feasible for RL settings that necessitate the use of function approximators. We generalize across weight changes and high-dimensional inputs by proposing a multi-objective Q-network whose outputs are conditioned on the relative importance of objectives and we introduce Diverse Experience Replay (DER) to counter the inherent non-stationarity of the Dynamic Weights setting. We perform an extensive experimental evaluation and compare our methods to adapted algorithms from Deep Multi-Task/Multi-Objective Reinforcement Learning and show that our proposed network in combination with DER dominates these adapted algorithms across weight change scenarios and problem domains.
","['Université Libre de Bruxelles', 'VUB', 'Vrije Universiteit Brussel', 'Vrije Universiteit Brussel', 'Vrije Universiteit Brussel']"
2019,Fingerprint Policy Optimisation for Robust Reinforcement Learning,"Supratik Paul, Michael A Osborne, Shimon Whiteson",https://icml.cc/Conferences/2019/Schedule?showEvent=3743,"Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to suboptimal policies, if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO), which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. Our experiments show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling, but are key to learning good policies.
","['University of Oxford', 'U Oxford', 'University of Oxford']"
2019,An Investigation into Neural Net Optimization via Hessian Eigenvalue Density,"Behrooz Ghorbani, Shankar Krishnan, Ying Xiao",https://icml.cc/Conferences/2019/Schedule?showEvent=4303,"To understand the dynamics of training in deep neural networks, we study the evolution of the Hessian eigenvalue density throughout the optimization process. In non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In a batch normalized network, these two effects are almost absent. We give a theoretical rationale to partially explain these phenomena. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications. 
","['Stanford University', 'Google', 'Google Inc']"
2019,Differentiable Linearized ADMM,"Xingyu Xie, Jianlong Wu, Guangcan Liu, Zhisheng Zhong, Zhouchen Lin",https://icml.cc/Conferences/2019/Schedule?showEvent=3629,"Recently, a number of learning-based optimization methods that combine data-driven architectures with the classical optimization algorithms have been proposed and explored, showing superior empirical performance in solving various ill-posed inverse problems, but there is still a scarcity of rigorous analysis about the convergence behaviors of learning-based optimization. In particular, most existing analyses are specific to unconstrained problems but cannot apply to the more general cases where some variables of interest are subject to certain constraints. In this paper, we propose Differentiable Linearized ADMM (D-LADMM) for solving the problems with linear constraints. Specifically, D-LADMM is a K-layer LADMM inspired deep neural network, which is obtained by firstly introducing some learnable weights in the classical Linearized ADMM algorithm and then generalizing the proximal operator to some learnable activation function. Notably, we rigorously prove that there exist a set of learnable parameters for D-LADMM to generate globally converged solutions, and we show that those desired parameters can be attained by training D-LADMM in a proper way. To the best of our knowledge, we are the first to provide the convergence analysis for the learning-based optimization method on constrained problems.
","['Peking Unversity', 'Peking University', 'Nanjing University of Information Science and Technology', 'Peking University', 'Peking University']"
2019,Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search,"Youhei Akimoto, Shinichi Shirakawa, Nozomu Yoshinari, Kento Uchida, Shota Saito, Kouhei Nishida",https://icml.cc/Conferences/2019/Schedule?showEvent=3959,"High sensitivity of neural architecture search (NAS) methods against their input such as step-size (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process. Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS. We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation. It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast). We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust). Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-the-art performances with low computational budgets both on image classification and inpainting tasks.
","['University of Tsukuba / RIKEN AIP', 'Yokohama National University', 'Yokohama National University', 'Yokohama National University', 'Yokohama National University', 'Shinshu University']"
2019,A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent,"YongQiang Cai, Qianxiao Li, Zuowei Shen",https://icml.cc/Conferences/2019/Schedule?showEvent=3821,"Despite its empirical success and recent theoretical progress, there generally lacks a quantitative analysis of the effect of batch normalization (BN) on the convergence and stability of gradient descent. In this paper, we provide such an analysis on the simple problem of ordinary least squares (OLS), where the precise dynamical properties of gradient descent (GD) is completely known, thus allowing us to isolate and compare the additional effects of BN. More precisely, we show that unlike GD, gradient descent with BN (BNGD) converges for arbitrary learning rates for the weights, and the convergence remains linear under mild conditions. Moreover, we quantify two different sources of acceleration of BNGD over GD -- one due to over-parameterization which improves the effective condition number and another due having a large range of learning rates giving rise to fast descent. These phenomena set BNGD apart from GD and could account for much of its robustness properties. These findings are confirmed quantitatively by numerical experiments, which further show that many of the uncovered properties of BNGD in OLS are also observed qualitatively in more complex supervised learning problems.
","['National University of Singapore', 'National University of Singapore; IHPC, Singapore', 'National University of Singapore']"
2019,The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study,"Daniel Park, Jascha Sohl-Dickstein, Quoc Le, Samuel Smith",https://icml.cc/Conferences/2019/Schedule?showEvent=3994,"We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base network, and then perform a large hyper-parameter search to study how the test error depends on learning rate, batch size, and network width. We find that the optimal SGD hyper-parameters are determined by a ""normalized noise scale,"" which is a function of the batch size, learning rate, and initialization conditions. In the absence of batch normalization, the optimal normalized noise scale is directly proportional to width. Wider networks, with their higher optimal noise scale, also achieve higher test accuracy. These observations hold for MLPs, ConvNets, and ResNets, and for two different parameterization schemes (""Standard"" and ""NTK""). We observe a similar trend with batch normalization for ResNets. Surprisingly, since the largest stable learning rate is bounded, the largest batch size consistent with the optimal normalized noise scale decreases as the width increases.
","['Google Brain', 'Google Brain', 'Google Brain', 'DeepMind']"
2019,AdaGrad stepsizes: sharp convergence over nonconvex landscapes,"Rachel Ward, Xiaoxia Wu, Leon Bottou",https://icml.cc/Conferences/2019/Schedule?showEvent=4063,"Adaptive gradient methods such as AdaGrad and its variants update the stepsize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widespread use in large-scale optimization for their ability to converge robustly, without the need to fine-tune
parameters such as the stepsize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex optimization.   We bridge this gap by providing strong theoretical guarantees for the convergence of AdaGrad over smooth, nonconvex landscapes. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary point at the $\mathcal{O}(\log(N)/\sqrt{N})$ rate in the stochastic setting, and at the optimal $\mathcal{O}(1/N)$ rate in the batch (non-stochastic) setting -- in this sense, our convergence guarantees are ``sharp''. In particular, both our theoretical results and extensive numerical experiments imply that AdaGrad-Norm is robust to the \emph{unknown Lipschitz constant and level of stochastic noise on the gradient}.","['University of Texas', 'The University of Texas at Austin', 'Facebook']"
2019,Beyond Backprop: Online Alternating Minimization with Auxiliary Variables,"Anna Choromanska, Benjamin Cowen, Sadhana Kumaravel, Ronny Luss, Mattia Rigotti, Irina Rish, Paolo DiAchille, Viatcheslav  Gurev, Brian Kingsbury, Ravi Tejwani, Djallel Bouneffouf",https://icml.cc/Conferences/2019/Schedule?showEvent=4325,"Despite significant recent advances in deep neural networks,  training them remains a challenge due to the highly non-convex nature of the objective function.  State-of-the-art methods rely on error backpropagation, which suffers from   several well-known issues, such as vanishing and exploding gradients, inability to handle non-differentiable nonlinearities and to parallelize weight-updates across layers, and biological implausibility. These limitations continue to motivate exploration of alternative training algorithms,   including several recently proposed auxiliary-variable methods  which break the complex nested objective function into local subproblems. However, those techniques are mainly offline (batch), which limits their applicability to   extremely large datasets, as well as to online, continual or reinforcement learning.  The main contribution of our work is  a    novel online (stochastic/mini-batch) alternating minimization (AM) approach  for training deep neural networks, together with the first theoretical convergence guarantees for AM in stochastic settings and promising empirical results  on a variety of architectures  and datasets.
","['New York University', 'NYU', 'IBM Research', 'IBM Research', 'IBM Research AI', 'IBM Research AI', 'IBM Research', 'IBM Research', 'IBM Research', 'MIT', 'IBM Research']"
2019,SWALP : Stochastic Weight Averaging in Low Precision Training,"Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Wilson, Christopher De Sa",https://icml.cc/Conferences/2019/Schedule?showEvent=3602,"Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings. 
","['Cornell University', 'Cornell University', 'Cornell', 'Cornell', 'Cornell University', 'Cornell']"
2019,Efficient optimization of loops and limits with randomized telescoping sums,"Alex Beatson, Ryan P Adams",https://icml.cc/Conferences/2019/Schedule?showEvent=3852,"We consider optimization problems in which the objective requires an inner loop with many steps or is the limit of a sequence of increasingly costly approximations.
Meta-learning, training recurrent neural networks, and optimization of the solutions to differential equations are all examples of optimization problems with this character.
In such problems, it can be expensive to compute the objective function value and its gradient, but truncating the loop or using less accurate approximations can induce biases that damage the overall solution.
We propose \emph{randomized telescope} (RT) gradient estimators, which represent the objective as the sum of a telescoping series and sample linear combinations of terms to provide cheap unbiased gradient estimates.
We identify conditions under which RT estimators achieve optimization convergence rates independent of the length of the loop or the required accuracy of the approximation.
We also derive a method for tuning RT estimators online to maximize a lower bound on the expected decrease in loss per unit of computation.
We evaluate our adaptive RT estimators on a range of applications including meta-optimization of learning rates, variational inference of ODE parameters, and training an LSTM to model long sequences.
","['Princeton University', 'Princeton University']"
2019,Self-similar Epochs: Value in arrangement,"Eliav Buchnik, Edith Cohen, Avinatan Hasidim, Yossi Matias",https://icml.cc/Conferences/2019/Schedule?showEvent=4107,"Optimization of machine learning models is commonly performed through stochastic gradient updates on randomly ordered training examples. This practice means that each fraction of an epoch comprises an independent random sample of the training data that may not preserve informative structure present in the full data.  We hypothesize that the training can be more effective with {\it self-similar}  arrangements that potentially allow each epoch to  provide benefits of multiple ones. We study this for  ``matrix factorization'' -- the common task of learning metric embeddings of entities such as queries, videos,  or words from example pairwise associations. We construct arrangements that preserve the weighted Jaccard similarities of  rows and columns and experimentally observe training acceleration of 3\%-37\% on synthetic and recommendation datasets.  Principled arrangements of training examples emerge as a novel and potentially powerful enhancement to SGD that merits further exploration.
","['Google & Tel Aviv University', 'Google Research and Tel Aviv University', 'Google', 'Google']"
2019,Adversarial Attacks on Node Embeddings via Graph Poisoning,"Aleksandar Bojchevski, Stephan Günnemann",https://icml.cc/Conferences/2019/Schedule?showEvent=4072,"The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for solving downstream tasks. However, despite the proliferation of such methods, there is currently no study of their robustness to adversarial attacks. We provide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial perturbations that poison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.
","['Technical University of Munich', 'Technical University of Munich']"
2019,First-Order Adversarial Vulnerability of Neural Networks and Input Dimension,"Carl-Johann Simon-Gabriel, Yann Ollivier, Leon Bottou, Bernhard Schölkopf, David Lopez-Paz",https://icml.cc/Conferences/2019/Schedule?showEvent=4017,"Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the L1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension-dependence persists after either usual or robust training, but gets attenuated with higher regularization.
","['Max-Planck-Institute for Intelligent Systems', 'Facebook Artificial Intelligence Research', 'Facebook', 'MPI for Intelligent Systems Tübingen, Germany', 'Facebook AI Research']"
2019,On Certifying Non-Uniform Bounds against Adversarial Attacks,"Chen Liu, Ryota Tomioka, Volkan Cevher",https://icml.cc/Conferences/2019/Schedule?showEvent=4033,"This work studies the robustness certification problem of neural network models,
which aims to find certified adversary-free regions as large as possible around data points.
In contrast to the existing approaches that seek regions bounded uniformly along all input features,
we consider non-uniform bounds and use it to study the decision boundary of neural network models.
We formulate our target as an optimization problem with nonlinear constraints.
Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits
so that the relaxed problem can be solved by the augmented Lagrangian method.
Our experiments show the non-uniform bounds have larger volumes than uniform ones.
Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability.
Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features' robustness.
","['EPFL', 'Microsoft Research Cambridge', 'EPFL']"
2019,Improving Adversarial Robustness via Promoting Ensemble Diversity,"Tianyu Pang, Kun Xu, Chao Du, Ning Chen, Jun Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3614,"Though deep neural networks have achieved significant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the outputs, which ignores the interaction among networks. This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members, and present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members. Our method is computationally efficient and compatible with the defense methods acting on individual networks. Empirical results on various datasets verify that our method can improve adversarial robustness while maintaining state-of-the-art accuracy on normal examples.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', '', 'Tsinghua University']"
2019,Adversarial camera stickers: A physical camera-based attack on deep learning systems,"Juncheng Li, Frank R Schmidt, Zico Kolter",https://icml.cc/Conferences/2019/Schedule?showEvent=4262,"Recent work has documented the susceptibility of deep learning systems to adversarial examples, but most such attacks directly manipulate the digital input to a classifier. Although a smaller line of work considers physical adversarial attacks, in all cases these involve manipulating the object of interest, e.g., putting a physical sticker on an object to misclassify it, or manufacturing an object specifically intended to be misclassified. In this work, we consider an alternative question: is it possible to fool deep classifiers, over all perceived objects of a certain type, by physically manipulating the camera itself? We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different (targeted) class. To accomplish this, we propose an iterative procedure for both updating the attack perturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is physically realizable). For example, we show that we can achieve physically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6% of the time. This presents a new class of physically-realizable threat models to consider in the context of adversarially robust machine learning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54
","['Carnegie Mellon University', 'Robert Bosch GmbH', 'Carnegie Mellon University / Bosch Center for AI']"
2019,Adversarial examples from computational constraints,"Sebastien Bubeck, Yin Tat Lee, Eric Price, Ilya Razenshteyn",https://icml.cc/Conferences/2019/Schedule?showEvent=3956,"Why are classifiers in high dimension vulnerable to “adversarial” perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints.
First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give two particular classification tasks where learning a robust classifier is computationally intractable. More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (non-robustly) by a simple linear separator,
(iii) yet are not efficiently robustly learnable, even for small perturbations. Specifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a cryptographic assumption. These examples give an exponential separation between classical learning and robust learning in the statistical query model or under a cryptographic assumption. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms. 
","['Microsoft Research', 'UW', 'UT-Austin', 'Microsoft Research Redmond']"
2019,POPQORN: Quantifying Robustness of Recurrent Neural Networks,"CHING-YUN KO, Zhaoyang Lyu, Tsui-Wei Weng, Luca Daniel, Ngai Wong, Dahua Lin",https://icml.cc/Conferences/2019/Schedule?showEvent=3701,"The vulnerability to adversarial attacks has been a critical issue for deep neural networks. Addressing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been developed to compute robustness quantification for neural networks, namely, certified lower bounds of the minimum adversarial perturbation. Such methods, however, were devised for feed-forward networks, e.g. multi-layer perceptron or convolutional networks. It remains an open problem to quantify robustness for recurrent networks, especially LSTM and GRU. 
For such networks, there exist additional challenges in computing the robustness quantification, such as handling the inputs at multiple steps and the interaction between gates and states. In this work, we propose POPQORN (Propagated-output Quantified Robustness for RNNs), a general algorithm to quantify robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its effectiveness on different network architectures and show that the robustness quantification on individual steps can lead to new insights.
","['The University of Hong Kong', 'The Chinese University of Hong Kong', 'MIT', 'Massachusetts Institute of Technology', 'The University of Hong Kong', 'The Chinese University of Hong Kong']"
2019,Using Pre-Training Can Improve Model Robustness and Uncertainty,"Dan Hendrycks, Kimin Lee, Mantas Mazeika",https://icml.cc/Conferences/2019/Schedule?showEvent=4237,"He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.
","['UC Berkeley', 'KAIST', 'University of Chicago']"
2019,Generalized No Free Lunch Theorem for Adversarial Robustness,Elvis Dohmatob,https://icml.cc/Conferences/2019/Schedule?showEvent=4225,"This manuscript presents some new impossibility results on adversarial robustness
in machine learning, a very important yet largely open problem. We show that if conditioned on a class label the data distribution satisfies the $W_2$ Talagrand
transportation-cost inequality (for example, this condition is satisfied if the conditional distribution has density which is log-concave; is the uniform measure on a compact Riemannian manifold with positive Ricci curvature, any classifier can be adversarially fooled with high probability once the perturbations are slightly greater than the natural noise level in the problem. We call this result The Strong ""No Free Lunch"" Theorem as some recent results (Tsipras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very particular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscript with some speculation on possible future research directions.",['Criteo AI Lab']
2019,PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach,"Tsui-Wei Weng, Pin-Yu Chen, Lam Nguyen, Mark Squillante, Akhilan Boopathy, Ivan Oseledets, Luca Daniel",https://icml.cc/Conferences/2019/Schedule?showEvent=4021," We propose a novel framework PROVEN to \textbf{PRO}babilistically \textbf{VE}rify \textbf{N}eural network's robustness with statistical guarantees. PROVEN provides probability certificates of neural network robustness when the input perturbation follow distributional characterization. Notably, PROVEN is derived from current state-of-the-art worst-case neural network robustness verification frameworks, and therefore it can provide probability certificates with little computational overhead on top of existing  methods such as Fast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR neural network models demonstrate our probabilistic approach can tighten up robustness certificate to around $1.8 \times$ and $3.5 \times$ with at least a $99.99\%$ confidence compared with the worst-case robustness certificate by CROWN and CNN-Cert.  ","['MIT', 'IBM Research AI', 'IBM Research, Thomas J. Watson Research Center', 'IBM Research', 'MIT', 'Skolkovo Institute of Science and Technology', 'Massachusetts Institute of Technology']"
2019,On Learning Invariant Representations for Domain Adaptation,"Han Zhao, Remi Tachet des Combes, Kun Zhang, Geoff Gordon",https://icml.cc/Conferences/2019/Schedule?showEvent=4264,"Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits \emph{conditional shift}: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of \emph{any} domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.

","['Carnegie Mellon University', 'Microsoft Research Montreal', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2019,Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models,"Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nati Srebro, Daniel Soudry",https://icml.cc/Conferences/2019/Schedule?showEvent=4224,"With an eye toward understanding complexity control in deep learning, we study how infinitesimal regularization or gradient descent optimization lead to margin maximizing solutions in both homogeneous and non homogeneous models, extending previous work that focused on infinitesimal regularization only in homogeneous models.  To this end we study the limit of loss minimization with a diverging norm constraint (the constrained path''), relate it to the limit of amargin path'' and characterize the resulting solution.  For non-homogeneous ensemble models, which output is a sum of homogeneous sub-models, we show that this solution discards the shallowest sub-models if they are unnecessary. For homogeneous models, we show convergence to a ``lexicographic max-margin solution'', and provide conditions under which max-margin solutions are also attained as the limit of unconstrained gradient descent.
","['Technion', 'Toyota Technological Institute at Chicago', 'University of Southern California', 'Toyota Technological Institute at Chicago', 'Technion']"
2019,Adversarial Generation of Time-Frequency Features with application in audio synthesis,"Andrés Marafioti, Nathanaël Perraudin, Nicki Holighaus, Piotr Majdak",https://icml.cc/Conferences/2019/Schedule?showEvent=4016,"Time-frequency (TF) representations provide powerful and intuitive features for the analysis of time series such as audio. But still, generative modeling of audio in the TF domain is a subtle matter. Consequently, neural audio synthesis widely relies on directly modeling the waveform and previous attempts at unconditionally synthesizing audio from neurally generated invertible TF features still struggle to produce audio at satisfying quality. In this article, focusing on the short-time Fourier transform, we discuss the challenges that arise in audio synthesis based on generated invertible TF features and how to overcome them. We demonstrate the potential of deliberate generative TF modeling by training a generative adversarial network (GAN) on short-time Fourier features. We show that by applying our guidelines, our TF-based network was able to outperform a state-of-the-art GAN generating waveforms directly, despite the similar architecture in the two networks. 
","['Austrian Academy of Sciences', 'Swiss Data Science Center', 'Acoustics Research Institute', 'Acoustics Research Institute']"
2019,On the Universality of Invariant Networks,"Haggai Maron, Ethan Fetaya, Nimrod Segol, Yaron Lipman",https://icml.cc/Conferences/2019/Schedule?showEvent=3813,"Constraining linear layers in neural networks to respect symmetry transformations from a group $G$ is a common design principle for invariant networks that has found many applications in machine learning. 		
In this paper, we consider a fundamental question that has received very little attention to date: Can these networks approximate any (continuous) invariant function? 		
We tackle the rather general case where $G\leq S_n$ (an arbitrary subgroup of the symmetric group) that acts on $\R^n$ by permuting coordinates. This setting includes several recent popular invariant networks. We present two main results: First, $G$-invariant networks are universal if high-order tensors are allowed. Second, there are groups $G$ for which higher-order tensors are unavoidable for obtaining universality. 		
$G$-invariant networks consisting of only first-order tensors are of special interest due to their practical value. We conclude the paper by proving a necessary condition for the universality of $G$-invariant networks that incorporate only first-order tensors. Lastly, we propose a conjecture stating that this condition is also sufficient. ","['Weizmann Institute of Science', 'University of Toronto', 'Weizmann Institute of Science', 'Weizmann Institute of Science']"
2019,Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,"Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, Ruosong Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=3996,"Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works:
(i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17].
(ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size.
(iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent.
The key idea is to track dynamics of training and generalization via properties of a related kernel.
","[' Princeton University and Institute for Advanced Study', 'Carnegie Mellon University', 'Princeton University', 'Princeton University', 'Carnegie Mellon University']"
2019,Gauge Equivariant Convolutional Networks and the Icosahedral CNN,"Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, Max Welling",https://icml.cc/Conferences/2019/Schedule?showEvent=3872,"The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning.
We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.
","['Qualcomm AI Research', 'University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam & Qualcomm']"
2019,Feature-Critic Networks for Heterogeneous Domain Generalization,"Yiying Li, Yongxin Yang, Wei Zhou, Timothy Hospedales",https://icml.cc/Conferences/2019/Schedule?showEvent=4052,"The well known domain shift issue causes model performance to degrade when deployed to a new target domain with different statistics to training. Domain adaptation techniques alleviate this, but need some instances from the target domain to drive adaptation. Domain generalisation is the recently topical problem of learning a model that generalises to unseen domains out of the box, and various approaches aim to train a domain-invariant feature extractor, typically by adding some manually designed losses. In this work, we propose a learning to learn approach, where the auxiliary loss that helps generalisation is itself learned. Beyond conventional domain generalisation, we consider a more challenging setting of heterogeneous domain generalisation, where the unseen domains do not share label space with the seen ones, and the goal is to train a feature representation that is useful off-the-shelf for novel data and novel categories. Experimental evaluation demonstrates that our method outperforms state-of-the-art solutions in both settings.
","['National University of Defense Technology', 'University of Edinburgh ', 'National University of Defense Technology', 'Samsung AI Centre / University of Edinburgh']"
2019,Learning to Convolve: A Generalized Weight-Tying Approach,"Nichita Diaconu, Daniel E Worrall",https://icml.cc/Conferences/2019/Schedule?showEvent=3761,"Recent work (Cohen & Welling, 2016) has shown that generalizations of convolutions, based on group theory, provide powerful inductive biases for learning. In these generalizations, filters are not only translated but can also be rotated, flipped, etc. However, coming up with exact models of how to rotate a 3x3 filter on a square pixel-grid is difficult. In this paper, we learn how to transform filters for use in the group convolution, focussing on roto-translation. For this, we learn a filter basis and all rotated versions of that filter basis. Filters are then encoded by a set of rotation invariant coefficients. To rotate a filter, we switch the basis. We demonstrate we can produce feature maps with low sensitivity to input rotations, while achieving high performance on MNIST and CIFAR-10.
","['University of Amsterdam', 'University of Amsterdam']"
2019,On Dropout and Nuclear Norm Regularization,"Poorya Mianjy, Raman Arora",https://icml.cc/Conferences/2019/Schedule?showEvent=4173,"We give a formal and complete characterization of the explicit regularizer induced by dropout in deep linear networks with squared loss. We show that (a) the explicit regularizer is composed of an $\ell_2$-path regularizer and other terms that are also re-scaling invariant, (b) the convex envelope of the induced regularizer is the squared nuclear norm of the network map, and (c) for a sufficiently large dropout rate, we characterize the global optima of the dropout objective. We validate our theoretical findings with empirical results.","['Johns Hopkins University', 'Johns Hopkins University']"
2019,Gradient Descent Finds Global Minima of Deep Neural Networks,"Simon Du, Jason Lee, Haochuan Li, Liwei Wang, Xiyu Zhai",https://icml.cc/Conferences/2019/Schedule?showEvent=3786,"Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.
","['Carnegie Mellon University', 'University of Southern California', 'Peking University', 'Peking University', 'Massachusetts Institute of Technology']"
2019,Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm,"Sepideh Mahabadi, Piotr Indyk, Shayan Oveis Gharan, Alireza Rezaei",https://icml.cc/Conferences/2019/Schedule?showEvent=3957,"``Composable core-sets'' are an efficient framework for solving optimization problems in massive data models. In this work, we consider efficient construction of composable core-sets for the determinant maximization problem.
This can also be cast as the MAP inference task for ``determinantal point processes"", that have recently gained a lot of interest for modeling diversity and fairness. The problem was recently studied in \cite{indyk2018composable}, where they designed composable core-sets with the optimal approximation bound of $O(k)^k$. On the other hand, the more practical ``Greedy"" algorithm has been previously used in similar contexts. In this work, first we provide a theoretical approximation guarantee of $C^{k^2}$ for the Greedy algorithm in the context of composable core-sets; Further, we propose to use a ``Local Search"" based algorithm that while being still practical, achieves a nearly optimal approximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms and show the effectiveness of our proposed algorithm on standard data sets.","['Toyota Technological Institute at Chicago', 'MIT', 'University of Washington', 'University of Washington']"
2019,Sublinear Time Nearest Neighbor Search over Generalized Weighted Space,"Yifan Lei, Qiang Huang, Mohan Kankanhalli, Anthony Tung",https://icml.cc/Conferences/2019/Schedule?showEvent=3631,"Nearest Neighbor Search (NNS) over generalized weighted space is a fundamental problem which has many applications in various fields. However, to the best of our knowledge, there is no sublinear time solution to this problem. Based on the idea of Asymmetric Locality-Sensitive Hashing (ALSH), we introduce a novel spherical asymmetric transformation and propose the first two novel weight-oblivious hashing schemes SL-ALSH and S2-ALSH accordingly. We further show that both schemes enjoy a quality guarantee and can answer the NNS queries in sublinear time. Evaluations over three real datasets demonstrate the superior performance of the two proposed schemes.
","['National University of Singapore', 'National University of Singapore', 'National University of Singapore,', 'NUS']"
2019,Compressing Gradient Optimizers via Count-Sketches,"Ryan Spring, Anastasios Kyrillidis, Vijai Mohan, Anshumali Shrivastava",https://icml.cc/Conferences/2019/Schedule?showEvent=3666,"Many popular first-order optimization methods accelerate the convergence rate of deep learning models. However, these algorithms require auxiliary variables, which cost additional memory proportional to the number of parameters in the model. The problem is becoming more severe as models grow larger to learn from complex, large-scale datasets. Our proposed solution is to maintain a linear sketch to compress the auxiliary variables. Our approach has the same performance as the full-sized baseline, while using less space for the auxiliary variables. Theoretically, we prove that count-sketch optimization maintains the SGD convergence rate, while gracefully reducing memory usage for large-models. We show a rigorous evaluation on popular architectures such as ResNet-18 and Transformer-XL. On the 1-Billion Word dataset, we save 25% of the memory used during training (7.7 GB instead of 10.8 GB) with minimal accuracy and performance loss. For an Amazon extreme classification task with over 49.5 million classes, we also reduce the training time by 38%, by increasing the mini-batch size 3.5x using our count-sketch optimizer.
","['Rice University', 'Rice University', 'www.amazon.com', 'Rice University']"
2019,Scalable Fair Clustering,"Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, Tal Wagner",https://icml.cc/Conferences/2019/Schedule?showEvent=3962,"We study the fair variant of the classic k-median problem introduced by (Chierichetti et al., NeurIPS 2017) in which the points are colored, and the goal is to minimize the same average distance objective as in the standard $k$-median problem while ensuring that all clusters have an ``approximately equal'' number of points of each color. 
(Chierichetti et al., NeurIPS 2017) proposed a two-phase algorithm for fair $k$-clustering. In the first step, the pointset is partitioned into subsets called fairlets that satisfy the fairness requirement and approximately preserve the k-median objective. In the second step, fairlets are merged into k clusters by one of the existing k-median algorithms. The running time of this algorithm is dominated by the first step, which takes super-quadratic time. 
In this paper, we present a practical approximate fairlet decomposition algorithm that runs in nearly linear time. ","['Toyota Technological Institute at Chicago (TTIC)', 'MIT', 'IBM Research', 'New Jersey Institute of Technology', 'Massachusetts Institute of Technology', 'MIT']"
2019,Conditional Gradient Methods via Stochastic Path-Integrated Differential Estimator,"Alp Yurtsever, Suvrit Sra, Volkan Cevher",https://icml.cc/Conferences/2019/Schedule?showEvent=4115,"We propose a class of variance-reduced stochastic conditional gradient methods. By adopting the recent stochastic path-integrated differential estimator technique (SPIDER) of Fang et. al. (2018) for the classical Frank-Wolfe (FW) method, we introduce SPIDER-FW for finite-sum minimization as well as the more general expectation minimization problems. SPIDER-FW enjoys superior complexity guarantees in the non-convex setting, while matching the best known FW variants in the convex case. We also extend our framework a la conditional gradient sliding (CGS) of Lan & Zhou. (2016), and propose SPIDER-CGS.
","['EPFL', 'MIT', 'EPFL']"
2019, Fault Tolerance in Iterative-Convergent Machine Learning,"Aurick Qiao, Bryon Aragam, Bingjing Zhang, Eric Xing",https://icml.cc/Conferences/2019/Schedule?showEvent=4193,"Machine learning (ML) training algorithms often possess an inherent self-correcting behavior due to their iterative- convergent nature. Recent systems exploit this property to achieve adaptability and efficiency in unreliable computing environments by relaxing the consistency of execution and allowing calculation errors to be self-corrected during training. However, the behavior of such systems are only well understood for specific types of calculation errors, such as those caused by staleness, reduced precision, or asynchronicity, and for specific algorithms, such as stochastic gradient descent. In this paper, we develop a general framework to quantify the effects of calculation errors on iterative-convergent algorithms. We then use this framework to derive a worst-case upper bound on the cost of arbitrary perturbations to model parameters during training and to design new strategies for checkpoint-based fault tolerance. Our system, SCAR, can reduce the cost of partial failures by 78%–95% when compared with traditional checkpoint-based fault tolerance across a variety of ML models and training algorithms, providing near-optimal performance in recovering from failures.
","['Petuum, Inc. and Carnegie Mellon University', 'Carnegie Mellon University', 'Petuum, Inc.', 'Petuum Inc. and CMU']"
2019,Static Automatic Batching In TensorFlow,Ashish Agarwal,https://icml.cc/Conferences/2019/Schedule?showEvent=3703,"Dynamic neural networks are becoming increasingly common, and yet it is hard to implement them efficiently. On-the-fly operation batching for such models is sub-optimal and suffers from run time overheads, while writing manually batched versions can be hard and error-prone. To address this we extend TensorFlow with pfor, a parallel-for loop optimized using static loop vectorization. With pfor, users can express computation using nested loops and conditional constructs, but get performance resembling that of a manually batched version. Benchmarks demonstrate speedups of one to two orders of magnitude on range of tasks, from jacobian computation, to Graph Neural Networks. 
",['Google Brain']
2019,Improving Neural Network Quantization without Retraining using Outlier Channel Splitting,"Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3981,"Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied topic of quantizing a floating-point model without (re)training. DNN weights and activations follow a bell-shaped distribution post-training, while practical hardware uses a linear quantization grid. This leads to challenges in dealing with outliers in the distribution. Prior work has addressed this by clipping the outliers or using specialized hardware. In this work, we propose outlier channel splitting (OCS), which duplicates channels containing outliers, then halves the channel values. The network remains functionally identical, but affected outliers are moved toward the center of the distribution. OCS requires no additional training and works on commodity hardware. Experimental evaluation on ImageNet classification and language modeling shows that OCS can outperform state-of-the-art clipping techniques with only minor overhead.
","['Cornell University', 'Cornell University', 'Cornell University', 'Cornell', 'Cornell Univeristy']"
2019,Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications,"Albert Gural, Boris Murmann",https://icml.cc/Conferences/2019/Schedule?showEvent=4308,"In the age of Internet of Things (IoT), embedded devices ranging from ARM Cortex M0s with hundreds of KB of RAM to Arduinos with 2KB RAM are expected to perform increasingly sophisticated classification tasks, such as voice and gesture recognition, activity tracking, and biometric security. While convolutional neural networks (CNNs), together with spectrogram preprocessing, are a natural solution to many of these classification tasks, storage of the network's activations often exceeds the hard memory constraints of embedded platforms. This paper presents memory-optimal direct convolutions as a way to push classification accuracy as high as possible given strict hardware memory constraints at the expense of extra compute. We therefore explore the opposite end of the compute-memory trade-off curve from standard approaches that minimize latency. We validate the memory-optimal CNN technique with an Arduino implementation of the 10-class MNIST classification task, fitting the network specification, weights, and activations entirely within 2KB SRAM and achieving a state-of-the-art classification accuracy for small-scale embedded systems of 99.15%.
","['Stanford University', 'Stanford University']"
2019,DL2: Training and Querying Neural Networks with Logic,"Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce Zhang, Martin Vechev",https://icml.cc/Conferences/2019/Schedule?showEvent=4331,"We present DL2, a system for training and querying neural networks with logical constraints. Using DL2, one can declaratively specify domain knowledge constraints to be enforced during training, as well as pose queries on the model to find inputs that satisfy a set of constraints. DL2 works by translating logical constraints into a loss function with desirable mathematical properties. The loss is then minimized with standard gradient-based methods. We evaluate DL2 by training networks with interesting constraints in unsupervised, semi-supervised and supervised settings. Our experimental evaluation demonstrates that DL2 is more expressive than prior approaches combining logic and neural networks, and its loss functions are better suited for optimization. Further, we show that for a number of queries, DL2 can find the desired inputs in seconds (even for large models such as ResNet-50 on ImageNet).
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2019,PA-GD: On the Convergence of Perturbed Alternating Gradient Descent to Second-Order Stationary Points for Structured Nonconvex Optimization,"Songtao Lu, Mingyi Hong, Zhengdao Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=4138,"Alternating gradient descent (A-GD) is a simple but popular algorithm in machine learning, which updates two blocks of variables in an alternating manner using gradient descent steps. In this paper, we consider a smooth unconstrained nonconvex optimization problem, and propose a perturbed A-GD (PA-GD) which is able to converge (with high probability) to the second-order stationary points (SOSPs) with a global sublinear rate. Existing analysis on A-GD type algorithm either only guarantees convergence to first-order solutions, or converges to second-order solutions asymptotically (without rates). To the best of our knowledge, this is the first alternating type algorithm that takes $\mathcal{O}(\text{polylog}(d)/\epsilon^2)$ iterations to achieve an ($\epsilon,\sqrt{\epsilon}$)-SOSP with high probability, where polylog$(d)$ denotes the polynomial of the logarithm with respect to problem dimension $d$.","['University of Minnesota Twin Cities', 'University of Minnesota', 'Iowa State University']"
2019,Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization,"Kaiyi Ji, Zhe Wang, Yi Zhou, Yingbin LIANG",https://icml.cc/Conferences/2019/Schedule?showEvent=3691,"Two types of zeroth-order stochastic algorithms have recently been designed for nonconvex optimization respectively based on the first-order techniques SVRG and SARAH/SPIDER. This paper addresses several important issues that are still open in these methods. First, all existing SVRG-type zeroth-order algorithms suffer from worse function query complexities than either zeroth-order gradient descent (ZO-GD) or stochastic gradient descent (ZO-SGD). In this paper, we propose a new algorithm ZO-SVRG-Coord-Rand and develop a new analysis for an existing ZO-SVRG-Coord algorithm proposed in Liu et al. 2018b, and show that both ZO-SVRG-Coord-Rand and ZO-SVRG-Coord (under our new analysis) outperform other exiting SVRG-type zeroth-order methods as well as ZO-GD and ZO-SGD. Second, the existing SPIDER-type algorithm SPIDER-SZO (Fang et al., 2018) has superior theoretical performance, but suffers from the generation of a large number of Gaussian random variables as well as a $\sqrt{\epsilon}$-level stepsize in practice. In this paper, we develop a new algorithm ZO-SPIDER-Coord, which is free from Gaussian variable generation and allows a large constant stepsize while maintaining the same convergence rate and query complexity, and we further show that ZO-SPIDER-Coord automatically achieves a linear convergence rate as the iterate enters into a local PL region without restart and algorithmic modification.","['The Ohio State University', 'Ohio State University', 'Duke University', 'The Ohio State University']"
2019,Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization,"Feihu Huang, Songcan Chen, Heng Huang",https://icml.cc/Conferences/2019/Schedule?showEvent=3615,"In this paper, we propose a faster stochastic alternating direction method of multipliers (ADMM) for nonconvex optimization by using a new stochastic path-integrated differential estimator (SPIDER), called as SPIDER-ADMM. Moreover, we prove that the SPIDER-ADMM achieves a record-breaking incremental first-order oracle (IFO) complexity for finding an ϵ-approximate solution. As one of major contribution of this paper, we provide a new theoretical analysis framework for nonconvex stochastic ADMM methods with providing the optimal IFO complexity. Based on this new analysis framework, we study the unsolved optimal IFO complexity of the existing non-convex SVRG-ADMM and SAGA-ADMM methods, and prove their the optimal IFO complexity. Thus, the SPIDER-ADMM improves the existing stochastic ADMM methods. Moreover, we extend SPIDER-ADMM to the online setting, and propose a faster online SPIDER-ADMM. Our theoretical analysis also derives the IFO complexity of the online SPIDER-ADMM. Finally, the experimental results on benchmark datasets validate that the proposed algorithms have faster convergence rate than the existing ADMM algorithms for nonconvex optimization.
","['University of Pittsburgh', 'Nanjing University of Aeronautics and Astronautics', 'University of Pittsburgh']"
2019,Lower Bounds for Smooth Nonconvex Finite-Sum Optimization,"Dongruo Zhou, Quanquan Gu",https://icml.cc/Conferences/2019/Schedule?showEvent=4168,"Smooth finite-sum optimization has been widely studied in both convex and nonconvex settings. However, existing lower bounds for finite-sum optimization are mostly limited to the setting where each component function is (strongly) convex, while the lower bounds for nonconvex finite-sum optimization remain largely unsolved. In this paper, we study the lower bounds for smooth nonconvex finite-sum optimization, where the objective function is the average of $n$ nonconvex component functions. We prove tight lower bounds for the complexity of finding $\epsilon$-suboptimal point and $\epsilon$-approximate stationary point in different settings, for a wide regime of the smallest eigenvalue of the Hessian of the objective function (or each component function). Given our lower bounds, we can show that existing algorithms including {KatyushaX} \citep{allen2018katyushax}, {Natasha} \citep{allen2017natasha} and {StagewiseKatyusha} \citep{yang2018does} have achieved optimal {Incremental First-order Oracle} (IFO) complexity (i.e., number of IFO calls) up to logarithm factors for nonconvex finite-sum optimization. We also point out potential ways to further improve these complexity results, in terms of making stronger assumptions or by a different convergence analysis.","['UCLA', 'University of California, Los Angeles']"
2019,Nonconvex Variance Reduced Optimization with Arbitrary Sampling,"Samuel Horvath, Peter Richtarik",https://icml.cc/Conferences/2019/Schedule?showEvent=3831,"We provide the first importance sampling variants of variance reduced algorithms for empirical risk minimization with non-convex loss functions. In particular, we analyze non-convex versions of \texttt{SVRG}, \texttt{SAGA} and \texttt{SARAH}. Our methods have the capacity to speed up the training process by  an order of magnitude compared to the state of the art on real datasets. Moreover, we also improve upon current mini-batch analysis of these methods by proposing  importance sampling for minibatches in this setting. Surprisingly, our approach can in some regimes lead to superlinear speedup with respect to the minibatch size, which is not usually present in stochastic optimization. All the above results follow from a general analysis of the methods which works with {\em arbitrary sampling}, i.e., fully general randomized strategy for the selection of subsets of examples to be sampled in each iteration. Finally, we also perform a novel importance sampling analysis of \texttt{SARAH} in the convex setting.
","['KAUST', 'KAUST']"
2019,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,"Sai Praneeth Reddy Karimireddy, Quentin Rebjock, Sebastian Stich, Martin Jaggi",https://icml.cc/Conferences/2019/Schedule?showEvent=4158,"Sign-based algorithms (e.g. signSGD) have been proposed as a biased gradient compression technique to alleviate the communication bottleneck in training large neural networks across multiple workers. We show simple convex counter-examples where signSGD does not converge to the optimum.
Further, even when it does converge, signSGD may generalize poorly when compared with SGD.  These issues arise because of the biased nature of the sign compression operator.
We then show that using error-feedback, i.e. incorporating the error made by the compression operator into the next step, overcomes these issues. We prove that our algorithm (EF-SGD) with arbitrary compression operator achieves the same rate of convergence as SGD without any additional assumptions. Thus EF-SGD achieves gradient compression for free. Our experiments thoroughly substantiate the theory.
","['EPFL', 'EPFL', 'EPFL', 'EPFL']"
2019,A Composite Randomized Incremental Gradient Method,"Junyu Zhang, Lin Xiao",https://icml.cc/Conferences/2019/Schedule?showEvent=3875,"We consider the problem of minimizing the composition of a smooth function (which can be nonconvex) and a smooth vector mapping, where both of them can be express as the average of a large number of components. We propose a composite randomized incremental gradient method by extending the SAGA framework. The gradient sample complexity of our method matches that of several recently developed methods based on SVRG in the general case. However, for structured problems where linear convergence rates can be obtained, our method can be much better for ill-conditioned problems. In addition, when the finite-sum structure only appear for the inner mapping, the sample complexity of our method is the same as that of SAGA for minimizing finite sum of smooth nonconvex functions, despite the additional outer composition and the stochastic composite gradients being biased in our case.
","['University of Minnesota, Twin Cities', 'Microsoft Research']"
2019,Optimal Continuous DR-Submodular Maximization and Applications to Provable Mean Field Inference,"Yatao Bian, Joachim Buhmann, Andreas Krause",https://icml.cc/Conferences/2019/Schedule?showEvent=4006,"Mean field inference for discrete graphical models is generally a highly nonconvex problem, which also holds for the class of probabilistic log-submodular models. Existing optimization methods, e.g., coordinate ascent  algorithms, typically only find local optima.
In this work we propose provable mean filed  methods for probabilistic  log-submodular models and its posterior agreement (PA) with strong approximation guarantees. The main algorithmic technique is a new Double Greedy scheme, termed DR-DoubleGreedy, for continuous DR-submodular maximization with box-constraints. It is a one-pass algorithm with linear time complexity, reaching the optimal 1/2 approximation ratio, which may be of independent interest. We validate the superior performance of our algorithms against baselines on both synthetic and real-world datasets.
","['ETH Zürich', 'ETH Zurich', 'ETH Zurich']"
2019,Multiplicative Weights Updates as a distributed constrained optimization algorithm: Convergence to second-order stationary points almost always,"Ioannis Panageas, Georgios Piliouras, xiao wang",https://icml.cc/Conferences/2019/Schedule?showEvent=3976,"Non-concave maximization has been the subject of much recent study in the optimization and machine learning communities, specifically in deep learning.
Recent papers  ([Ge et al. 2015, Lee et al 2017] and references therein) indicate that first order methods work well and avoid saddles points.  Results as in [Lee \etal 2017], however, are limited to the \textit{unconstrained} case or for cases where the critical points are in the interior of the feasibility set, which fail to capture some of the most interesting applications. In this paper we focus on \textit{constrained} non-concave maximization. We analyze a variant of a well-established algorithm in machine learning called Multiplicative Weights Update (MWU) for the maximization problem $\max_{\mathbf{x} \in D} P(\mathbf{x})$, where $P$ is non-concave, twice continuously differentiable and $D$ is a product of simplices. We show that MWU converges almost always for small enough stepsizes to critical points that satisfy the second order KKT conditions,
by combining techniques from dynamical systems as well as taking advantage of a recent connection between Baum Eagon inequality and MWU [Palaiopanos et al 2017].","['SUTD', 'Singapore University of Technology and Design', 'Singapore university of technology and design']"
2019,Katalyst: Boosting Convex Katayusha for  Non-Convex Problems with a  Large Condition Number,"Zaiyi Chen, Yi Xu, Haoyuan Hu, Tianbao Yang",https://icml.cc/Conferences/2019/Schedule?showEvent=4230,"An important class of non-convex objectives that has wide applications in machine learning consists of  a sum of $n$ smooth functions and a non-smooth convex function. Tremendous studies have been devoted to conquering these problems by leveraging one of the two types of variance reduction techniques, i.e., SVRG-type that computes a full gradient occasionally and SAGA-type that maintains $n$ stochastic gradients at every iteration.  In practice, SVRG-type is preferred to SAGA-type due to its potentially less memory costs. 
An interesting question that has been largely ignored is how to improve the complexity of variance reduction methods for problems with a large condition number that measures the degree to which the objective is close to a convex function.  In this paper, we present a simple but non-trivial boosting of a  state-of-the-art SVRG-type method for convex problems (namely Katyusha) to enjoy an improved complexity for solving non-convex problems with a large condition number (that is close to a convex function). To the best of our knowledge, its complexity has the best dependence on $n$ and the degree of non-convexity, and also matches that of a recent SAGA-type accelerated stochastic algorithm for a constrained non-convex smooth optimization problem.  
","['Cainiao AI', 'The University of Iowa', 'Artificial Intelligence Department, Zhejiang Cainiao Supply Chain Management Co.', 'The University of Iowa']"
2019,Safe Policy Improvement with Baseline Bootstrapping,"Romain Laroche, Paul TRICHELAIR, Remi Tachet des Combes",https://icml.cc/Conferences/2019/Schedule?showEvent=3681,"This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a policy that is guaranteed to perform at least as well as the baseline policy used to collect the data. 
	    Our approach, called SPI with Baseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows paradigm: it bootstraps the trained policy with the baseline when the uncertainty is high. 
	    Our first algorithm, $\Pi_b$-SPIBB, comes with SPI theoretical guarantees. 
	    We also implement a variant, $\Pi_{\leq b}$-SPIBB, that is even more efficient in practice. 
	    We apply our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the superiority of SPIBB with respect to existing algorithms, not only in safety but also in mean performance. 
	    Finally, we implement a model-free version of SPIBB and show its benefits on a navigation task with deep RL implementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network representation able to train efficiently and reliably from batch data, without any interaction with the environment.","['Microsoft Research', 'Mila - Quebec AI Institute/McGill University', 'Microsoft Research Montreal']"
2019,Distributional Reinforcement Learning for Efficient Exploration,"Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, Yaoliang Yu",https://icml.cc/Conferences/2019/Schedule?showEvent=4297,"In distributional reinforcement learning (RL), the estimated distribution of value functions model both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method achieves 483 % average gain across 49 games in cumulative rewards over QR-DQN. We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves nearoptimal safety rewards twice faster than QRDQN.
","['University of Alberta', 'Huawei Technologies', 'University of Alberta', 'University of Waterloo', 'University of Waterloo']"
2019,Optimistic Policy Optimization via Multiple Importance Sampling,"Matteo Papini, Alberto Maria Metelli, Lorenzo Lupo, Marcello Restelli",https://icml.cc/Conferences/2019/Schedule?showEvent=4049,"Policy Search (PS) is an effective approach to Reinforcement Learning (RL) for solving
control tasks with continuous state-action spaces. In this paper, we address the exploration-exploitation trade-off in PS by proposing an approach based on Optimism in the Face of Uncertainty. We cast the PS problem as a suitable Multi Armed Bandit (MAB) problem, defined over the policy parameter space, and we propose a class of algorithms that effectively exploit the problem structure, by leveraging Multiple Importance Sampling to perform an off-policy estimation of the expected return.
We show that the regret of the proposed approach is bounded by $\widetilde{\mathcal{O}}(\sqrt{T})$ for both discrete and continuous parameter spaces. Finally, we evaluate our algorithms on tasks of varying difficulty, comparing them with existing MAB and RL algorithms.","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano']"
2019,Neural Logic Reinforcement Learning,"zhengyao jiang, Shan Luo",https://icml.cc/Conferences/2019/Schedule?showEvent=3834,"Deep reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However, most DRL algorithms suffer a problem of generalising the learned policy, which makes the policy performance largely affected even by minor modifications of the training environment. Except that, the use of deep neural networks makes the learned policies hard to be interpretable. To address these two challenges, we propose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to represent the policies in reinforcement learning by first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming that have demonstrated significant advantages in terms of interpretability and generalisability in supervised tasks. Extensive experiments conducted on cliff-walking and blocks manipulation tasks demonstrate that NLRL can induce interpretable policies achieving near-optimal performance while showing good generalisability to environments of different initial states and problem sizes.
","['University of Liverpool', 'University of Liverpool']"
2019,Learning to Collaborate in Markov Decision Processes,"Goran Radanovic, Rati Devidze, David Parkes, Adish Singla",https://icml.cc/Conferences/2019/Schedule?showEvent=3599,"We consider a two-agent MDP framework where agents repeatedly solve a task in a collaborative setting. We study the problem of designing a learning algorithm for the first agent (A1) that facilitates a successful collaboration even in cases when the second agent (A2) is adapting its policy in an unknown way. The key challenge in our setting is that the first agent faces non-stationarity in rewards and transitions because of the adaptive behavior of the second agent.

We design novel online learning algorithms for agent A1 whose regret decays as $O(T^{1-\frac{3}{7} \cdot \alpha})$ with $T$ learning episodes provided that the magnitude of agent A2's policy changes between any two consecutive episodes are upper bounded by $O(T^{-\alpha})$. Here, the parameter $\alpha$ is assumed to be strictly greater than $0$, and we show that this assumption is necessary provided that the {\em learning parity with noise} problem is computationally hard. We show that sub-linear regret of agent A1 further implies near-optimality of the agents' joint return for MDPs that manifest the properties of a {\em smooth} game. ","['Harvard University', 'Max Planck Institute for Software Systems', 'Harvard University', 'Max Planck Institute (MPI-SWS)']"
2019,Predictor-Corrector Policy Optimization,"Ching-An Cheng, Xinyan Yan, Nathan Ratliff, Byron Boots",https://icml.cc/Conferences/2019/Schedule?showEvent=3970,"We present a predictor-corrector framework, called PicCoLO, that can transform a first-order model-free reinforcement or imitation learning algorithm into a new hybrid method that leverages predictive models to accelerate policy learning. The new ``PicCoLOed'' algorithm optimizes a policy by recursively repeating two steps: In the Prediction Step, the learner uses a model to predict the unseen future gradient and then applies the predicted estimate to update the policy; in the Correction Step, the learner runs the updated policy in the environment, receives the true gradient, and then corrects the policy using the gradient error. Unlike previous algorithms, PicCoLO corrects for the mistakes of using imperfect predicted gradients and hence does not suffer from model bias. The development of PicCoLO is made possible by a novel reduction from predictable online learning to adversarial online learning,  which provides a systematic way to modify existing first-order algorithms to achieve the optimal regret with respect to predictable information. We show, in both theory and simulation, that the convergence rate of several first-order model-free algorithms can be improved by PicCoLO. 
","['Georgia Tech', 'Georgia Tech', 'NVIDIA', 'Georgia Tech']"
2019,Learning a Prior over Intent via Meta-Inverse Reinforcement Learning,"Kelvin Xu, Ellis Ratner, Anca Dragan, Sergey Levine, Chelsea Finn",https://icml.cc/Conferences/2019/Schedule?showEvent=3995,"A significant challenge for the practical application of reinforcement learning to real world problems is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert demonstrations. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ""prior"" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.
","['University of California, Berkeley', 'University of California, Berkeley', 'EECS Department, University of California, Berkeley', 'UC Berkeley', 'Stanford, Google, UC Berkeley']"
2019,DeepMDP: Learning Continuous Latent Space Models for Representation Learning,"Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, Marc Bellemare",https://icml.cc/Conferences/2019/Schedule?showEvent=4097,"Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a \texit{DeepMDP}, a parameterized latent space model that is trained via the minimization of two tractable latent space losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the embedding function as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.
","['Google Brain', 'Google Brain', 'Johns Hopkins University', 'Google Brain', 'Google Brain']"
2019,Importance Sampling Policy Evaluation with an Estimated Behavior Policy,"Josiah Hanna, Scott Niekum, Peter Stone",https://icml.cc/Conferences/2019/Schedule?showEvent=4167,"We consider the problem of off-policy evaluation in Markov decision processes. Off-policy evaluation is the task of evaluating the expected return of one policy with data generated by a different, behavior policy. Importance sampling is a technique for off-policy evaluation that re-weights off-policy returns to account for differences in the likelihood of the returns between the two policies. In this paper, we study importance sampling with an estimated behavior policy where the behavior policy estimate comes from the same set of data used to compute the importance sampling estimate. We find that this estimator often lowers the mean squared error of off-policy evaluation compared to importance sampling with the true behavior policy or using a behavior policy that is estimated from a separate data set. Intuitively, estimating the behavior policy in this way corrects for error due to sampling in the action-space. Our empirical results also extend to other popular variants of importance sampling and show that estimating a non-Markovian behavior policy can further lower large-sample mean squared error even when the true behavior policy is Markovian.
","['UT Austin', 'University of Texas at Austin', 'University of Texas at Austin']"
2019,Learning from a Learner,"alexis jacq, Matthieu Geist, Ana Paiva, Olivier Pietquin",https://icml.cc/Conferences/2019/Schedule?showEvent=4077,"In this paper, we propose a novel setting for Inverse Reinforcement Learning (IRL), namely ""Learning from a Learner"" (LfL). As opposed to standard IRL, it does not consist in learning a reward by observing an optimal agent but from observations of another learning (and thus sub-optimal) agent. To do so, we leverage the fact that the observed agent's policy is assumed to improve over time. The ultimate goal of this approach is to recover the actual environment's reward and to allow the observer to outperform the learner. To recover that reward in practice, we propose methods based on the entropy-regularized policy iteration framework. We discuss different approaches to learn solely from trajectories in the state-action space. We demonstrate the genericity of our method by observing agents implementing various reinforcement learning algorithms. Finally, we show that, on both discrete and continuous state/action tasks, the observer's performance (that optimizes the recovered reward) can surpass those of the observed agent. 
","['EPFL', 'Google', 'INESC-ID U of Lisbon', 'GOOGLE BRAIN']"
2019,Separable value functions across time-scales,"Joshua Romoff, Peter Henderson, Ahmed Touati, Yann Ollivier, Joelle Pineau, Emma Brunskill",https://icml.cc/Conferences/2019/Schedule?showEvent=4227,"In many finite horizon episodic reinforcement learning (RL) settings, it is desirable to optimize for the undiscounted return - in settings like Atari, for instance, the goal is to collect the most points while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temporal discounting is often applied to optimize over a shorter effective planning horizon. This comes at the cost of potentially biasing the optimization target away from the undiscounted goal. In settings where this bias is unacceptable - where the system must optimize for longer horizons at higher discounts - the target of the value function approximator may increase in variance leading to difficulties in learning. We present an extension of temporal difference (TD) learning, which we call TD($\Delta$), that breaks down a value function into a series of components based on the differences between value functions with smaller discount factors. The separation of a longer horizon value function into these components has useful properties in scalability and performance. We discuss these properties and show theoretic and empirical improvements over standard TD learning in certain settings.
","['McGill University', 'Stanford University', 'MILA / FAIR', 'Facebook Artificial Intelligence Research', 'McGill University / Facebook', 'Stanford University']"
2019,Learning Action Representations for Reinforcement Learning,"Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, Philip Thomas",https://icml.cc/Conferences/2019/Schedule?showEvent=3974,"Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori.  We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken.  We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.
","['University of Massachusetts Amherst', 'Adobe Research', 'UMass Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst']"
2019,Bayesian Counterfactual Risk Minimization,"Ben London, Ted Sandler",https://icml.cc/Conferences/2019/Schedule?showEvent=3876,"We present a Bayesian view of counterfactual risk minimization (CRM) for offline learning from logged bandit feedback. Using PAC-Bayesian analysis, we derive a new generalization bound for the truncated inverse propensity score estimator. We apply the bound to a class of Bayesian policies, which motivates a novel, potentially data-dependent, regularization technique for CRM. Experimental results indicate that this technique outperforms standard $L_2$ regularization, and that it is competitive with variance regularization while being both simpler to implement and more computationally efficient.","['Amazon', 'Amazon.com']"
2019,Per-Decision Option Discounting,"Anna Harutyunyan, Peter Vrancx, Philippe Hamel, Ann Nowe, Doina Precup",https://icml.cc/Conferences/2019/Schedule?showEvent=3865,"In order to solve complex problems an agent must be able to reason over a sufficiently long horizon. Temporal abstraction, commonly modeled through options, offers the ability to reason at many timescales, but the horizon length is still determined by the discount factor of the underlying Markov Decision Process. We propose a modification to the options framework that naturally scales the agent's horizon with option length. We show that the proposed option-step discount controls a bias-variance trade-off, with larger discounts (counter-intuitively) leading to less estimation variance.
","['DeepMind', 'PROWLER.io', 'Deepmind', 'VU Brussel', 'DeepMind']"
2019,Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds,"Andrea Zanette, Emma Brunskill",https://icml.cc/Conferences/2019/Schedule?showEvent=4172,"Strong worst-case performance bounds for episodic reinforcement learning exist 
but fortunately in practice RL algorithms perform much better than 
such bounds would predict. Algorithms and theory that provide strong 
problem-dependent bounds could help illuminate the key features of what 
makes a RL problem hard and reduce the barrier to using RL algorithms 
in practice. As a step towards this
we derive an algorithm and analysis for finite horizon discrete MDPs 
with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL 
environment has special features but without apriori 
knowledge of the environment from the algorithm. As a result of our analysis, 
we also help address an open learning theory question~\cite{jiang2018open} 
about episodic MDPs with a constant upper-bound on the sum of rewards, 
providing a regret bound function of the number of episodes with no 
dependence on the horizon.
","['Stanford University', 'Stanford University']"
2019,A Theory of Regularized Markov Decision Processes,"Matthieu Geist, Bruno Scherrer, Olivier Pietquin",https://icml.cc/Conferences/2019/Schedule?showEvent=4025,"Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.
","['Google', 'INRIA', 'GOOGLE BRAIN']"
2019,Discovering Options for Exploration by Minimizing Cover Time,"Yuu Jinnai, Jee Won Park, David Abel, George Konidaris",https://icml.cc/Conferences/2019/Schedule?showEvent=4279,"One of the main challenges in reinforcement learning is solving tasks with sparse reward. We show that the difficulty of discovering a distant rewarding state in an MDP is bounded by the expected cover time of a random walk over the graph induced by the MDP's transition dynamics. We therefore propose to accelerate exploration by constructing options that minimize cover time. We introduce a new option discovery algorithm that diminishes the expected cover time by connecting the most distant states in the state-space graph with options. We show empirically that the proposed algorithm improves learning in several domains with sparse rewards.
","['Brown University', 'Brown University', 'Brown University', 'Brown']"
2019,Policy Certificates: Towards Accountable Reinforcement Learning,"Christoph Dann, Lihong Li, Wei Wei, Emma Brunskill",https://icml.cc/Conferences/2019/Schedule?showEvent=3749,"The performance of a reinforcement learning algorithm can vary drastically during learning because of exploration. Existing algorithms provide little information about the quality of their current policy before executing it, and thus have limited use in high-stakes applications like healthcare. We address this lack of accountability by proposing that algorithms output policy certificates. These certificates bound the sub-optimality and return of the policy in the next episode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and present a new  framework for theoretical analysis that guarantees the quality of their policies and certificates. For tabular MDPs, we show that computing certificates can even improve the sample-efficiency of optimism-based exploration. As a result, one of our algorithms is the first to achieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm also matches (and in some settings slightly improves upon) existing minimax regret bounds.
","['Carnegie Mellon University', 'Google Research', 'Google', 'Stanford University']"
2019,The Value Function Polytope in Reinforcement Learning,"Robert Dadashi, Marc Bellemare, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans",https://icml.cc/Conferences/2019/Schedule?showEvent=4096,"We establish geometric and topological properties of the space of value functions in finite state-action Markov decision processes. 
Our main contribution is the characterization of the nature of its shape: a general polytope (Aigner et al., 2010). To demonstrate this result, we exhibit several properties of the structural relationship between policies and value functions including the line theorem, which shows that the value functions of policies constrained on all but one state describe a line segment. Finally, we use this novel perspective and introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.
","['Google AI Residency Program', 'Google Brain', 'Université de Montréal', 'Google', 'Google / University of Alberta']"
2019,Data Shapley:  Equitable Valuation of Data for Machine Learning,"Amirata Ghorbani, James Zou",https://icml.cc/Conferences/2019/Schedule?showEvent=4290,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. 
For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data.
In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2)  low Shapley value data effectively capture outliers and corruptions; 3)  high Shapley value data inform what type of new data to acquire to improve the predictor.  ","['Stanford', 'Stanford University']"
2019,Feature Grouping as a Stochastic Regularizer for High-Dimensional Structured Data,"Sergul Aydore, Thirion Bertrand, Gael Varoquaux",https://icml.cc/Conferences/2019/Schedule?showEvent=4127,"In many applications where collecting data is expensive, for example neuroscience or medical imaging, the sample size is typically small compared to the feature dimension. These datasets call for intelligent regularization that exploits known structure, such as correlations between the features arising from the measurement device. However, existing structured regularizers need specially crafted solvers, which are difficult to apply to complex models. We propose a new regularizer specifically designed to leverage structure in the data in a way that can be applied efficiently to complex models. Our approach relies on feature grouping, using a fast clustering algorithm inside a stochastic gradient descent loop: given a
family of feature groupings that capture feature covariations, we randomly select these groups at each iteration. Experiments on two real-world datasets demonstrate that the proposed approach produces models that generalize better than those trained with conventional regularizers, and also improves convergence speed, and has a linear computational cost.
","['Stevens Institute of Technology', 'inria', 'Inria']"
2019,Metric-Optimized Example Weights,"Sen Zhao, Mahdi Milani Fard, Harikrishna Narasimhan, Maya Gupta",https://icml.cc/Conferences/2019/Schedule?showEvent=3932,"Real-world machine learning applications often have complex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between complex test metrics and cost-weighted learning, we propose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examples are learned to optimize the test metric on a validation set. These metric-optimized example weights can be learned for any test metric, including black box and customized ones for specific applications. We illustrate the performance of the proposed method on diverse public benchmark datasets and real-world applications. We also provide a generalization bound for the method.
","['Google Research', 'Google', 'Google Research', 'Google']"
2019,Improving Model Selection by Employing the Test Data,"Max Westphal, Werner Brannath",https://icml.cc/Conferences/2019/Schedule?showEvent=3649,"Model selection and evaluation are usually strictly separated by means of data splitting to enable an unbiased estimation and a simple statistical inference for the unknown generalization performance of the final prediction model. We investigate the properties of novel evaluation strategies, namely when the final model is selected based on empirical performances on the test data. To guard against selection induced overoptimism, we employ a parametric multiple test correction based on the approximate multivariate distribution of performance estimates. Our numerical experiments involve training common machine learning algorithms (EN, CART, SVM, XGB) on various artificial classification tasks. At its core, our proposed approach improves model selection in terms of the expected final model performance without introducing overoptimism. We furthermore observed a higher probability for a successful evaluation study, making it easier in practice to empirically demonstrate a sufficiently high predictive performance.
","['University of Bremen', 'University of Bremen']"
2019,Topological Data Analysis of Decision Boundaries with Application to Model Selection,"Karthikeyan Ramamurthy, Kush Varshney, Krishnan Mody",https://icml.cc/Conferences/2019/Schedule?showEvent=3938,"We propose the labeled Cech complex, the plain labeled Vietoris-Rips complex, and the locally scaled labeled Vietoris-Rips complex to perform persistent homology inference of decision boundaries in classification tasks. We provide theoretical conditions and analysis for recovering the homology of a decision boundary from samples. Our main objective is quantification of deep neural network complexity to enable matching of datasets to pre-trained models to facilitate the functioning of AI marketplaces; we report results for experiments using MNIST, FashionMNIST, and CIFAR10.
","['IBM Research', 'IBM Research AI', 'New York University']"
2019,Contextual Memory Trees,"Wen Sun, Alina Beygelzimer, Hal Daume, John Langford, Paul Mineiro",https://icml.cc/Conferences/2019/Schedule?showEvent=4174,"We design and study a  Contextual Memory Tree (CMT), a learning memory controller that inserts new memories into an experience store of unbounded size. It operates online and is designed to efficiently query for memories from that store, supporting logarithmic time insertion and retrieval operations. Hence CMT can be integrated into existing statistical learning algorithms as an augmented memory unit without substantially increasing training and inference computation.  Furthermore CMT operates as a reduction to classification, allowing it to benefit from advances in representation or architecture.  We demonstrate the efficacy of CMT by augmenting existing multi-class and multi-label classification algorithms with CMT and observe statistical improvement. We also test CMT learning on several image-captioning tasks to demonstrate that it performs computationally better than a simple nearest neighbors memory system while benefitting from reward learning.
","['Carnegie Mellon University', 'Yahoo Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft']"
2019,Sparse Extreme Multi-label Learning with Oracle Property,"Weiwei Liu, Xiaobo Shen",https://icml.cc/Conferences/2019/Schedule?showEvent=3806,"The pioneering work of sparse local embeddings for extreme classification (SLEEC) (Bhatia et al., 2015) has shown great promise in multi-label learning. Unfortunately, the statistical rate of convergence and oracle property of SLEEC are still not well understood. To fill this gap, we present a unified framework for SLEEC with nonconvex penalty. Theoretically, we rigorously prove that our proposed estimator enjoys oracle property (i.e., performs as well as if the underlying model were known beforehand), and obtains a desirable statistical convergence rate. Moreover, we show that under a mild condition on the magnitude of the entries in the underlying model, we are able to obtain an improved convergence rate. Extensive numerical experiments verify our theoretical findings and the superiority of our proposed estimator.
","['Wuhan University', 'Nanjing University of Science and Technology']"
2019,Shape Constraints for Set Functions,"Andrew Cotter, Maya Gupta, Heinrich Jiang, Erez Louidor, James Muller, Taman Narayan, Serena Wang, Tao Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=4160,"Set functions predict a label from a permutation-invariant variable-size collection of feature vectors. We propose making set functions more understandable and regularized by capturing domain knowledge through shape constraints. We show how prior work in monotonic constraints can be adapted to set functions, and then propose two new shape constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shape constraints with a deep lattice network. We propose a nonlinear estimation strategy we call the semantic feature engine that uses set functions with the proposed shape constraints to estimate labels for compound sparse categorical features. Experiments on real-world data show the achieved accuracy is similar to deep sets or deep neural networks, but provides guarantees on the model behavior, which makes it easier to explain and debug. 
","['Google AI', 'Google', 'Google Research', 'Google, Inc.', 'Google', 'Google', 'Google', 'Google']"
2019,On The Power of Curriculum Learning in Training Deep Networks,"Guy Hacohen, Daphna Weinshall",https://icml.cc/Conferences/2019/Schedule?showEvent=3720,"Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher"" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.
","['Hebrew University of Jerusalem', 'Hebrew University of Jerusalem, Israel']"
2019,Voronoi Boundary Classification: A High-Dimensional Geometric Approach via Weighted Monte Carlo Integration,"Vladislav Polianskii, Florian T. Pokorny",https://icml.cc/Conferences/2019/Schedule?showEvent=4132,"Voronoi cell decompositions provide a classical avenue to classification. Typical approaches however only utilize point-wise cell-membership information by means of nearest neighbor queries and do not utilize further geometric information about Voronoi cells since the computation of Voronoi diagrams is prohibitively expensive in high dimensions. We propose a Monte-Carlo integration based approach that instead computes a weighted integral over the boundaries of Voronoi cells, thus incorporating additional information about the Voronoi cell structure. We demonstrate the scalability of our approach in up to 3072 dimensional spaces and analyze convergence based on the number of Monte Carlo samples and choice of weight functions. Experiments comparing our approach to Nearest Neighbors, SVM and Random Forests indicate that while our approach performs similarly to Random Forests for large data sizes, the algorithm exhibits non-trivial data-dependent performance characteristics for smaller datasets and can be analyzed in terms of a geometric confidence measure, thus adding to the repertoire of geometric approaches to classification while having the benefit of not requiring any model changes or retraining as new training samples or classes are added.
","['KTH Royal Institute of Technology', 'KTH Royal Institute of Technology']"
2019,Robust Decision Trees Against Adversarial Examples,"Hongge Chen, Huan Zhang, Duane Boning, Cho-Jui Hsieh",https://icml.cc/Conferences/2019/Schedule?showEvent=4169,"Although adversarial examples and model robust-ness have been extensively studied in the context of neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree-based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worst-case perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees—a naive approach to finding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in the saddlepoint problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting systems such as XGBoost.  Experimental results on real world datasets demonstrate that the proposed algorithms can significantly improve the robustness of tree-based models against adversarial examples.
","['MIT', 'UCLA', 'MIT', 'UCLA']"
2019,Automatic Classifiers as Scientific Instruments: One Step Further Away from Ground-Truth,"Jacob Whitehill, Anand Ramakrishnan",https://icml.cc/Conferences/2019/Schedule?showEvent=3562,"Automatic machine learning-based detectors of various psychological and social phenomena (e.g., emotion, stress, engagement) have great potential to advance basic science. However, when a detector d is trained to approximate an existing measurement tool (e.g., a questionnaire, observation protocol), then care must be taken when interpreting measurements collected using d since they are one step further removed from the under- lying construct. We examine how the accuracy of d, as quantified by the correlation q of d’s out- puts with the ground-truth construct U, impacts the estimated correlation between U (e.g., stress) and some other phenomenon V (e.g., academic performance). In particular: (1) We show that if the true correlation between U and V is r, then the expected sample correlation, over all vectors T n whose correlation with U is q, is qr. (2) We derive a formula for the probability that the sample correlation (over n subjects) using d is positive given that the true correlation is negative (and vice-versa); this probability can be substantial (around 20 − 30%) for values of n and q that have been used in recent affective computing studies. (3) With the goal to reduce the variance of correlations estimated by an automatic detector, we show that training multiple neural networks d(1) , . . . , d(m) using different training architectures and hyperparameters for the same detection task provides only limited “coverage” of T^n.
","['Worcester Polytechnic Institute', 'Worcester Polytechnic Institute']"
2019,"Look Ma, No Latent Variables: Accurate Cutset Networks via Compilation","Tahrima Rahman, Shasha Jin, Vibhav Gogate",https://icml.cc/Conferences/2019/Schedule?showEvent=3939,"Tractable probabilistic models obviate the need for unreliable approximate inference approaches and as a result often yield accurate query answers in practice. However, most tractable models that achieve state-of-the-art generalization performance (measured using test set likelihood score) use latent variables. Such models admit poly-time marginal (MAR) inference but do not admit poly-time (full) maximum-a-posteriori (MAP) inference. To address this problem, in this paper, we propose a novel approach for inducing cutset networks, a well-known tractable, highly interpretable representation that does not use latent variables and admits linear time MAR as well as MAP inference. Our approach addresses a major limitation of existing techniques that learn cutset networks from data in that their accuracy is quite low as compared to latent variable models such as ensembles of cutset networks and sum-product networks. The key idea in our approach is to construct deep cutset networks by not only learning them from data but also compiling them from a more accurate latent tractable model. We show experimentally that our new approach yields more accurate MAP estimates as compared with existing approaches and significantly improves the test set log-likelihood score of cutset networks bringing them closer in terms of generalization performance to latent variable models.
","['University of Texas at Dallas', 'The University of Texas at Dallas', 'The University of Texas at Dallas']"
2019,Optimal Transport for structured data with application on graphs,"Titouan Vayer, Nicolas Courty, Romain Tavenard, Chapel Laetitia, Remi Flamary",https://icml.cc/Conferences/2019/Schedule?showEvent=4110,"This work considers the problem of computing distances between structured
objects such as undirected graphs, seen as probability distributions in a
specific metric space. We consider a new transportation distance (
i.e. that minimizes a total cost of transporting probability masses) that unveils
the geometric nature of the structured objects space. Unlike Wasserstein or
Gromov-Wasserstein metrics that focus solely and respectively on features (by
considering a metric in the feature space) or structure (by seeing structure as
a metric space), our new distance exploits jointly both information, and is
consequently called Fused Gromov-Wasserstein  (FGW). After discussing its
properties and computational aspects, we show results on a graph classification
task, where our method outperforms both graph kernels and
deep graph convolutional networks.  Exploiting further on the metric properties
of FGW, interesting geometric objects such as Fr{\'e}chet means or barycenters
of graphs are illustrated and discussed in a clustering context.
","['IRISA', 'IRISA, Universite Bretagne-Sud', 'LETG-Rennes /  IRISA-Obelix', 'IRISA', ""Université côte d'Azur""]"
2019,Learning Optimal Linear Regularizers,Matthew Streeter,https://icml.cc/Conferences/2019/Schedule?showEvent=3760,"We present algorithms for efficiently learning regularizers that improve generalization.  Our approach is based on the insight that regularizers can be viewed as upper bounds on the generalization gap, and that reducing the slack in the bound can improve performance on test data.  For a broad class of regularizers, the hyperparameters that give the best upper bound can be computed using linear programming.  Under certain Bayesian assumptions, solving the LP lets us ""jump"" to
the optimal hyperparameters given very limited data.  This suggests a natural algorithm for tuning regularization hyperparameters, which we show to be effective on both real and synthetic data.
",['Google']
2019,On Symmetric Losses for Learning from Corrupted Labels,"Nontawat Charoenphakdee, Jongyeong Lee, Masashi Sugiyama",https://icml.cc/Conferences/2019/Schedule?showEvent=3697,"This paper aims to provide a better understanding of a symmetric loss. 
First, we emphasize that using a symmetric loss is advantageous in the balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization from corrupted labels. 
Second, we prove general theoretical properties of symmetric losses, including a classification-calibration condition, excess risk bound, conditional risk minimizer, and AUC-consistency condition. 
Third, since all nonnegative symmetric losses are non-convex, we propose a convex barrier hinge loss that benefits significantly from the symmetric condition, although it is not symmetric everywhere. 
Finally, we conduct experiments to validate the relevance of the symmetric condition. 
","['The University of Tokyo / RIKEN', 'The University of Tokyo/RIKEN', 'RIKEN / The University of Tokyo']"
2019,AUCµ: A Performance Metric for Multi-Class Machine Learning Models,"Ross Kleiman, University of Wisconsin David Page",https://icml.cc/Conferences/2019/Schedule?showEvent=4252,"The area under the receiver operating characteristic curve (AUC) is arguably the most common metric in machine learning for assessing the quality of a two-class classification model. As the number and complexity of machine learning applications grows, so too does the need for measures that can gracefully extend to classification models trained for more than two classes. Prior work in this area has proven computationally intractable and/or inconsistent with known properties
of AUC, and thus there is still a need for an improved multi-class efficacy metric. We provide in this work a multi-class extension of AUC that we call AUCµ that is derived from first principles of the binary class AUC. AUCµ has similar computational complexity to AUC and maintains the properties of AUC critical to its interpretation and use.
","['University of Wisconsin-Madison', 'University of Wisconsin, Madison']"
2019,Regularization in directable environments with application to Tetris,"Jan Malte Lichtenberg, Ozgur Simsek",https://icml.cc/Conferences/2019/Schedule?showEvent=3930,"Learning from small data sets is difficult in the absence of specific domain knowledge. We present a regularized linear model called STEW that benefits from a generic and prevalent form of prior knowledge:  feature directions. STEW shrinks weights toward each other, converging to an equal-weights solution in the limit of infinite regularization. We provide theoretical results on the equal-weights solution that explains how STEW can productively trade-off bias and variance. Across a wide range of learning problems, including Tetris,  STEW outperformed existing linear models, including ridge regression, the Lasso, and the non-negative Lasso, when feature directions were known. The model proved to be robust to unreliable (or absent) feature directions, still outperforming alternative models under diverse conditions. Our results in Tetris were obtained by using a novel approach to learning in sequential decision environments based on multinomial logistic regression.
","['University of Bath', 'University of Bath']"
2019,Improved Dynamic Graph Learning through Fault-Tolerant Sparsification,"Chunjiang Zhu, Sabine Storandt, Kam-Yiu Lam, Song Han, Jinbo Bi",https://icml.cc/Conferences/2019/Schedule?showEvent=4263,"Graph sparsification has been used to improve the computational cost of learning over graphs, e.g., Laplacian-regularized estimation and graph semi-supervised learning (SSL). However, when graphs vary over time, repeated sparsification requires polynomial order computational cost per update. We propose a new type of graph sparsification namely fault-tolerant (FT) sparsification to significantly reduce the cost to only a constant. Then the computational cost of subsequent graph learning tasks can be significantly improved with limited loss in their accuracy. In particular, we give theoretical analyze to upper bound the loss in the accuracy of the subsequent Laplacian-regularized estimation and graph SSL, due to the FT sparsification. In addition, FT spectral sparsification can be generalized to FT cut sparsification, for cut-based graph learning. Extensive experiments have confirmed the computational efficiencies and accuracies of the proposed methods for learning on dynamic graphs.
","['University of Connecticut', 'University of Wuerzburg', '', '', 'University of Connecticut']"
2019,Heterogeneous Model Reuse via Optimizing Multiparty Multiclass Margin,"Xi-Zhu Wu, Song Liu, Zhi-Hua Zhou",https://icml.cc/Conferences/2019/Schedule?showEvent=4036,"Nowadays, many problems require learning a model from data owned by different participants who are restricted to share their examples due to privacy concerns, which is referred to as multiparty learning in the literature. In conventional multiparty learning, a global model is usually trained from scratch via a communication protocol, ignoring the fact that each party may already have a local model trained on her own dataset. In this paper, we define a multiparty multiclass margin to measure the global behavior of a set of heterogeneous local models, and propose a general learning method called HMR (Heterogeneous Model Reuse) to optimize the margin. Our method reuses local models to approximate a global model, even when data are non-i.i.d distributed among parties, by exchanging few examples under predefined budget. Experiments on synthetic and real-world data covering different multiparty scenarios show the effectiveness of our proposal.
","['Nanjing University', 'University of Bristol', 'Nanjing University']"
2019,Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff,"Yochai Blau, Tomer Michaeli",https://icml.cc/Conferences/2019/Schedule?showEvent=3643,"Lossy compression algorithms are typically designed and analyzed through the lens of Shannon's rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate. However, in recent years, it has become increasingly accepted that ""low distortion"" is not a synonym for ""high perceptual quality"", and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical definition of perceptual quality recently proposed by Blau & Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST example.
","['Technion', 'Technion']"
2019,Collaborative Channel Pruning for Deep Networks,"Hanyu Peng, Jiaxiang Wu, Shifeng Chen, Junzhou Huang",https://icml.cc/Conferences/2019/Schedule?showEvent=3988,"Deep networks have achieved impressive performance in various domains, but their applications
are largely limited by the prohibitive computational overhead. In this paper, we propose a novel
algorithm, namely collaborative channel pruning
(CCP), to reduce the computational overhead with
negligible performance degradation. The joint
impact of pruned/preserved channels on the loss
function is quantitatively analyzed, and such interchannel dependency is exploited to determine
which channels to be pruned. The channel selection problem is then reformulated as a constrained 0-1 quadratic optimization problem, and the Hessian matrix, which is essential in constructing the above optimization, can be efficiently approximated. Empirical evaluation on two benchmark data sets indicates that our proposed CCP algorithm achieves higher classification accuracy with similar computational complexity than other stateof-the-art channel pruning algorithms
","['Shenzhen Institutes of Advanced Technology,Chinese Academy of Sciences', 'Tencent AI Lab', 'SIAT', 'University of Texas at Arlington / Tencent AI Lab']"
2019,"Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization","Eldad Meller, Alexander Finkelstein, Uri Almog, Mark Grobman",https://icml.cc/Conferences/2019/Schedule?showEvent=4099,"Quantization of neural networks has become common practice, driven by the need for efficient implementations of deep neural networks on embedded devices. In this paper, we exploit an oft-overlooked degree of freedom in most networks - for a given layer, individual output channels can be scaled by any factor provided that the corresponding weights of the next layer are inversely scaled. Therefore, a given network has many factorizations which change the weights of the network without changing its function. We present a conceptually simple and easy to implement method that uses this property and show that proper factorizations significantly decrease the degradation caused by quantization. We show improvement on a wide variety of networks and achieve state-of-the-art degradation results for MobileNets. While our focus is on quantization, this type of factorization is applicable to other domains such as network-pruning, neural nets regularization and network interpretability.
","['Hailo', 'Hailo Technologies', 'Hailo Technologies', 'Hailo Technologies']"
2019,GDPP: Learning Diverse Generations using Determinantal Point Processes,"Mohamed Elfeki, Camille Couprie, Morgane Riviere, Mohamed Elhoseiny",https://icml.cc/Conferences/2019/Schedule?showEvent=3621,"Generative models have proven to be an outstanding tool for representing high-dimensional probability distributions and generating realistic looking images. An essential characteristic of generative models is their ability to produce multi-modal outputs. However, while training, they are often susceptible to mode collapse, that is models are limited in mapping input noise to only a few modes of the true data distribution. In this work, we draw inspiration from Determinantal Point Process (DPP) to propose an unsupervised penalty loss that alleviates mode collapse while producing higher quality samples. DPP is an elegant probabilistic measure used to model negative correlations within a subset and hence quantify its diversity. We use DPP kernel to model the diversity in real data as well as in synthetic data. Then, we devise an objective term that encourages generator to synthesize data with a similar diversity to real data. In contrast to previous state-of-the-art generative models that tend to use additional trainable parameters or complex training paradigms, our method does not change the original training scheme. Embedded in an adversarial training and variational autoencoder, our Generative DPP approach shows a consistent resistance to mode-collapse on a wide-variety of synthetic data and natural image datasets including MNIST, CIFAR10, and CelebA, while outperforming state-of-the-art methods for data-efficiency, generation quality, and convergence-time whereas being 5.8x faster than its closest competitor.
","['University of Central Florida', 'FAIR', 'Facebook Artificial Intelligence Research', 'KAUST and Baidu SVAIL']"
2019,Co-Representation Network for Generalized Zero-Shot Learning,"Fei Zhang, Guangming Shi",https://icml.cc/Conferences/2019/Schedule?showEvent=3811,"Generalized zero-shot learning is a significant topic but faced with bias problem, which leads to unseen classes being easily misclassified into seen classes. Hence we propose a embedding model called co-representation network to learn a more uniform visual embedding space that effectively alleviates the bias problem and helps with classification. We mathematically analyze our model and find it learns a projection with high local linearity, which is proved to cause less bias problem. The network consists of a cooperation module for representation and a relation module for classification, it is simple in structure and can be easily trained in an end-to-end manner. Experiments show that our method outperforms existing generalized zero-shot learning methods on several  benchmark datasets.
","['Xidian University', 'Xidian University']"
2019,GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects,"Edward Smith, Scott Fujimoto, Adriana Romero Soriano, David Meger",https://icml.cc/Conferences/2019/Schedule?showEvent=3656,"Mesh models are a promising approach for encoding the structure of 3D objects. Current mesh reconstruction systems predict uniformly distributed vertex locations of a predetermined graph through a series of graph convolutions, leading to compromises with respect to performance or resolution. In this paper, we argue that the graph representation of geometric objects allows for additional structure, which should be leveraged for enhanced reconstruction. Thus, we propose a system which properly benefits from the advantages of the geometric structure of graph-encoded objects by introducing (1) a graph convolutional update preserving vertex information; (2) an adaptive splitting heuristic allowing detail to emerge; and (3) a training objective operating both on the local surfaces defined by vertices as well as the global structure defined by the mesh. Our proposed method is evaluated on the task of 3D object reconstruction from images with the ShapeNet dataset, where we demonstrate state of the art performance, both visually and numerically, while having far smaller space requirements by generating adaptive meshes.
","['McGill University', 'McGill University', 'FAIR', 'McGill University']"
2019,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,"Mingxing Tan, Quoc Le",https://icml.cc/Conferences/2019/Schedule?showEvent=3661,"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.
","['Google Brain', 'Google Brain']"
2019,Geometry Aware Convolutional Filters for Omnidirectional Images Representation,"Renata Khasanova, Pascal Frossard",https://icml.cc/Conferences/2019/Schedule?showEvent=3612,"Due to their wide field of view, omnidirectional cameras are frequently used by autonomous vehicles, drones and robots for navigation and other computer vision tasks. The images captured by such cameras, are often analyzed and classified with techniques designed for planar images that unfortunately fail to properly handle the native geometry of such images and therefore results in suboptimal performance. In this paper we aim at improving popular deep convolutional neural networks so that they can properly take into account the specific properties of omnidirectional data. In particular we propose an algorithm that adapts convolutional layers, which often serve as a core building block of a CNN, to the properties of omnidirectional images. Thus, our filters have a shape and size that adapt to the location on the omnidirectional image. We show that our method is not limited to spherical surfaces and is able to incorporate the knowledge about any kind of projective geometry inside the deep learning network. As depicted by our experiments, our method outperforms the existing deep neural network techniques for omnidirectional image classification and compression tasks.
","['Ecole Polytechnique Federale de Lausanne (EPFL)', 'EPFL']"
2019,A Personalized Affective Memory Model for Improving Emotion Recognition,"Pablo Barros, German Parisi, Stefan Wermter",https://icml.cc/Conferences/2019/Schedule?showEvent=3828,"Recent models of emotion recognition strongly rely on supervised deep learning solutions for the distinction of general emotion expressions. However, they are not reliable when recognizing online and personalized facial expressions, e.g., for person-specific affective understanding. In this paper, we present a neural model based on a conditional adversarial autoencoder to learn how to represent and edit general emotion expressions. We then propose Grow-When-Required networks as personalized affective memories to learn individualized aspects of emotional expressions. Our model achieves state-of-the-art performance on emotion recognition when evaluated on in-the-wild datasets. Furthermore, our experiments include ablation studies and neural visualizations in order to explain the behavior of our model.
","['University of Hamburg', 'University of Hamburg', 'University of Hamburg']"
2019,Temporal Gaussian Mixture Layer for Videos,"AJ Piergiovanni, Michael Ryoo",https://icml.cc/Conferences/2019/Schedule?showEvent=3578,"We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal convolutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that are fully differentiable. We present our fully convolutional video models with multiple TGM layers for activity detection. The extensive experiments on multiple datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outperforming the state-of-the-arts.
","['Indiana University', 'Indiana University / Google Brain']"
2019,Regret Circuits: Composability of Regret Minimizers,"Gabriele Farina, Christian Kroer, Tuomas Sandholm",https://icml.cc/Conferences/2019/Schedule?showEvent=3856,"Regret minimization is a powerful tool for solving large-scale problems; it was recently used in breakthrough results for large-scale extensive-form game solving. This was achieved by composing simplex regret minimizers into an overall regret-minimization framework for extensive-form game strategy spaces. In this paper we study the general composability of regret minimizers. We derive a calculus for constructing regret minimizers for composite convex sets that are obtained from convexity-preserving operations on simpler convex sets. We show that local regret minimizers for the simpler sets can be combined with additional regret minimizers into an aggregate regret minimizer for the composite set. As one application, we show that the CFR framework can be constructed easily from our framework. We also show ways to include curtailing (constraining) operations into our framework. For one, they enable the construction of CFR generalization for extensive-form games with general convex strategy constraints that can cut across decision points.
","['Carnegie Mellon University', 'Columbia University', 'Carnegie Mellon University']"
2019,Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function,"Arvind Raghunathan, Anoop Cherian, Devesh Jha",https://icml.cc/Conferences/2019/Schedule?showEvent=4156,"Computing Nash equilibrium (NE) of multi-player games has witnessed renewed interest due to recent advances in generative adversarial networks. However, computing equilibrium efficiently is challenging. To this end, we introduce the Gradient-based Nikaido-Isoda (GNI) function which serves: (i) as a merit function, vanishing only at the first-order stationary points of each player's optimization problem, and (ii) provides error bounds to a stationary Nash point. Gradient descent is shown to converge sublinearly to a first-order stationary point of the GNI function. For the particular case of bilinear min-max games and multi-player quadratic games, the GNI function is convex.  Hence, the application of gradient descent in this case yields linear convergence to an NE (when one exists). In our numerical experiments, we observe that the GNI formulation always converges to the first-order stationary point of each player's optimization problem.
","['Mitsubishi Electric Research Laboratories', 'MERL', 'Mitsubishi Electric Research Labs']"
2019,Stable-Predictive Optimistic Counterfactual Regret Minimization,"Gabriele Farina, Christian Kroer, Noam Brown, Tuomas Sandholm",https://icml.cc/Conferences/2019/Schedule?showEvent=3839,"The CFR framework has been a powerful tool for solving large-scale extensive-form games in practice. However, the theoretical rate at which past CFR-based algorithms converge to the Nash equilibrium is on the order of $O(T^{-1/2})$, where $T$ is the number of iterations. In contrast, first-order methods can be used to achieve a $O(T^{-1})$ dependence on iterations, yet these methods have been less successful in practice. In this work we present the first CFR variant that breaks the square-root dependence on iterations. By combining and extending recent advances on predictive and stable regret minimizers for the matrix-game setting we show that it is possible to leverage ``optimistic'' regret minimizers to achieve a $O(T^{-3/4})$ convergence rate within CFR. This is achieved by introducing a new notion of stable-predictivity, and by setting the stability of each counterfactual regret minimizer relative to its location in the decision tree. Experiments show that this method is faster than the original CFR algorithm, although not as fast as newer variants, in spite of their worst-case $O(T^{-1/2})$ dependence on iterations.","['Carnegie Mellon University', 'Columbia University', 'CMU', 'Carnegie Mellon University']"
2019,When Samples Are Strategically Selected,"Hanrui Zhang, Yu Cheng, Vincent Conitzer",https://icml.cc/Conferences/2019/Schedule?showEvent=4103,"In standard classification problems, the assumption is that the entity making the decision (the {\em principal}) has access to {\em all} the samples.  However, in many contexts, she either does not have direct access to the samples, or can inspect only a limited set of samples and does not know which are the most relevant ones.  In such cases, she must rely on another party (the {\em agent}) to either provide the samples or point out the most relevant ones.  If the agent has a different objective, then the principal cannot trust the submitted samples to be representative.  She must set a {\em policy} for how she makes decisions, keeping in mind the agent's incentives.  In this paper, we introduce a theoretical framework for this problem and provide key structural and computational results.
","['Duke University', 'Duke University', 'Duke']"
2019,Statistical Foundations of Virtual Democracy,"Anson Kahng, Min Kyung Lee, Ritesh Noothigattu, Ariel Procaccia, Christos-Alexandros Psomas",https://icml.cc/Conferences/2019/Schedule?showEvent=3565,"Virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. One of the key questions is which aggregation method -- or voting rule -- to use; we offer a novel statistical viewpoint that provides guidance. Specifically, we seek voting rules that are robust to prediction errors, in that their output on people's true preferences is likely to coincide with their output on noisy estimates thereof. We prove that the classic Borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of pairwise-majority consistent rules is not. Our empirical results further support, and more precisely measure, the robustness of Borda count. 
","['Carnegie Mellon University', 'CMU', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2019,Optimal Auctions through Deep Learning,"Paul Duetting, Zhe Feng, Harikrishna Narasimhan, David Parkes, Sai Srivatsa Ravindranath",https://icml.cc/Conferences/2019/Schedule?showEvent=3967,"Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981. Even after 30-40 years of intense research the problem remains unsolved for seemingly simple multi-bidder, multi-item settings. In this work, we initiate the exploration of the use of tools from deep learning for the automated design of optimal auctions. We model an auction as a multi-layer neural network, frame optimal auction design as a constrained learning problem, and show how it can be solved using standard pipelines. We prove generalization bounds and present extensive experiments, recovering essentially all known analytical solutions for  multi-item settings, and obtaining novel mechanisms for settings in which the optimal mechanism is unknown.
","['London School of Economics', 'Harvard University', 'Google Research', 'Harvard University', 'Harvard University']"
2019,Learning to Clear the Market,"Weiran Shen, Sébastien Lahaie, Renato Leme",https://icml.cc/Conferences/2019/Schedule?showEvent=4192,"The problem of market clearing is to set a price for an item such that quantity demanded equals quantity supplied. In this work, we cast the problem of predicting clearing prices into a learning framework and use the resulting models to perform revenue optimization in auctions and markets with contextual information. The economic intuition behind market clearing allows us to obtain fine-grained control over the aggressiveness of the resulting pricing policy, grounded in theory. To evaluate our approach, we fit a model of clearing prices over a massive dataset of bids in display ad auctions from a major ad exchange. The learned prices outperform other modeling techniques in the literature in terms of revenue and efficiency trade-offs. Because of the convex nature of the clearing loss function, the convergence rate of our method is as fast as linear regression.
","['Tsinghua University', 'Google', 'Google Research']"
2019,Learning to bid in revenue-maximizing auctions,"Thomas Nedelec, Noureddine El Karoui, Vianney Perchet",https://icml.cc/Conferences/2019/Schedule?showEvent=4090,"We consider the problem of the optimization of bidding strategies in prior-dependent revenue-maximizing auctions, when the seller fixes the reserve prices based on the bid distributions. Our study is done in the setting where one bidder is strategic. Using a variational approach, we study the complexity of the original objective and we introduce a relaxation of the objective functional in  order to use gradient descent methods. Our approach is simple, general and can be applied to various value distributions and revenue-maximizing mechanisms. The new strategies we derive yield  massive uplifts compared to the traditional truthfully bidding strategy. 
","['ENS Paris Saclay - Criteo AI Lab', 'Criteo AI Lab and UC, Berkeley', 'ENS Paris Saclay & Criteo AI Lab']"
2019,Open-ended learning in symmetric zero-sum games,"David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jaderberg, Thore Graepel",https://icml.cc/Conferences/2019/Schedule?showEvent=4161,"Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them winner' andloser'. If the game is approximately transitive, then self-play generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield open-ended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSROrN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of  agents than existing algorithms. We apply PSROrN to two highly nontransitive resource allocation games and find that PSRO_rN consistently outperforms the existing alternatives.
","['DeepMind', 'DeepMind', '', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2019,Deep Counterfactual Regret Minimization,"Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm",https://icml.cc/Conferences/2019/Schedule?showEvent=3751,"Counterfactual Regret Minimization (CFR) is the leading algorithm for solving large imperfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is typically applied before running CFR. The abstracted game is solved with tabular CFR, and its solution is mapped back to the full game. This process can be problematic because aspects of abstraction are often manual and domain specific, abstraction algorithms may miss important strategic nuances of the game, and there is a chicken-and-egg problem because determining a good abstraction requires knowledge of the equilibrium of the game. This paper introduces \emph{Deep Counterfactual Regret Minimization}, a form of CFR that obviates the need for abstraction by instead using deep neural networks to approximate the behavior of CFR in the full game. We show that Deep CFR is principled and achieves strong performance in large poker games. This is the first non-tabular variant of CFR to be successful in large games.
","['Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research', 'Carnegie Mellon University']"
2019,Generalized Approximate Survey Propagation for High-Dimensional Estimation,"Carlo Lucibello, Luca Saglietti, Yue Lu",https://icml.cc/Conferences/2019/Schedule?showEvent=4031,"In Generalized Linear Estimation (GLE) problems, we seek to estimate a signal that is observed through a linear transform followed by a component-wise, possibly nonlinear and noisy, channel. In the Bayesian optimal setting, Generalized Approximate Message Passing (GAMP) is known to achieve optimal performance for GLE. However, its performance can significantly deteriorate whenever there is a mismatch between the assumed and the true generative model, a situation frequently encountered in practice. In this paper, we propose a new algorithm, named Generalized Approximate Survey Propagation (GASP), for solving GLE in the presence of prior or model misspecifications. As a prototypical example, we consider the phase retrieval problem, where we show that GASP outperforms the corresponding GAMP, reducing the reconstruction threshold and, for certain choices of its parameters, approaching Bayesian optimal performance. Furthermore, we present a set of state evolution equations that can precisely characterize the performance of GASP in the high-dimensional limit.
","['Bocconi University', 'Microsoft Research', 'Harvard University, USA']"
2019,Boosted Density Estimation Remastered,"Zac Cranko, Richard Nock",https://icml.cc/Conferences/2019/Schedule?showEvent=3788,"    There has recently been a steady increase in the number iterative
    approaches to density estimation. However, an accompanying burst
    of formal convergence guarantees has not followed; all results pay
    the price of heavy assumptions which are often unrealistic or hard
    to check. The \emph{Generative Adversarial Network (GAN)}
    literature --- seemingly orthogonal to the aforementioned pursuit
    --- has had the side effect of a renewed interest in variational
    divergence minimisation (notably $f$-GAN). We show how to combine
    this latter approach and the classical boosting theory in supervised
    learning to get the first density estimation algorithm that provably
    achieves geometric convergence under very weak assumptions. We do so by a trick allowing to combine
    \textit{classifiers} as the sufficient statistics of an
    exponential family. Our analysis includes an improved variational characterisation of $f$-GAN.","['ANU', 'Data61, The Australian National University and the University of Sydney']"
2019,Inference and Sampling of $K_{33}$-free Ising Models,"Valerii Likhosherstov, Yury Maximov, Misha Chertkov",https://icml.cc/Conferences/2019/Schedule?showEvent=4294,"We call an Ising model tractable when it is possible to compute its partition function value (statistical inference) in polynomial time. The tractability also implies an ability to sample configurations of this model in polynomial time. The notion of tractability extends the basic case of planar zero-field Ising models. Our starting point is to describe algorithms for the basic case, computing partition function and sampling efficiently. Then, we extend our tractable inference and sampling algorithms to models whose triconnected components are either planar or graphs of $O(1)$ size. In particular, it results in a polynomial-time inference and sampling algorithms for $K_{33}$ (minor)-free topologies of zero-field Ising models---a generalization of planar graphs with a potentially unbounded genus.","['Skolkovo Institute of Science and Technology', 'Los Alamos National Laboratory - LANL', 'University of Arizona ']"
2019,Random Matrix Improved Covariance Estimation for a Large Class of Metrics,"Malik TIOMOKO A, Romain Couillet, Florent BOUCHARD, Guillaume GINOLHAC",https://icml.cc/Conferences/2019/Schedule?showEvent=3740,"Relying on recent advances in statistical estimation of covariance distances based on random matrix theory, this article proposes an improved covariance and precision matrix estimation for a wide family of metrics. 
The method is shown to largely outperform the sample covariance matrix estimate and to compete with state-of-the-art methods, while at the same time being computationally simpler and faster. Applications to linear and quadratic discriminant analyses also show significant gains, therefore suggesting practical interest to statistical machine learning.
","['Université Paris Sud', 'CentralSupélec', 'LISTIC, Université Savoie Mont-Blanc', 'Université Savoie Mont-Blanc']"
2019,Dual Entangled Polynomial Code: Three-Dimensional Coding for Distributed Matrix Multiplication,"Pedro Soto, Jun Li, Xiaodi Fan",https://icml.cc/Conferences/2019/Schedule?showEvent=3960,"Matrix multiplication is a fundamental building block in various machine learning algorithms. When the matrix comes from a large dataset, the multiplication can be split into multiple tasks which calculate the multiplication of submatrices on different nodes. As some nodes may be stragglers, coding schemes have been proposed to tolerate stragglers in such distributed matrix multiplication. However, existing coding schemes typically split the matrices in only one or two dimensions, limiting their capabilities to handle large-scale matrix multiplication. Three-dimensional coding, however, does not have any code construction that achieves the optimal number of tasks required for decoding, with the best result achieved by entangled polynomial (EP) codes. In this paper, we propose dual entangled polynomial (DEP) codes that require around 25% fewer tasks than EP codes by executing two matrix multiplications on each task. With experiments in a real cloud environment, we show that DEP codes can also save the decoding overhead and memory consumption of tasks.
","['Florida International University', 'Florida International University', 'Florida International University']"
2019,Neural Joint Source-Channel Coding,"Kristy Choi, Kedar Tatwawadi, Aditya Grover, Tsachy Weissman, Stefano Ermon",https://icml.cc/Conferences/2019/Schedule?showEvent=3941,"For reliable transmission across a noisy communication channel, classical results from information theory show that it is asymptotically optimal to separate out the source and channel coding processes. However, this decomposition can fall short in the finite bit-length regime, as it requires non-trivial tuning of hand-crafted codes and assumes infinite computational power for decoding. In this work, we propose to jointly learn the encoding and decoding processes using a new discrete variational autoencoder model. By adding noise into the latent codes to simulate the channel during training, we learn to both compress and error-correct given a fixed bit-length and computational budget. We obtain codes that are not only competitive against several separation schemes, but also learn useful robust representations of the data for downstream tasks such as classification. Finally, inference amortization yields an extremely fast neural decoder, almost an order of magnitude faster compared to standard decoding methods based on iterative belief propagation.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
2019,Doubly-Competitive Distribution Estimation,"Yi Hao, Alon Orlitsky",https://icml.cc/Conferences/2019/Schedule?showEvent=3929,"Distribution estimation is a statistical-learning cornerstone. Its classical \emph{min-max} formulation minimizes the estimation error for the worst distribution, hence under-performs for practical distributions that, like power-law, are often rather simple. Modern research has therefore focused on two frameworks: \emph{structural} estimation that improves learning accuracy by assuming a simple structure of the underlying distribution; and \emph{competitive}, or \emph{instance-optimal}, estimation that achieves the performance of a genie aided estimator up to a small excess error that vanishes as the sample size grows, regardless of the distribution. This paper combines and strengthens the two frameworks. It designs a single estimator whose excess error vanishes both at a universal rate as the sample size grows, as well as when the (unknown) distribution gets simpler. We show that the resulting algorithm significantly improves the performance guarantees for numerous competitive- and structural-estimation results. The algorithm runs in near-linear time and is robust to model misspecification and domain-symbol permutations. 
","['University of California, San Diego', 'UCSD']"
2019,Homomorphic Sensing,"Manolis Tsakiris, Liangzu Peng",https://icml.cc/Conferences/2019/Schedule?showEvent=3741,"A recent line of research termed ""unlabeled sensing"" and ""shuffled linear regression"" has been exploring under great generality the recovery of signals from subsampled and permuted measurements; a challenging problem in diverse fields of data science and machine learning. In this paper we introduce an abstraction of this problem which we call ""homomorphic sensing"". Given a linear subspace and a finite set of linear transformations we develop an algebraic theory which establishes conditions guaranteeing that points in the subspace are uniquely determined from their homomorphic image under some transformation in the set. As a special case, we recover known conditions for unlabeled sensing, as well as new results and extensions. On the algorithmic level we exhibit two dynamic programming based algorithms, which to the best of our knowledge are the first working solutions for the unlabeled sensing problem for small dimensions. One of them, additionally based on branch-and-bound, when applied to image registration under affine transformations, performs on par with or outperforms state-of-the-art methods on benchmark datasets.
","['Johns Hopkins University', 'ShanghaiTech University']"
2019,Phaseless PCA: Low-Rank Matrix Recovery from Column-wise Phaseless Measurements,"Seyedehsara Nayer, Praneeth Narayanamurthy, Namrata Vaswani",https://icml.cc/Conferences/2019/Schedule?showEvent=3894,"This work proposes the first set of simple, practically useful, and provable algorithms for two inter-related problems. (i) The first is low-rank matrix recovery from magnitude-only (phaseless) linear projections of each of its columns. This finds important applications in phaseless dynamic imaging, e.g., Fourier Ptychographic imaging of live biological specimens. Our guarantee shows that, in the regime of small ranks, the sample complexity required is only a little larger than the order-optimal one, and much smaller than what standard (unstructured) phase retrieval methods need. %Moreover our algorithm is fast and memory-efficient if only the minimum required number of measurements is used (ii) The second problem we study is a dynamic extension of the above: it allows the low-dimensional subspace from which each image/signal (each column of the low-rank matrix) is generated to change with time. We introduce a simple algorithm that is provably correct as long as the subspace changes are piecewise constant.
","['Iowa State University', 'Iowa State University', 'Iowa State University']"
2019,Rate Distortion For Model Compression:From Theory To Practice,"Weihao Gao, Yu-Han Liu, Chong Wang, Sewoong Oh",https://icml.cc/Conferences/2019/Schedule?showEvent=3765,"The enormous size of modern deep neural net-works makes it challenging to deploy those models in memory and communication limited scenarios. Thus, compressing a trained model without a significant loss in performance has become an increasingly important task. Tremendous advances has been made recently, where the main technical building blocks are pruning, quantization, and low-rank factorization. In this paper, we propose principled approaches to improve upon the common heuristics used in those building blocks, by studying the fundamental limit for model compression via the rate distortion theory. We prove a lower bound for the rate distortion function for model compression and prove its achievability for linear models. Although this achievable compression scheme is intractable in practice, this analysis motivates a novel objective function for model compression, which can be used to improve classes of model compressor such as pruning or quantization. Theoretically, we prove that the proposed scheme is optimal for compressing one-hidden-layer ReLU neural networks. Empirically,we show that the proposed scheme improves upon the baseline in the compression-accuracy tradeoff.
","['University of Illinois at Urbana-Champaign', 'Google', 'ByteDance Inc.', 'University of Washington']"
2019,Formal Privacy for Functional Data with Gaussian Perturbations,"Ardalan Mirshani, Matthew Reimherr, Aleksandra Slavković",https://icml.cc/Conferences/2019/Schedule?showEvent=4269,"Motivated by the rapid rise in statistical tools in {\it Functional Data Analysis}, we consider the Gaussian mechanism for achieving differential privacy (DP) with parameter estimates taking values in a, potentially infinite-dimensional, separable Banach space.  Using classic results from probability theory, we show how densities over function spaces can be utilized to achieve the desired DP bounds.  This extends prior results of Hall et al (2013) to a much broader class of statistical estimates and summaries, including ``path level"" summaries, nonlinear functionals, and full function releases.  By focusing on Banach spaces, we provide a deeper picture of the challenges for privacy with complex data, especially the role regularization plays in balancing utility and privacy.  Using an application to penalized smoothing, we highlight this balance in the context of mean function estimation.  Simulations and an  application to {diffusion tensor imaging} are briefly presented, with extensive additions included in a supplement.
","['The Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University']"
2019,Graphical-model based estimation and inference for differential privacy,"Ryan McKenna, Daniel Sheldon, Gerome Miklau",https://icml.cc/Conferences/2019/Schedule?showEvent=3639,"Many privacy mechanisms reveal high-level information about a data distribution through noisy measurements. It is common to use this information to estimate the answers to new queries. In this work, we provide an approach to solve this estimation problem efficiently using graphical models, which is particularly effective when the distribution is high-dimensional but the measurements are over low-dimensional marginals. We show that our approach is far more efficient than existing
estimation techniques from the privacy literature and that it can improve the accuracy and scalability of many state-of-the-art mechanisms.
","['UMass Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts, Amherst']"
2019,White-box vs Black-box: Bayes Optimal Strategies for Membership Inference,"Alexandre Sablayrolles, Douze Matthijs, Cordelia Schmid, Yann Ollivier, Herve Jegou",https://icml.cc/Conferences/2019/Schedule?showEvent=4066,"Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet. 
","['Facebook AI Research', 'Facebook AI Research', 'Inria/Google', 'Facebook Artificial Intelligence Research', 'Facebook AI Research']"
2019,An Optimal Private Stochastic-MAB Algorithm based on Optimal Private Stopping Rule,"Touqir Sajed, Or Sheffet",https://icml.cc/Conferences/2019/Schedule?showEvent=4245,"We present a provably optimal differentially private algorithm for the stochastic multi-arm bandit problem, as opposed to the private analogue of the UCB-algorithm (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016) which doesn't meet the recently discovered lower-bound of $\Omega \left(\frac{K\log(T)}{\epsilon} \right)$ (Shariff and Sheffet, 2018). Our construction is based on a different algorithm, Successive Elimination (Even-Dar et al., 2002), that repeatedly pulls all remaining arms until an arm is found to be suboptimal and is then eliminated. In order to devise a private analogue of Successive Elimination we visit the problem of private \emph{stopping rule}, that takes as input a stream of i.i.d samples from an unknown distribution and returns a \emph{multiplicative} $(1 \pm \alpha)$-approximation of the distribution's mean, and prove the optimality of our private stopping rule. We then present the private Successive Elimination algorithm which meets both the non-private lower bound (Lai and Robbins, 1985) and the above-mentioned private lower bound. We also compare empirically the performance of our algorithm with the private UCB algorithm.","['University of Alberta', 'University of Alberta']"
2019,Sublinear Space Private Algorithms Under the Sliding Window Model,Jalaj Upadhyay,https://icml.cc/Conferences/2019/Schedule?showEvent=3867,"The Differential privacy overview of Apple states, ``Apple retains the collected data for a maximum of three months."" Analysis of recent data is formalized by the {\em sliding window model}. This begs the question: what is the price of privacy in the sliding window model? In this paper, we study heavy hitters in the sliding window model with window size $w$. Previous works of Chan et al. (2012) estimates heavy hitters with an error of order $\theta w$ for a constant $\theta >0$. In this paper, we give an efficient differentially private algorithm to estimate heavy hitters in the sliding window model with $\widetilde O(w^{3/4})$ additive error and using $\widetilde O(\sqrt{w})$ space.
",['Johns Hopkins University']
2019,Locally Private Bayesian Inference for Count Models,"Aaron Schein, Steven Wu, Alexandra Schofield, Mingyuan Zhou, Hanna Wallach",https://icml.cc/Conferences/2019/Schedule?showEvent=4109,"We present a general and modular method for privacy-preserving
Bayesian inference for Poisson factorization, a broad class of models
that includes some of the most widely used models in the social
sciences. Our method satisfies limited-precision local privacy, a
generalization of local differential privacy that we introduce to
formulate appropriate privacy guarantees for sparse count data. We
present an MCMC algorithm that approximates the posterior distribution
over the latent variables conditioned on data that has been locally
privatized by the geometric mechanism. Our method is based on two
insights: 1) a novel reinterpretation of the geometric mechanism in
terms of the Skellam distribution and 2) a general theorem that
relates the Skellam and Bessel distributions. We demonstrate our
method's utility using two case studies that involve real-world email
data. We show that our method consistently outperforms the commonly
used naive approach, wherein inference proceeds as usual, treating
the locally privatized data as if it were not privatized.
","['UMass Amherst', 'University of Minnesota', 'Cornell University', 'University of Texas at Austin', 'Microsoft Research']"
2019,Low Latency Privacy Preserving Inference,"Alon Brutzkus, Ran Gilad-Bachrach, Oren Elisha",https://icml.cc/Conferences/2019/Schedule?showEvent=3801,"When applying machine learning to sensitive data, one has to find a balance between accuracy, information security, and computational-complexity. Recent studies combined Homomorphic Encryption with neural networks to make inferences while protecting against information leakage. However, these methods are limited by the width and depth of neural networks that can be used (and hence the accuracy) and exhibit high latency even for relatively simple networks. In this study we provide two solutions that address these limitations. In the first solution, we present more than 10\times improvement in latency and enable inference on wider networks compared to prior attempts with the same level of security. The improved performance is achieved by novel methods to represent the data during the computation. In the second solution, we apply the method of transfer learning to provide private inference services using deep networks with latency of \sim0.16 seconds. We demonstrate the efficacy of our methods on several computer vision tasks.
","['Tel Aviv University', 'Microsoft Research', 'Microsoft']"
2019,Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters,"Jayadev Acharya, Ziteng Sun",https://icml.cc/Conferences/2019/Schedule?showEvent=3570,"We consider the problems of distribution estimation, and heavy hitter (frequency) estimation under privacy, and communication constraints. While the  constraints have been studied separately, optimal schemes for one are sub-optimal for the other. We propose a sample-optimal $\eps$-locally differentially private (LDP) scheme for distribution estimation, where each user communicates one bit, and requires \emph{no} public randomness. We also show that Hadamard Response, a recently proposed scheme for $\eps$-LDP distribution estimation is also utility-optimal for heavy hitters estimation. Our final result shows that unlike distribution estimation, without public randomness, any utility-optimal heavy hitter estimation algorithm must require $\Omega(\log n)$ bits of communication per user.","['Cornell University', 'Cornell/Google']"
2019,Poission Subsampled R\'enyi Differential Privacy,"Yuqing Zhu, Yu-Xiang Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=4317,"We consider the problem of privacy-amplification by under the Renyi Differential Privacy framework. This is the main technique underlying the moments accountants (Abadi et al., 2016) for differentially private deep learning. 
Unlike previous attempts on this problem which deals with Sampling with Replacement, we consider the Poisson subsampling scheme which selects each data point independently with a coin toss. This allows us to significantly simplify and tighten the bounds for the RDP of subsampled mechanisms and derive numerically stable approximation schemes.  In particular, for subsampled Gaussian mechanism and subsampled Laplace mechanism, we prove an analytical formula of their RDP that exactly matches the lower bound. The result is the first of its kind and we numerically demonstrate an order of magnitude improvement in the privacy-utility tradeoff. 
","['UC Santa Barbara', 'UC Santa Barbara']"
2019,Benefits and Pitfalls of the Exponential Mechanism with Applications to Hilbert Spaces and Functional PCA,"Jordan Awan, Ana Kenney, Matthew Reimherr, Aleksandra Slavković",https://icml.cc/Conferences/2019/Schedule?showEvent=3732,"The exponential mechanism is a fundamental tool of Differential Privacy (DP) due to its strong privacy guarantees and flexibility. We study its extension to settings with summaries based on infinite dimensional outputs such as with functional data analysis, shape analysis, and nonparametric statistics. We show that the mechanism must be designed with respect to a specific base measure over the output space, such as a Gaussian process. We provide a positive result that establishes a Central Limit Theorem for the exponential mechanism quite broadly. We also provide a negative result, showing that the  magnitude of noise introduced for privacy is asymptotically non-negligible relative to the statistical estimation error. We develop an $\ep$-DP mechanism for functional principal component analysis, applicable in separable Hilbert spaces, and demonstrate its performance via simulations and applications to two datasets.","['Pennsylvania State University', 'Penn State University', 'Pennsylvania State University', 'Pennsylvania State University']"
2019,Refined Complexity of PCA with Outliers,"Kirill Simonov, Fedor Fomin, Petr Golovach, Fahad Panolan",https://icml.cc/Conferences/2019/Schedule?showEvent=3827,"Principal component analysis (PCA) is one of the most fundamental procedures in exploratory data analysis and is the basic step in applications ranging from    quantitative finance and bioinformatics to image analysis and neuroscience. However, it is well-documented that the applicability of PCA in many real scenarios could be  constrained by an ""immune deficiency"" to outliers such as corrupted observations. We consider the following algorithmic question about the PCA with outliers. For a set of $n$ points in $\mathbb{R}^{d}$, how to learn a subset of  points, say 1% of the total number of points, such that the remaining  part of the points is best fit into some unknown  $r$-dimensional subspace? We provide a rigorous algorithmic analysis of the problem. We show that the problem is solvable in time $n^{O(d^2)}$. In particular, for constant dimension the problem is solvable in polynomial time. We complement the algorithmic result by the lower bound, showing that unless Exponential Time Hypothesis fails, in time $f(d)n^{o(d)}$, for any function $f$ of $d$, it is impossible not only to solve the problem exactly but even to approximate it within a constant factor.","['University of Bergen', 'University of Bergen', 'University of Bergen', 'University of Bergen']"
2019,On Efficient Optimal Transport: An Analysis of Greedy and Accelerated Mirror Descent Algorithms,"Darren Lin, Nhat Ho, Michael Jordan",https://icml.cc/Conferences/2019/Schedule?showEvent=3800,"We provide theoretical analyses for two algorithms that solve the regularized optimal transport (OT) problem between two discrete probability measures with at most $n$ atoms. We show that a greedy variant of the classical Sinkhorn algorithm, known as the \emph{Greenkhorn algorithm}, can be improved to $\bigOtil\left(n^2/\varepsilon^2\right)$, improving on the best known complexity bound of $\bigOtil\left(n^2/\varepsilon^3\right)$. This matches the best known complexity bound for the Sinkhorn algorithm and helps explain why the Greenkhorn algorithm outperforms the Sinkhorn algorithm in practice. Our proof technique is based on a primal-dual formulation and provide a \textit{tight} upper bound for the dual solution, leading to a class of \emph{adaptive primal-dual accelerated mirror descent} (APDAMD) algorithms.  We prove that the complexity of these algorithms is $\bigOtil\left(n^2\sqrt{\gamma}/\varepsilon\right)$ in which $\gamma \in (0, n]$ refers to some constants in the Bregman divergence. Experimental results on synthetic and real datasets demonstrate the favorable performance of the Greenkhorn and APDAMD algorithms in practice.","['UC Berkeley', 'University of California, Berkeley', 'UC Berkeley']"
2019,Passed & Spurious: Descent Algorithms and Local Minima in Spiked Matrix-Tensor Models,"Stefano Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, Lenka Zdeborova",https://icml.cc/Conferences/2019/Schedule?showEvent=3706,"In this work we analyse quantitatively the interplay between the loss landscape and performance of descent algorithms in a prototypical inference problem, the spiked matrix-tensor model. We study a loss function that is the negative log-likelihood of the model. We analyse the number of local minima at a fixed distance from the signal/spike with the Kac-Rice formula, and locate trivialization of the landscape at large signal-to-noise ratios. We evaluate analytically the performance of a gradient flow algorithm using integro-differential PDEs as developed in physics of disordered systems for the Langevin dynamics.
We analyze the performance of an approximate message passing algorithm estimating the maximum likelihood configuration via its state evolution. We conclude by comparing the above results: while we observe a
drastic slow down of the gradient flow dynamics even in the region
where the landscape is trivial, both the analyzed algorithms are shown
to perform well even in the part of the region of parameters where
spurious local minima are present. 
","['Institut de Physique Théorique', 'ENS', 'Institut de Physique Théorique', 'CNRS']"
2019,Teaching a black-box learner,"Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, Jerry Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3964,"One widely-studied model of {\it teaching} calls for a teacher to provide the minimal set of labeled examples that uniquely specifies a target concept. The assumption is that the teacher knows the learner's hypothesis class, which is often not true of real-life teaching scenarios. We consider the problem of teaching a learner whose representation and hypothesis class are unknown---that is, the learner is a black box. We show that a teacher who does not interact with the learner can do no better than providing random examples. We then prove, however, that with interaction, a teacher can efficiently find a set of teaching examples that is a provably good approximation to the optimal set. As an illustration, we show how this scheme can be used to {\it shrink} training sets for any family of classifiers: that is, to find an approximately-minimal subset of training instances that yields the same classifier as the entire set.
","['UC San Diego', 'Columbia University', 'UC San Diego/NTENT', 'University of Wisconsin-Madison']"
2019,PAC Learnability of Node Functions in Networked Dynamical Systems,"Abhijin Adiga, Chris J Kuhlman, Madhav Marathe, S. S. Ravi, Anil  Vullikanti",https://icml.cc/Conferences/2019/Schedule?showEvent=4122,"We consider the PAC learnability of the local functions at the vertices of a discrete networked dynamical system, assuming that the underlying network is known. Our focus is on the learnability of threshold functions. We show that several variants of threshold functions are PAC learnable and provide tight bounds on the sample complexity. In general, when the input consists of positive and negative examples, we show that the concept class of threshold functions is not efficiently PAC learnable, unless NP = RP. Using a dynamic programming approach, we show efficient PAC learnability when the number of negative examples is small. We also present an efficient learner which is consistent with all the positive examples and at least (1-1/e) fraction of the negative examples. This algorithm is based on maximizing a submodular function under matroid constraints. By performing experiments on both synthetic and real-world networks, we study how the network structure and sample complexity influence the quality of the inferred system.
","['University of Virginia', 'Biocomplexity Institute & Initiative, University of Virginia', 'Biocomplexity Institute & Initiative, University of Virginia', 'University of Virginia and University at Albany -- SUNY', 'Biocomplexity Institute and Dept of Computer Science, University of Virginia']"
2019,Online learning with kernel losses,"Niladri Chatterji, Aldo Pacchiano, Peter Bartlett",https://icml.cc/Conferences/2019/Schedule?showEvent=4222,"We present a generalization of the adversarial linear bandits framework, where the underlying losses are kernel functions (with an associated reproducing kernel Hilbert space) rather than linear functions. We study a version of the exponential weights algorithm and bound its regret in this setting. Under conditions on the eigen-decay of the kernel we provide a sharp characterization of the regret for this algorithm. When we have polynomial eigen-decay ($\mu_j \le \mathcal{O}(j^{-\beta})$), we find that the regret is bounded by $\mathcal{R}_n \le \mathcal{O}(n^{\beta/2(\beta-1)})$. While under the assumption of exponential eigen-decay ($\mu_j \le \mathcal{O}(e^{-\beta j })$) we get an even tighter bound on the regret $\mathcal{R}_n \le \tilde{\mathcal{O}}(n^{1/2})$. When the eigen-decay is polynomial we also show a \emph{non-matching} minimax lower bound on the regret of $\mathcal{R}_n \ge \Omega(n^{(\beta+1)/2\beta})$ and a lower bound of $\mathcal{R}_n \ge \Omega(n^{1/2})$ when the decay in the eigen-values is exponentially fast.

We also study the full information setting when the underlying losses are kernel functions and present an adapted exponential weights algorithm and a conditional gradient descent algorithm.","['UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2019,Nearest Neighbor and Kernel Survival Analysis: Nonasymptotic Error Bounds and Strong Consistency Rates,George Chen,https://icml.cc/Conferences/2019/Schedule?showEvent=3653,"We establish the first nonasymptotic error bounds for Kaplan-Meier-based nearest neighbor and kernel survival probability estimators where feature vectors reside in metric spaces. Our bounds imply rates of strong consistency for these nonparametric estimators and, up to a log factor, match an existing lower bound for conditional CDF estimation. Our proof strategy also yields nonasymptotic guarantees for nearest neighbor and kernel variants of the Nelson-Aalen cumulative hazards estimator. We experimentally compare these methods on four datasets.
We find that for the kernel survival estimator, a good choice of kernel is one learned using random survival forests.
",['Carnegie Mellon University']
2019,Fast Rates for a kNN Classifier Robust to Unknown Asymmetric Label Noise,"Henry Reeve, Ata Kaban",https://icml.cc/Conferences/2019/Schedule?showEvent=3851,"We consider classification in the presence of class-dependent asymmetric label noise with unknown noise probabilities. In this setting, identifiability conditions are known, but additional assumptions were shown to be required for finite sample rates, and so far only the parametric rate has been obtained. Assuming these identifiability conditions, together with a measure-smoothness condition on the regression function and Tsybakov’s margin condition, we show that the Robust kNN classifier of Gao et al. attains, the mini-max optimal rates of the noise-free setting, up to a log factor, even when trained on data with unknown asymmetric label noise. Hence, our results provide a solid theoretical backing for this empirically successful algorithm. By contrast the standard kNN is not even consistent in the setting of asymmetric label noise. A key idea in our analysis is a simple kNN based method for estimating the maximum of a function that requires far less assumptions than existing mode estimators do, and which may be of independent interest for noise proportion estimation and randomised optimisation problems.
","['University of Birmingham', 'University of Birmingham']"
2019,Uniform Convergence Rate of the Kernel Density Estimator Adaptive to Intrinsic Volume Dimension,"Jisu Kim, Jaehyeok Shin, Alessandro Rinaldo, Larry Wasserman",https://icml.cc/Conferences/2019/Schedule?showEvent=4108,"We derive concentration inequalities for the supremum norm of the difference between a kernel density estimator (KDE) and its point-wise expectation that hold uniformly over the selection of the bandwidth and under weaker conditions on the kernel and the data generating distribution than previously used in the literature. We first propose a novel concept, called the volume dimension, to measure the intrinsic dimension of the support of a probability distribution based on the rates of decay of the probability of vanishing Euclidean balls. Our bounds depend on the volume dimension and generalize the existing bounds derived in the literature. In particular, when the data-generating distribution has a bounded Lebesgue density or is supported on a sufficiently well-behaved lower-dimensional manifold, our bound recovers the same convergence rate depending on the intrinsic dimension of the support as ones known in the literature. At the same time, our results apply to more general cases, such as the ones of distribution with unbounded densities or supported on a mixture of manifolds with different dimensions. Analogous bounds are derived for the derivative of the KDE, of any order. Our results are generally applicable but are especially useful for problems in geometric inference and topological data analysis, including level set estimation, density-based clustering, modal clustering and mode hunting, ridge estimation and persistent homology.
","['Inria Saclay', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2019,Maximum Likelihood Estimation for  Learning Populations of Parameters,"Ramya Korlakai Vinayak, Weihao Kong, Gregory Valiant, Sham Kakade",https://icml.cc/Conferences/2019/Schedule?showEvent=4310,"Consider a setting with $N$ independent individuals, each with an unknown parameter, $p_i \in [0, 1]$ drawn from some unknown distribution $P^\star$. After observing the outcomes of $t$ independent Bernoulli trials, i.e., $X_i \sim \text{Binomial}(t, p_i)$ per individual, our objective is to accurately estimate $P^\star$ in the sparse regime, namely when $t \ll N$. This problem arises in numerous domains, including the social sciences, psychology, health-care, and biology, where the size of the population under study is usually large yet the number of observations per individual is often limited. 

Our main result shows that, in this sparse regime where $t \ll N$, the maximum likelihood estimator (MLE) is both statistically minimax optimal and efficiently computable. Precisely, for sufficiently large $N$, the MLE achieves the information theoretic optimal error bound of $\mathcal{O}(\frac{1}{t})$ for $t < c\log{N}$, with regards to the earth mover's distance (between the estimated and true distributions). More generally, in an exponentially large interval of $t$ beyond $c \log{N}$, the MLE achieves the minimax error bound of $\mathcal{O}(\frac{1}{\sqrt{t\log N}})$. In contrast, regardless of how large $N$ is, the naive ""plug-in"" estimator for this problem only achieves the sub-optimal error of $\Theta(\frac{1}{\sqrt{t}})$. Empirically, we also demonstrate the MLE performs well on both synthetic as well as real datasets.","['University of Washington', 'Stanford University', 'Stanford University', 'University of Washington']"
2019,Projection onto Minkowski Sums with Application to Constrained Learning,"Joong-Ho (Johann) Won, Jason Xu, Kenneth Lange",https://icml.cc/Conferences/2019/Schedule?showEvent=4073,"We introduce block descent algorithms for projecting onto Minkowski sums of sets. Projection onto such sets is a crucial step in many statistical learning problems, and may regularize complexity of solutions to an optimization problem or arise in dual formulations of penalty methods. We show that projecting onto the Minkowski sum admits simple, efficient algorithms when complications such as overlapping constraints pose challenges to existing methods. We prove that our algorithm converges linearly when sets are strongly convex or satisfy an error bound condition, and extend the theory and methods to encompass non-convex sets as well. We demonstrate empirical advantages in runtime and accuracy over competitors in applications to $\ell_{1,p}$-regularized learning, constrained lasso, and overlapping group lasso.","['Seoul National University', 'Duke University', 'UCLA']"
2019,Blended Conditonal Gradients,"Gábor Braun, Sebastian Pokutta, Dan Tu, Stephen Wright",https://icml.cc/Conferences/2019/Schedule?showEvent=4183,"We present a blended conditional gradient approach for minimizing a smooth convex function over a polytope P, combining the Frank–Wolfe algorithm (also called conditional gradient) with gradient-based steps, different from away steps and pairwise steps, but still achieving linear convergence for strongly convex functions, along with good practical performance. Our approach retains all favorable properties of conditional gradient algorithms, notably avoidance of projections onto P and maintenance of iterates as sparse convex combinations of a limited number of extreme points of P. The algorithm is lazy, making use of inexpensive inexact solutions of the linear programming subproblem that characterizes the conditional gradient approach. It decreases measures of optimality (primal and dual gaps) rapidly, both in the number of iterations and in wall-clock time, outperforming even the lazy conditional gradient algorithms of Braun et al. 2017. We also present a streamlined version of the algorithm that applies when P is the probability simplex.
","['Georgia Institute of Technology', 'Georgia Tech', 'GEORGIA TECH', 'University of Wisconsin-Madison']"
2019,Acceleration of SVRG and Katyusha X by Inexact Preconditioning,"Yanli Liu, Fei Feng, Wotao Yin",https://icml.cc/Conferences/2019/Schedule?showEvent=4233,"Empirical risk minimization is an important class of optimization problems with many popular machine learning applications, and stochastic variance reduction methods are popular choices for solving them. Among these methods, SVRG and Katyusha X (a Nesterov accelerated SVRG) achieve fast convergence without substantial memory requirement. In this paper, we propose to accelerate these two algorithms by \textit{inexact preconditioning}, the proposed methods employ \textit{fixed} preconditioners, although the subproblem in each epoch becomes harder, it suffices to apply \textit{fixed} number of simple subroutines to solve it inexactly, without losing the overall convergence. As a result, this inexact preconditioning strategy gives provably better iteration complexity and gradient complexity over SVRG and Katyusha X. We also allow each function in the finite sum to be nonconvex while the sum is strongly convex. In our numerical experiments, we observe an on average $8\times$ speedup on the number of iterations and $7\times$ speedup on runtime.","['UCLA math', 'UCLA', 'Alibaba US']"
2019,Characterization of Convex Objective Functions and Optimal Expected Convergence Rates for SGD,"Marten van Dijk, Lam Nguyen, PHUONG_HA NGUYEN, Dzung Phan",https://icml.cc/Conferences/2019/Schedule?showEvent=3843,"We study Stochastic Gradient Descent (SGD) with diminishing step sizes for convex objective functions. We introduce a definitional framework and theory that defines and characterizes a core property, called curvature, of convex objective functions. In terms of curvature we can derive a new inequality that can be used to compute an optimal sequence of diminishing step sizes by solving a differential equation. Our exact solutions confirm known results in literature and allows us to fully characterize a new regularizer with its corresponding expected convergence rates.
","['University of Connecticut', 'IBM Research, Thomas J. Watson Research Center', 'University of Connecticut', 'IBM T.J. Watson Research Center']"
2019,A Conditional-Gradient-Based Augmented Lagrangian Framework,"Alp Yurtsever, Olivier Fercoq, Volkan Cevher",https://icml.cc/Conferences/2019/Schedule?showEvent=4129,"This paper considers a generic convex minimization template with affine constraints over a compact domain, which covers key semidefinite programming applications. The existing conditional gradient methods either do not apply to our template or are too slow in practice. To this end, we propose a new conditional gradient method, based on a unified treatment of smoothing and augmented Lagrangian frameworks. The proposed method maintains favorable properties of the classical conditional gradient method, such as cheap linear minimization oracle calls and sparse representation of the decision variable.  We prove $O(1/\sqrt{k})$ convergence rate for our method in the objective residual and the feasibility gap. This rate is essentially the same as the state of the art CG-type methods for our problem template, but the proposed method is arguably superior in practice compared to existing methods in various applications.","['EPFL', 'Télécom ParisTech, Université Paris-Saclay', 'EPFL']"
2019,SGD: General Analysis and Improved Rates,"Robert Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, Peter Richtarik",https://icml.cc/Conferences/2019/Schedule?showEvent=4155,"We propose a  general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm.  Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form  minibatches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before.  Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful  stepsize-switching rules  which describe when one should switch from a constant to a decreasing stepsize regime. 
","['Telecom Paristech', 'The University of Edinburgh', 'KAUST', 'King Abdullah University of Science and Technology', 'Moscow Institute of Physics and Technology', 'KAUST']"
2019,Curvature-Exploiting Acceleration of Elastic Net Computations,"Vien Mai, Mikael Johansson",https://icml.cc/Conferences/2019/Schedule?showEvent=4157,"This paper introduces an efficient second-order method for solving the elastic net problem. Its key innovation is a computationally efficient technique for injecting curvature information in the optimization process which admits a strong theoretical performance guarantee. In particular, we show improved run time over popular first-order methods and quantify the speed-up in terms of statistical measures of the data matrix. The improved time complexity is the result of an extensive exploitation of the problem structure and a careful combination of second-order information, variance reduction techniques, and momentum acceleration. Beside theoretical speed-up, experimental results demonstrate great practical performance benefits of curvature information, especially for ill-conditioned data sets.
","['KTH Royal Institute of Technology', 'KTH Royal Institute of Technology']"
2019,Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,"Anastasiia Koloskova, Sebastian Stich, Martin Jaggi",https://icml.cc/Conferences/2019/Schedule?showEvent=4005,"We consider decentralized stochastic optimization with the objective function (e.g. data samples for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a fixed communication graph. To address the communication bottleneck,  the nodes compress (e.g. quantize or sparsify) their model updates. We cover both unbiased and biased compression operators with quality denoted by \delta <= 1 (\delta=1 meaning no compression).
We (i) propose a novel gossip-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O(1/(nT) + 1/(T \rho^2 \delta)^2) for strongly convex objectives, where T denotes the number of iterations and \rho the eigengap of the connectivity matrix. We (ii) present a novel gossip algorithm, CHOCO-GOSSIP, for the average consensus problem that converges in time O(1/(\rho^2\delta) \log (1/\epsilon)) for accuracy \epsilon > 0. This is (up to our knowledge) the first gossip algorithm that supports arbitrary compressed messages for \delta > 0 and still exhibits linear convergence. We (iii) show in experiments that both of our algorithms do outperform the respective state-of-the-art baselines and CHOCO-SGD can reduce communication by at least two orders of magnitudes.
","['EPFL', 'EPFL', 'EPFL']"
2019,Safe Grid Search with Optimal Complexity,"Eugene Ndiaye, Tam Le, Olivier Fercoq, Joseph Salmon, Ichiro Takeuchi",https://icml.cc/Conferences/2019/Schedule?showEvent=3812,"Popular machine learning estimators involve regularization parameters that can be challenging to tune, and standard strategies rely on grid search for this task.
In this paper, we revisit the techniques of approximating the regularization  path up to predefined tolerance $\epsilon$ in a unified framework and show that its complexity is $O(1/\sqrt[d]{\epsilon})$ for uniformly convex loss of order $d \geq 2$ and $O(1/\sqrt{\epsilon})$ for Generalized Self-Concordant functions. This framework encompasses least-squares but also logistic regression, a case that as far as we know was not handled as precisely in previous works. We leverage our technique to provide refined bounds on the validation error as well as a practical algorithm for hyperparameter tuning. The latter has global convergence guarantee when targeting a prescribed accuracy on the validation set. Last but not least, our approach helps relieving the practitioner from the (often neglected) task of selecting a stopping criterion when optimizing over the training set: our method automatically calibrates this criterion based on the targeted accuracy on the validation set.","['RIKEN AIP', 'RIKEN AIP', 'Télécom ParisTech, Université Paris-Saclay', 'Université de Montpellier', 'Nagoya Institute of Technology / RIKEN']"
2019,SAGA with Arbitrary Sampling,"Xun Qian, Zheng Qu, Peter Richtarik",https://icml.cc/Conferences/2019/Schedule?showEvent=3969,"We study the problem of minimizing the average of a very large number of smooth functions, which is of key importance in  training supervised  learning models.  One of the most celebrated methods in this context is the SAGA algorithm of Defazio et al. (2014). Despite years of research on the topic, a general-purpose version of SAGA---one that would include arbitrary importance sampling and minibatching schemes---does not exist.  We remedy this situation and propose a general and flexible variant of SAGA following the arbitrary sampling paradigm. We perform an iteration complexity analysis of the method, largely possible due to the construction of  new stochastic Lyapunov functions. We establish linear convergence rates in the smooth and  strongly convex regime, and  under certain error bound conditions also in a  regime without strong convexity.  Our rates match those of the primal-dual method Quartz (Qu et al., 2015) for which an arbitrary sampling analysis is available, which makes a significant step towards closing the gap in our understanding of complexity of primal and dual methods for finite sum problems. Finally, we show through experiments that specific variants of our general SAGA method can perform better in practice than other competing methods.
","['KAUST', 'The University of Hong Kong', 'KAUST']"
2019,Natural Analysts in Adaptive Data Analysis,"Tijana Zrnic, Moritz Hardt",https://icml.cc/Conferences/2019/Schedule?showEvent=3916,"Adaptive data analysis is frequently criticized for its pessimistic generalization guarantees. The source of these pessimistic bounds is a model that permits arbitrary, possibly adversarial analysts that optimally use information to bias results. While being a central issue in the field, still lacking are notions of natural analysts that allow for more optimistic bounds faithful to the reality that typical analysts aren't adversarial. In this work, we propose notions of natural analysts that smoothly interpolate between the optimal non-adaptive bounds and the best-known adaptive generalization bounds. To accomplish this, we model the analyst's knowledge as evolving according to the rules of an unknown dynamical system that takes in revealed information and outputs new statistical queries to the data. This allows us to restrict the analyst through different natural control-theoretic notions. One such notion corresponds to a recency bias, formalizing an inability to arbitrarily use distant information. Another complementary notion formalizes an anchoring bias, a tendency to weight initial information more strongly. Both notions come with quantitative parameters that smoothly interpolate between the non-adaptive case and the fully adaptive case, allowing for a rich spectrum of intermediate analysts that are neither non-adaptive nor adversarial. Natural not only from a cognitive perspective, we show that our notions also capture standard optimization methods, like gradient descent in various settings. This gives a new interpretation to the fact that gradient descent tends to overfit much less than its adaptive nature might suggest.
","['University of California, Berkeley', 'University of California, Berkeley']"
2019,CapsAndRuns: An Improved Method for Approximately Optimal Algorithm Configuration,"Gellért Weisz, András György, Csaba Szepesvari",https://icml.cc/Conferences/2019/Schedule?showEvent=3650,"We consider the problem of configuring general-purpose solvers to run efficiently on problem instances drawn from an unknown distribution, a problem of major interest in solver autoconfiguration. Following previous work, we focus on designing algorithms that find a configuration with near-optimal expected capped runtime while doing the least amount of work, with the cap chosen in a configuration-specific way so that most instances are solved. In this paper we present a new algorithm, CapsAndRuns, which finds a near-optimal configuration while using time that scales (in a problem dependent way) with the optimal expected capped runtime, significantly strengthening previous results which could only guarantee a bound that scaled with the potentially much larger optimal expected uncapped runtime. The new algorithm is simpler and more intuitive than the previous methods: first it estimates the optimal runtime cap for each configuration, then it uses a Bernstein race to find a near optimal configuration given the caps. Experiments verify that our method can significantly outperform its competitors.
","['DeepMind', 'DeepMind', 'DeepMind/University of Alberta']"
2019,Leveraging Low-Rank Relations Between Surrogate Tasks in Structured Prediction,"Giulia Luise, Dimitrios Stamos, Massimiliano Pontil, Carlo Ciliberto",https://icml.cc/Conferences/2019/Schedule?showEvent=4011,"We study the interplay between surrogate methods for structured prediction and techniques from multitask learning designed to leverage relationships between surrogate outputs. 
We propose an efficient algorithm based on trace norm regularization which, differently from previous methods, does not require explicit knowledge of the coding/decoding functions of the surrogate framework. 
As a result, our algorithm can be applied to the broad class of problems in which the surrogate space is large or even infinite dimensional. We study excess risk bounds for trace norm regularized structured prediction proving the consistency and learning rates for our estimator. We also identify relevant regimes in which our approach can enjoy better generalization performance than previous methods. 
Numerical experiments on ranking problems indicate that enforcing low-rank relations among surrogate outputs may indeed provide a significant advantage in practice.
","['University College London', 'University College London', 'Istituto Italiano di Tecnologia and University College London', 'Imperial College London']"
2019,Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,"Andrew Cotter, Maya Gupta, Heinrich Jiang, Nati Srebro, Karthik Sridharan, Serena Wang, Blake Woodworth, Seungil You",https://icml.cc/Conferences/2019/Schedule?showEvent=3659,"Classifiers can be trained with data-dependent constraints to satisfy fairness goals, reduce churn, achieve a targeted false positive rate, or other policy goals. We study the generalization performance for such constrained optimization problems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To improve generalization, we frame the problem as a two-player game where one player optimizes the model parameters on a training dataset, and the other player enforces the constraints on an independent validation dataset. We build on recent work in two-player constrained optimization to show that if one uses this two-dataset approach, then constraint generalization can be significantly improved. As we illustrate experimentally, this approach works not only in theory, but also in practice.
","['Google AI', 'Google', 'Google Research', 'Toyota Technological Institute at Chicago', 'Cornell University', 'Google', 'TTI-Chicago', 'Kakao Mobility']"
2019,Optimality Implies Kernel Sum Classifiers are Statistically Efficient,"Raphael Meyer, Jean Honorio",https://icml.cc/Conferences/2019/Schedule?showEvent=4299,"We propose a novel combination of optimization tools with learning theory bounds in order to analyze the sample complexity of optimal kernel sum classifiers. This contrasts the typical learning theoretic results which hold for all (potentially suboptimal) classifiers. Our work also justifies assumptions made in prior work on multiple kernel learning. As a byproduct of our analysis, we also provide a new form of Rademacher complexity for hypothesis classes containing only optimal classifiers.
","['Purdue University', 'Purdue University']"
2019,The Implicit Fairness Criterion of Unconstrained Learning,"Lydia T. Liu, Max Simchowitz, Moritz Hardt",https://icml.cc/Conferences/2019/Schedule?showEvent=4223,"We clarify what fairness guarantees we can and cannot expect to follow from unconstrained machine learning. Specifically, we show that in many settings, unconstrained learning on its own implies group calibration, that is, the outcome variable is conditionally independent of group membership given the score. 
A lower bound confirms the optimality of our upper bound. Moreover, we prove that as the excess risk of the learned score decreases, the more strongly it violates separation and independence, two other standard fairness criteria. Our results challenge the view that group calibration necessitates an active intervention, suggesting that often we ought to think of it as a byproduct of unconstrained machine learning. 
","['University of California Berkeley', 'UC Berkeley', 'University of California, Berkeley']"
2019,Weak Detection of Signal in the Spiked Wigner Model,"Hye Won Chung, Ji Oon Lee",https://icml.cc/Conferences/2019/Schedule?showEvent=3665,"We consider the problem of detecting the presence of the signal in a rank-one signal-plus-noise data matrix. In case the signal-to-noise ratio is under the threshold below which a reliable detection is impossible, we propose a hypothesis test based on the linear spectral statistics of the data matrix. When the noise is Gaussian, the error of the proposed test is optimal as it matches the error of the likelihood ratio test that minimizes the sum of the Type-I and Type-II errors. The test is data-driven and does not depend on the distribution of the signal or the noise. If the density of the noise is known, it can be further improved by an entrywise transformation to lower the error of the test.
","['KAIST', 'KAIST']"
2019,Rademacher Complexity for Adversarially Robust Generalization,"Dong Yin, Kannan Ramchandran, Peter Bartlett",https://icml.cc/Conferences/2019/Schedule?showEvent=3688,"Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence; moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data.
In this paper, we focus on $\ell_\infty$ attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity.
For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded $\ell_1$ norm, and our results also extend to multi-class linear classifiers; in addition, for (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists.
We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting.
Our results indicate that having $\ell_1$ norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting.
We demonstrate experimental results that validate our theoretical findings.","['UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2019,Provably efficient RL with Rich Observations via Latent State Decoding,"Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, John Langford",https://icml.cc/Conferences/2019/Schedule?showEvent=3757,"We study the exploration problem in episodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumptions, we demonstrate how to estimate a mapping from the observations to latent states inductively through a sequence of regression and clustering steps---where previously decoded latent states provide labels for later regression problems---and use it to construct good exploration policies. We provide finite-sample guarantees on the quality of the learned state decoding function and exploration policies, and complement our theory with an empirical evaluation on a class of hard exploration problems. Our method exponentially improves over $Q$-learning with na\""ive exploration, even when $Q$-learning has cheating access to latent states.","['Carnegie Mellon University', 'Microsoft Research', 'University of Illinois at Urbana-Champaign', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2019,Information-Theoretic Considerations in Batch Reinforcement Learning,"Jinglin Chen, Nan Jiang",https://icml.cc/Conferences/2019/Schedule?showEvent=4162,"Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.
","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']"
2019,A Contrastive Divergence for Combining Variational Inference and MCMC,"Francisco Ruiz, Michalis Titsias",https://icml.cc/Conferences/2019/Schedule?showEvent=3854,"We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).
","['University of Cambridge / Columbia University', 'DeepMind']"
2019,Calibrated Approximate Bayesian Inference,"Hanwen Xing, Geoff Nicholls, Jeong Lee",https://icml.cc/Conferences/2019/Schedule?showEvent=4074,"We give a general purpose computational framework for estimating the bias in coverage resulting from making approximations in Bayesian inference. Coverage is the probability credible sets cover true parameter values. We show how to estimate the actual coverage an approximation scheme achieves when the ideal observation model and the prior can be simulated, but have been replaced, in the Monte Carlo, with approximations as they are intractable.  Coverage estimation procedures given in Lee et al. (2018) work well on simple problems, but are biased, and do not scale well, as those authors note.   For example, the methods of Lee et al. (2018) fail for calibration of an approximate completely collapsed MCMC algorithm for partition structure in a Dirichlet process for clustering group labels in a hierarchical model. By exploiting the symmetry of the coverage error under permutation of low level group labels and smoothing with Bayesian Additive Regression Trees, we are able to show that the original approximate inference had poor coverage and should not be trusted. 
","['University of Oxford', 'University of Oxford', 'University of Auckland']"
2019,Moment-Based Variational Inference for Markov Jump Processes,"Christian Wildner, Heinz Koeppl",https://icml.cc/Conferences/2019/Schedule?showEvent=4135,"We propose moment-based variational inference as a flexible framework for approximate smoothing of latent Markov jump processes. The main ingredient of our approach is to partition the set of all transitions of the latent process into classes. This allows to express the Kullback-Leibler divergence from the approximate to the posterior process in terms of a set of moment functions that arise naturally from the chosen partition. To illustrate possible choices of the partition, we consider special classes of jump processes that frequently occur in applications. We then extend the results to latent parameter inference and demonstrate the method on several examples.
","['TU Darmstadt', 'TU Darmstadt']"
2019,Understanding MCMC Dynamics as Flows on the Wasserstein Space,"Chang Liu, Jingwei Zhuo, Jun Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3822,"It is known that the Langevin dynamics used in MCMC is the gradient flow of the KL divergence on the Wasserstein space, which helps convergence analysis and inspires recent particle-based variational inference methods (ParVIs). But no more MCMC dynamics is understood in this way. In this work, by developing novel concepts, we propose a theoretical framework that recognizes a general MCMC dynamics as the fiber-gradient Hamiltonian flow on the Wasserstein space of a fiber-Riemannian Poisson manifold. The ""conservation + convergence"" structure of the flow gives a clear picture on the behavior of general MCMC dynamics. The framework also enables ParVI simulation of MCMC dynamics, which enriches the ParVI family with more efficient dynamics, and also adapts ParVI advantages to MCMCs. We develop two ParVI methods for a particular MCMC dynamics and demonstrate the benefits in experiments.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2019,LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data Approximations,"Brian Trippe, Jonathan Huggins, Raj Agrawal, Tamara Broderick",https://icml.cc/Conferences/2019/Schedule?showEvent=3862,"Due to the ease of modern data collection, applied statisticians often have access to a large set of covariates that they wish to relate to some observed outcome. 
 Generalized linear models (GLMs) offer a particularly interpretable framework for such an analysis.  In these high-dimensional problems, the number of covariates is often large relative to the number of observations, so we face non-trivial inferential uncertainty;  a Bayesian approach allows coherent quantification of this uncertainty. Unfortunately, existing methods for Bayesian inference in GLMs require running times roughly cubic in parameter dimension,  and so are limited to settings with at most tens of thousand parameters.  We propose to reduce time and memory costs with a low-rank approximation of the data in an approach we call LR-GLM. When used with the Laplace approximation or Markov chain Monte Carlo, LR-GLM provides a full Bayesian posterior approximation  and admits running times reduced by a full factor of the parameter dimension.  We rigorously establish the quality of our approximation and show how the choice of rank allows a tunable  computational--statistical trade-off.  Experiments support our theory and demonstrate the efficacy of LR-GLM on real large-scale datasets.
","['MIT', 'Harvard', 'MIT', 'MIT']"
2019,Amortized Monte Carlo Integration,"Adam Golinski, Frank Wood, Tom Rainforth",https://icml.cc/Conferences/2019/Schedule?showEvent=3869,"Current approaches to amortizing Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions—a computational pipeline which is inefficient when the target function(s) are known upfront. In this paper, we address this inefficiency by introducing AMCI, a method for amortizing Monte Carlo integration directly. AMCI operates similarly to amortized inference but produces three distinct amortized proposals, each tailored to a different component of the overall expectation calculation. At runtime, samples are produced separately from each amortized proposal, before being combined to an overall estimate of the expectation. We show that while existing approaches are fundamentally limited in the level of accuracy they can achieve, AMCI can theoretically produce arbitrarily small errors for any integrable target function using only a single sample from each proposal at runtime. We further show that it is able to empirically outperform the theoretically optimal selfnormalized importance sampler on a number of example problems. Furthermore, AMCI allows not only for amortizing over datasets but also amortizing over target functions.
","['University of Oxford', 'University of British Columbia', 'University of Oxford']"
2019,Stein Point Markov Chain Monte Carlo,"Wilson Ye Chen, Alessandro Barp, Francois-Xavier Briol, Jackson Gorham, Mark Girolami, Lester Mackey, Chris Oates",https://icml.cc/Conferences/2019/Schedule?showEvent=3730,"An important task in machine learning and statistics is the approximation of a probability measure by an empirical measure supported on a discrete point set. Stein Points are a class of algorithms for this task, which proceed by sequentially minimising a Stein discrepancy between the empirical measure and the target and, hence, require the solution of a non-convex optimisation problem to obtain each new point. This paper removes the need to solve this optimisation problem by, instead, selecting each new point based on a Markov chain sample path. This significantly reduces the computational cost of Stein Points and leads to a suite of algorithms that are straightforward to implement. The new algorithms are illustrated on a set of challenging Bayesian inference problems, and rigorous theoretical guarantees of consistency are established.
","['The Institute of Statistical Mathematics', 'Imperial College London', 'University of Cambridge', 'OPENDOOR', 'Imperial College London', 'Microsoft Research', 'Newcastle University']"
2019,Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations,"Wu Lin, Mohammad Emtiyaz Khan, Mark Schmidt",https://icml.cc/Conferences/2019/Schedule?showEvent=3799,"Natural-gradient methods enable fast and simple algorithms for variational inference, but due to computational difficulties, their use is mostly limited to minimal exponential-family (EF) approximations.
In this paper, we extend their application to estimate structured approximations such as mixtures of EF distributions.
Such approximations can fit complex, multimodal posterior distributions and are generally more accurate than unimodal EF approximations.
By using a minimal conditional-EF representation of such approximations, we derive simple natural-gradient updates. 
Our empirical results demonstrate a faster convergence of our natural-gradient method compared to black-box gradient-based methods. Our work expands the scope of natural gradients for Bayesian inference and makes them more widely applicable than before.
","['University of British Columbia', 'RIKEN', 'University of British Columbia']"
2019,Particle Flow Bayes' Rule,"Xinshi Chen, Hanjun Dai, Le Song",https://icml.cc/Conferences/2019/Schedule?showEvent=4188,"We present a particle flow realization of Bayes' rule, where an ODE-based neural operator is used to transport particles from a prior to its posterior after a new observation. We prove that such an ODE operator exists. Its neural parameterization can be trained in a meta-learning framework, allowing this operator to reason about the effect of an individual observation on the posterior, and thus generalize across different priors, observations and to sequential Bayesian inference. We demonstrated the generalization ability of our particle flow Bayes operator in several canonical and high dimensional examples.
","['Georgia Institution of Technology', 'Georgia Tech', 'Georgia Institute of Technology']"
2019,Correlated Variational Auto-Encoders,"Da Tang, Dawen Liang, Tony Jebara, Nicholas Ruozzi",https://icml.cc/Conferences/2019/Schedule?showEvent=3953,"Variational Auto-Encoders (VAEs) are capable of learning latent representations for high dimensional data. However, due to the i.i.d. assumption, VAEs only optimize the singleton variational distributions and fail to account for the correlations between data points, which might be crucial for learning latent representations from dataset where a priori we know correlations exist. We propose Correlated Variational Auto-Encoders (CVAEs) that can take the correlation structure into consideration when learning latent representations with VAEs. CVAEs apply a prior based on the correlation structure. To address the intractability introduced by the correlated prior, we develop an approximation by average of a set of tractable lower bounds over all maximal acyclic subgraphs of the undirected correlation graph. Experimental results on matching and link prediction on public benchmark rating datasets and spectral clustering on a synthetic dataset show the effectiveness of the proposed method over baseline algorithms.
","['Columbia University', 'Netflix', 'Columbia and Netflix', 'UT Dallas']"
2019,Towards a Unified Analysis of Random Fourier Features,"Zhu Li, Jean-Francois Ton, Dino Oglic, Dino Sejdinovic",https://icml.cc/Conferences/2019/Schedule?showEvent=4235,"Random Fourier features is a widely used, simple, and effective technique for scaling up kernel methods. The existing theoretical analysis of the approach, however, remains focused on specific learning tasks and typically gives pessimistic bounds which are at odds with the empirical results. We tackle these problems and provide the first unified risk analysis of learning with random Fourier features using the squared error and Lipschitz continuous loss functions. In our bounds, the trade-off between the computational cost and the expected risk convergence rate is problem specific and expressed in terms of the regularization parameter and the \emph{number of effective degrees of freedom}. We study both the standard random Fourier features method for which we improve the existing bounds on the number of features required to guarantee the corresponding minimax risk convergence rate of kernel ridge regression, as well as a data-dependent modification which samples features proportional to \emph{ridge leverage scores} and further reduces the required number of features. As ridge leverage scores are expensive to compute, we devise a simple approximation scheme which provably reduces the computational cost without loss of statistical efficiency.
","['University of Oxford', 'University of Oxford', ""King's College London"", 'University of Oxford']"
2019,Learning deep kernels for exponential family densities,"Li Kevin Wenliang, D.J. Sutherland, Heiko Strathmann, Arthur Gretton",https://icml.cc/Conferences/2019/Schedule?showEvent=4255,"The kernel exponential family is a rich class of distributions, which can be fit efficiently and with statistical guarantees by score matching. Being required to choose a priori a simple kernel such as the Gaussian, however, limits its practical applicability. We provide a scheme for learning a kernel parameterized by a deep network, which can find complex location-dependent local features of the data geometry. This gives a very rich class of density models, capable of fitting complex structures on moderate-dimensional problems. Compared to deep density models fit via maximum likelihood, our approach provides a complementary set of strengths and tradeoffs: in empirical studies, the former can yield higher likelihoods, whereas the latter gives better estimates of the gradient of the log density, the score, which describes the distribution's shape.
","['Gatsby Unit, University College London', 'Gatsby unit, University College London', 'University College London', 'Gatsby Computational Neuroscience Unit']"
2019,Bayesian Deconditional Kernel Mean Embeddings,"Kelvin Hsu, Fabio Ramos",https://icml.cc/Conferences/2019/Schedule?showEvent=3983,"Conditional kernel mean embeddings form an attractive nonparametric framework for representing conditional means of functions, describing the observation processes for many complex models. However, the recovery of the original underlying function of interest whose conditional mean was observed is a challenging inference task. We formalize deconditional kernel mean embeddings as a solution to this inverse problem, and show that it can be naturally viewed and used as a nonparametric Bayes' rule. Critically, we introduce the notion of task transformed Gaussian processes and establish deconditional kernel means embeddings as their posterior predictive mean. This connection provides Bayesian interpretations and uncertainty estimates for deconditional kernel means, explains their regularization hyperparameters, and provides a marginal likelihood for kernel hyperparameter learning. They further enable practical applications such as learning sparse representations for big data and likelihood-free inference.
","['University of Sydney, CSIRO', 'NVIDIA, University of Sydney']"
2019,A Kernel Perspective for Regularizing Deep Neural Networks,"Alberto Bietti, Gregoire Mialon, Dexiong Chen, Julien Mairal",https://icml.cc/Conferences/2019/Schedule?showEvent=3832,"We propose a new point of view for regularizing deep neural networks by using the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm cannot be computed, it admits upper and lower approximations leading to various practical strategies. Specifically, this perspective (i) provides a common umbrella for many existing regularization principles, including spectral norm and gradient penalties, or adversarial training, (ii) leads to new effective regularization penalties, and (iii) suggests hybrid strategies combining lower and upper bounds to get better approximations of the RKHS norm. We experimentally show this approach to be effective when learning on small datasets, or to obtain adversarially robust models.
","['Inria', 'Inria', 'Inria', 'Inria']"
2019,A Persistent Weisfeiler--Lehman Procedure for Graph Classification,"Bastian Rieck, Christian Bock, Karsten Borgwardt",https://icml.cc/Conferences/2019/Schedule?showEvent=3725,"The Weisfeiler--Lehman graph kernel exhibits competitive performance
in many graph classification tasks. However, its subtree features are not able to capture connected components and cycles, topological features known for characterising graphs. To extract such features, we leverage propagated node label
information and transform unweighted graphs into metric ones. This permits us to augment the subtree features with topological information obtained using persistent homology, a concept from topological data analysis. Our method, which we formalise as a generalisation of Weisfeiler--Lehman subtree features, exhibits favourable classification accuracy and its improvements in predictive performance are mainly driven by including cycle information.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2019,Rehashing Kernel Evaluation in High Dimensions,"Paris Siminelakis, Kexin Rong, Peter Bailis, Moses Charikar, Philip Levis",https://icml.cc/Conferences/2019/Schedule?showEvent=4232,"Kernel methods are effective but do not scale well to large scale data, especially in high dimensions where the geometric data structures used to accelerate kernel evaluation suffer from the curse of dimensionality. 
Recent theoretical advances have  proposed fast kernel evaluation algorithms leveraging hashing techniques with worst-case asymptotic improvements. However, these advances are largely confined to the theoretical realm due to concerns such as super-linear preprocessing time and diminishing gains in non-worst case datasets. In this paper, we close the gap between theory and practice by addressing these challenges via provable and practical procedures for adaptive sample size selection, preprocessing time reduction, and refined variance bounds that quantify the data-dependent performance of random sampling and hashing-based kernel evaluation methods. Our experiments show that these new tools offer up to $10\times$ improvement in evaluation time on a range of synthetic and real-world datasets.","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
2019,Large-Scale Sparse Kernel Canonical Correlation Analysis,"Viivi Uurtio, Sahely Bhadra, Juho Rousu",https://icml.cc/Conferences/2019/Schedule?showEvent=3793,"This paper presents gradKCCA, a large-scale sparse non-linear canonical correlation method. Like Kernel Canonical Correlation Analysis (KCCA), our method finds non-linear relations through kernel functions, but it does not rely on a kernel matrix, a known bottleneck for scaling up kernel methods. gradKCCA corresponds to solving KCCA with the additional constraint that the canonical projection directions in the kernel-induced feature space have preimages in the original data space. Firstly, this modification allows us to very efficiently maximize kernel canonical correlation through an alternating projected gradient algorithm working in the original data space. Secondly, we can control the sparsity of the projection directions by constraining the $\ell_1$ norm of the preimages of the projection directions, facilitating the interpretation of the discovered patterns, which is not available through KCCA. Our empirical experiments demonstrate that gradKCCA outperforms state-of-the-art CCA methods in terms of speed and robustness to noise both in simulated and real-world datasets.","['Aalto University', 'Indian Institute of Technology Palakkad', 'Aalto University']"
2019,A Kernel Theory of Modern Data Augmentation,"Tri Dao, Albert Gu, Alexander J Ratner, Virginia Smith, Christopher De Sa, Christopher Re",https://icml.cc/Conferences/2019/Schedule?showEvent=3611,"Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be approximated by first-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.
","['Stanford', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Cornell', 'Stanford']"
2019,kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable Selection,"Lotfi Slim, Clément Chatelain, Chloe-Agathe Azencott, Jean-Philippe Vert",https://icml.cc/Conferences/2019/Schedule?showEvent=4164,"Model selection is an essential task for many applications in scientific discovery. The most common approaches rely on univariate linear measures of association between each feature and the outcome. Such classical selection procedures fail to take into account nonlinear effects and interactions between features. Kernel-based selection procedures have been proposed as a solution. However, current strategies for kernel selection fail to measure the significance of a joint model constructed through the combination of the basis kernels. In the present work, we exploit recent advances in post-selection inference to propose a valid statistical test for the association of a joint model of the selected kernels with the outcome. The kernels are selected via a step-wise procedure which we model as a succession of quadratic constraints in the outcome variable. 
","['Mines ParisTech (ARMINES)', 'Sanofi R&D', 'MINES ParisTech', 'Google']"
2019,Scalable Learning in Reproducing Kernel Krein Spaces,"Dino Oglic, Thomas Gaertner",https://icml.cc/Conferences/2019/Schedule?showEvent=3763,"We provide the first mathematically complete derivation of the Nyström method for low-rank approximation of indefinite kernels and propose an efficient method for finding an approximate eigendecomposition of such kernel matrices. Building on this result, we devise highly scalable methods for learning in reproducing kernel Krein spaces. The devised approaches provide a principled and theoretically well-founded means to tackle large scale learning problems with indefinite kernels. The main motivation for our work comes from problems with structured representations (e.g., graphs, strings, time-series), where it is relatively easy to devise a pairwise (dis)similarity function based on intuition and/or knowledge of domain experts. Such functions are typically not positive definite and it is often well beyond the expertise of practitioners to verify this condition. The effectiveness of the devised approaches is evaluated empirically using indefinite kernels defined on structured and vectorial data representations.
","[""King's College London"", 'The University of Nottingham']"
2019,Dirichlet Simplex Nest and Geometric Inference,"Mikhail Yurochkin, Aritra Guha, Yuekai Sun, XuanLong Nguyen",https://icml.cc/Conferences/2019/Schedule?showEvent=3943,"We propose Dirichlet Simplex Nest, a class of probabilistic models suitable for a variety of data types, and develop fast and provably accurate inference algorithms by accounting for the model's convex geometry and low dimensional simplicial structure. By exploiting the connection to Voronoi tessellation and properties of Dirichlet distribution, the proposed inference algorithm is shown to achieve consistency and strong error bound guarantees on a range of model settings and data distributions. The effectiveness of our model and the learning algorithm is demonstrated by simulations and by analyses of text and financial data.
","['IBM Research, MIT-IBM Watson AI Lab', 'U Michigan', 'University of Michigan', 'University of Michigan']"
2019,Bayesian leave-one-out cross-validation for large data,"Måns Magnusson, Michael Andersen, Johan Jonasson, Aki Vehtari",https://icml.cc/Conferences/2019/Schedule?showEvent=3825,"Model inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.
","['Aalto University', 'Aalto University', 'Chalmers University of Technology', 'Aalto University']"
2019,Rao-Blackwellized Stochastic Gradients for Discrete Distributions,"Runjing Liu, Jeffrey Regier, Nilesh Tripuraneni, Michael Jordan, Jon McAuliffe",https://icml.cc/Conferences/2019/Schedule?showEvent=4277,"We wish to compute the gradient of an expectation
over a finite or countably infinite sample
space having K ≤ ∞ categories. When K is indeed
infinite, or finite but very large, the relevant
summation is intractable. Accordingly, various
stochastic gradient estimators have been proposed.
In this paper, we describe a technique that can be
applied to reduce the variance of any such estimator,
without changing its bias—in particular,
unbiasedness is retained. We show that our technique
is an instance of Rao-Blackwellization, and 
we demonstrate the improvement it yields on a 
semi-supervised classification problem and a pixel attention task.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'Voleon Group and University of California at Berkeley']"
2019,Neurally-Guided Structure Inference,"Sidi Lu, Jiayuan Mao, Josh Tenenbaum, Jiajun Wu",https://icml.cc/Conferences/2019/Schedule?showEvent=4250,"Most structure inference methods either rely on exhaustive search or are purely data-driven. Exhaustive search robustly infers the structure of arbitrarily complex data, but it is slow. Data-driven methods allow efficient inference, but do not generalize when test data have more complex structures than training data. In this paper, we propose a hybrid inference algorithm, the Neurally-Guided Structure Inference (NG-SI), keeping the advantages of both search-based and data-driven methods. The key idea of NG-SI is to use a neural network to guide the hierarchical, layer-wise search over the compositional space of structures. We evaluate our algorithm on two representative structure inference tasks: probabilistic matrix decomposition and symbolic program parsing. It outperforms data-driven and search-based alternatives on both tasks.
","['Shanghai Jiao Tong University', 'Tsinghua University and MIT CSAIL', 'MIT', 'MIT']"
2019,Bayesian Joint Spike-and-Slab Graphical Lasso,"Zehang Li, Tyler Mccormick, Samuel Clark",https://icml.cc/Conferences/2019/Schedule?showEvent=3635,"In this article, we propose a new class of priors for Bayesian inference with multiple Gaussian graphical models. We introduce Bayesian treatments of two popular procedures, the group graphical lasso and the fused graphical lasso, and extend them to a continuous spike-and-slab framework to allow self-adaptive shrinkage and model selection simultaneously.  We develop an EM algorithm that performs fast and dynamic explorations of posterior modes. Our approach selects sparse models efficiently and automatically with substantially smaller bias than would be induced by alternative regularization procedures. The performance of the proposed methods are demonstrated through simulation and two real data examples.
","['Yale School of Public Health', 'University of Washington', 'The Ohio State University']"
2019,Rotation Invariant Householder Parameterization for Bayesian PCA,"Rajbir-Singh Nirwan, Nils Bertschinger",https://icml.cc/Conferences/2019/Schedule?showEvent=3805,"We consider probabilistic PCA and related factor models from a Bayesian perspective. These models are in general not identifiable as the likelihood has a rotational symmetry. This gives rise to complicated posterior distributions with continuous subspaces of equal density and thus hinders efficiency of inference as well as interpretation of obtained parameters. In particular, posterior averages over factor loadings become meaningless and only model predictions are unambiguous. Here, we propose a parameterization based on Householder transformations, which remove the rotational symmetry of the posterior. Furthermore, by relying on results from random matrix theory, we establish the parameter distribution which leaves the model unchanged compared to the original rotationally symmetric formulation. In particular, we avoid the need to compute the Jacobian determinant of the parameter transformation. This allows us to efficiently implement probabilistic PCA in a rotation invariant fashion in any state of the art toolbox. Here, we implemented our model in the probabilistic programming language Stan and illustrate it on several examples.
","['Frankfurt Institute for Advanced Studies', 'Frankfurt Institute for Advanced Studies ']"
2019,A Framework for Bayesian Optimization in Embedded Subspaces,"Amin Nayebi, Alexander Munteanu, Matthias Poloczek",https://icml.cc/Conferences/2019/Schedule?showEvent=4216,"We present a theoretically founded approach for high-dimensional Bayesian optimization based on low-dimensional subspace embeddings.
We prove that the error in the Gaussian process model is bounded tightly when going from the original high-dimensional search domain to the low-dimensional embedding.  This implies that the optimization process in the low-dimensional embedding proceeds essentially as if it were run directly on an unknown active subspace of low dimensionality.  The argument applies to a large class of algorithms and GP models, including non-stationary kernels. Moreover, we provide an efficient implementation based on hashing and demonstrate empirically that this subspace embedding achieves considerably better results than the previously proposed methods for high-dimensional BO based on Gaussian matrix projections and structure-learning.
","['University of Arizona', 'TU Dortmund', 'Uber AI Labs & The University of Arizona']"
2019,Convolutional Poisson Gamma Belief Network,"CHAOJIE WANG, Bo Chen, SUCHENG XIAO, Mingyuan Zhou",https://icml.cc/Conferences/2019/Schedule?showEvent=3613,"For text analysis, one often resorts to a lossy representation that either completely ignores word order or embeds each word as a low-dimensional dense feature vector. In this paper, we propose convolutional Poisson factor analysis (CPFA) that directly operates on a lossless representation that processes the words in each document as a sequence of high-dimensional one-hot vectors. To boost its performance, we further propose the convolutional Poisson gamma belief network (CPGBN) that couples CPFA with the gamma belief network via a novel probabilistic pooling layer. CPFA forms words into phrases and captures very specific phrase-level topics, and CPGBN further builds a hierarchy of increasingly more general phrase-level topics. For efficient inference, we develop both a Gibbs sampler and a Weibull distribution based convolutional variational auto-encoder. Experimental results demonstrate that CPGBN can extract high-quality text latent representations that capture the word order information, and hence can be leveraged as a building block to enrich a wide variety of existing latent variable models that ignore word order.
","['XIDIAN UNIVERSITY', 'School of Electronic Engineering, Xidian University', 'XIDIAN UNIVERSITY', 'University of Texas at Austin']"
2019,Automatic Posterior Transformation for Likelihood-Free Inference,"David Greenberg, Marcel Nonnenmacher, Jakob Macke",https://icml.cc/Conferences/2019/Schedule?showEvent=4268,"How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.
","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich']"
2019,Active Learning for Decision-Making from Imbalanced Observational Data,"Iiris Sundin, Peter Schulam, Eero Siivola, Aki Vehtari, Suchi Saria, Samuel Kaski",https://icml.cc/Conferences/2019/Schedule?showEvent=4171,"Machine learning can help personalized decision support by learning models to predict individual treatment effects (ITE). This work studies the reliability of prediction-based decision-making in a task of deciding which action $a$ to take for a target unit after observing its covariates $\tilde{x}$ and predicted outcomes $\hat{p}(\tilde{y} \mid \tilde{x}, a)$. An example case is personalized medicine and the decision of which treatment to give to a patient. A common problem when learning these models from observational data is imbalance, that is, difference in treated/control covariate distributions, which is known to increase the upper bound of the expected ITE estimation error. We propose to assess the decision-making reliability by estimating the ITE model's Type S error rate, which is the probability of the model inferring the sign of the treatment effect wrong. Furthermore, we use the estimated reliability as a criterion for active learning, in order to collect new (possibly expensive) observations, instead of making a forced choice based on unreliable predictions. We demonstrate the effectiveness of this decision-making aware active learning in two decision-making tasks: in simulated data with binary outcomes and in a medical dataset with synthetic and continuous treatment outcomes.","['Aalto University', 'Johns Hopkins University', 'Aalto University', 'Aalto University', 'Johns Hopkins University', 'Aalto University']"
2019,Validating Causal Inference Models via Influence Functions,"Ahmed Alaa, Mihaela van der Schaar",https://icml.cc/Conferences/2019/Schedule?showEvent=3882,"The problem of estimating causal effects of treatments from observational data falls beyond the realm of supervised learning — because counterfactual data is inaccessible, we can never observe the true causal effects. In the absence of ""supervision"", how can we evaluate the performance of causal inference methods? In this paper, we use influence functions — the functional derivatives of a loss function — to develop a model validation procedure that estimates the estimation error of causal inference methods. Our procedure utilizes a Taylor-like expansion to approximate the loss function of a method on a given dataset in terms of the influence functions of its loss on a ""synthesized"", proximal dataset with known causal effects. Under minimal regularity assumptions, we show that our procedure is consistent and efficient. Experiments on 77 benchmark datasets show that using our procedure, we can accurately predict the comparative performances of state-of-the-art causal inference methods applied to a given observational study.
","['UCLA', 'UCLA']"
2019,"Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks","Charith Mendis, Alex Renda, Dr.Saman Amarasinghe, Michael Carbin",https://icml.cc/Conferences/2019/Schedule?showEvent=4057,"Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers. Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation. In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions. Ithemal uses a hierarchical LSTM--based approach to predict throughput based on the opcodes and operands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers. In particular, our model has less than half the error of state-of-the-art analytical models (LLVM's llvm-mca and Intel's IACA). Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.
","['MIT', 'MIT', 'Massachusetts institute of technology', 'MIT']"
2019,Learning to Groove with Inverse Sequence Transformations,"Jon Gillick, Adam Roberts, Jesse Engel, Douglas Eck, David Bamman",https://icml.cc/Conferences/2019/Schedule?showEvent=3931,"We explore models for translating abstract musical ideas (scores, rhythms) into expressive performances using seq2seq and recurrent variational information bottleneck (VIB) models. Though seq2seq models usually require painstakingly aligned corpora, we show that it is possible to adapt an approach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix, Vid2Vid) to sequences, creating large volumes of paired data by performing simple transformations and training generative models to plausibly invert these transformations. Music, and drumming in particular, provides a strong test case for this approach because many common transformations (quantization, removing voices) have clear semantics, and learning to invert them has real-world applications.  Focusing on the case of drum set players, we create and release a new dataset for this purpose, containing over 13 hours of recordings by professional drummers aligned with fine-grained timing and dynamics information.  We also explore some of the creative potential of these models, demonstrating improvements on state-of-the-art methods for Humanization (instantiating a performance from a musical score).
","['UC Berkeley', 'Google Brain', 'Google Brain', 'Google Brain', 'UC Berkeley']"
2019,Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game AI,"Lei Han, Peng Sun, Yali Du, Jiechao Xiong, Qing Wang, Xinghai Sun, Han Liu, Tong Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3838,"We consider the problem of multi-agent reinforcement learning (MARL) in video game AI, where the agents are located in a spatial grid-world environment and the number of agents varies both within and across episodes. The challenge is to flexibly control an arbitrary number of agents while achieving effective collaboration. Existing MARL methods usually suffer from the trade-off between these two considerations. To address the issue, we propose a novel architecture that learns a spatial joint representation of all the agents and outputs grid-wise actions. Each agent will be controlled independently by taking the action from the grid it occupies. By viewing the state information as a grid feature map, we employ a convolutional encoder-decoder as the policy network. This architecture naturally promotes agent communication because of the large receptive field provided by the stacked convolutional layers. Moreover, the spatially shared convolutional parameters enable fast parallel exploration that the experiences discovered by one agent can be immediately transferred to others. The proposed method can be conveniently integrated with general reinforcement learning algorithms, e.g., PPO and Q-learning. We demonstrate the effectiveness of the proposed method in extensive challenging multi-agent tasks in StarCraft II.
","['Tencent AI Lab', 'Tencent AI Lab', 'University of Technology Sydney', 'Tencent AI Lab', 'Tencent AI Lab', 'Tencent AI Lab', 'Northwestern', 'Tecent AI Lab']"
2019,HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving,"Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy, Stewart Wilcox",https://icml.cc/Conferences/2019/Schedule?showEvent=3906,"We present an environment, benchmark, and deep learning driven automated theorem prover for higher-order logic. Higher-order interactive theorem provers enable the formalization of arbitrary mathematical theories and thereby present an interesting challenge for deep learning. We provide an open-source framework based on the HOL Light theorem prover that can be used as a reinforcement learning environment. HOL Light comes with a broad coverage of basic mathematical theorems on calculus and the formal proof of the Kepler conjecture, from which we derive a challenging benchmark for automated reasoning approaches. We also present a deep reinforcement learning driven automated theorem prover, DeepHOL, that gives strong initial results on this benchmark.
","['Google Research', 'Google', 'Google', 'Google', 'Googl']"
2019,Molecular Hypergraph Grammar with Its Application to Molecular Optimization,Hiroshi Kajino,https://icml.cc/Conferences/2019/Schedule?showEvent=3670,"Molecular optimization aims to discover novel molecules with desirable properties, and its two fundamental challenges are: (i) it is not trivial to generate valid molecules in a controllable way due to hard chemical constraints such as the valency conditions, and (ii) it is often costly to evaluate a property of a novel molecule, and therefore, the number of property evaluations is limited. These challenges are to some extent alleviated by a combination of a variational autoencoder (VAE) and Bayesian optimization (BO), where VAE converts a molecule into/from its latent continuous vector, and BO optimizes a latent continuous vector (and its corresponding molecule) within a limited number of property evaluations. While the most recent work, for the first time, achieved 100% validity, its architecture is rather complex due to auxiliary neural networks other than VAE, making it difficult to train. This paper presents a molecular hypergraph grammar variational autoencoder (MHG-VAE), which uses a single VAE to achieve 100% validity. Our idea is to develop a graph grammar encoding the hard chemical constraints, called molecular hypergraph grammar (MHG), which guides VAE to always generate valid molecules. We also present an algorithm to construct MHG from a set of molecules.
",['MIT-IBM Watson AI Lab / IBM Research']
2019,Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance,"Dasaem Jeong, Taegyun Kwon, Yoojin Kim, Juhan Nam",https://icml.cc/Conferences/2019/Schedule?showEvent=4004,"Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be played simultaneously by the polyphonic nature and each of them has its own duration. In this paper, we represent the unique form of musical score using graph neural network and apply it for rendering expressive piano performance from the music score. Specifically, we design the model using note-level gated graph neural network and measure-level hierarchical attention network with bidirectional long short-term memory with an iterative feedback method. In addition, to model different styles of performance for a given input score, we employ a variational auto-encoder. The result of the listening test shows that our proposed model generated more human-like performances compared to a baseline model and a hierarchical attention network model that handles music score as a word-like sequence.
","['KAIST', 'KAIST', 'KAIST', 'KAIST']"
2019,Learning to Prove Theorems via Interacting with Proof Assistants,"Kaiyu Yang, Jia Deng",https://icml.cc/Conferences/2019/Schedule?showEvent=3658,"Humans prove theorems by relying on substantial high-level reasoning and problem-specific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, representing theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the problem of using machine learning to automate the interaction with proof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written proofs from 123 projects developed with the Coq proof assistant. We develop ASTactic, a deep learning-based model that generates tactics as programs in the form of abstract syntax trees (ASTs). Experiments show that ASTactic trained on CoqGym can generate effective tactics and can be used to prove new theorems not previously provable by automated methods. Code is available at https://github.com/princeton-vl/CoqGym.
","['Princeton University', 'Princeton University']"
2019,Circuit-GNN: Graph Neural Networks for Distributed Circuit Design,"GUO ZHANG, Hao He, Dina Katabi",https://icml.cc/Conferences/2019/Schedule?showEvent=4281,"We present Circuit-GNN, a graph neural network (GNN) model for designing distributed circuits. Today, designing distributed circuits is a slow process that can take months from an expert engineer.  Our model both automates and speeds up the process. The model learns to simulate the electromagnetic (EM) properties of distributed circuits. Hence, it can be used to replace traditional EM simulators, which typically take tens of minutes for each design iteration. Further, by leveraging neural networks' differentiability, we can use our model to solve the inverse problem -- i.e., given desirable EM specifications, we propagate the gradient to optimize the circuit parameters and topology to satisfy the specifications. 
We exploit the flexibility of GNN to create one model that works for different circuit topologies. 
We compare our model with a commercial simulator showing that it reduces simulation time by four orders of magnitude.  We also demonstrate the value of our model by using it to design a Terahertz channelizer, a difficult task that requires a specialized expert.  The results show that our model produces a channelizer whose performance is as good as a manually optimized design, and can save the expert several weeks of topology and parameter optimization. Most interestingly, our model comes up with new designs that differ from the limited templates commonly used by engineers in the field, hence significantly expanding the design space. 
","['MIT', 'Massachusetts Institute of Technology', 'MIT']"
2019,Learning to Optimize Multigrid PDE Solvers,"Daniel Greenfeld, Meirav Galun, Ronen Basri, Irad Yavneh, Ron Kimmel",https://icml.cc/Conferences/2019/Schedule?showEvent=3877,"Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientific disciplines.  A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from discretized PDEs to prolongation operators for a broad class of 2D diffusion problems. We train a neural network once for the entire class of PDEs, using an efficient and unsupervised loss function. Our tests demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.
","['Weizmann Institute of Science', 'Weizmann Institute of Science', 'Weizmann Institute of Science', 'Technion', 'Technion']"
2019,A Block Coordinate Descent Proximal Method for Simultaneous Filtering and Parameter Estimation,"Ramin Raziperchikolaei, Harish Bhat",https://icml.cc/Conferences/2019/Schedule?showEvent=4187,"We propose and analyze a block coordinate descent proximal algorithm (BCD-prox) for simultaneous filtering and parameter estimation of ODE models. As we show on ODE systems with up to d=40 dimensions, as compared to state-of-the-art methods, BCD-prox exhibits increased robustness (to noise, parameter initialization, and hyperparameters), decreased training times, and improved accuracy of both filtered states and estimated parameters. We show how BCD-prox can be used with multistep numerical discretizations, and we establish convergence of BCD-prox under hypotheses that include real systems of interest.
","['UC MErced', 'University of California, Merced']"
2019,Learning Hawkes Processes Under Synchronization Noise,"William Trouleau, Jalal Etesami, Matthias Grossglauser, Negar Kiyavash, Patrick Thiran",https://icml.cc/Conferences/2019/Schedule?showEvent=3756,"Multivariate Hawkes processes (MHP) are widely used in a variety of fields to model the occurrence of discrete events. Prior work on learning MHPs has only focused on inference in the presence of perfect traces without noise. We address the problem of learning the causal structure of MHPs when observations are subject to an unknown delay. In particular, we introduce the so-called synchronization noise, where the stream of events generated by each dimension is subject to a random and unknown time shift. We characterize the robustness of the classic maximum likelihood estimator to synchronization noise, and we introduce a new approach for learning the causal structure in the presence of noise. Our experimental results show that our approach accurately recovers the causal structure of MHPs for a wide range of noise levels, and significantly outperforms classic estimation methods.
","['EPFL', 'Bosch Research Center for AI, Germany', 'EPFL', 'Georgia Institute of Technology', 'EPFL']"
2019,Generative Adversarial User Model for Reinforcement Learning Based Recommendation System,"Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, Le Song",https://icml.cc/Conferences/2019/Schedule?showEvent=4202,"There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.
","['Georgia Institution of Technology', 'Georgia Tech', 'Ant Financial', 'Ant Financial', 'Ant Financial Services Group', 'Georgia Institute of Technology']"
2019,A Statistical Investigation of Long Memory in Language and Music,"Alexander Greaves-Tunnell, Zaid Harchaoui",https://icml.cc/Conferences/2019/Schedule?showEvent=3937,"Representation and learning of long-range dependencies is a central challenge confronted in modern applications of machine learning to sequence data. Yet despite the prominence of this issue, the basic problem of measuring long-range dependence, either in a given data source or as represented in a trained deep model, remains largely limited to heuristic tools. We contribute a statistical framework for investigating long-range dependence in current applications of deep sequence modeling, drawing on the well-developed theory of long memory stochastic processes. This framework yields testable implications concerning the relationship between long memory in real-world data and its learned representation in a deep learning architecture, which are explored through a semiparametric framework adapted to the high-dimensional setting. 
","['University of Washington', 'University of Washington']"
2019,Deep Factors for Forecasting,"Yuyang Wang, Alex Smola, Danielle Robinson, Jan Gasthaus, Dean Foster, Tim Januschowski",https://icml.cc/Conferences/2019/Schedule?showEvent=3881,"Producing probabilistic forecasts for large collections of similar and/or dependent time series is a practically highly relevant, yet challenging task. Classical time series models fail to capture complex patterns in the data and multivariate techniques struggle to scale to large problem sizes, but their reliance on strong structural assumptions makes them data-efficient and allows them to provide estimates of uncertainty. The converse is true for models based on deep neural networks, which can learn complex patterns and dependencies given enough data. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component.  It also handles uncertainty through a local classical model. We provide both theoretical and empirical evidence for the soundness of our approach through a necessary and sufficient decomposition of exchangeable time series into a global and a local part and extensive experiments.  Our experiments demonstrate the advantages of our model both in term of data efficiency and computational complexity.
","['AWS AI Labs', 'Amazon', 'Amazon Web Services', 'Amazon Research', 'Amazon', 'Amazon Research']"
2019,Weakly-Supervised Temporal Localization via Occurrence Count Learning,"Julien Schroeter, Kirill Sidorov, David Marshall",https://icml.cc/Conferences/2019/Schedule?showEvent=3674,"We propose a novel model for temporal detection and localization which allows the training of deep neural networks using only counts of event occurrences as training labels. This powerful weakly-supervised framework alleviates the burden of the imprecise and time consuming process of annotating event locations in temporal data. Unlike existing methods, in which localization is explicitly achieved by design, our model learns localization implicitly as a byproduct of learning to count instances. This unique feature is a direct consequence of the model's theoretical properties. We validate the effectiveness of our approach in a number of experiments (drum hit and piano onset detection in audio, digit detection in images) and demonstrate performance comparable to that of fully-supervised state-of-the-art methods, despite much weaker training requirements.
","['Cardiff University', 'Cardiff University', 'Cardiff University']"
2019,Switching Linear Dynamics for Variational Bayes Filtering,"Philip Becker-Ehmck, Jan Peters, Patrick van der Smagt",https://icml.cc/Conferences/2019/Schedule?showEvent=3860,"System identification of complex and nonlinear systems is a central problem for model predictive control and model-based reinforcement learning. 
Despite their complexity, such systems can often be approximated well by a set of linear dynamical systems if broken into appropriate subsequences. 
This mechanism not only helps us find good approximations of dynamics, but also gives us deeper insight into the underlying system. 
Leveraging Bayesian inference, Variational Autoencoders and Concrete relaxations, we show how to learn a richer and more meaningful state space, e.g. encoding joint constraints and collisions with walls in a maze, from partial and high-dimensional observations.
This representation translates into a gain of accuracy of learned dynamics showcased on various simulated tasks.
","['Volkswagen Group', 'TU Darmstadt', 'Volkswagen Group']"
2019,Imputing Missing Events in Continuous-Time Event Streams,"Hongyuan Mei, Guanghui Qin, Jason Eisner",https://icml.cc/Conferences/2019/Schedule?showEvent=3675,"Events in the world may be caused by other, unobserved events. We consider sequences of events in continuous time. Given a probability model of complete sequences, we propose particle smoothing---a form of sequential importance sampling---to impute the missing events in an incomplete sequence. We develop a trainable family of proposal distributions based on a type of bidirectional continuous-time LSTM: Bidirectionality lets the proposals condition on future observations, not just on the past as in particle filtering. Our method can sample an ensemble of possible complete sequences (particles), from which we form a single consensus prediction that has low Bayes risk under our chosen loss metric. We experiment in multiple synthetic and real domains, using different missingness mechanisms, and modeling the complete sequences in each domain with a neural Hawkes process (Mei & Eisner 2017). On held-out incomplete sequences, our method is effective at inferring the ground-truth unobserved events, with particle smoothing consistently improving upon particle filtering.
","['Johns Hopkins University', 'Peking University', 'Johns Hopkins University + Microsoft Semantic Machines']"
2019,Understanding and Controlling Memory in Recurrent Neural Networks,"Doron Haviv, Alexander Rivkind, Omri Barak",https://icml.cc/Conferences/2019/Schedule?showEvent=3902,"To be effective in sequential data processing, Recurrent Neural Networks (RNNs) are required to keep track of past events by creating memories. While the relation  between memories and the network’s hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network’s hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a ’slow point’. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.
","['Technion', 'Weizmann Institute of Science', 'Technion - Israeli Institute of Technology']"
2019,Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces,"Philipp Becker, Harit Pandya, Gregor Gebhardt, Cheng  Zhao, C. James  Taylor, Gerhard Neumann",https://icml.cc/Conferences/2019/Schedule?showEvent=3721,"In order to integrate uncertainty estimates into deep time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with deep learning models, however, such approaches typically rely on approximate inference tech-
niques such as variational inference which makes learning more complex and often less scalable due to approximation errors.
We propose a new deep approach to Kalman filtering which can be learned directly in an end-to-end manner using backpropagation without additional approximations.
Our approach uses a high-dimensional factorized latent state representation for which the Kalman updates simplify to scalar operations and thus avoids hard to backpropagate, computationally heavy and potentially unstable matrix inversions.
Moreover, we use locally linear dynamic models to efficiently propagate the latent state to the next time step.
The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter & Schmidhuber, 1997) but uses an explicit representation of uncertainty.
As shown by our experiments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly improved prediction performance and outperforms various recent generative models on an image imputation task.
","['Bosch Center for Artificial Intelligence, University of Tübingen', 'University of Lincoln', 'TU Darmstadt', 'Birmingham University', 'Lancaster University', 'Lincoln University']"
2019,Subspace Robust Wasserstein Distances,"François-Pierre Paty, Marco Cuturi",https://icml.cc/Conferences/2019/Schedule?showEvent=4320,"Making sense of Wasserstein distances between discrete measures in high-dimensional settings remains a challenge. Recent work has advocated a two-step approach to improve robustness and facilitate the computation of optimal transport, using for instance projections on random real lines, or a preliminary quantization of the measures to reduce the size of their support. We propose in this work a max-min'' robust variant of the Wasserstein distance by considering the maximal possible distance that can be realized between two measures, assuming they can be projected orthogonally on a lower k-dimensional subspace. Alternatively, we show that the correspondingmin-max'' OT problem has a tight convex relaxation which can be cast as that of finding an optimal transport plan with a low transportation cost, where the cost is alternatively defined as the sum of the k largest eigenvalues of the second order moment matrix of the displacements (or matchings) corresponding to that plan (the usual OT definition only considers the trace of that matrix). We show that both quantities inherit several favorable properties from the OT geometry. We propose two algorithms to compute the latter formulation using entropic regularization, and illustrate the interest of this approach empirically.
","['ENSAE Paris', 'Google and CREST/ENSAE']"
2019,Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models,"Kaspar Märtens, Kieran Campbell, Christopher Yau",https://icml.cc/Conferences/2019/Schedule?showEvent=4154,"The interpretation of complex high-dimensional data typically requires the use of dimensionality reduction techniques to extract explanatory low-dimensional representations. However, in many real-world problems these representations may not be sufficient to aid interpretation on their own, and it would be desirable to interpret the model in terms of the original features themselves. Our goal is to characterise how feature-level variation depends on latent low-dimensional representations, external covariates, and non-linear interactions between the two. In this paper, we propose to achieve this through a structured kernel decomposition in a hybrid Gaussian Process model which we call the Covariate Gaussian Process Latent Variable Model (c-GPLVM). We demonstrate the utility of our model on simulated examples and applications in disease progression modelling from high-dimensional gene expression data in the presence of additional phenotypes. In each setting we show how the c-GPLVM can extract low-dimensional structures from high-dimensional data sets whilst allowing a breakdown of feature-level variability that is not present in other commonly used dimensionality reduction approaches.
","['University of Oxford', 'University of British Columbia', 'University of Birmingham']"
2019,Active Manifolds: A non-linear analogue to Active Subspaces,"Robert Bridges, Anthony Gruber, Christopher Felder, Miki Verma, Chelsey Hoff",https://icml.cc/Conferences/2019/Schedule?showEvent=4239,"We present an approach to analyze $C^1(\mathbb{R}^m)$ functions that addresses limitations present in the Active Subspaces (AS) method of Constantine et al. (2014; 2015). Under appropriate hypotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, which can be exploited for approximation or analysis, especially when $m$ is large (high-dimensional input space). We provide theorems justifying our AM technique and an algorithm permitting functional approximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examples, we show AM reduces approximation error by an order of magnitude compared to AS, at the expense of more computation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who apply AS to analyze a magnetohydrodynamic power generator model, and compare the performance of AM on the same data. Our analysis provides detailed information not captured by AS, exhibiting the influence of each parameter individually along an active manifold.  Overall, AM represents a novel technique for analyzing functional models with benefits including:  reducing $m$-dimensional analysis to a 1-D analogue, permitting more accurate regression than AS (at more computational expense), enabling more informative sensitivity analysis, and granting accessible visualizations (2-D plots) of parameter sensitivity along the AM.","['Oak Ridge National Laboratory', 'Texas Tech University', 'Washington University in St. Louis', 'Oak Ridge National Laboratory', 'N/A']"
2019,Optimal Minimal Margin Maximization with Boosting,"Alexander Mathiasen, Kasper Green Larsen, Allan Grønlund",https://icml.cc/Conferences/2019/Schedule?showEvent=3731,"Boosting algorithms iteratively produce linear combinations of more and more base hypotheses and it has been observed experimentally that the generalization error keeps improving even after achieving zero training error. One popular explanation attributes this to improvements in margins. A common goal in a long line of research, is to obtain large margins using as few base hypotheses as possible, culminating with the AdaBoostV algorithm by Rätsch and Warmuth [JMLR’05]. The AdaBoostV algorithm was later conjectured to yield an optimal trade-off between number of hypotheses trained and the minimal margin over all training points (Nie, Warmuth, Vishwanathan and Zhang [JMLR’13]). Our main contribution is a new algorithm refuting this conjecture. Furthermore, we prove a lower bound which implies that our new algorithm is optimal.
","['Aarhus University', 'Aarhus University, MADALGO', 'Aarhus University, MADALGO']"
2019,Generalized Linear Rule Models,"Dennis Wei, Sanjeeb Dash, Tian Gao, Oktay Gunluk",https://icml.cc/Conferences/2019/Schedule?showEvent=3933,"This paper considers generalized linear models using rule-based features, also referred to as rule ensembles, for regression and probabilistic classification. Rules facilitate model interpretation while also capturing nonlinear dependences and interactions. Our problem formulation accordingly trades off rule set complexity and prediction accuracy. Column generation is used to optimize over an exponentially large space of rules without pre-generating a large subset of candidates or greedily boosting rules one by one. The column generation subproblem is solved using either integer programming or a heuristic optimizing the same objective. In experiments involving logistic and linear regression, the proposed methods obtain better accuracy-complexity trade-offs than existing rule ensemble algorithms. At one end of the trade-off, the methods are competitive with less interpretable benchmark models.
","['IBM Research', 'IBM Research', 'IBM Research', 'IBM Research']"
2019,"Fast Incremental von Neumann Graph Entropy Computation: Theory, Algorithm, and Applications","Pin-Yu Chen, Lingfei Wu, Sijia Liu, Indika Rajapakse",https://icml.cc/Conferences/2019/Schedule?showEvent=3574,"The von Neumann graph entropy (VNGE) facilitates measurement of information divergence and distance between graphs in a graph sequence. It has been successfully applied to various learning tasks driven by network-based data. While effective, VNGE is computationally demanding as it requires the full eigenspectrum of the graph Laplacian matrix.  In this paper, we propose a new computational framework, Fast Incremental von Neumann Graph EntRopy (FINGER), which approaches VNGE with a performance guarantee. FINGER reduces the cubic complexity of VNGE to linear complexity in the number of nodes and edges, and thus enables online computation based on incremental graph changes. We also show asymptotic equivalence of FINGER to the exact VNGE, and derive its approximation error bounds. Based on FINGER, we propose efficient algorithms for computing Jensen-Shannon distance between graphs. Our experimental results on different random graph models demonstrate the computational efficiency and the asymptotic equivalence of FINGER. In addition, we apply FINGER to two real-world applications and one synthesized anomaly detection dataset, and corroborate its superior performance over seven baseline graph similarity methods.
","['IBM Research AI', 'IBM Research', 'MIT-IBM Watson AI Lab', '']"
2019,Variational Inference for sparse network reconstruction from count data,"Julien Chiquet, Stephane Robin, Mahendra Mariadassou",https://icml.cc/Conferences/2019/Schedule?showEvent=3682,"Networks provide a natural yet statistically grounded way to depict and understand how a set of entities interact. However, in many situations interactions are not directly observed and the network needs to be reconstructed based on observations collected for each entity. Our work focuses on the situation where these observations consist of counts. A typical example is the reconstruction of an ecological network based on abundance data. In this setting, the abundance of a set of species is collected in a series of samples and/or environments and we aim at inferring direct interactions between the species. The abundances at hand can be, for example, direct counts of individuals (ecology of macro-organisms) or read counts resulting from metagenomic sequencing (microbial ecology). 
Whatever the approach chosen to infer such a network, it has to account for the peculiaraties of the data at hand. The first, obvious one, is that the data are counts, i.e. non continuous. Also, the observed counts often vary over many orders of magnitude and are more dispersed than expected under a simple model, such as the Poisson distribution. The observed counts may also result from different sampling efforts in each sample and/or for each entity, which hampers direct comparison. Furthermore, because the network is supposed to reveal only direct interactions, it is highly desirable to account for covariates describing the environment to avoid spurious edges.
Many methods of network reconstruction from count data have been proposed. In the context of microbial ecology, most methods (SparCC, REBACCA, SPIEC-EASI, gCODA, BanOCC) rely on a two-step strategy: transform the counts to pseudo Gaussian observations using simple transforms before moving back to the setting of Gaussian Graphical Models, for which state of the art methods exist to infer the network, but only in a Gaussian world. In this work, we consider instead a full-fledged probabilistic model with a latent layer where the counts follow Poisson distributions, conditional to latent (hidden) Gaussian correlated variables. In this model, known as Poisson log-normal (PLN), the dependency structure is completely captured by the latent layer and we model counts, rather than transformations thereof. To our knowledge, the PLN framework is quite new and has only been used by two other recent methods (Mint and plnDAG) to reconstruct networks from count data. In this work, we use the same mathematical framework but adopt a different optimization strategy which alleviates the whole optimization process. We also fully exploit the connection between the PLN framework and generalized linear models to account for the peculiarities of microbiological data sets.
The network inference step is done as usual by adding sparsity inducing constraints on the inverse covariance matrix of the latent Gaussian vector to select only the most important interactions between species. Unlike the usual Gaussian setting, the penalized likelihood is generally not tractable in this framework. We resort instead to a variational approximation for parameter inference and solve the corresponding optimization problem by alternating a gradient descent on the variational parameters and a graphical-Lasso step on the covariance matrix. We also select the sparsity parameter using the resampling-based StARS procedure.
We show that the sparse PLN approach has better performance than existing methods on simulated datasets and that it extracts relevant signal from microbial ecology datasets. We also show that the inference scales to datasets made up of hundred of species and samples, in line with other methods in the field.
In short, our contributions to the field are the following: we extend the use of PLN distributions in network inference by (i) accounting for covariates and offset and thus removing some spurious edges induced by confounding factors, (ii) accounting for different sampling effort to  integrate data sets from different sources and thus infer interactions between different types of organisms (e.g. bacteria - fungi), (iii) developing an inference procedure based on the iterative optimization of a well defined objective function. Our objective function is a provable lower bound of the observed likelihood and our procedure accounts for the uncertainty associated with the estimation of the latent variable, unlike the algorithm presented in Mint and plnDAG.
","['INRA / AgroParisTech / Paris Saclay', 'INRA / AgroParisTech / Paris Saclay', 'INRA']"
2019,Simplifying Graph Convolutional Networks,"Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, Kilian Weinberger",https://icml.cc/Conferences/2019/Schedule?showEvent=3683,"Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. 
GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. 
In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. 
We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. 
Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications.
Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN. 
","['Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Shanghai Jiao Tong University', 'Cornell University']"
2019,Robust Influence Maximization for Hyperparametric Models,"Dimitrios Kalimeris, Gal Kaplun, Yaron Singer",https://icml.cc/Conferences/2019/Schedule?showEvent=3771,"In this paper we study the problem of robust influence
maximization in the independent cascade
model under a hyperparametric assumption. In
social networks users influence and are influenced
by individuals with similar characteristics and
as such they are associated with some features.
A recent surging research direction in influence
maximization focuses on the case where the edge
probabilities on the graph are not arbitrary but are
generated as a function of the features of the users
and a global hyperparameter. We propose a model
where the objective is to maximize the worst-case
number of influenced users for any possible value
of that hyperparameter. We provide theoretical
results showing that proper robust solution in our
model is NP-hard and an algorithm that achieves
improper robust optimization. We make-use of
sampling based techniques and of the renowned
multiplicative weight updates algorithm. Additionally
we validate our method empirically and
prove that it outperforms the state-of-the-art robust
influence maximization techniques.
","['Harvard University', 'Harvard', 'Harvard']"
2019,"HyperGAN: A Generative Model for Diverse, Performant Neural Networks","Neale Ratzlaff, Fuxin Li",https://icml.cc/Conferences/2019/Schedule?showEvent=3923,"We introduce HyperGAN, a generative model that learns to generate all the parameters of a deep neural network. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of generated samples with a classification loss. This is equivalent to minimizing the KL-divergence between the distribution of generated parameters, and the unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while also generating a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty estimates than standard ensembles. This is evidenced by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples.
","['Oregon State University', 'Oregon State University']"
2019,Rates of Convergence for Sparse Variational Gaussian Process Regression,"David Burt, Carl E Rasmussen, Mark van der Wilk",https://icml.cc/Conferences/2019/Schedule?showEvent=3737,"Excellent variational approximations to Gaussian process posteriors have been developed which avoid the $\mathcal{O}\left(N^3\right)$ scaling with dataset size $N$. They reduce the computational cost to $\mathcal{O}\left(NM^2\right)$, with $M\ll N$ the number of \emph{inducing variables}, which summarise the process. While the computational cost seems to be linear in $N$, the true complexity of the algorithm depends on how $M$ must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, $M=\mathcal{O}(\log^D N)$ suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase $M$ in continual learning scenarios.","['University of Cambridge', 'Cambridge University', 'PROWLER.io']"
2019,Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations,"Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem",https://icml.cc/Conferences/2019/Schedule?showEvent=4085,"The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms.
In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions.
We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data.
Then, we train more than $12000$ models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets.
We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision.
Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. 
Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.","['ETH Zurich - Max Planck Institute', 'MPI for Intelligent Systems', 'Google Brain', 'ETH Zurich', 'Google Brain', 'MPI for Intelligent Systems Tübingen, Germany', 'Google Brain']"
2019,Sum-of-Squares Polynomial Flow,"Priyank Jaini, Kira A. Selby, Yaoliang Yu",https://icml.cc/Conferences/2019/Schedule?showEvent=3638,"Triangular map is a recent construct in probability theory that allows one to transform any source probability density function to any target density function. 
Based on triangular maps, we propose a general framework for high-dimensional density estimation, by specifying one-dimensional transformations (equivalently conditional densities) and appropriate conditioner networks. This framework (a) reveals the commonalities and differences of existing autoregressive and flow based methods, (b) allows a unified understanding of the limitations and representation power of these recent approaches and, (c) motivates us to uncover a new Sum-of-Squares (SOS) flow that is interpretable, universal, and easy to train. We perform several synthetic experiments on various density geometries to demonstrate the benefits (and short-comings) of such transformations. SOS flows achieve competitive results in simulations and several real-world datasets. 
","['University of Waterloo, Vector Institute', 'University of Waterloo', 'University of Waterloo']"
2019,FloWaveNet : A Generative Flow for Raw Audio,"Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, Sungroh Yoon",https://icml.cc/Conferences/2019/Schedule?showEvent=4060,"Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in practical applications due to its ancestral sampling scheme. The recently suggested Parallel WaveNet and ClariNet has achieved real-time audio synthesis capability by incorporating inverse autoregressive flow (IAF) for parallel sampling. However, these approaches require a two-stage training pipeline with a well-trained teacher network and can only produce natural sound by using probability distillation along with heavily-engineered auxiliary loss terms. We propose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training procedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently parallel due to the characteristics of generative flow. The model can efficiently sample raw audio in real-time, with clarity comparable to previous two-stage parallel models. The code and samples for all models, including our FloWaveNet, are available on GitHub.
","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Kakao Corp.', 'Seoul National University']"
2019,Are Generative Classifiers More Robust to Adversarial Attacks?,"Yingzhen Li, John Bradshaw, Yash Sharma",https://icml.cc/Conferences/2019/Schedule?showEvent=3897,"There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers, which only model the conditional distribution of the labels given the inputs. In this paper, we propose and investigate the deep Bayes classifier, which improves classical naive Bayes with conditional deep generative models. We further develop detection methods for adversarial examples, which reject inputs with low likelihood under the generative model. Experimental results suggest that deep Bayes classifiers are more robust than deep discriminative classifiers, and that the proposed detection methods are effective against many recently proposed attacks.
","['Microsoft Research Cambridge', 'University of Cambridge', 'Universitat Tubingen/CIN']"
2019,"A Gradual, Semi-Discrete Approach to Generative Network Training via Explicit Wasserstein Minimization","Yucheng Chen, Matus Telgarsky, Chao Zhang, Bolton Bailey, Daniel Hsu, Jian Peng",https://icml.cc/Conferences/2019/Schedule?showEvent=4293,"This paper provides a simple procedure to fit generative networks to target distributions, with the goal of a small Wasserstein distance (or other optimal transport costs). The approach is based on two principles: (a) if the source randomness of the network is a continuous distribution (the ""semi-discrete"" setting), then the Wasserstein distance is realized by a deterministic optimal transport mapping; (b) given an optimal transport mapping between a generator network and a target distribution, the Wasserstein distance may be decreased via a regression between the generated data and the mapped target points. The procedure here therefore alternates these two steps, forming an optimal transport and regressing against it, gradually adjusting the generator network towards the target distribution. Mathematically, this approach is shown to minimize the Wasserstein distance to both the empirical target distribution, and also its underlying population counterpart. Empirically, good performance is demonstrated on the training and testing sets of the MNIST and Thin-8 data. The paper closes with a discussion of the unsuitability of the Wasserstein distance for certain tasks, as has been identified in prior work (Arora et al., 2017; Huang et al., 2017).
","['University of Illinois at Urbana-Champaign', 'UIUC', 'University of Illinois, Urbana Champaign', 'University of Illinois', 'Columbia University', 'UIUC']"
2019,Disentangling Disentanglement in Variational Autoencoders,"Emile Mathieu, Tom Rainforth, N Siddharth, Yee-Whye Teh",https://icml.cc/Conferences/2019/Schedule?showEvent=3855,"We develop a generalisation of disentanglement in variational autoencoders (VAEs)---decomposition of the latent representation---characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the $\beta$-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.
","['University of Oxford', 'University of Oxford', 'Unversity of Oxford', 'Oxford and DeepMind']"
2019,EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE,"Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Sebastian Nowozin, Cheng Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3924,"Many real-life decision making situations allow further relevant information to be acquired at a specific cost, for example, in assessing the health status of a patient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment.  Acquiring more relevant information enables better decision making, but may be costly.  How can we trade off the desire to make good decisions by acquiring further information with the cost of performing that acquisition? To this end, we propose a principled framework, named \emph{EDDI} (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian experimental design. In EDDI, we propose a novel \emph{partial variational autoencoder} (Partial VAE) to predict missing data entries problematically given any subset of the observed ones, and combine it with an acquisition function that maximizes expected information gain on a set of target variables. We show cost reduction at the same decision quality and improved decision quality at the same cost in multiple machine learning benchmarks and two real-world health-care applications. 
","['University of Cambridge', 'Microsoft Research', 'Microsoft Research Cambridge', 'University of Cambridge', 'MSR Cambridge', 'Microsoft Research, Cambridge']"
2019,A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning,"Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, Masanori Koyama",https://icml.cc/Conferences/2019/Schedule?showEvent=3795,"Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called hyperbolic wrapped distribution, a wrapped normal distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.
","['The University of Tokyo', 'Preferred Networks', 'Preferred Networks, Inc.', 'Preferred Networks Inc. ']"
2019,Emerging Convolutions for Generative Normalizing Flows,"Emiel Hoogeboom, Rianne Van den Berg, Max Welling",https://icml.cc/Conferences/2019/Schedule?showEvent=4088,"Generative flows are attractive because they admit exact likelihood optimization and efficient image synthesis. Recently, Kingma & Dhariwal (2018) demonstrated with Glow that generative flows are capable of generating high quality images. We generalize the 1 × 1 convolutions proposed in Glow to invertible d × d convolutions, which are more flexible since they operate on both channel and spatial axes. We propose two methods to produce invertible convolutions, that have receptive fields identical to standard convolutions: Emerging convolutions are obtained by chaining specific autoregressive convolutions, and periodic convolutions are decoupled in the frequency domain. Our experiments show that the flexibility of d × d convolutions significantly improves the performance of generative flow models on galaxy images, CIFAR10 and ImageNet.
","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam & Qualcomm']"
2019,A Large-Scale Study on Regularization and Normalization in GANs,"Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, Sylvain Gelly",https://icml.cc/Conferences/2019/Schedule?showEvent=4146,"Generative adversarial networks (GANs) are a class of deep generative models which aim to learn a target distribution in an unsupervised fashion. While they were successfully applied to many problems, training a GAN is a notoriously challenging task and requires a significant number of hyperparameter tuning, neural architecture engineering, and a non-trivial amount of ``tricks"". The success in many practical applications coupled with the lack of a measure to quantify the failure modes of GANs resulted in a plethora of proposed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of GANs from a practical perspective. We discuss and evaluate common pitfalls and reproducibility issues, open-source our code on Github, and provide pre-trained models on TensorFlow Hub.
","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']"
2019,Variational Annealing of GANs: A Langevin Perspective,"Chenyang Tao, Shuyang Dai, Liqun Chen, Ke Bai, Junya Chen, Chang Liu, RUIYI (ROY) ZHANG, Georgiy Bobashev, Lawrence Carin",https://icml.cc/Conferences/2019/Schedule?showEvent=4271,"The generative adversarial network (GAN) has received considerable attention recently as a model for data synthesis, without an explicit specification of a likelihood function. There has been commensurate interest in leveraging likelihood estimates to improve GAN training. To enrich the understanding of this fast-growing yet almost exclusively heuristic-driven subject, we elucidate the theoretical roots of some of the empirical attempts to stabilize and improve GAN training with the introduction of likelihoods. We highlight new insights from variational theory of diffusion processes to derive a likelihood-based regularizing scheme for GAN training, and present a novel approach to train GANs with an unnormalized distribution instead of empirical samples. To substantiate our claims, we provide experimental evidence on how our theoretically-inspired new algorithms improve upon current practice.
","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Duke U', 'Tsinghua University', 'Duke University', 'RTI International', 'Duke']"
2019,Invertible Residual Networks,"Jens Behrmann, Will Grathwohl, Tian Qi Chen, David Duvenaud, Joern-Henrik Jacobsen",https://icml.cc/Conferences/2019/Schedule?showEvent=4295,"We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.
","['University of Bremen', 'University of Toronto', 'U of Toronto', 'University of Toronto', 'Vector Institute and University of Toronto']"
2019,NAS-Bench-101: Towards Reproducible Neural Architecture Search,"Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, Frank Hutter",https://icml.cc/Conferences/2019/Schedule?showEvent=4009,"Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.
","['Ambient.ai', 'University of Freiburg', 'Google', 'Google Inc.', 'Google Brain', 'University of Freiburg and Bosch Center for Artificial Intelligence']"
2019,Approximated Oracle Filter Pruning for Destructive CNN Width Optimization,"XIAOHAN DING, guiguang ding, Yuchen Guo, Jungong Han, Chenggang Yan",https://icml.cc/Conferences/2019/Schedule?showEvent=3835,"It is not easy to design and run Convolutional Neural Networks (CNNs) due to: 1) finding the optimal number of filters (i.e., the width) at each layer is tricky, given an architecture; and 2) the computational intensity of CNNs impedes the deployment on computationally limited devices. Oracle Pruning is designed to remove the unimportant filters from a well-trained CNN, which estimates the filters' importance by ablating them in turn and evaluating the model, thus delivers high accuracy but suffers from intolerable time complexity, and requires a given resulting width but cannot automatically find it. To address these problems, we propose Approximated Oracle Filter Pruning (AOFP), which keeps searching for the least important filters in a binary search manner, makes pruning attempts by masking out filters randomly, accumulates the resulting errors, and finetunes the model via a multi-path framework. As AOFP enables simultaneous pruning on multiple layers, we can prune an existing very deep CNN with acceptable time cost, negligible accuracy drop, and no heuristic knowledge, or re-design a model which exerts higher accuracy and faster inference.
","['Tsinghua University', 'Tsinghua University, China', 'Tsinghua University', 'Lancaster University', 'Hangzhou Dianzi University']"
2019,LegoNet: Efficient Convolutional Neural Networks with Lego Filters,"Zhaohui Yang, Yunhe Wang, Chuanjian Liu, Hanting Chen, Chunjing Xu, Boxin Shi, Chao Xu, Chang Xu",https://icml.cc/Conferences/2019/Schedule?showEvent=3712,"This paper aims to build efficient convolutional neural networks using a set of Lego filters. Many successful building blocks, e.g., inception and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recognition tasks. Beyond these high-level modules, we suggest that an ordinary filter in the neural network can be upgraded to a sophisticated module as well. Filter modules are established by assembling a shared set of Lego filters that are often of much lower dimensions. Weights in Lego filters and binary masks to stack Lego filters for these filter modules can be simultaneously optimized in an end-to-end manner as usual. Inspired by network engineering, we develop a split-transform-merge strategy for an efficient convolution by exploiting intermediate Lego feature maps. The compression and acceleration achieved by Lego Networks using the proposed Lego filters have been theoretically discussed. Experimental results on benchmark datasets and deep models demonstrate the advantages of the proposed Lego filters and their potential real-world applications on mobile devices.
","['Peking University', 'Peking University', ""Huawei Noah's Ark Lab"", 'Peking University', ""Huawei Noah's Ark Lab"", 'Peking University', 'Peking University', 'University of Sydney']"
2019,Sorting Out Lipschitz Function Approximation,"Cem Anil, James Lucas, Roger Grosse",https://icml.cc/Conferences/2019/Schedule?showEvent=3940,"Training neural networks under a strict Lipschitz constraint is useful for provable adversarial robustness, generalization bounds, interpretable gradients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation  is 1-Lipschitz. The challenge is to do this while maintaining the expressive power. We identify a necessary property for such an architecture: each of the layers must preserve the gradient norm during backpropagation. Based on this, we propose to combine a gradient norm preserving activation function, GroupSort, with norm-constrained weight matrices. We show that norm-constrained GroupSort architectures are universal Lipschitz function approximators. Empirically, we show that norm-constrained GroupSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterparts and can achieve provable adversarial robustness guarantees with little cost to accuracy. 
","['University of Toronto', 'University of Toronto', 'University of Toronto and Vector Institute']"
2019,"Graph Element Networks: adaptive, structured computation and memory","Ferran Alet, Adarsh Keshav Jeewajee, Maria Bauza Villalonga, Alberto Rodriguez, Tomas Lozano-Perez, Leslie Kaelbling",https://icml.cc/Conferences/2019/Schedule?showEvent=3979,"We explore the use of graph neural networks (GNNs) to model spatial processes in which there is no a priori graphical structure.  Similar to finite element analysis, we assign nodes of a GNN to spatial locations and use a computational process defined on the graph to model the relationship between an initial function defined over a space and a resulting function in the same space.   We use GNNs as a computational substrate, and show that the locations of the nodes in space as well as their connectivity can be optimized to focus on the most complex parts of the space.  Moreover, this representational strategy allows the learned input-output relationship to generalize over the size of the underlying space and run the same model at different levels of precision, trading computation for accuracy.  We demonstrate this method on a traditional PDE problem, a physical prediction problem from robotics, and learning to predict scene images from novel viewpoints.
","['MIT', 'Massachusetts Institute of Technology', 'MIT', 'MIT', 'MIT', '(organization)']"
2019,Training CNNs with Selective Allocation of Channels,"Jongheon Jeong, Jinwoo Shin",https://icml.cc/Conferences/2019/Schedule?showEvent=3973,"Recent progress in deep convolutional neural networks (CNNs) have enabled a simple paradigm of architecture design: larger models typically achieve better accuracy. Due to this, in modern CNN architectures, it becomes more important to design models that generalize well under certain resource constraints, e.g. the number of parameters. In this paper, we propose a simple way to improve the capacity of any CNN model having large-scale features, without adding more parameters. In particular, we modify a standard convolutional layer to have a new functionality of channel-selectivity, so that the layer is trained to select important channels to re-distribute their parameters. Our experimental results under various CNN architectures and datasets demonstrate that the proposed new convolutional layer allows new optima that generalize better via efficient resource utilization, compared to the baseline.
","['KAIST', 'KAIST, AITRICS']"
2019,Equivariant Transformer Networks,"Kai Sheng Tai, Peter Bailis, Gregory Valiant",https://icml.cc/Conferences/2019/Schedule?showEvent=3908,"How can prior knowledge on the transformation invariances of a domain be incorporated into the architecture of a neural network? We propose Equivariant Transformers (ETs), a family of differentiable image-to-image mappings that improve the robustness of models towards pre-defined continuous transformation groups. Through the use of specially-derived canonical coordinate systems, ETs incorporate functions that are equivariant by construction with respect to these transformations. We show empirically that ETs can be flexibly composed to improve model robustness towards more complicated transformation groups in several parameters. On a real-world image classification task, ETs improve the sample efficiency of ResNet classifiers, achieving relative improvements in error rate of up to 15% in the limited data regime while increasing model parameter count by less than 1%.
","['Stanford University', 'Stanford University', 'Stanford University']"
2019,Overcoming Multi-model Forgetting,"Yassine Benyahia, Kaicheng Yu, Kamil Bennani-Smires, Martin Jaggi, Anthony C. Davison, Mathieu Salzmann, Claudiu Musat",https://icml.cc/Conferences/2019/Schedule?showEvent=3710,"We identify a phenomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model's shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks.
","['IPROVA', 'EPFL', 'Swisscom', 'EPFL', 'EPFL', 'EPFL', 'Swisscom']"
2019,Bayesian Nonparametric Federated Learning of Neural Networks,"Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, Yasaman Khazaeni",https://icml.cc/Conferences/2019/Schedule?showEvent=3892,"In federated learning problems, data is scattered across different servers and exchanging or pooling it is often impractical or prohibited. We develop a Bayesian nonparametric framework for federated learning with neural networks. Each data server is assumed to provide local neural network weights, which are modeled through our framework. We then develop an inference approach that allows us to synthesize a more expressive global network without additional supervision, data pooling and with as few as a single communication round. We then demonstrate the efficacy of our approach on federated learning problems simulated from two popular image classification datasets.
","['IBM Research AI', 'IBM Research', 'IBM Research', 'IBM', 'MIT-IBM Watson AI Lab, IBM Research', 'IBM Research']"
2019,How does Disagreement Help Generalization against Label Corruption?,"Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, Masashi Sugiyama",https://icml.cc/Conferences/2019/Schedule?showEvent=3772,"Learning with noisy labels is one of the hottest problems in weakly-supervised learning. Based on memorization effects of deep neural networks, training on small-loss instances becomes very promising for handling noisy labels. This fosters the state-of-the-art approach ""Co-teaching"" that cross-trains two deep neural networks using the small-loss trick. However, with the increase of epochs, two networks converge to a consensus and Co-teaching reduces to the self-training MentorNet. To tackle this issue, we propose a robust learning paradigm called Co-teaching+, which bridges the ""Update by Disagreement'' strategy with the original Co-teaching. First, two networks feed forward and predict all data, but keep prediction disagreement data only. Then, among such disagreement data, each network selects its small-loss data, but back propagates the small-loss data from its peer network and updates its own parameters. Empirical results on benchmark datasets demonstrate that Co-teaching+ is much superior to many state-of-the-art methods in the robustness of trained models.
","['University of Technology Sydney', 'RIKEN-AIP', 'University of Technology Sydney', 'RIKEN', 'University of Technology Sydney', 'RIKEN / The University of Tokyo']"
2019,EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis,"Chaoqi Wang, Roger Grosse, Sanja Fidler, Guodong Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=4306,"Reducing the test time resource requirements of a neural network while preserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce a novel network reparameterization based on the Kronecker-factored eigenbasis (KFE), and then apply Hessian-based structured pruning methods in this basis. As opposed to existing Hessian-based pruning algorithms which do pruning in parameter coordinates, our method works in the KFE where different weights are approximately independent, enabling accurate pruning and fast computation. We demonstrate empirically the effectiveness of the proposed method through extensive experiments. In particular, we highlight that the improvements are especially significant for more challenging datasets and networks. With negligible loss of accuracy, an iterative-pruning version gives a 10x reduction in model size and a 8x reduction in FLOPs on wide ResNet32. 
","['University of Toronto', 'University of Toronto and Vector Institute', 'University of Toronto, NVIDIA', 'University of Toronto']"
2019,Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment,"Chen Huang, Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista Martin, Shih-Yu Sun, Carlos Guestrin, Joshua M Susskind",https://icml.cc/Conferences/2019/Schedule?showEvent=3769,"In most machine learning training paradigms a fixed, often handcrafted, loss function is assumed to be a good proxy for an underlying evaluation metric. In this work we assess this assumption by meta-learning an adaptive loss function to directly optimize the evaluation metric. We propose a sample efficient reinforcement learning approach for adapting the loss dynamically during training. We empirically show how this formulation improves performance by simultaneously optimizing the evaluation metric and smoothing the loss landscape. We verify our method in metric learning and classification scenarios, showing considerable improvements over the state-of-the-art on a diverse set of tasks. Importantly, our method is applicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned policies are transferable across tasks and data, demonstrating the versatility of the method. 
","['Apple Inc.', 'Apple', 'Apple', 'Apple Inc.', 'Apple', 'Apple & Univesity of Washington', 'Apple, Inc.']"
2019,Deep Compressed Sensing,"Yan Wu, Mihaela Rosca, Timothy Lillicrap",https://icml.cc/Conferences/2019/Schedule?showEvent=4326,"Compressed sensing (CS) provides an elegant framework for recovering sparse signals from compressed measurements. For example, CS can exploit the structure of natural images and recover an image from only a few random measurements. Unlike popular autoencoding models, reconstruction in CS is posed as an optimisation problem that is separate from sensing. CS is flexible and data efficient, but its application has been restricted by the strong assumption of sparsity and costly reconstruction process. A recent approach that combines CS with neural network generators has removed the constraint of sparsity, but reconstruction remains slow. Here we propose a novel framework that significantly improves both the performance and speed of signal recovery by jointly training a generator and the optimisation process for reconstruction via meta-learning. We explore training the measurements with different objectives, and derive a family of models based on minimising measurement errors. We show that Generative Adversarial Nets (GANs) can be viewed as a special case in this family of models. Borrowing insights from the CS perspective, we develop a novel way of improving GANs using gradient information from the discriminator.
","['DeepMind', 'DeepMind', 'Google DeepMind']"
2019,Differentiable Dynamic Normalization for Learning Deep Representation,"Ping Luo, Peng Zhanglin, Shao Wenqi, Zhang ruimao, Ren jiamin, Wu lingyun",https://icml.cc/Conferences/2019/Schedule?showEvent=3623,"This work presents Dynamic Normalization (DN), which is able to learn arbitrary normalization operations for different convolutional layers in a deep ConvNet. Unlike existing normalization approaches that predefined computations of the statistics (mean and variance), DN learns to estimate them. DN has several appealing benefits. First, it adapts to various networks, tasks, and batch sizes. Second, it can be easily implemented and trained in a differentiable end-to-end manner with merely small number of parameters. Third, its matrix formulation represents a wide range of normalization methods, shedding light on analyzing them theoretically. Extensive studies show that DN outperforms its counterparts in CIFAR10 and ImageNet.
","['The University of Hong Kong', 'SenseTime', 'CUHK', 'cuhk', 'sensetime', 'sensetime']"
2019,Toward Understanding the Importance of Noise in Training Neural Networks,"Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, Tuo Zhao",https://icml.cc/Conferences/2019/Schedule?showEvent=4316,"Numerous empirical evidence has corroborated that the noise plays a crucial rule in effective and efficient training of deep neural networks. The theory behind, however, is still largely unknown. This paper studies this fundamental problem through training a simple two-layer convolutional neural network model. Although training such a network requires to solve a non-convex optimization problem with a spurious local optimum and a global optimum, we prove that a perturbed gradient descent algorithm in conjunction with noise annealing is guaranteed to converge to a global optimum in polynomial time with arbitrary initialization. This implies that the noise enables the algorithm to efficiently escape from the spurious local optimum. Numerical experiments are provided to support our theory.
","['Peking University', 'Georgia Institute of Technolodgy', 'Georgia Tech', 'Peking University', '', 'Georgia Institute of Technology']"
2019,Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group,"Mario Lezcano Casado, David Martínez-Rubio",https://icml.cc/Conferences/2019/Schedule?showEvent=3707,"We introduce a novel approach to perform first-order optimization with orthogonal and unitary constraints.
This approach is based on a parametrization stemming from Lie group theory through the exponential map. 
The parametrization transforms the constrained optimization problem into an unconstrained one over a Euclidean space, for which common first-order optimization methods can be used.
The theoretical results presented are general enough to cover the special orthogonal group, the unitary group and, in general, any connected compact Lie group.
We discuss how this and other parametrizations can be computed efficiently through an implementation trick, making numerically complex parametrizations usable at a negligible runtime cost in neural networks.
In particular, we apply our results to RNNs with orthogonal recurrent weights, yielding a new architecture called expRNN.
We demonstrate how our method constitutes a more robust approach to optimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test RNNs.
","['Univeristy of Oxford', 'University of Oxford']"
2019,Breaking Inter-Layer Co-Adaptation by Classifier Anonymization,"Ikuro Sato, Kohta Ishikawa, Guoqing Liu, Masayuki Tanaka",https://icml.cc/Conferences/2019/Schedule?showEvent=3713,"This study addresses an issue of co-adaptation between a feature extractor and a classifier in a neural network. A naive joint optimization of a feature extractor and a classifier often brings situations in which an excessively complex feature distribution adapted to a very specific classifier degrades the test performance. We introduce a method called Feature-extractor Optimization through Classifier Anonymization (FOCA), which is designed to avoid an explicit co-adaptation between a feature extractor and a particular classifier by using many randomly-generated, weak classifiers during optimization. We put forth a mathematical proposition that states the FOCA features form a point-like distribution within the same class in a class-separable fashion under special conditions. Real-data experiments under more general conditions provide supportive evidences.
","['Denso IT Laboratory, Inc.', 'DENSO IT Laboratory', 'Denso IT Laboratory', 'National Institute of Advanced Industrial Science and Technology, Japan']"
2019,Understanding the Impact of Entropy on Policy Optimization,"Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, Dale Schuurmans",https://icml.cc/Conferences/2019/Schedule?showEvent=4104,"Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.
","['Mila - McGill University', 'Google', 'Google Brain', 'Google / University of Alberta']"
2019,"Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning","Casey Chu, Jose Blanchet, Peter Glynn",https://icml.cc/Conferences/2019/Schedule?showEvent=3944,"The goal of this paper is to provide a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.
","['Stanford University', 'Stanford University', 'Stanford University']"
2019,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,"Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, Nando de Freitas",https://icml.cc/Conferences/2019/Schedule?showEvent=3608,"We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. 
Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. 
Consequently, the influence reward opens up a window of new opportunities for research in this area.
","['MIT', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Princeton University', 'DeepMind', 'DeepMind']"
2019,Maximum Entropy-Regularized Multi-Goal Reinforcement Learning,"Rui Zhao, Xudong Sun, Volker Tresp",https://icml.cc/Conferences/2019/Schedule?showEvent=3833,"In Multi-Goal Reinforcement Learning, an agent learns to achieve multiple goals with a goal-conditioned policy. During learning, the agent first collects the trajectories into a replay buffer, and later these trajectories are selected randomly for replay. However, the achieved goals in the replay buffer are often biased towards the behavior policies. From a Bayesian perspective, when there is no prior knowledge about the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first propose a novel multi-goal RL objective based on weighted entropy. This objective encourages the agent to maximize the expected return, as well as to achieve more diverse goals.  Secondly, we developed a maximum entropy-based prioritization framework to optimize the proposed objective. For evaluation of this framework, we combine it with Deep Deterministic Policy Gradient, both with or without Hindsight Experience Replay. On a set of multi-goal robotic tasks of OpenAI Gym, we compare our method with other baselines and show promising improvements in both performance and sample-efficiency. 
","['Siemens & Ludwig Maximilian University of Munich', 'Ludwig Maximilian University of Munich', 'Siemens AG and University of Munich']"
2019,Imitating Latent Policies from Observation,"Ashley Edwards, Himanshu Sahni, Yannick Schroecker, Charles Isbell",https://icml.cc/Conferences/2019/Schedule?showEvent=3971,"In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github.com/ashedwards/ILPO.
","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']"
2019,SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning,"Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, Sergey Levine",https://icml.cc/Conferences/2019/Schedule?showEvent=4307,"Model-based reinforcement learning (RL) has proven to be a data efficient approach for learning control tasks but is difficult to utilize in domains with complex observations such as images. In this paper, we present a method for learning representations that are suitable for iterative model-based policy improvement, even when the underlying dynamical system has complex dynamics and image observations, in that these representations are optimized for inferring simple dynamics and cost models given data from the current policy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our approach on a range of robotics tasks, including manipulation with a real-world robotic arm directly from images. We find that our method produces substantially better final performance than other model-based RL methods while being significantly more efficient than model-free RL.
","['UC Berkeley', 'UCSD', 'UC Berkeley', 'OpenAI / UC Berkeley', 'Google Brain', 'UC Berkeley']"
2019,Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement Learning,"Seungyul Han, Youngchul Sung",https://icml.cc/Conferences/2019/Schedule?showEvent=3671,"In importance sampling (IS)-based reinforcement learning algorithms such as Proximal Policy Optimization (PPO), IS weights are typically clipped to avoid large variance in learning. However, policy update from clipped statistics induces large bias in tasks with high action dimensions, and bias from clipping makes it difficult to reuse old samples with large IS weights. In this paper, we consider PPO, a representative on-policy algorithm, and propose its improvement by dimension-wise IS weight clipping which separately clips the IS weight of each action dimension to avoid large bias and adaptively controls the IS weight to bound policy update from the current policy. This new technique enables efficient learning for high action-dimensional tasks and reusing of old samples like in off-policy learning to increase the sample efficiency. Numerical results show that the proposed new algorithm outperforms PPO and other RL algorithms in various Open AI Gym tasks.
","['KAIST', 'KAIST']"
2019,Structured agents for physical construction,"Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly Stachenfeld, Pushmeet Kohli, Peter Battaglia, Jessica Hamrick",https://icml.cc/Conferences/2019/Schedule?showEvent=4079,"Physical construction---the ability to compose objects, subject to physical dynamics, to serve some function---is fundamental to human intelligence. We introduce a suite of challenging physical construction tasks inspired by how children play with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of deep reinforcement learning agents fare on these challenges, and introduce several new approaches which provide superior performance. Our results show that agents which use structured representations (e.g., objects and scene graphs) and structured policies (e.g., object-centric actions) outperform those which use less structured representations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outperform strictly model-free agents in our most challenging construction problems. We conclude that approaches which combine structured representations and reasoning with powerful learning are a key path toward agents that possess rich intuitive physics, scene understanding, and planning.
","['Google DeepMind', 'DeepMind', 'DeepMind', 'Google', 'DeepMind', 'DeepMind', 'DeepMind']"
2019,Learning Novel Policies For Tasks,"Yunbo Zhang, Wenhao Yu, Greg Turk",https://icml.cc/Conferences/2019/Schedule?showEvent=4274,"In this work, we present a reinforcement learning algorithm that can find a variety of policies (novel policies) for a task that is given by a task reward function. Our method does this by creating a second reward function that recognizes previously seen state sequences and rewards those by novelty, which is measured using autoencoders that have been trained on state sequences from previously discovered policies. We present a two-objective update technique for policy gradient algorithms in which each update of the policy is a compromise between improving the task reward and improving the novelty reward. Using this method, we end up with a collection of policies that solves a given task as well as carrying out action sequences that are distinct from one another. We demonstrate this method on maze navigation tasks, a reaching task for a simulated robot arm, and a locomotion task for a hopper. We also demonstrate the effectiveness of our approach on deceptive tasks in which policy gradient methods often get stuck.
","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']"
2019,Taming MAML: Efficient unbiased meta-reinforcement learning,"Hao Liu, Richard Socher, Caiming Xiong",https://icml.cc/Conferences/2019/Schedule?showEvent=4182,"While meta reinforcement learning (Meta-RL) methods have achieved remarkable success, obtaining correct and low variance estimates for policy gradients remains a significant challenge. In particular, estimating a large Hessian, poor sample efficiency and unstable training continue to make Meta-RL difficult. We propose a surrogate objective function named, Taming MAML (TMAML), that adds control variates into gradient estimation via automatic differentiation. TMAML improves the quality of gradient estimation by reducing variance without introducing bias. We further propose a version of our method that extends the meta-learning framework to learning the control variates themselves, enabling efficient and scalable learning from a distribution of MDPs. We empirically compare our approach with MAML and other variance-bias trade-off methods including DICE, LVC, and action-dependent control variates. Our approach is easy to implement and outperforms existing methods in terms of the variance and accuracy of gradient estimation, ultimately yielding higher performance across a variety of challenging Meta-RL environments.
","['Salesforce Research, UC Berkeley', 'Salesforce', 'Salesforce']"
2019,Self-Supervised Exploration via Disagreement,"Deepak Pathak, Dhiraj Gandhi, Abhinav Gupta",https://icml.cc/Conferences/2019/Schedule?showEvent=4234,"Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github.io/exploration-by-disagreement/
","['UC Berkeley', 'Carnegie Mellon University Robotics Institute', 'Carnegie Mellon University']"
2019,Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,"Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, Deirdre Quillen",https://icml.cc/Conferences/2019/Schedule?showEvent=3641,"Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.
","['UC Berkeley', 'UC Berkeley', 'Stanford, Google, UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2019,The Natural Language of Actions,"Guy Tennenholtz, Shie Mannor",https://icml.cc/Conferences/2019/Schedule?showEvent=3873,"We introduce Act2Vec, a general framework for learning context-based action representation for Reinforcement Learning. Representing actions in a vector space help reinforcement learning algorithms achieve better performance by grouping similar actions and utilizing relations between different actions. We show how prior knowledge of an environment can be extracted from demonstrations and injected into action vector representations that encode natural compatible behavior. We then use these for augmenting state representations as well as improving function approximation of Q-values. We visualize and test action embeddings in three domains including a drawing task, a high dimensional navigation task, and the large action space domain of StarCraft II.
","['Technion', 'Technion']"
2019,Control Regularization for Reduced Variance Reinforcement Learning,"Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, Joel Burdick",https://icml.cc/Conferences/2019/Schedule?showEvent=3640,"Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in performance from run to run using different initializations/seeds. Focusing on problems arising in continuous control, we propose a functional regularization approach to augmenting model-free RL. In particular, we regularize the behavior of the deep policy to be similar to a policy prior, i.e., we regularize in function space. We show that functional regularization yields a bias-variance trade-off, and propose an adaptive tuning strategy to optimize this trade-off. When the policy prior has control-theoretic stability guarantees, we further show that this regularization approximately preserves those stability guarantees throughout learning. We validate our approach empirically on a range of settings, and demonstrate significantly reduced variance, guaranteed dynamic stability, and more efficient learning than deep RL alone.
","['California Institute of Technology', 'Rice University', 'University of Michigan', 'Rice University', 'Caltech', 'Caltech']"
2019,On the Generalization Gap in Reparameterizable Reinforcement Learning,"Huan Wang, Stephan Zheng, Caiming Xiong, Richard Socher",https://icml.cc/Conferences/2019/Schedule?showEvent=4206,"Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumptions of traditional supervised learning theory do not apply. We focus on the special class of reparameterizable RL problems, where the trajectory distribution can be decomposed using the reparametrization trick. For this problem class, estimating the expected return is efficient and the trajectory can be computed deterministically given peripheral random variables, which enables us to study reparametrizable RL using supervised learning and transfer learning theory. Through these relationships, we derive guarantees on the gap between the expected and empirical return for both intrinsic and external errors, based on Rademacher complexity as well as the PAC-Bayes bound. Our bound suggests the generalization capability of reparameterizable RL is related to multiple factors including ``smoothness'' of the environment transition, reward and agent policy function class. We also empirically verify the relationship between the generalization gap and these factors through simulations.
","['Salesforce Research', 'Salesforce Research', 'Salesforce', 'Salesforce']"
2019,Trajectory-Based Off-Policy Deep Reinforcement Learning,"Andreas Doerr, Michael Volpp, Marc Toussaint, Sebastian Trimpe, Christian Daniel",https://icml.cc/Conferences/2019/Schedule?showEvent=4203,"Policy gradient methods are powerful reinforcement learning algorithms and have been demonstrated to solve many complex tasks. However, these methods are also data-inefficient, afflicted with high variance gradient estimates, and frequently get stuck in local optima. This work addresses these weaknesses by combining recent improvements in the reuse of off-policy data and exploration in parameter space with deterministic behavioral policies. The resulting objective
is amenable to standard neural network optimization strategies like stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. Incorporation of previous rollouts via importance sampling greatly improves data-efficiency, whilst
stochastic optimization schemes facilitate the escape from local optima. We evaluate the proposed approach on a series of continuous control benchmark tasks. The results show that the proposed algorithm is able to successfully and reliably learn solutions using fewer system interactions than standard policy gradient methods.
","['Bosch Center for Artificial Intelligence, Max Planck Institute for Intelligent Systems', 'Bosch Center for Artificial Intelligence', 'University Stuttgart', 'Max Planck Institute for Intelligent Systems', 'Bosch Center for Artificial Intelligence']"
2019,A Deep Reinforcement Learning Perspective on Internet Congestion Control,"Nathan Jay, Noga H. Rotman, Brighten  Godfrey, Michael Schapira, Aviv Tamar",https://icml.cc/Conferences/2019/Schedule?showEvent=4028,"We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources' data-transmission rates to efficiently utilize network capacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training deep network policies that capture intricate patterns in data traffic and network conditions, and leverage this to outperform the state-of-the-art. We also highlight significant challenges facing real-world adoption of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reproducibility of our results, we present a test suite for RL-guided congestion control based on the OpenAI Gym interface.
","['University of Illinois Urbana-Champaign', 'Hebrew University of Jerusalem', 'University of Illinois Urbana-Champaign', 'Hebrew University of Jerusalem', 'Technion']"
2019,Model-Based Active Exploration,"Pranav Shyam, Wojciech Jaśkowski, Faustino  Gomez",https://icml.cc/Conferences/2019/Schedule?showEvent=4091,"Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient {\em active} exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.
","['NNAISENSE', 'NNAISENSE', 'NNAISENSE SA']"
2019,Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,"Daniel Brown, Wonjoon Goo, Prabhat  Nagarajan, Scott Niekum",https://icml.cc/Conferences/2019/Schedule?showEvent=4186,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.
","['University of Texas at Austin', 'University of Texas at Austin', 'Preferred Networks', 'University of Texas at Austin']"
2019,Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN,"dror freirich, Tzahi Shimkin, Ron Meir, Aviv Tamar",https://icml.cc/Conferences/2019/Schedule?showEvent=3848,"The recently proposed distributional approach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this formulation, DiRL can be seen as learning a deep generative model of the value distribution, driven by the discrepancy between the distribution of the current value, and the distribution of the sum of current reward and next value. We use this insight to propose a GAN-based approach to DiRL, which leverages the strengths of GANs in learning distributions of high dimensional data. In particular, we show that our GAN approach can be used for DiRL with multivariate rewards, an important setting which cannot be tackled with prior methods. The multivariate setting also allows us to unify learning the distribution of values and state transitions, and we exploit this idea to devise a novel exploration method that is driven by the discrepancy in estimating both values and states.
","['Technion', 'Technion Israeli Institute of Technology', 'Technion Israeli Institute of Technology', 'Technion']"
2019,A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs,"Jingkai Mao, Jakob Foerster, Tim Rocktäschel, Maruan Al-Shedivat, Gregory Farquhar, Shimon Whiteson",https://icml.cc/Conferences/2019/Schedule?showEvent=3768,"By enabling correct differentiation in Stochastic Computation Graphs (SCGs), the infinitely differentiable Monte-Carlo estimator (DiCE) can generate correct estimates for the higher order gradients that arise in, e.g., multi-agent reinforcement learning and meta-learning. However, the baseline term in DiCE that serves as a control variate for reducing variance applies only to first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient estimate. It reuses the same baseline function (e.g., the state-value function in reinforcement learning) already used for the first order baseline. We provide theoretical analysis and numerical evaluations of this new baseline, which demonstrate that it can dramatically reduce the variance of DiCE's second order gradient estimators and also show empirically that it reduces the variance of third and fourth order gradients. This computational tool can be easily used to estimate higher order gradients with unprecedented efficiency and simplicity wherever automatic differentiation is utilised, and it has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.
","['Man AHL', 'Facebook AI Research', 'University of Oxford', 'Carnegie Mellon University', 'University of Oxford', 'University of Oxford']"
2019,Remember and Forget for Experience Replay,"Guido Novati, Petros Koumoutsakos",https://icml.cc/Conferences/2019/Schedule?showEvent=4287,"Experience replay (ER) is a fundamental component of off-policy deep reinforcement learning (RL). ER recalls experiences from past iterations to compute gradient estimates for the current policy, increasing data-efficiency. However, the accuracy of such updates may deteriorate when the policy diverges from past behaviors and can undermine the performance of ER. Many algorithms mitigate this issue by tuning hyper-parameters to slow down policy changes. An alternative is to actively enforce the similarity between policy and the  experiences in the replay memory. We introduce Remember and Forget Experience Replay (ReF-ER), a novel method that can enhance RL algorithms with parameterized policies. ReF-ER (1) skips gradients computed from experiences that are too unlikely with the current policy and (2) regulates policy changes within a trust region of the replayed behaviors. We couple ReF-ER with Q-learning, deterministic policy gradient and off-policy gradient methods. We find that ReF-ER consistently improves the performance of continuous-action, off-policy RL on fully observable benchmarks and partially observable flow control problems.
","['ETH Zurich', 'ETH Zurich']"
2019,Tensor Variable Elimination for Plated Factor Graphs,"Fritz Obermeyer, Elias Bingham, Martin Jankowiak, Neeraj Pradhan, Justin Chiu, Alexander Rush, Noah Goodman",https://icml.cc/Conferences/2019/Schedule?showEvent=4278,"A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a compact way to express repeated structure when compared to plate diagrams for directed graphical models. To exploit efficient tensor algebra in graphs with plates of variables, we generalize undirected factor graphs to plated factor graphs and variable elimination to a tensor variable elimination algorithm that operates directly on plated factor graphs. Moreover, we generalize complexity bounds based on treewidth and characterize the class of plated factor graphs for which inference is tractable. As an application, we integrate tensor variable elimination into the Pyro probabilistic programming language to enable exact inference in discrete latent variable models with repeated structure. We validate our methods with experiments on both directed and undirected graphical models, including applications to polyphonic music modeling, animal movement modeling, and latent sentiment analysis.
","['Uber AI Labs', 'Uber AI Labs', 'Uber AI Labs', 'Uber AI Labs', 'Harvard', 'Harvard University', 'Uber AI Labs']"
2019,Predicate Exchange: Inference with Declarative Knowledge,"Zenna Tavares, Javier Burroni, Edgar Minasyan, Armando Solar-Lezama, Rajesh Ranganath",https://icml.cc/Conferences/2019/Schedule?showEvent=3972,"Programming languages allow us to express complex predicates, but existing inference methods are unable to condition probabilistic models on most of them. To support a broader class of predicates, we develop an inference procedure called predicate exchange, which softens predicates. A soft predicate quantifies the extent to which values of model variables are consistent with its hard counterpart. We substitute the likelihood term in the Bayesian posterior with a soft predicate, and develop a variant of replica exchange MCMC to draw posterior samples. We implement predicate exchange as a language agnostic tool which performs a nonstandard execution of a probabilistic program.  We demonstrate the approach on sequence models of health and inverse rendering. 
","['MIT', 'UMass Amherst', 'Princeton University', 'MIT', 'New York University']"
2019,Discriminative Regularization for Latent Variable Models with Applications to Electrocardiography,"Andrew Miller, Ziad Obermeyer, John Cunningham, Sendhil Mullainathan",https://icml.cc/Conferences/2019/Schedule?showEvent=3736,"Generative models often use latent variables to represent structured variation in high-dimensional data, such as images and medical waveforms. However, these latent variables may ignore subtle, yet meaningful features in the data. Some features may predict an outcome of interest (e.g. heart attack) but account for only a small fraction of variation in the data. We propose a generative model training objective that uses a black-box discriminative model as a regularizer to learn representations that preserve this predictive variation. With these discriminatively regularized latent variable models, we visualize and measure variation in the data that influence a black-box predictive model, enabling an expert to better understand each prediction. With this technique, we study models that use electrocardiograms to predict outcomes of clinical interest. We measure our approach on synthetic and real data with statistical summaries and an experiment carried out by a physician.
","['Columbia University', 'UC Berkeley', 'Columbia', 'University of Chicago']"
2019,Hierarchical Decompositional Mixtures of Variational Autoencoders,"Ping Liang Tan, Robert Peharz",https://icml.cc/Conferences/2019/Schedule?showEvent=4034,"Variational autoencoders (VAEs) have received considerable attention, since they allow us to learn expressive neural density estimators effectively and efficiently. However, learning and inference in VAEs is still problematic due to the sensitive interplay between the generative model and the inference network. Since these problems become generally more severe in high dimensions, we propose a novel hierarchical mixture model over low-dimensional VAE experts. Our model decomposes the overall learning problem into many smaller problems, which are coordinated by the hierarchical mixture, represented by a sum-product network. In experiments we show that our models outperform classical VAEs on almost all of our experimental benchmarks. Moreover, we show that our model is highly data efficient and degrades very gracefully in extremely low data regimes.ow data regimes.
","['University of Cambridge', 'University of Cambridge']"
2019,Finding Mixed Nash Equilibria of Generative Adversarial Networks,"Ya-Ping Hsieh, Chen Liu, Volkan Cevher",https://icml.cc/Conferences/2019/Schedule?showEvent=4102,"Generative adversarial networks (GANs) are known to achieve the state-of-the-art performance on various generative tasks, but these results come at the expense of a notoriously difficult training phase. Current training strategies typically draw a connection to optimization theory, whose scope is restricted to local convergence due to the presence of non-convexity. In this work, we tackle the training of GANs by rethinking the problem formulation from the mixed Nash Equilibria (NE) perspective. Via a classical lifting trick, we show that essentially all existing GAN objectives can be relaxed into their mixed strategy forms, whose global optima can be solved via sampling, in contrast to the exclusive use of optimization framework in previous work. We further propose a mean-approximation sampling scheme, which allows to systematically exploit methods for bi-affine games to delineate novel, practical training algorithms of GANs. Finally, we provide experimental evidence that our approach yields comparable or superior results to contemporary training algorithms, and outperforms classical methods such as SGD, Adam, and RMSProp. 
","['EPFL', 'EPFL', 'EPFL']"
2019,CompILE: Compositional Imitation Learning and Execution,"Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, Peter Battaglia",https://icml.cc/Conferences/2019/Schedule?showEvent=4035,"We introduce Compositional Imitation Learning and Execution (CompILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. CompILE uses a novel unsupervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-composed and executed to perform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We evaluate CompILE in a challenging 2D multi-task environment and a continuous control task, and show that it can find correct task boundaries and event encodings in an unsupervised manner. Latent codes and associated behavior policies discovered by CompILE can be used by a hierarchical agent, where the high-level policy selects actions in the latent code space, and the low-level, task-specific policies are simply the learned decoders. We found that our CompILE-based agent could learn given only sparse rewards, where agents without task-specific policies struggle.
","['University of Amsterdam', 'DeepMind', 'Georgia Tech', 'Deepmind', 'DeepMind', 'Facebook AI Research / UCL', 'DeepMind', 'DeepMind']"
2019,Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data,"Luigi Antelmi, Nicholas Ayache, Philippe Robert, Marco Lorenzi",https://icml.cc/Conferences/2019/Schedule?showEvent=4163,"Interpretable modeling of heterogeneous data channels is essential in medical applications, for example when jointly analyzing clinical scores and medical images.
Variational Autoencoders (VAE) are powerful generative models that learn representations of complex data.
The flexibility of VAE may come at the expense of lack of interpretability in describing the joint relationship between heterogeneous data.
To tackle this problem, in this work we extend the variational framework of VAE to bring parsimony and interpretability when jointly account for latent relationships across multiple channels.
In the latent space, this is achieved by constraining the variational distribution of each channel to a common target prior.
Parsimonious latent representations are enforced by variational dropout.
Experiments on synthetic data show that our model correctly identifies the prescribed latent dimensions and data relationships across multiple testing scenarios.
When applied to imaging and clinical data, our method allows to identify the joint effect of age and pathology in describing clinical condition in a large scale clinical cohort.
","['UCA, Inria', 'INRIA', 'CMMR Nice', 'Inria UCA,']"
2019,Deep Generative Learning via Variational Gradient Flow,"Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, Shunkang Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3826,"We propose a framework to learn deep generative models via \textbf{V}ariational \textbf{Gr}adient Fl\textbf{ow} (VGrow) on probability spaces. The evolving distribution that asymptotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the $f$-divergence between them. We prove that the evolving distribution coincides with the pushforward distribution through the infinitesimal time composition of residual maps that are perturbations of the identity map along the vector field. The vector field depends on the density ratio of the pushforward distribution and the target distribution, which can be consistently learned from a binary classification problem. Connections of our proposed VGrow method with other popular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of deep generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffreys divergences as well as our newly discovered ``logD'' divergence which serves as the objective function of the logD-trick GAN. Experimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving competitive performance with state-of-the-art GANs.","[""Xi'an Jiaotong University"", 'Zhongnan University of Ecomomics and Law', 'HKUST', '', 'HKUST', 'HKUST']"
2019,Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,"Jonathan Ho, Peter Chen, Aravind Srinivas, Rocky Duan, Pieter Abbeel",https://icml.cc/Conferences/2019/Schedule?showEvent=3759,"Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models.
","['UC Berkeley', 'Covariant.ai', 'UC Berkeley', 'University of California, Berkeley', 'OpenAI / UC Berkeley']"
2019,Learning Neurosymbolic Generative Models via Program Synthesis,"Halley R Young, Osbert Bastani, Mayur Naik",https://icml.cc/Conferences/2019/Schedule?showEvent=3775,"Generative models have become significantly more powerful in recent years. However, these models continue to have difficulty capturing global structure in data. For example, images of buildings typically contain spatial patterns such as windows repeating at regular intervals, but state-of-the-art models have difficulty generating these patterns. We propose to address this problem by incorporating programs representing global structure into generative models—e.g., a 2D for-loop may represent a repeating pattern of windows—along with a framework for learning these models by leveraging program synthesis to obtain training data. On both synthetic and real-world data, we demonstrate that our approach substantially outperforms state-of-the-art at both generating and completing images with global structure.
","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']"
2019,Theoretically Principled Trade-off between Robustness and Accuracy,"Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, Michael Jordan",https://icml.cc/Conferences/2019/Schedule?showEvent=3942,"We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by 11.41% in terms of mean L_2 perturbation distance.
","['CMU & TTIC', 'University of Virginia', 'University of California, Berkeley', 'Petuum Inc. and CMU', 'UC Berkeley', 'UC Berkeley']"
2019,The Odds are Odd: A Statistical Test for Detecting Adversarial Examples,"Kevin Roth, Yannic Kilcher, Thomas Hofmann",https://icml.cc/Conferences/2019/Schedule?showEvent=4113,"We investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2019,ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation,"Yuzhe Yang, GUO ZHANG, Zhi Xu, Dina Katabi",https://icml.cc/Conferences/2019/Schedule?showEvent=4204,"Deep neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examples. In contrast, the performance of defense techniques still lags behind. This paper proposes ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: first pixels are randomly dropped from the image; then, the image is reconstructed using ME. We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans typically rely on such global structures in classifying images, the process makes the network mode compatible with human perception. We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outperforms prior techniques, improving robustness against both black-box and white-box attacks.
","['MIT', 'MIT', 'MIT', 'MIT']"
2019,Certified Adversarial Robustness via Randomized Smoothing,"Jeremy Cohen, Elan Rosenfeld, Zico Kolter",https://icml.cc/Conferences/2019/Schedule?showEvent=3907,"We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm.  While this ""randomized smoothing"" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise.  We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with L2 norm less than 0.5 (=127/255).  Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies.
The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University / Bosch Center for AI']"
2019,"Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition","Yao Qin, Nicholas Carlini, Garrison Cottrell, Ian Goodfellow, Colin Raffel",https://icml.cc/Conferences/2019/Schedule?showEvent=4300,"Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples on speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes progress on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Then, we make progress towards physical-world audio adversarial examples by constructing perturbations which remain effective even after applying highly-realistic simulated environmental distortions.
","['University of California, San Diego', 'Google', 'University of California, San Diego', 'Google Brain', 'Google']"
2019,Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization,"Seungyong Moon, Gaon An, Hyun Oh Song",https://icml.cc/Conferences/2019/Schedule?showEvent=4042,"Solving for adversarial examples with projected gradient descent has been demonstrated to be highly effective in fooling the neural network based classifiers. However, in the black-box setting, the attacker is limited only to the query access to the network and solving for a successful adversarial example becomes much more difficult. To this end, recent methods aim at estimating the true gradient signal based on the input queries but at the cost of excessive queries. We propose an efficient discrete surrogate to the optimization problem which does not require estimating the gradient and consequently becomes free of the first order update hyperparameters to tune. Our experiments on Cifar-10 and ImageNet show the state of the art black-box attack performance with significant reduction in the required queries compared to a number of recently proposed methods. The source code is available at https://github.com/snu-mllab/parsimonious-blackbox-attack.
","['Seoul National University', 'Seoul National University', 'Seoul National University']"
2019,Wasserstein Adversarial Examples via Projected Sinkhorn Iterations,"Eric Wong, Frank R Schmidt, Zico Kolter",https://icml.cc/Conferences/2019/Schedule?showEvent=4229,"A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by $\ell_p$ norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which can naturally represent ``standard'' image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for approximate projection onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers, and code for all experiments in the paper is available at https://github.com/locuslab/projected_sinkhorn. ","['Carnegie Mellon University', 'Robert Bosch GmbH', 'Carnegie Mellon University / Bosch Center for AI']"
2019,Transferable Clean-Label Poisoning Attacks on Deep Neural Nets,"Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, Tom Goldstein",https://icml.cc/Conferences/2019/Schedule?showEvent=4267,"In this paper, we explore clean-label poisoning attacks on deep convolutional networks with access to neither the network's output nor its architecture or parameters. Our goal is to ensure that after injecting the poisons into the training data, a model with unknown architecture and parameters trained on that data will misclassify the target image into a specific class. To achieve this goal, we generate multiple poison images from the base class by adding small perturbations which cause the poison images to trap the target image within their convex polytope in feature space. We also demonstrate that using Dropout during crafting of the poisons and enforcing this objective in multiple layers enhances transferability, enabling attacks against both the transfer learning and end-to-end training settings. We demonstrate transferable attack success rates of over 50% by poisoning only 1% of the training set.
","['University of Maryland', 'University of Maryland and EY LLP', 'University of Maryland, College Park', 'United States Naval Academy', 'Cornell University', 'University of Maryland']"
2019,NATTACK: Learning the Distributions of Adversarial Examples for an Improved  Black-Box  Attack on Deep Neural Networks,"Yandong li, Lijun Li, Liqiang Wang, Tong Zhang, Boqing Gong",https://icml.cc/Conferences/2019/Schedule?showEvent=3662,"Powerful adversarial attack methods are vital for understanding how to construct robust deep neural networks (DNNs) and for thoroughly testing defense techniques. In this paper, we propose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques developed recently. Instead of searching for an ""optimal"" adversarial example for a benign input to a targeted DNN, our algorithm finds a probability density distribution over a small region centered around the input, such that a sample drawn from this  distribution is likely an adversarial example, without the need of accessing the DNN's internal layers or weights. Our approach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examples are not as transferable across defended DNNs as them across vanilla DNNs.
","['University of Central Florida', 'Beihang University', 'University of Central Florida', 'Tencent', 'Google']"
2019,Simple Black-box Adversarial Attacks,"Chuan Guo, Jacob Gardner, Yurong You, Andrew Wilson, Kilian Weinberger",https://icml.cc/Conferences/2019/Schedule?showEvent=3754,"We propose an intriguingly simple method for the construction of adversarial images in the black-box setting. In constrast to the white-box scenario, constructing black-box adversarial images has the additional constraint on query budget, and efficient attacks remain an open problem to date. With only the mild assumption of requiring continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simple iterative principle: we randomly sample a vector from a predefined orthonormal basis and either add or subtract it to the target image. Despite its simplicity, the proposed method can be used for both untargeted and targeted attacks -- resulting in previously unprecedented query efficiency in both settings. We demonstrate the efficacy and efficiency of our algorithm on several real world settings including the Google Cloud Vision API. We argue that our proposed algorithm should serve as a strong baseline for future black-box attacks, in particular because it is  extremely fast and its implementation requires less than 20 lines of PyTorch code. 
","['Cornell University', 'Uber AI Labs', 'Cornell University', 'Cornell University', 'Cornell University']"
2019,Causal Identification under Markov Equivalence: Completeness Results,"Amin Jaber, Jiji Zhang, Elias Bareinboim",https://icml.cc/Conferences/2019/Schedule?showEvent=3841,"Causal effect identification is the task of determining whether a causal distribution is computable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this problem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumption in many settings. In this paper, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in particular, a partial ancestral graph (PAG). This is attractive because a PAG can be learned directly from data, and the scientist does not need to commit to a particular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is complete. We derive a complete algorithm for identification given a PAG. This implies that whenever the causal effect is identifiable, the algorithm returns a valid identification expression; alternatively, it will throw a failure condition, which means that the effect is provably not identifiable. We further provide a graphical characterization of non-identifiability of causal effects in PAGs.
","['Purdue University', 'Lingnan U', 'Purdue']"
2019,Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models,"Michael Oberst, David Sontag",https://icml.cc/Conferences/2019/Schedule?showEvent=4272,"We introduce an off-policy evaluation procedure for highlighting episodes where applying a reinforcement learned (RL) policy is likely to have produced a substantially different outcome than the observed policy.  In particular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite partially observable Markov Decision Processes (POMDPs).  We see this as a useful procedure for off-policy ``debugging'' in high-risk settings (e.g., healthcare); by decomposing the expected difference in reward between the RL and observed policy into specific episodes, we can identify episodes where the counterfactual difference in reward is most dramatic.  This in turn can be used to facilitate review of specific episodes by domain experts. We demonstrate the utility of this procedure with a synthetic environment of sepsis management.
","['MIT', 'Massachusetts Institute of Technology']"
2019,Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models,"Biwei Huang, Kun Zhang, Mingming Gong, Clark Glymour",https://icml.cc/Conferences/2019/Schedule?showEvent=4045,"  In many scientific fields, such as economics and neuroscience, we are often faced with nonstationary time series, and concerned with both finding causal relations and forecasting the values of variables of interest, both of which are particularly challenging in such nonstationary environments. In this paper, we study causal discovery and forecasting for nonstationary time series. By exploiting a particular type of state-space model to represent the processes, we show that nonstationarity helps to identify the causal structure, and that forecasting naturally benefits from learned causal knowledge. Specifically, we allow changes in both causal strengths and noise variances in the nonlinear state-space models, which, interestingly, renders both the causal structure and model parameters identifiable. Given the causal model, we treat forecasting as a problem in Bayesian inference in the causal model, which exploits the time-varying property of the data and adapts to new observations in a principled manner.  Experimental results on synthetic and real-world data sets demonstrate the efficacy of the proposed methods.

","['Carnegie Mellon University', 'Carnegie Mellon University', 'University of Pittsburgh & CMU', 'Carnegie Mellon University']"
2019,Classifying Treatment Responders Under Causal Effect Monotonicity,Nathan Kallus,https://icml.cc/Conferences/2019/Schedule?showEvent=4296,"In the context of individual-level causal inference, we study the problem of predicting whether someone will respond or not to a treatment based on their features and past examples of features, treatment indicator (e.g., drug/no drug), and a binary outcome (e.g., recovery from disease). As a classification task, the problem is made difficult by not knowing the example outcomes under the opposite treatment indicators. We assume the effect is monotonic, as in advertising's effect on a purchase or bail-setting's effect on reappearance in court: either it would have happened regardless of treatment, not happened regardless, or happened only depending on exposure to treatment. Predicting whether the latter is latently the case is our focus. While previous work focuses on conditional average treatment effect estimation, formulating the problem as a classification task allows us to develop new tools more suited to this problem. By leveraging monotonicity, we develop new discriminative and generative algorithms for the responder-classification problem. We explore and discuss connections to corrupted data and policy learning. We provide an empirical study with both synthetic and real datasets to compare these specialized algorithms to standard benchmarks.
",['Cornell University']
2019,Learning Models from Data with Measurement Error: Tackling Underreporting,"Roy Adams, Yuelong Ji, Xiaobin Wang, Suchi Saria",https://icml.cc/Conferences/2019/Schedule?showEvent=4067,"Measurement error in observational datasets can lead to systematic bias in inferences based on these datasets. As studies based on observational data are increasingly used to inform decisions with real-world impact, it is critical that we develop a robust set of techniques for analyzing and adjusting for these biases. In this paper we present a method for estimating the distribution of an outcome given a binary exposure that is subject to underreporting. Our method is based on a missing data view of the measurement error problem, where the true exposure is treated as a latent variable that is marginalized out of a joint model. We prove three different conditions under which the outcome distribution can still be identified from data containing only error-prone observations of the exposure. We demonstrate this method on synthetic data and analyze its sensitivity to near violations of the identifiability conditions. Finally, we use this method to estimate the effects of maternal smoking and heroin use during pregnancy on childhood obesity, two import problems from public health. Using the proposed method, we estimate these effects using only subject-reported drug use data and refine the range of estimates generated by a sensitivity analysis-based approach. Further, the estimates produced by our method are consistent with existing literature on both the effects of maternal smoking and the rate at which subjects underreport smoking.
","['Johns Hopkins University', 'Johns Hopkins University', 'Johns Hopkins University', 'Johns Hopkins University']"
2019,Adjustment Criteria for Generalizing Experimental Findings,"Juan Correa, Jin Tian, Elias Bareinboim",https://icml.cc/Conferences/2019/Schedule?showEvent=4283,"Generalizing causal effects from a controlled experiment to settings beyond the particular study population is arguably one of the central tasks found in empirical circles. 
While a proper design and careful execution of the experiment would  support, under mild conditions, the validity of inferences about the population in which the experiment was conducted, two challenges make the extrapolation step to different populations somewhat involved, namely, transportability and sampling selection bias. 
The former is concerned with disparities in the distributions and causal mechanisms between the domain (i.e., settings, population, environment) where the experiment is conducted and where the inferences are intended; the latter with distortions in the sample's proportions due to preferential selection of units into the study. 
In this paper, we investigate the assumptions and machinery necessary for using \textit{covariate adjustment} to correct for the biases generated by both of these problems, and generalize experimental data to infer causal effects in a new domain. We derive complete graphical conditions to determine if a set of covariates is admissible for adjustment in this new setting. Building on the graphical characterization, we develop an efficient algorithm that enumerates all possible admissible sets with poly-time delay guarantee; this can be useful for when some variables are preferred over the others due to different costs or amenability to measurement.
","['Purdue University', 'Iowa State University', 'Purdue']"
2019,Conditional Independence in Testing Bayesian Networks,"Yujia Shen, Haiying Huang, Arthur Choi, Adnan Darwiche",https://icml.cc/Conferences/2019/Schedule?showEvent=4324,"Testing Bayesian Networks (TBNs) were introduced recently to represent a set of distributions, one of which is selected based on the given evidence and used for reasoning. TBNs are more expressive than classical Bayesian Networks (BNs): Marginal queries correspond to multi-linear functions in BNs and to piecewise multi-linear functions in TBNs. Moreover, TBN queries are universal approximators, like neural networks. In this paper, we study conditional independence in TBNs, showing that it can be inferred from d-separation as in BNs. We also study the role of TBN expressiveness and independence in dealing with the problem of learning with incomplete models (i.e., ones that miss nodes or edges from the data-generating model). Finally, we illustrate our results on a number of concrete examples, including a case study on Hidden Markov Models.
","['UCLA', 'UCLA', 'UCLA', 'UCLA']"
2019,Sensitivity Analysis of Linear Structural Causal Models,"Carlos Cinelli, Daniel Kumor, Bryant Chen, Judea Pearl, Elias Bareinboim",https://icml.cc/Conferences/2019/Schedule?showEvent=4243,"Causal inference requires assumptions about the data generating process, many of which are unverifiable from the data. Given that some causal assumptions might be uncertain or disputed, formal methods are needed to quantify how sensitive research conclusions are to violations of those assumptions. Although an extensive literature exists on the topic, most results are limited to specific model structures, while a general-purpose algorithmic framework for sensitivity analysis is still lacking. In this paper, we develop a formal, systematic approach to sensitivity analysis for arbitrary linear Structural Causal Models (SCMs).  We start by formalizing sensitivity analysis as a constrained identification problem. We then develop an efficient, graph-based identification algorithm that exploits non-zero constraints on both directed and bidirected edges. This allows researchers to systematically derive sensitivity curves for a target causal quantity with an arbitrary set of path coefficients and error covariances as sensitivity parameters. These results can be used to display the degree to which violations of causal assumptions affect the target quantity of interest, and to judge, on scientific grounds, whether problematic degrees of violations are plausible.
","['UCLA', 'Purdue University', 'Brex', 'UCLA', 'Purdue']"
2019,More Efficient Off-Policy Evaluation through Regularized Targeted Learning,"Aurelien Bibaut, Ivana Malenica, Nikos Vlassis, Mark van der Laan",https://icml.cc/Conferences/2019/Schedule?showEvent=4251,"We study the problem of off-policy evaluation (OPE) in Reinforcement Learning (RL), where the aim is to estimate the performance of a new policy given historical data that may have been generated by a different policy, or policies. In particular, we introduce a novel doubly-robust estimator for the OPE problem in RL, based on the Targeted Maximum Likelihood Estimation principle from the statistical causal inference literature. We also introduce several variance reduction techniques that lead to impressive performance gains in off-policy evaluation. We show empirically that our estimator uniformly wins over existing off-policy evaluation methods across multiple RL environments and various levels of model misspecification. Finally, we further the existing theoretical analysis of estimators for the RL off-policy estimation problem by showing their $O_P(1/\sqrt{n})$ rate of convergence and characterizing their asymptotic distribution.","['UC Berkeley', 'UC Berkeley', 'Netflix', 'UC Berkeley']"
2019,Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding,"Muhammad Osama, Dave Zachariah, Thomas Schön",https://icml.cc/Conferences/2019/Schedule?showEvent=3804,"We address the problem of inferring the causal effect of an exposure on an outcome across space, using observational data. The data is possibly subject to unmeasured confounding variables which, in a standard approach, must be adjusted for by estimating a nuisance function. Here we develop a method that eliminates the nuisance function, while mitigating the resulting errors-in-variables. The result is a robust and accurate inference method for spatially varying heterogeneous causal effects. The properties of the method are demonstrated on synthetic as well as real data from Germany and the US.
","['Uppsala University', 'Uppsala University', 'Uppsala University']"
2019,Adversarially Learned Representations for Information Obfuscation and Inference,"Martin A Bertran, Natalia Martinez Gil, Afroditi Papadaki, Qiang Qiu, Miguel Rodrigues, Galen Reeves, Guillermo Sapiro",https://icml.cc/Conferences/2019/Schedule?showEvent=4236,"Data collection and sharing are pervasive aspects of modern society. This process can either be voluntary, as in the case of a person taking a facial image to unlock his/her phone, or incidental, such as traffic cameras collecting videos on pedestrians. An undesirable side effect of these processes is that shared data can carry information about attributes that users might consider as sensitive, even when such information is of limited use for the task. It is therefore desirable for both data collectors and users to design procedures that minimize sensitive information leakage. Balancing the competing objectives of providing meaningful individualized service levels and inference while obfuscating sensitive information is still an open problem. In this work, we take an information theoretic approach that is implemented as an unconstrained adversarial game between Deep Neural Networks in a principled, data-driven manner. This approach enables us to learn domain-preserving stochastic transformations that maintain performance on existing algorithms while minimizing sensitive information leakage.
","['Duke University', 'Duke University', 'University College London', 'Duke University', 'University College London', 'Duke', 'Duke University']"
2019,Adaptive Neural Trees,"Ryutaro Tanno, Kai Arulkumaran, Daniel Alexander, Antonio Criminisi, Aditya Nori",https://icml.cc/Conferences/2019/Schedule?showEvent=3645,"Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs), a model that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the predictive task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.
","['University College London', 'Imperial College London', 'University College London', 'Microsoft', 'Microsoft Research Cambridge']"
2019,Connectivity-Optimized Representation Learning via Persistent Homology,"Christoph Hofer, Roland Kwitt, Marc Niethammer, Mandar Dixit",https://icml.cc/Conferences/2019/Schedule?showEvent=3818,"We study the problem of learning representations with controllable connectivity properties. This is beneficial in situations when the imposed structure can be leveraged upstream. In particular, we control the connectivity of an autoencoder's latent space via a novel type of loss, operating on information from persistent homology. Under mild conditions, this loss is differentiable and we present a theoretical analysis of the properties induced by the loss. We choose one-class learning as our upstream task and demonstrate that the imposed structure enables informed parameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on computer vision data, these one-class models exhibit competitive performance and, in a low sample size regime, outperform other methods by a large margin. Notably, our results indicate that a single 
autoencoder, trained on auxiliary (unlabeled) data, yields a mapping into latent space that can be reused across datasets for one-class learning.
","['University of Salzburg', 'University of Salzburg', 'UNC', 'Microsoft']"
2019,Minimal Achievable Sufficient Statistic Learning,"Milan Cvitkovic, Günther Koliander",https://icml.cc/Conferences/2019/Schedule?showEvent=4189,"We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a machine learning training objective for which the minima are minimal sufficient statistics with respect to a class of functions being optimized over (e.g., deep networks). In deriving MASS Learning, we also introduce Conserved Differential Information (CDI), an information-theoretic quantity that — unlike standard mutual information — can be usefully applied to deterministically-dependent continuous random variables like the input and output of a deep network. In a series of experiments, we show that deep networks trained with MASS Learning achieve competitive performance on supervised learning, regularization, and uncertainty quantification benchmarks.
","['California Institute of Technology', 'Austrian Academy of Sciences']"
2019,Learning to Route in Similarity Graphs,"Dmitry Baranchuk, Dmitry Persiyanov, Anton Sinitsin, Artem Babenko",https://icml.cc/Conferences/2019/Schedule?showEvent=4084,"Recently similarity graphs became the leading paradigm for efficient nearest neighbor search, outperforming traditional tree-based and LSH-based methods. Similarity graphs perform the search via greedy routing: a query traverses the graph and in each vertex moves to the adjacent vertex that is the closest to this query. In practice, similarity graphs are often susceptible to local minima, when queries do not reach its nearest neighbors, getting stuck in suboptimal vertices. In this paper we propose to learn the routing function that overcomes local minima via incorporating information about the graph global structure. In particular, we augment the vertices of a given graph with additional representations that are learned to provide the optimal routing from the start vertex to the query nearest neighbor. By thorough experiments, we demonstrate that the proposed learnable routing successfully diminishes the local minima problem and significantly improves the overall search performance.
","['Yandex', 'Yandex', 'HSE', 'Yandex']"
2019,Invariant-Equivariant Representation Learning for Multi-Class Data,Ilya Feige,https://icml.cc/Conferences/2019/Schedule?showEvent=3719,"Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach is based primarily on the strategic routing of data through the two latent variables, and thus is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.
",['Faculty']
2019,Infinite Mixture Prototypes for Few-shot Learning,"Kelsey Allen, Evan Shelhamer, Hanul Shin, Josh Tenenbaum",https://icml.cc/Conferences/2019/Schedule?showEvent=4256,"We propose infinite mixture prototypes to adaptively represent both simple and complex data distributions for few-shot learning. Infinite mixture prototypes combine deep representation learning with Bayesian nonparametrics, representing each class by a set of clusters, unlike existing prototypical methods that represent each class by a single cluster. By inferring the number of clusters, infinite mixture prototypes interpolate between nearest neighbor and prototypical representations in a learned feature space, which improves accuracy and robustness in the few-shot regime. We show the importance of adaptive capacity for capturing complex data distributions such as super-classes (like alphabets in character recognition), with 10-25% absolute accuracy improvements over prototypical networks, while still maintaining or improving accuracy on  standard few-shot learning benchmarks. By clustering labeled and unlabeled data with the same rule, infinite mixture prototypes achieve state-of-the-art semi-supervised accuracy, and can perform purely unsupervised clustering, unlike existing fully- and semi-supervised prototypical methods.
","['Massachusetts Institute of Technology', 'UC Berkeley', 'Massachusetts Institute of Technology', 'MIT']"
2019,MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing,"Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, Aram Galstyan",https://icml.cc/Conferences/2019/Schedule?showEvent=3702,"Existing popular methods for semi-supervised learning with Graph Neural Networks (such as the Graph Convolutional Network) provably cannot learn a general class of neighborhood mixing relationships. To address this weakness, we propose a new model, MixHop, that can learn these relationships, including difference operators, by repeatedly mixing feature representations of neighbors at various distances. MixHop requires no additional memory or computational complexity, and outperforms on challenging baselines. In addition, we propose sparsity regularization that allows us to visualize how the network prioritizes neighborhood information across different graph datasets. Our analysis of the learned architectures reveals that neighborhood mixing varies per datasets.
","['USC Information Sciences Institute', 'Google AI', 'Google Research', 'University of Southern California', 'ISI, University of Southern California', 'University of Southern California', 'University of Southern California', 'USC ISI']"
2019,Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting,"Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, Caiming Xiong",https://icml.cc/Conferences/2019/Schedule?showEvent=3842,"Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. 
Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.  
","['NC State University', 'Salesforce Research', 'NC State University', 'Salesforce', 'Salesforce']"
2019,Exploration Conscious Reinforcement Learning Revisited,"Lior Shani, Yonathan Efroni, Shie Mannor",https://icml.cc/Conferences/2019/Schedule?showEvent=3577,"The Exploration-Exploitation tradeoff arises in Reinforcement Learning when one cannot tell if a policy is optimal. Then, there is a constant need to explore new actions instead of exploiting past experience. In practice, it is common to resolve the tradeoff by using a fixed exploration mechanism, such as  $\epsilon$-greedy exploration or by adding Gaussian noise, while still trying to learn an optimal policy. In this work, we take a different approach and study exploration-conscious criteria, that result in optimal policies with respect to the exploration mechanism. Solving these criteria, as we establish, amounts to solving a surrogate Markov Decision Process. We continue and analyze properties of exploration-conscious optimal policies and characterize two general approaches to solve such criteria. Building on the approaches, we apply simple changes in existing tabular and deep Reinforcement Learning algorithms and empirically demonstrate superior performance relatively to their non-exploration-conscious counterparts, both for discrete and continuous action spaces.","['Technion', 'Technion', 'Technion']"
2019,Complexity of Linear Regions in Deep Networks,"Boris Hanin, David Rolnick",https://icml.cc/Conferences/2019/Schedule?showEvent=3567,"It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the number of linear regions grows exponentially with depth; it is not clear where within this range most networks fall in practice, either before or after training. In this paper, we provide a mathematical framework to count the number of linear regions of a piecewise linear network and measure the volume of the boundaries between these regions. In particular, we prove that for networks at initialization, the average number of regions along any one-dimensional subspace grows linearly in the total number of neurons, far below the exponential upper bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exponential, an intuition that matches our empirical observations. We conclude that the practical expressivity of neural networks is likely far below that of the theoretical maximum, and that this gap can be quantified.
","['Texas A&M and Facebook AI Research', 'University of Pennsylvania']"
2019,On Connected Sublevel Sets in Deep Learning,Quynh Nguyen,https://icml.cc/Conferences/2019/Schedule?showEvent=4246,"This paper shows that every sublevel set of the loss function of a class of deep over-parameterized neural nets with piecewise linear activation functions is connected and unbounded. This implies that the loss has no bad local valleys and all of its global minima are connected within a unique and potentially very large global valley.
",['Saarland University']
2019,Adversarial Examples Are a Natural Consequence of Test Error in Noise,"Justin Gilmer, Nicolas Ford, Nicholas Carlini, Ekin Dogus Cubuk",https://icml.cc/Conferences/2019/Schedule?showEvent=3917,"Over the last few years, the phenomenon of \emph{adversarial examples} --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, and therefore the adversarial robustness and corruption robustness research programs are closely related. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.

","['Google Brain', 'Google Brain', 'Google', 'Google Brain']"
2019,Greedy Layerwise Learning Can Scale To ImageNet,"Eugene Belilovsky, Michael Eickenberg, Edouard Oyallon",https://icml.cc/Conferences/2019/Schedule?showEvent=3588,"Shallow supervised 1-hidden layer neural networks have a number of favorable properties that make them easier to interpret, analyze, and optimize than their deep counterparts, but lack their representational power. Here we use 1-hidden layer learning problems to sequentially build deep networks layer by layer, which can inherit properties from shallow networks. Contrary to previous approaches using shallow networks, we focus on problems where deep learning is reported as critical for success. We thus study CNNs on image classification tasks using the large-scale ImageNet dataset and the CIFAR-10 dataset. Using a simple set of ideas for architecture and training we find that solving sequential 1-hidden-layer auxiliary problems lead to a CNN that exceeds AlexNet performance on ImageNet. Extending this training methodology to construct individual layers by solving 2-and-3-hidden layer auxiliary problems, we obtain an 11-layer network that exceeds several members of the VGG model family on ImageNet, and can train a VGG-11 model to the same accuracy as end-to-end learning. To our knowledge, this is the first competitive alternative to end-to-end training of CNNs that can scale to ImageNet. We illustrate several interesting properties of these models and conduct a range of experiments to study the properties this training induces on the intermediate layers.
","['Mila, University of Montreal', 'UC Berkeley', 'CentraleSupélec']"
2019,On the Impact of the Activation function on Deep Neural Networks Training,"Soufiane Hayou, Arnaud Doucet, Judith Rousseau",https://icml.cc/Conferences/2019/Schedule?showEvent=3911,"The weight initialization and the activation function of deep neural networks have a crucial impact on the performance of the training procedure. An inappropriate selection can lead to the loss of information of the input during forward propagation and the exponential vanishing/exploding of gradients during back-propagation. Understanding the theoretical properties of untrained random networks is key to identifying which deep networks may be trained successfully as recently demonstrated by Samuel et al. (2017) who showed that for deep feedforward neural networks only a specific choice of hyperparameters known as the `Edge of Chaos' can lead to good performance. While the work by Samuel et al. (2017) discuss trainability issues, we focus here on training acceleration and overall performance. We give a comprehensive theoretical analysis of the Edge of Chaos and show that we can indeed tune the initialization parameters and the activation function in order to accelerate the training and improve the performance.
","['University of Oxford', 'Oxford University', 'University of Oxford']"
2019,Estimating Information Flow in Deep Neural Networks,"Ziv Goldfeld, Ewout van den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury, Yury Polyanskiy",https://icml.cc/Conferences/2019/Schedule?showEvent=4128,"We study the estimation of the mutual information I(X;Tℓ) between the input X to a deep neural network (DNN) and the output vector Tℓ of its ℓ-th hidden layer (an “internal representation”). Focusing on feedforward networks with fixed weights and noisy internal representations, we develop a rigorous framework for accurate estimation of I(X;Tℓ). By relating I(X;Tℓ) to information transmission over additive white Gaussian noise channels, we reveal that compression, i.e. reduction in I(X;Tℓ) over the course of training, is driven by progressive geometric clustering of the representations of samples from the same class. Experimental results verify this connection. Finally, we shift focus to purely deterministic DNNs, where I(X;Tℓ) is provably vacuous, and show that nevertheless, these models also cluster inputs belonging to the same class. The binning-based approximation of I(X;T_ℓ) employed in past works to measure compression is identified as a measure of clustering, thus clarifying that these experiments were in fact tracking the same clustering phenomenon. Leveraging the clustering perspective, we provide new evidence that compression and generalization may not be causally related and discuss potential future research ideas.
","['MIT', 'IBM', 'IBM', 'IBM', 'IBM Research AI', 'IBM Research', 'MIT']"
2019,The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects,"Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, Jinwen Ma",https://icml.cc/Conferences/2019/Schedule?showEvent=3792,"Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. 
Along this line, we  study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics.
Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function.
Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency.
We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to  escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well.
We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).
","['Peking University', 'Johns Hopkins University', 'Peking University', 'Princeton University', 'Peking University']"
2019,Characterizing Well-Behaved vs. Pathological Deep Neural Networks,Antoine Labatie,https://icml.cc/Conferences/2019/Schedule?showEvent=4205,"We introduce a novel approach, requiring only mild assumptions, for the characterization of deep neural networks at initialization. Our approach applies both to fully-connected and convolutional networks and easily incorporates batch normalization and skip-connections. Our key insight is to consider the evolution with depth of statistical moments of signal and noise, thereby characterizing the presence or absence of pathologies in the hypothesis space encoded by the choice of hyperparameters. We establish: (i) for feedforward networks, with and without batch normalization, the multiplicativity of layer composition inevitably leads to ill-behaved moments and pathologies; (ii) for residual networks with batch normalization, on the other hand, skip-connections induce power-law rather than exponential behaviour, leading to well-behaved moments and no pathology.
",['Labatie-AI']
2019,Understanding Geometry of Encoder-Decoder CNNs,"Jong Chul Ye, woonkyoung Sung",https://icml.cc/Conferences/2019/Schedule?showEvent=4095,"Encoder-decoder networks using convolutional neural network (CNN) architecture have been extensively used in deep learning literatures thanks to its excellent performance for various inverse problems in computer vision, medical imaging, etc.
However,  it is still difficult to obtain coherent geometric view why such an architecture gives the desired performance. Inspired by recent theoretical understanding on  generalizability, expressivity and optimization landscape of neural networks, as well as the theory of   convolutional framelets, here we provide a unified theoretical framework  that leads to a better understanding of geometry of encoder-decoder CNNs. Our unified mathematical framework shows that encoder-decoder CNN architecture is closely related to nonlinear basis representation
using  combinatorial convolution frames, whose expressibility increases exponentially with the network depth. We also  demonstrate  the importance of skipped connection  in terms of expressibility,  and  optimization landscape. 
","['""Department of Bio and Brain Engineering, KAIST, Korea""', 'KAIST']"
2019,Traditional and Heavy Tailed Self Regularization in Neural Network Models,"Michael Mahoney, Charles H Martin",https://icml.cc/Conferences/2019/Schedule?showEvent=3986,"Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify \emph{5+1 Phases of Training}, corresponding to increasing amounts of \emph{Implicit Self-Regularization}.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a ``size scale'' separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of \emph{Heavy-Tailed Self-Regularization}, similar to the self-organization seen in the statistical physics of disordered systems.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.
","['UC Berkeley', 'Calculation Consulting']"
2019,Almost surely constrained convex optimization,"Olivier Fercoq, Ahmet Alacaoglu, Ion Necoara, Volkan Cevher",https://icml.cc/Conferences/2019/Schedule?showEvent=4012,"We propose a stochastic gradient framework for solving stochastic composite convex optimization problems with (possibly) infinite number of linear inclusion constraints that need to be satisfied almost surely. We use smoothing and homotopy techniques to handle constraints without the need for matrix-valued projections. We show for our stochastic gradient algorithm $\mathcal{O}(\log(k)/\sqrt{k})$ convergence rate for general convex objectives and $\mathcal{O}(\log(k)/k)$ convergence rate for restricted strongly convex objectives. 
These rates are known to be optimal up to logarithmic factor, even without constraints. 
We conduct numerical experiments on basis pursuit, hard margin support vector machines and portfolio optimization problems and show that our algorithm achieves state-of-the-art practical performance.","['Télécom ParisTech, Université Paris-Saclay', 'EPFL', 'University Bucharest', 'EPFL']"
2019,Generalized Majorization-Minimization,"Sobhan Naderi Parizi, Kun He, Reza Aghajani, Stan Sclaroff, Pedro Felzenszwalb",https://icml.cc/Conferences/2019/Schedule?showEvent=3690,"Non-convex optimization is ubiquitous in machine learning. Majorization-Minimization (MM) is a powerful iterative procedure for optimizing non-convex functions that works by optimizing a sequence of bounds on the function. In MM, the bound at each iteration is required to touch the objective function at the optimizer of the previous bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and propose a new optimization framework, named Generalized Majorization-Minimization (G-MM), that is more flexible. For instance, G-MM can incorporate application-specific biases into the optimization procedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show empirically that they consistently outperform their MM counterparts in optimizing non-convex objectives. In particular, G-MM algorithms appear to be less sensitive to initialization.
","['Google Inc.', 'Facebook Reality Labs', 'University of California San Diego', 'Boston University', 'Brown University']"
2019,On the Computation and Communication Complexity of Parallel SGD with Dynamic Batch Sizes for Stochastic Non-Convex Optimization,"Hao Yu, rong jin",https://icml.cc/Conferences/2019/Schedule?showEvent=3781,"For SGD based distributed stochastic optimization, computation complexity, measured by the convergence rate in terms of the number of stochastic gradient calls, and communication complexity, measured by the number of inter-node communication rounds, are two most important performance metrics.  The classical data-parallel implementation of SGD over N workers can achieve linear speedup of its convergence rate but incurs an inter-node communication round at each batch. We study the benefit of using dynamically increasing batch sizes in parallel SGD for stochastic non-convex optimization by charactering the attained convergence rate and the required number of communication rounds.  We show that for stochastic non-convex optimization under the P-L condition, the classical data-parallel SGD with exponentially increasing batch sizes can achieve the fastest known $O(1/(NT))$ convergence with linear speedup using only $\log(T)$ communication rounds. For general stochastic non-convex optimization, we propose a Catalyst-like algorithm to achieve the fastest known $O(1/\sqrt{NT})$ convergence with only $O(\sqrt{NT}\log(\frac{T}{N}))$ communication rounds.  ","['Alibaba Group (US) Inc', 'alibaba group']"
2019,Simple Stochastic Gradient Methods for Non-Smooth Non-Convex Regularized Optimization,"Michael Metel, Akiko Takeda",https://icml.cc/Conferences/2019/Schedule?showEvent=3711,"Our work focuses on stochastic gradient methods for optimizing a smooth non-convex loss function with a non-smooth non-convex regularizer. Research on this class of problem is quite limited, and until recently no non-asymptotic convergence results have been reported. We present two simple stochastic gradient algorithms, for finite-sum and general stochastic optimization problems, which have superior convergence complexities compared to the current state-of-the-art. We also compare our algorithms' performance in practice for empirical risk minimization. 
","['RIKEN Center for Advanced Intelligence Project', 'The University of Tokyo / RIKEN']"
2019,Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization,"zhenxun zhuang, Ashok Cutkosky, Francesco Orabona",https://icml.cc/Conferences/2019/Schedule?showEvent=3935,"Stochastic Gradient Descent (SGD) has played a central role in machine learning. However, it requires a carefully hand-picked stepsize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a plethora of adaptive gradient-based algorithms have emerged to ameliorate this problem. In this paper, we propose new surrogate losses to cast the problem of learning the optimal stepsizes for the stochastic optimization of a non-convex smooth objective function onto an online convex optimization problem. This allows the use of no-regret online algorithms to compute optimal stepsizes on the fly. In turn, this results in a SGD algorithm with self-tuned stepsizes that guarantees convergence rates that are automatically adaptive to the level of noise.
","['Boston University', 'Google', 'Stony Brook University']"
2019,Efficient Dictionary Learning with Gradient Descent,"Dar Gilboa, Sam Buchanan, John Wright",https://icml.cc/Conferences/2019/Schedule?showEvent=3789,"Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. 
","['Columbia University', 'Columbia University', 'Columbia University, USA']"
2019,Plug-and-Play Methods Provably Converge with Properly Trained Denoisers,"Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, Wotao Yin",https://icml.cc/Conferences/2019/Schedule?showEvent=4180,"Plug-and-play (PnP) is a non-convex framework that integrates modern denoising priors, such as BM3D or deep learning-based denoisers, into ADMM or other proximal algorithms. An advantage of PnP is that one  can use pre-trained denoisers when there is not sufficient data for end-to-end training. Although PnP has been recently studied extensively with great empirical success, theoretical analysis addressing even the most basic question of convergence has been insufficient. In this paper, we theoretically establish convergence of PnP-FBS and PnP-ADMM, without using diminishing stepsizes, under a certain Lipschitz condition on the denoisers. We then propose real spectral normalization, a technique for training deep learning-based denoisers to satisfy the proposed Lipschitz condition. Finally, we present experimental results validating the theory.
","['University of California, Los Angeles', 'University of California, Los Angeles (UCLA)', 'TAMU', 'Texas A&M University', 'Texas A&M University', 'Alibaba US']"
2019,Riemannian adaptive stochastic gradient algorithms on matrix manifolds,"Hiroyuki Kasai, Pratik Kumar Jawanpuria, Bamdev Mishra",https://icml.cc/Conferences/2019/Schedule?showEvent=3989,"Adaptive stochastic gradient algorithms in the Euclidean space have attracted much attention lately. Such explorations on Riemannian manifolds, on the other hand, are relatively new, limited, and challenging. This is because of the intrinsic non-linear structure of the underlying manifold and the absence of a canonical coordinate system. In machine learning applications, however, most manifolds of interest are represented as matrices with notions of row and column subspaces. In addition, the implicit manifold-related constraints may also lie on such subspaces. For example, the Grassmann manifold is the set of column subspaces. To this end, such a rich structure should not be lost by transforming matrices to just a stack of vectors while developing optimization algorithms on manifolds. We propose novel stochastic gradient algorithms for problems on Riemannian matrix manifolds by adapting the row and column subspaces of gradients. Our algorithms are provably convergent and they achieve the convergence rate of order $O(log(T)/sqrt(T))$, where $T$ is the number of iterations. Our experiments illustrate that the proposed algorithms outperform existing Riemannian adaptive stochastic algorithms. ","['The University of Electro-Communications', 'Microsoft', 'Microsoft']"
2019,Stochastic Optimization for DC Functions and Non-smooth Non-convex Regularizers with Non-asymptotic Convergence,"Yi Xu, Qi Qi, Qihang Lin, rong jin, Tianbao Yang",https://icml.cc/Conferences/2019/Schedule?showEvent=3951,"Difference of convex (DC) functions cover a broad family of non-convex and possibly non-smooth and non-differentiable functions, and have wide applications in machine learning and statistics. Although deterministic algorithms for DC functions have been extensively studied, stochastic optimization that is more suitable for learning with big data remains under-explored. In this paper, we propose new stochastic optimization algorithms and study their first-order convergence theories for solving a broad family of DC functions. We improve the existing algorithms and theories of stochastic optimization for DC functions from both practical and theoretical perspectives. Moreover, we extend the proposed stochastic algorithms for DC functions to solve problems with a general non-convex non-differentiable regularizer, which does not necessarily have a DC decomposition but enjoys an efficient proximal mapping.  To the best of our knowledge, this is the first work that gives the first non-asymptotic convergence for solving non-convex optimization whose objective has a general non-convex non-differentiable regularizer.
","['The University of Iowa', 'The University of Iowa', 'Univ Iowa', 'alibaba group', 'The University of Iowa']"
2019,Alternating Minimizations Converge to Second-Order Optimal Solutions,"Qiuwei Li, Zhihui Zhu, Gongguo Tang",https://icml.cc/Conferences/2019/Schedule?showEvent=3745,"This work studies the second-order convergence for both standard alternating minimization and proximal alternating minimization. We show that under mild assumptions on the (nonconvex) objective function, both algorithms avoid strict saddles almost surely from random initialization. Together with known first-order convergence results, this implies both algorithms converge to a second-order stationary point. This solves an open problem for the second-order convergence of alternating minimization algorithms that have been widely used in practice to solve large-scale nonconvex problems due to their simple implementation, fast convergence, and superb empirical performance.
","['Colorado School of Mines', 'Johns Hopkins University', 'Colorado School of Mines']"
2019,Provably Efficient Imitation Learning from Observation Alone,"Wen Sun, Anirudh Vemula, Byron Boots, Drew Bagnell",https://icml.cc/Conferences/2019/Schedule?showEvent=3927,"We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-dependent policies by minimizing an Integral Probability Metric between the observation distributions of the expert policy and the learner. FAIL provably learns a near-optimal policy with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The resulting theory extends the domain of provably sample efficient learning algorithms beyond existing results that typically only consider tabular RL settings or settings that require access to a near-optimal reset distribution.  We also demonstrate the efficacy ofFAIL on multiple OpenAI Gym control tasks.
","['Carnegie Mellon University', 'CMU', 'Georgia Tech', 'Carnegie Mellon University']"
2019,Dead-ends and Secure Exploration in Reinforcement Learning,"Mehdi Fatemi, Shikhar Sharma, Harm van Seijen, Samira Ebrahimi Kahou",https://icml.cc/Conferences/2019/Schedule?showEvent=3651,"Many interesting applications of reinforcement learning (RL) involve MDPs that include numerous dead-end"" states. Upon reaching a dead-end state, the agent continues to interact with the environment in a dead-end trajectory before reaching an undesired terminal state, regardless of whatever actions are chosen. The situation is even worse when existence of many dead-end states is coupled with distant positive rewards from any initial state (we term this as Bridge Effect). Hence, conventional exploration techniques often incur prohibitively many training steps before convergence. To deal with the bridge effect, we propose a condition for exploration, called security. We next establish formal results that translate the security condition into the learning problem of an auxiliary value function. This new value function is used to capany"" given exploration policy and is guaranteed to make it secure. As a special case, we use this theory and introduce secure random-walk. We next extend our results to the deep RL settings by identifying and addressing two main challenges that arise. Finally, we empirically compare secure random-walk with standard benchmarks in two sets of experiments including the Atari game of Montezuma's Revenge. 
","['Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2019,Statistics and Samples in Distributional Reinforcement Learning,"Mark Rowland, Robert Dadashi, Saurabh Kumar, Remi Munos, Marc Bellemare, Will Dabney",https://icml.cc/Conferences/2019/Schedule?showEvent=3583,"We present a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomposed as the combination of some statistical estimator and a method for imputing a return distribution consistent with that set of statistics. With this new understanding, we are able to provide improved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based upon estimation of the expectiles of the return distribution. We compare EDRL with existing methods on a variety of MDPs to illustrate concrete aspects of our analysis, and develop a deep RL variant of the algorithm, ER-DQN, which we evaluate on the Atari-57 suite of games.
","['DeepMind', 'Google AI Residency Program', 'Google', 'DeepMind', 'Google Brain', 'DeepMind']"
2019,Hessian Aided Policy Gradient,"Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, Chao Mi",https://icml.cc/Conferences/2019/Schedule?showEvent=4137,"	Reducing the variance of estimators for policy gradient has long been the focus of reinforcement learning research.
	While classic algorithms like REINFORCE find an $\epsilon$-approximate first-order stationary point in $\OM({1}/{\epsilon^4})$ random trajectory simulations, no provable  improvement on the complexity has been made so far.
	This paper presents a Hessian aided policy gradient method with the first improved sample complexity of $\OM({1}/{\epsilon^3})$.
	While our method exploits information from the policy Hessian, it can be implemented in linear time with respect to the parameter dimension and is hence applicable to sophisticated DNN parameterization.
	Simulations on standard tasks validate the efficiency of our method.","['Zhejiang University', 'University of Pennsylvania', 'University of Pennsylvania', 'Zhejiang University', 'Zhejiang University']"
2019,Provably Efficient Maximum Entropy Exploration,"Elad Hazan, Sham Kakade, Karan Singh, Abby Van Soest",https://icml.cc/Conferences/2019/Schedule?showEvent=4322,"Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For example, one natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense.  We provide an efficient algorithm to optimize such such intrinsically defined objectives, when given access to a black box planning oracle (which is robust to function approximation). Furthermore, when restricted to the tabular setting where we have sample based access to the MDP, our proposed algorithm is provably efficient, both in terms of its sample and computational complexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.
","['Princeton University', 'University of Washington', 'Princeton University', 'Princeton University']"
2019,Combining parametric and nonparametric models for off-policy evaluation,"Omer Gottesman, Yao Liu, Scott Sussex, Emma Brunskill, Finale Doshi-Velez",https://icml.cc/Conferences/2019/Schedule?showEvent=4093,"We consider a model-based approach to perform batch off-policy evaluation in reinforcement learning. Our method takes a mixture-of-experts approach to combine parametric and non-parametric models of the environment such that the final value estimate has the least expected error. We do so by first estimating the local accuracy of each model and then using a planner to select which model to use at every time step as to minimize the return error estimate along entire trajectories. Across a variety of domains, our mixture-based approach outperforms the individual models alone as well as state-of-the-art importance sampling-based estimators.
","['Harvard University', 'Stanford University', 'Harvard University', 'Stanford University', 'Harvard University']"
2019,Sample-Optimal Parametric Q-Learning Using Linearly Additive Features,"Lin Yang, Mengdi Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=3991,"
Consider a Markov decision process (MDP) that admits a set of state-action features, which can linearly express the process's probabilistic transition model. We propose a parametric Q-learning algorithm that finds an approximate-optimal policy using a sample size proportional to the feature dimension $K$ and invariant with respect to the size of the state space. To further improve its sample efficiency, we exploit the monotonicity property and intrinsic noise structure of the Bellman operator, provided the existence of anchor state-actions that imply implicit non-negativity in the feature space. We augment the algorithm using techniques of variance reduction, monotonicity preservation, and confidence bounds. It is proved to find a policy which is $\epsilon$-optimal from any initial state with high probability using $\widetilde{O}(K/\epsilon^2(1-\gamma)^3)$ sample transitions for arbitrarily large-scale MDP with a discount factor $\gamma\in(0,1)$. A matching information-theoretical lower bound is proved, confirming the sample optimality of the proposed method with respect to all parameters (up to polylog factors).","['Princeton', 'Princeton University']"
2019,Transfer of Samples in Policy Search via Multiple Importance Sampling,"Andrea Tirinzoni, Mattia Salvini, Marcello Restelli",https://icml.cc/Conferences/2019/Schedule?showEvent=4094,"We consider the transfer of experience samples in reinforcement learning. Most of the previous works in this context focused on value-based settings, where transferring instances conveniently reduces to the transfer of (s,a,s',r) tuples. In this paper, we consider the more complex case of reusing samples in policy search methods, in which the agent is required to transfer entire trajectories between environments with different transition models. By leveraging ideas from multiple importance sampling, we propose robust gradient estimators that effectively achieve this goal, along with several techniques to reduce their variance. In the case where the transition models are known, we theoretically establish the robustness to the negative transfer for our estimators. In the case of unknown models, we propose a method to efficiently estimate them when the target task belongs to a finite set of possible tasks and when it belongs to some reproducing kernel Hilbert space. We provide empirical results to show the effectiveness of our estimators.
","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano']"
2019,Action Robust Reinforcement Learning and Applications in Continuous Control,"Chen Tessler, Chen Tessler, Yonathan Efroni, Shie Mannor",https://icml.cc/Conferences/2019/Schedule?showEvent=3585,"A policy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. 
Specifically, we consider two scenarios in which the agent attempts to perform an action $\action$, and (i) with probability $\alpha$, an alternative adversarial action $\bar \action$ is taken, or (ii) an adversary adds a perturbation to the selected action in the case of continuous action space. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrupt forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our approach to deep reinforcement learning (DRL) and provide extensive experiments in the various MuJoCo domains. 
Our experiments show that not only does our approach produce robust policies, but it also improves the performance in the absence of perturbations. 
This generalization indicates that action-robustness can be thought of as implicit regularization in RL problems.","['Technion', 'Technion', 'Technion', 'Technion']"
2019,Kernel-Based Reinforcement Learning in Robust Markov Decision Processes,"Shiau Hong Lim, Arnaud Autef",https://icml.cc/Conferences/2019/Schedule?showEvent=3984,"The robust Markov decision processes (MDP) framework aims to address the problem of parameter uncertainty due to model mismatch, approximation errors or even adversarial behaviors. It is especially relevant when deploying the learned policies in real-world applications. Scaling up the robust MDP framework to large or continuous state space remains a challenging problem. The use of function approximation in this case is usually inevitable and this can only amplify the problem of model mismatch and parameter uncertainties. It has been previously shown that, in the case of MDPs with state aggregation, the robust policies enjoy a tighter performance bound compared to standard solutions due to its reduced sensitivity to approximation errors. We extend these results to the much larger class of kernel-based approximators and show, both analytically and empirically that the robust policies can significantly outperform the non-robust counterpart.
","['IBM Research', 'Ecole Polytechnique']"
2019,Optimal Algorithms for Lipschitz Bandits with Heavy-tailed Rewards,"Shiyin Lu, Guanghui Wang, Yao Hu, Lijun Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3798,"We study Lipschitz bandits, where a learner repeatedly plays one arm from an infinite arm set and then receives a stochastic reward whose expectation is a Lipschitz function of the chosen arm. Most of existing work assume the reward distributions are bounded or at least sub-Gaussian, and thus do not apply to heavy-tailed rewards arising in many real-world scenarios such as web advertising and financial markets. To address this limitation, in this paper we relax the assumption on rewards to allow arbitrary distributions that have finite $(1+\epsilon)$-th moments for some $\epsilon \in (0, 1]$, and propose algorithms that enjoy a sublinear regret of $\widetilde{O}(T^{(d_z\epsilon + 1)/(d_z \epsilon + \epsilon + 1)})$ where $T$ is the time horizon and $d_z$ is the zooming dimension. The key idea is to exploit the Lipschitz property of the expected reward function by adaptively discretizing the arm set, and employ upper confidence bound policies with robust mean estimators designed for heavy-tailed distributions. Furthermore, we provide a lower bound for Lipschitz bandits with heavy-tailed rewards, and show that our algorithms are optimal in terms of $T$. Finally, we conduct numerical experiments to demonstrate the effectiveness of our algorithms.","['Nanjing University', 'Nanjing University', 'Alibaba Youku Cognitive and Intelligent Lab', 'Nanjing University']"
2019,Target Tracking for Contextual Bandits: Application to Demand Side Management,"Margaux Brégère, Pierre Gaillard, Yannig Goude, Gilles Stoltz",https://icml.cc/Conferences/2019/Schedule?showEvent=4050,"We propose a contextual-bandit approach for demand side management by offering price incentives. More precisely, a target mean consumption is set at each round and the mean consumption is modeled as a complex function of the distribution of prices sent and of some contextual variables such as the temperature, weather, and so on. The performance of our strategies is measured in quadratic losses through a regret criterion. We offer $T^{2/3}$ upper bounds on this regret (up to poly-logarithmic terms)---and even faster rates under stronger assumptions---for strategies inspired by standard strategies for contextual bandits (like LinUCB, see Li et al., 2010). Simulations on a real data set gathered by UK Power Networks, in which price incentives were offered, show that our strategies are effective and may indeed manage demand response by suitably picking the price levels.","['CNRS Université Paris-Sud, Inria Paris, EDF R&D', 'INRIA Paris', 'EDF Lab Paris-Saclay', 'Université paris Sud']"
2019,Correlated bandits or: How to minimize mean-squared error online,"Vinay Praneeth Boda, Prashanth L.A.",https://icml.cc/Conferences/2019/Schedule?showEvent=3870,"While the objective in traditional multi-armed bandit problems is to  find the arm with the highest mean, in many settings, finding an arm that best captures information about other arms is of interest.  This objective, however, requires learning the underlying correlation structure and not just the means.  Sensors placement for industrial surveillance and cellular network monitoring are a few applications, where the underlying correlation structure plays an important role. Motivated by such applications, we formulate the correlated bandit problem, where the objective is to find the arm with the lowest mean-squared error (MSE) in estimating all the arms. To this end, we derive first an MSE estimator based on sample variances/covariances and show that our estimator exponentially concentrates around the true MSE. Under a best-arm identification framework, we propose a successive rejects type algorithm and provide bounds on the probability of error in identifying the best arm. Using minimax theory, we also derive fundamental performance limits for the correlated bandit problem. 
","['LinkedIn Corp.', 'IIT Madras']"
2019,Stay With Me: Lifetime Maximization Through Heteroscedastic Linear Bandits With Reneging,"Ping-Chun Hsieh, Xi Liu, Anirban  Bhattacharya, P R Kumar",https://icml.cc/Conferences/2019/Schedule?showEvent=3928,"Sequential decision making for lifetime maximization is a critical problem in many real-world applications, such as medical treatment and portfolio selection. In these applications, a ``reneging'' phenomenon, where participants may disengage from future interactions after observing an unsatisfiable outcome, is rather prevalent. To address the above issue, this paper proposes a model of heteroscedastic linear bandits with reneging, which allows each participant to have a distinct ``satisfaction level,"" with any interaction outcome falling short of that level resulting in that participant reneging. Moreover, it allows the variance of the outcome to be context-dependent. Based on this model, we develop a UCB-type policy, namely HR-UCB, and prove that it achieves $\mathcal{O}\big(\sqrt{{T}(\log({T}))^{3}}\big)$ regret. Finally, we validate the performance of HR-UCB via simulations. ","['Texas A&M University', 'Texas A&M University', 'Texas A&M University', 'Texas A & M University']"
2019,"Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits","Branislav Kveton, Csaba Szepesvari, Sharan Vaswani, Zheng Wen, Tor Lattimore, Mohammad Ghavamzadeh",https://icml.cc/Conferences/2019/Schedule?showEvent=3694,"We propose a bandit algorithm that explores by randomizing its history of rewards. Specifically, it pulls the arm with the highest mean reward in a non-parametric bootstrap sample of its history with pseudo rewards. We design the pseudo rewards such that the bootstrap mean is optimistic with a sufficiently high probability. We call our algorithm Giro, which stands for garbage in, reward out. We analyze Giro in a Bernoulli bandit and derive a $O(K \Delta^{-1} \log n)$ bound on its $n$-round regret, where $\Delta$ is the difference in the expected rewards of the optimal and the best suboptimal arms, and $K$ is the number of arms. The main advantage of our exploration design is that it easily generalizes to structured problems. To show this, we propose contextual Giro with an arbitrary reward generalization model. We evaluate Giro and its contextual variant on multiple synthetic and real-world problems, and observe that it performs well.","['Google Research', 'DeepMind/University of Alberta', 'Mila, University of Montreal', 'Adobe Research', 'DeepMind', 'Facebook AI Research']"
2019,Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously,"Julian Zimmert, Haipeng Luo, Chen-Yu Wei",https://icml.cc/Conferences/2019/Schedule?showEvent=3810,"We develop the first general semi-bandit algorithm that simultaneously achieves $\mathcal{O}(\log T)$ regret for stochastic environments
and $\mathcal{O}(\sqrt{T})$ regret for adversarial environments
without knowledge of the regime or the number of rounds $T$.
The leading problem-dependent constants of our bounds are not only optimal in some worst-case sense studied previously,
but also optimal for two concrete instances of semi-bandit problems.
Our algorithm and analysis extend the recent work of (Zimmert & Seldin, 2019) for the special case of multi-armed bandits,
but importantly requires a novel hybrid regularizer designed specifically for semi-bandit.
Experimental results on synthetic data show that our algorithm indeed performs well uniformly over different environments.
We finally provide a preliminary extension of our results to the full bandit feedback.","['University of Copenhagen', 'University of Southern California', 'University of Southern California']"
2019,Bilinear Bandits with Low-rank Structure ,"Kwang-Sung Jun, Rebecca Willett, Stephen Wright, Robert Nowak",https://icml.cc/Conferences/2019/Schedule?showEvent=3780,"  We introduce the bilinear bandit problem with low-rank structure in which an action takes the form of a pair of arms from two different entity types, and the reward is a bilinear function of the known feature vectors of the arms.  The unknown in the problem is a $d_1$ by $d_2$ matrix $\mathbf{\Theta}^*$ that defines the reward, and has low rank $r \ll \min\{d_1,d_2\}$.  Determination of $\mathbf{\Theta}^*$ with this low-rank structure poses a significant challenge in finding the right exploration-exploitation tradeoff.  In this work, we propose a new two-stage algorithm called ``Explore-Subspace-Then-Refine'' (ESTR). The first stage is an explicit subspace exploration, while the second stage is a linear bandit algorithm called ``almost-low-dimensional OFUL'' (LowOFUL) that exploits and further refines the estimated subspace via a regularization technique.  We show that the regret of ESTR is $\widetilde{\mathcal{O}}((d_1+d_2)^{3/2} \sqrt{r T})$ where $\widetilde{\mathcal{O}}$ hides logarithmic factors and $T$ is the time horizon, which improves upon the regret of $\widetilde{\mathcal{O}}(d_1d_2\sqrt{T})$ attained for a na\""ive linear bandit reduction.  We conjecture that the regret bound of ESTR is unimprovable up to polylogarithmic factors, and our preliminary experiment shows that ESTR outperforms a na\""ive linear bandit reduction.","['Boston University', 'U Chicago', 'University of Wisconsin-Madison', 'University of Wisconsion-Madison']"
2019,Online Learning to Rank with Features,"Shuai Li, Tor Lattimore, Csaba Szepesvari",https://icml.cc/Conferences/2019/Schedule?showEvent=3782,"We introduce a new model for online ranking in which the click probability factors into an examination and attractiveness function and the attractiveness function is a linear function of a feature vector and an unknown parameter. Only relatively mild assumptions are made on the examination function. A novel algorithm for this setup is analysed, showing that the dependence on the number of items is replaced by a dependence on the dimension, allowing the new algorithm to handle a large number of items. When reduced to the orthogonal case, the regret of the algorithm improves on the state-of-the-art.
","['The Chinese University of Hong Kong', 'DeepMind', 'DeepMind/University of Alberta']"
2019,On the Design of Estimators for Bandit Off-Policy Evaluation,"Nikos Vlassis, Aurelien Bibaut, Maria Dimakopoulou, Tony Jebara",https://icml.cc/Conferences/2019/Schedule?showEvent=4218,"Off-policy evaluation is the problem of estimating the value of a target policy using data collected under a different policy. Given a base estimator for bandit off-policy evaluation and a parametrized class of control variates, we address the problem of computing a control variate in that class that reduces the risk of the base estimator. We derive the population risk as a function of the class parameters and we establish conditions that guarantee risk improvement. We present our main results in the context of multi-armed bandits, and we propose a simple design for contextual bandits that gives rise to an estimator that is shown to perform well in multi-class cost-sensitive classification datasets.
","['Netflix', 'UC Berkeley', 'Netflix', 'Netflix']"
2019,Dynamic Learning with Frequent New Product Launches: A Sequential Multinomial Logit Bandit Problem,"Junyu Cao, Wei Sun",https://icml.cc/Conferences/2019/Schedule?showEvent=3954,"Motivated by the phenomenon that companies introduce new products to keep abreast with customers' rapidly changing tastes, we consider a novel online learning setting where a profit-maximizing seller needs to learn customers' preferences through offering recommendations, which may contain existing products and new products that are launched in the middle of a selling period. We propose a sequential multinomial logit (SMNL) model to characterize customers' behavior when product recommendations are presented in tiers. For the offline version with known customers' preferences, we propose a polynomial-time algorithm and characterize the properties of the optimal tiered product recommendation. For the online problem, we propose a learning algorithm and quantify its regret bound. Moreover, we extend the setting to incorporate a constraint which ensures every new product is learned to a given accuracy. Our results demonstrate the tier structure can be used to mitigate the risks associated with learning new products.
","['University of California Berkeley', 'IBM Research']"
2019,Context-Aware Zero-Shot Learning for Object Recognition,"Eloi Zablocki, Patrick Bordes, Laure Soulier, Benjamin Piwowarski, Patrick Gallinari",https://icml.cc/Conferences/2019/Schedule?showEvent=4039,"Zero-Shot  Learning  (ZSL)  aims  at  classifying unlabeled objects by leveraging auxiliary knowledge, such as semantic representations. A limitation of previous approaches is that only intrinsic properties of objects, e.g. their visual appearance, are taken into account while their context, e.g. the surrounding objects in the image, is ignored. Following the intuitive principle that objects tend to be found in certain contexts but not others, we propose a new and challenging approach, context-aware ZSL, that leverages semantic representations in a new way to model the conditional likelihood of an object to appear in a given context. Finally, through extensive experiments conducted on Visual Genome, we show that contextual information can substantially improve the standard ZSL approach and is robust to unbalanced classes.
","['Sorbonne Université, LIP6', ""Laboratoire d'Informatique de PARIS VI"", 'Sorbonne Université', 'Sorbonne Université', 'LIP6, Sorbonne Universite']"
2019,Band-limited Training and Inference for Convolutional Neural Networks,"Adam Dziedzic, John Paparrizos, Sanjay Krishnan, Aaron Elmore, Michael Franklin",https://icml.cc/Conferences/2019/Schedule?showEvent=3587,"The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter applies to the entire frequency spectrum of the input data. We explore artificially constraining the frequency spectra of these filters and data, called band-limiting, during training. The frequency domain constraints apply to both the feed-forward and back-propagation steps. Experimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this compression scheme and results suggest that CNNs learn to leverage lower-frequency components. In particular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high prediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other compression schemes. 
","['University of Chicago', 'University of Chicago', 'U Chicago', 'University of Chicago', 'University of Chicago']"
2019,Learning Classifiers for Target Domain with Limited or No Labels,"Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama",https://icml.cc/Conferences/2019/Schedule?showEvent=3837,"In computer vision applications, such as domain adaptation (DA), few shot learning (FSL) and zero-shot learning (ZSL), we encounter new objects and environments, for which insufficient examples exist to allow for training “models from scratch,” and methods that adapt existing models, trained on the presented training environment, to the new scenario are required. We propose a novel visual attribute encoding method that encodes each image as a low-dimensional probability vector
composed of prototypical part-type probabilities. The prototypes are learnt to be representative of all training data. At test-time we utilize this encoding as an input to a classifier. At test-time we freeze the encoder and only learn/adapt the classifier component to limited annotated labels in FSL; new semantic attributes in ZSL. We conduct extensive experiments on benchmark datasets. Our method outperforms state-of-art methods trained for the specific contexts (ZSL, FSL, DA).
","['Boston University', 'Boston University', 'Boston University']"
2019,Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules,"Daniel Ho, Eric Liang, Peter Chen, Ion Stoica, Pieter Abbeel",https://icml.cc/Conferences/2019/Schedule?showEvent=3664,"A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.
","['UC Berkeley', 'UC Berkeley', 'Covariant.ai', 'UC Berkeley', 'UC Berkeley']"
2019,Anomaly Detection With Multiple-Hypotheses Predictions,"Duc Tam Nguyen, Zhongyu Lou, Michael Klar, Thomas Brox",https://icml.cc/Conferences/2019/Schedule?showEvent=3586,"In one-class-learning tasks, only the normal case (foreground) can be modeled with data, whereas the variation of all possible anomalies is too erratic to be described by samples. Thus, due to the lack of representative data, the wide-spread discriminative approaches cannot cover such learning tasks, and rather generative models,which attempt to learn the input density of the foreground, are used. However, generative models suffer from a large input dimensionality (as in images) and are typically inefficient learners.We propose to learn the data distribution of the foreground more efficiently with a multi-hypotheses autoencoder. Moreover, the model is criticized by a discriminator, which prevents artificial data modes not supported by data, and which enforces diversity across hypotheses. Our multiple-hypotheses-based anomaly detection framework allows the reliable identification of out-of-distribution samples. For anomaly detection on CIFAR-10, it yields up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%.
","['University of Freiburg', 'Bosch', 'Bosch', 'University of Freiburg']"
2019,Kernel Mean Matching for Content Addressability of GANs,"Wittawat Jitkrittum, Wittawat Jitkrittum, Patsorn Sangkloy, Muhammad Waleed Gondal, Amit Raj, James Hays, Bernhard Schölkopf",https://icml.cc/Conferences/2019/Schedule?showEvent=3606,"We propose a novel procedure which adds ""content-addressability"" to any given unconditional implicit model e.g., a generative adversarial network (GAN). The procedure allows users to control the generative process by specifying a set (arbitrary size) of desired examples based on which similar samples are generated from the model. The proposed approach, based on kernel mean matching, is applicable to any generative models which transform latent vectors to samples, and does not require retraining of the model. Experiments on various high-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our approach is able to generate images which are consistent with the input set, while retaining the image quality of the original model. To our knowledge, this is the first work that attempts to construct, at test time, a content-addressable generative model from a trained marginal model. 
","['Max Planck Institute for Intelligent Systems', 'Max Planck Institute for Intelligent Systems', 'Georgia Institution of Technology', 'Max Planck Institute for Intelligent Systems', 'Georgia Institute of Technology', 'Georgia Institute of Technology, USA', 'MPI for Intelligent Systems Tübingen, Germany']"
2019,Neural Inverse Knitting: From Images to Manufacturing Instructions,"Alexandre Kaspar, Tae-Hyun Oh, Liane Makatura, Petr Kellnhofer, Wojciech Matusik",https://icml.cc/Conferences/2019/Schedule?showEvent=3617,"Motivated by the recent potential of mass customization brought by whole-garment knitting machines, we introduce the new problem of automatic machine instruction generation using a single image of the desired physical product, which we apply to machine knitting. We propose to tackle this problem by directly learning to synthesize regular machine instructions from real images. We create a cured dataset of real samples with their instruction counterpart and propose to use synthetic images to augment it in a novel way. We theoretically motivate our data mixing framework and show empirical results suggesting that making real images look more synthetic is beneficial in our problem setup.
","['MIT CSAIL', 'MIT CSAIL', 'MIT', 'MIT', 'MIT']"
2019,Making Convolutional Networks Shift-Invariant Again,Richard Zhang,https://icml.cc/Conferences/2019/Schedule?showEvent=4179,"Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks leads to performance degradation; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling. The technique is general and can be incorporated across layer types and applications, such as image classification and conditional image generation. In addition to increased shift-invariance, we also observe, surprisingly, that anti-aliasing boosts accuracy in ImageNet classification, across several commonly-used architectures. This indicates that anti-aliasing serves as effective regularization. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks.
",['Adobe']
2019,Generative Modeling of Infinite Occluded Objects for Compositional Scene Representation,"Jinyang Yuan, Bin Li, Xiangyang Xue",https://icml.cc/Conferences/2019/Schedule?showEvent=4032,"We present a deep generative model which explicitly models object occlusions for compositional scene representation. Latent representations of objects are disentangled into location, size, shape, and appearance, and the visual scene can be generated compositionally by integrating these representations and an infinite-dimensional binary vector indicating presences of objects in the scene. By training the model to learn spatial dependences of pixels in the unsupervised setting, the number of objects, pixel-level segregation of objects, and presences of objects in overlapping regions can be estimated through inference of latent variables. Extensive experiments conducted on a series of specially designed datasets demonstrate that the proposed method outperforms two state-of-the-art methods when object occlusions exist.
","['Fudan University', 'Fudan University', 'Fudan University']"
2019,IMEXnet - A Forward Stable Deep Neural Network,"Eldad Haber, Keegan Lensink, Eran Treister, Lars Ruthotto",https://icml.cc/Conferences/2019/Schedule?showEvent=3861,"Deep convolutional neural networks have revolutionized many machine learning and computer vision tasks, however, some remaining key challenges limit their wider use. These challenges include improving the network's robustness to perturbations of the input image and the limited ``field of view'' of convolution operators. We introduce the IMEXnet that addresses these challenges by adapting semi-implicit methods for partial differential equations. Compared to similar explicit networks, such as residual networks, our network is more stable, which has recently shown to reduce the sensitivity to small changes in the input features and improve generalization. The addition of an implicit step connects all pixels in each channel of the image and therefore addresses the field of view problem while still being comparable to standard convolutions in terms of the number of parameters and computational complexity. We also present a new dataset for semantic segmentation and demonstrate the effectiveness of our architecture using the NYU Depth dataset.
","['University of British Columbia', 'The University of British Columbia', '', 'Emory University']"
2019,Do ImageNet Classifiers Generalize to ImageNet?,"Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar",https://icml.cc/Conferences/2019/Schedule?showEvent=4270,"We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly ""harder"" images than those found in the original test sets.
","['Berkeley', 'University of California Berkeley', 'University of California, Berkeley', 'UC Berkeley']"
2019,Exploring the Landscape of Spatial Robustness,"Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry",https://icml.cc/Conferences/2019/Schedule?showEvent=4080,"The study of adversarial robustness has so far largely focused on perturbations
bound in $\ell_p$-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of
neural network--based classifiers to rotations and translations. While data
augmentation offers relatively small robustness, we use ideas from robust
optimization and test-time input aggregation to significantly improve robustness.
Finally we find that, in contrast to the $\ell_p$-norm case, first-order
methods cannot reliably find worst-case perturbations. This highlights
spatial robustness as a fundamentally different setting requiring additional
study.
","['MIT', 'MIT', 'MIT', 'UC Berkeley', 'MIT']"
2019,Sever: A Robust Meta-Algorithm for Stochastic Optimization,"Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, Alistair Stewart",https://icml.cc/Conferences/2019/Schedule?showEvent=3572,"In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, possesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself, it only requires computing the top singular vector of a certain n×d matrix. We apply Sever on a drug design dataset and a spam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the spam dataset, with 1% corruptions, we achieved 7.4% test error, compared to 13.4%−20.5% for the baselines, and 3% error on the uncorrupted dataset. Similarly, on the drug design dataset, with 10% corruptions, we achieved 1.42 mean-squared error test error, compared to 1.51-2.33 for the baselines, and 1.23 error on the uncorrupted dataset.
","['USC', 'MIT', 'UCSD', 'Microsoft Research', 'University of California, Berkeley', 'University of Southern California']"
2019,Analyzing Federated Learning through an Adversarial Lens,"Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, Seraphin Calo ",https://icml.cc/Conferences/2019/Schedule?showEvent=3853,"Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server to train an overall global model. In this work, we explore how the federated learning setting gives rise to a new threat, namely model poisoning, which differs from traditional data poisoning. Model poisoning is carried out by an adversary controlling a small number of malicious agents (usually 1) with the aim of causing the global model to misclassify a set of chosen inputs with high conﬁdence. We explore a number of strategies to carry out this attack on deep neural networks, starting with targeted model poisoning using a simple boosting of the malicious agent’s update to overcome the effects of other agents. We also propose two critical notions of stealth to detect malicious updates. We bypass these by including them in the adversarial objective to carry out stealthy model poisoning. We improve its stealth with the use of an alternating minimization strategy which alternately optimizes for stealth and the adversarial objective. We also empirically demonstrate that Byzantine-resilient aggregation strategies are not robust to our attacks. Our results indicate that highly constrained adversaries can carry out model poisoning attacks while maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develop effective defense strategies.
","['Princeton University', 'IBM T. J. Watson Research Center', 'Princeton University', 'IBM Research']"
2019,Fairwashing: the risk of rationalization,"Ulrich AIVODJI, Hiromi Arai, Olivier Fortineau, Sébastien Gambs, Satoshi Hara, Alain Tapp",https://icml.cc/Conferences/2019/Schedule?showEvent=4244,"Black-box explanation is the problem of explaining how a machine learning model -- whose internal logic is hidden to the auditor and generally complex -- produces its outcomes. 
Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. 
While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. 
In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. 
Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. 
We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.
","['UQAM', 'RIKEN AIP', 'Ensta Paristech', 'UQAM', 'Osaka University', 'Université de Montréal']"
2019,Understanding the Origins of Bias in Word Embeddings,"Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, Richard Zemel",https://icml.cc/Conferences/2019/Schedule?showEvent=3620,"Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning 
systems can amplify stereotypes in important contexts. Although some methods have been developed to mitigate this problem, how word embedding biases arise during training is poorly understood. In this work we develop a technique to address this question. Given a word embedding, our method reveals how perturbing the training corpus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikipedia and New York Times corpora, and find it to be very accurate.
","['University of Toronto', 'University of Toronto', 'University of Toronto', 'Vector Institute']"
2019,Bias Also Matters: Bias Attribution for Deep Neural Network Explanation,"Shengjie Wang, Tianyi Zhou, Jeff Bilmes",https://icml.cc/Conferences/2019/Schedule?showEvent=3905,"The gradient of a deep neural network (DNN) w.r.t. the input provides information that can be used to explain the output prediction in terms of the input features and has been widely studied to assist in interpreting DNNs. In a linear model (i.e., g(x) = wx + b), the gradient corresponds to the weights w. Such a model can reasonably locally-linearly approximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The bias b, however, is usually overlooked in attribution methods. In this paper, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of predictions, it can also play a significant role in understanding DNN behavior. We propose a backpropagation-type algorithm “bias back-propagation (BBp)” that starts at the output layer and iteratively attributes the bias of each layer to its input nodes as well as combining the resulting bias term of the previous layer. Together with the backpropagation of the gradient generating w, we can fully recover the locally linear model g(x) = wx + b. In experiments, we show that BBp can generate complementary and highly interpretable explanations.
","['""University of Washington, Seattle""', 'University of Washington', 'UW']"
2019,Interpreting Adversarially Trained Convolutional Neural Networks,"Tianyuan Zhang, Zhanxing Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3778,"We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We  design systematic approaches to interpret AT-CNNs in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we find that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. 
We validate our hypothesis from two aspects.  First, we compare the salience maps of AT-CNNs and standard CNNs on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of CNNs is sensitive to dramatically different types of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data,  saturated images and patch-shuffled ones, and then evaluate the classification accuracy of AT-CNNs and normal CNNs on these datasets. 
Our findings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interpretation perspective. 
","['Peking University', 'Peking University']"
2019,Counterfactual Visual Explanations,"Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, Stefan Lee",https://icml.cc/Conferences/2019/Schedule?showEvent=4068,"In this work, we develop a technique to produce counterfactual visual explanations. Given a `query' image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c'$. To do this, we select a `distractor' image $I'$ that the system predicts as class $c'$ and identify spatial regions in $I$ and $I'$ such that replacing the identified region in $I$ with the identified region in $I'$ would push the system towards classifying $I$ as $c'$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples. ","['Georgia Tech', 'Siemens Corporate Technology', 'Siemens Corporation', 'Georgia Institute of Technology / Facebook AI Research', 'Georgia Tech & Facebook AI Research', 'Georgia Institute of Technology']"
2019,Data Poisoning Attacks on Stochastic Bandits,"Fang Liu, Ness Shroff",https://icml.cc/Conferences/2019/Schedule?showEvent=3961,"Stochastic multi-armed bandits form a class of online learning problems that have important applications in online recommendation systems, adaptive medical treatment, and many others. Even though potential attacks against these learning algorithms may hijack their behavior, causing catastrophic loss in real-world applications, little is known about adversarial attacks on bandit algorithms. In this paper, we propose a framework of offline attacks on bandit algorithms and study convex optimization based attacks on several popular bandit algorithms. We show that the attacker can force the bandit algorithm to pull a target arm with high probability by a slight manipulation of the rewards in the data. Then we study a form of online attacks on bandit algorithms and propose an adaptive attack strategy against any bandit algorithm without the knowledge of the bandit algorithm. Our adaptive attack strategy can hijack the behavior of the bandit algorithm to suffer a linear regret with only a logarithmic cost to the attacker. Our results demonstrate a significant security threat to stochastic bandits. 
","['The Ohio State University', 'The Ohio State University']"
2019,On the Convergence and Robustness of Adversarial Training,"Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, Quanquan Gu",https://icml.cc/Conferences/2019/Schedule?showEvent=3680,"Improving the robustness of deep neural networks (DNNs) to adversarial examples is an important yet challenging problem for secure deep learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max optimization problem, with the \textit{inner maximization} generating adversarial examples by maximizing the classification loss, and the \textit{outer minimization} finding model parameters by minimizing the loss on adversarial examples generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this paper, we propose such a criterion, namely First-Order Stationary Condition for constrained optimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examples found in the inner maximization. With FOSC, we find that to ensure better robustness, it is essential to use adversarial examples with better convergence quality at the \textit{later stages} of training. Yet at the early stages, high convergence quality adversarial examples are not necessary and may even lead to poor robustness. Based on these observations, we propose a \textit{dynamic} training strategy to gradually increase the convergence quality of the generated adversarial examples, which significantly improves the robustness of adversarial training. Our theoretical and empirical results show the effectiveness of the proposed method. 
","['Tsinghua University', 'The University of Melbourne', 'The University of Melbourne', 'JD AI Research', 'JD', 'University of California, Los Angeles']"
2019,Learning with Bad Training Data via Iterative Trimmed Loss Minimization,"Yanyao Shen, Sujay Sanghavi",https://icml.cc/Conferences/2019/Schedule?showEvent=4165,"In this paper, we study a simple and generic framework to tackle the problem of learning model parameters when a fraction of the training samples are corrupted. Our approach is motivated by a simple observation: in a variety of such settings, the evolution of training accuracy (as a function of training epochs) is different for clean samples and bad samples. We propose to iteratively minimize the trimmed loss, by alternating between (a) selecting  samples with lowest current loss, and (b)  retraining a model on only these samples. Analytically, we characterize the statistical performance and convergence rate of the algorithm for simple and natural linear and non-linear models. Experimentally, we demonstrate its effectiveness in three settings: (a) deep image classifiers with errors only in labels, (b) generative adversarial networks with bad training images, and (c) deep image classifiers with adversarial (image, label) pairs (i.e., backdoor attacks). For the well-studied setting of random label noise, our algorithm achieves  state-of-the-art performance without having access to any a-priori guaranteed clean samples. 
","['UT Austin', 'UT Austin']"
2019,On discriminative learning of prediction uncertainty,"Vojtech Franc, Daniel Prusa",https://icml.cc/Conferences/2019/Schedule?showEvent=4098,"In classification with a reject option, the classifier is allowed in uncertain cases to abstain from prediction. The classical cost based model of an optimal classifier with a reject option requires the cost of rejection to be defined explicitly. An alternative bounded-improvement model, avoiding the notion of the reject cost, seeks for a classifier with a guaranteed selective risk and maximal cover. We prove that both models share the same class of optimal strategies, and we provide an explicit relation between the reject cost and the target risk being the parameters of the two models. An optimal rejection strategy for both models is based on thresholding the conditional risk defined by posterior probabilities which are usually unavailable. We propose a discriminative algorithm learning an uncertainty function which preserves ordering of the input space induced by the conditional risk, and hence can be used to construct optimal rejection strategies.
","['Czech Technical University in Prague', 'Czech technical university in Prague']"
2019,Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels,"Pengfei Chen, Ben Liao, Guangyong Chen, Shengyu Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3593,"Noisy labels are ubiquitous in real-world datasets, which poses a challenge for robustly training deep neural networks (DNNs) as DNNs usually have the high capacity to memorize the noisy labels. In this paper, we find that the test accuracy can be quantitatively characterized in terms of the noise ratio in datasets. In particular, the test accuracy is a quadratic function of the noise ratio in the case of symmetric noise, which explains the experimental findings previously published. Based on our analysis, we apply cross-validation to randomly split noisy datasets, which identifies most samples that have correct labels. Then we adopt the Co-teaching strategy which takes full advantage of the identified samples to train DNNs robustly against noisy labels. Compared with extensive state-of-the-art methods, our strategy consistently improves the generalization performance of DNNs under both synthetic and real-world training noise.
","['The Chinese University of Hong Kong', 'Tencent', 'Tencent', 'Tencent; The Chinese University of Hong Kong']"
2019,Does Data Augmentation Lead to Positive Margin?,"Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, Dimitris Papailiopoulos",https://icml.cc/Conferences/2019/Schedule?showEvent=3770,"Data augmentation (DA) is commonly used during model training, as it significantly improves test error and model robustness. DA artificially expands the training set by applying random noise, rotations, crops, or even adversarial perturbations to the input data. Although DA is widely used, its capacity to provably improve robustness is not fully understood. In this work, we analyze the robustness that DA begets by quantifying the margin that DA enforces on empirical risk minimizers. We first focus on linear separators, and then a class of nonlinear models whose labeling is constant within small convex hulls of data points. We present lower bounds on the number of augmented data points required for non-zero margin, and show that commonly used DA techniques may only introduce significant margin after adding exponentially many points to the data set.
","['University of Wisconsin - Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'UW-Madison', 'University of Wisconsin-Madison']"
2019,Robust Learning from Untrusted Sources,"Nikola Konstantinov, Christoph H. Lampert",https://icml.cc/Conferences/2019/Schedule?showEvent=3816,"Modern machine learning methods often require more data for training than a single expert can provide. Therefore, it has become a standard procedure to collect data from multiple external sources, \eg via crowdsourcing. Unfortunately, the quality of these sources is not always guaranteed. As further complications, the data might be stored in a distributed way, or might even have to remain private. In this work, we address the question of how to learn robustly in such scenarios. Studying the problem through the lens of statistical learning theory, we derive a procedure that allows for learning from all available sources, yet automatically suppresses irrelevant or corrupted data. We show by extensive experiments that our method provides significant improvements over alternative approaches from robust statistics and distributed optimization.
","['IST Austria', 'IST Austria']"
2019,SELFIE: Refurbishing Unclean Samples for Robust Deep Learning,"Hwanjun Song, Minseok Kim, Jae-Gil Lee",https://icml.cc/Conferences/2019/Schedule?showEvent=3582,"Owing to the extremely high expressive power of deep neural networks, their side effect is to totally memorize training data even when the labels are extremely noisy. To overcome overfitting on the noisy labels, we propose a novel robust training method called SELFIE. Our key idea is to selectively refurbish and exploit unclean samples that can be corrected with high precision, thereby gradually increasing the number of available training samples. Taking advantage of this design, SELFIE effectively prevents the risk of noise accumulation from the false correction and fully exploits the training data. To validate the superiority of SELFIE, we conducted extensive experimentation using four real-world or synthetic data sets. The result showed that SELFIE remarkably improved absolute test error compared with two state-of-the-art methods.
","['KAIST', 'KAIST', 'KAIST']"
2019,Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance,"Cong Xie, Sanmi Koyejo, Indranil Gupta",https://icml.cc/Conferences/2019/Schedule?showEvent=3783,"We present Zeno, a technique to make distributed machine learning, particularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. Zeno generalizes previous results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to suspect workers that are potentially defective. Since this is likely to lead to false positives, we use a ranking-based preference mechanism. We prove the convergence of SGD for non-convex problems under these scenarios. Experimental results show that Zeno outperforms existing approaches.
","['UIUC', 'Illinois / Google', 'UIUC']"
2019,Concentration Inequalities for Conditional Value at Risk,"Philip Thomas, Erik Learned-Miller",https://icml.cc/Conferences/2019/Schedule?showEvent=4265,"In this paper we derive new concentration inequalities for the conditional value at risk (CVaR) of a random variable, and compare them to the previous state of the art (Brown, 2007).  We show analytically that our lower bound is strictly tighter than Brown's, and empirically that this difference is significant.  While our upper bound may be looser than Brown's in some cases, we show empirically that in most cases our bound is significantly tighter.  After discussing when each upper bound is superior, we conclude with empirical results which suggest that both of our bounds will often be significantly tighter than Brown's.
","['University of Massachusetts Amherst', 'University of Massachusetts, Amherst']"
2019,Data Poisoning Attacks in Multi-Party Learning,"Saeed Mahloujifar, Mohammad Mahmoody, Ameer Mohammed",https://icml.cc/Conferences/2019/Schedule?showEvent=3817,"In this work, we demonstrate universal multi-party poisoning attacks that adapt and apply to any multi-party learning process with arbitrary interaction pattern between the parties. More generally, we introduce and study $(k,p)$-poisoning attacks in which an adversary controls $k\in[m]$ of the parties, and for each corrupted party $P_i$, the adversary submits some poisoned data $T'_i$ on behalf of $P_i$ that is still ""$(1-p)$-close"" to the correct data $T_i$ (e.g., $1-p$ fraction of $T'_i$ is still honestly generated).We prove that for any ""bad"" property $B$ of the final trained hypothesis $h$ (e.g., $h$ failing on a particular test example or having ""large"" risk) that has an arbitrarily small constant probability of happening without the attack, there always is a $(k,p)$-poisoning attack that increases the probability of $B$ from $\mu$ to by $\mu^{1-p \cdot k/m} = \mu + \Omega(p \cdot k/m)$. Our attack only uses clean labels, and it is online, as it only knows the the data shared so far.","['University of Virginia', 'University of Virginia', 'Kuwait University']"
2019,Distributed Weighted Matching via Randomized Composable Coresets,"Sepehr Assadi, Mohammad Hossein Bateni, Vahab Mirrokni",https://icml.cc/Conferences/2019/Schedule?showEvent=4194,"Maximum weight matching is one of the most fundamental combinatorial optimization problems with a wide range of applications in data mining and bioinformatics. Developing distributed weighted matching algorithms has been challenging due to the sequential nature of efficient algorithms for this problem. In this paper, we develop a simple distributed algorithm for the problem on general graphs with approximation guarantee of 2 + eps that (nearly) matches that of the sequential greedy algorithm. A key advantage of this algorithm is that it can be easily implemented in only two rounds of computation in modern parallel computation frameworks such as MapReduce. We also demonstrate the efficiency of our algorithm in practice on various graphs (some with half a trillion edges) by achieving objective values always close to what is achievable in the centralized setting.
","['Princeton University', 'Google Research', 'Google Research']"
2019,Multivariate Submodular Optimization,"Richard Santiago, F. Bruce Shepherd",https://icml.cc/Conferences/2019/Schedule?showEvent=4248,"Submodular functions have found a wealth of new applications in data science and machine learning models in recent years. This has been coupled with many algorithmic advances in the area of submodular optimization: (SO) $\min/\max~f(S): S \in \mathcal{F}$, where $\mathcal{F}$ is a given family of feasible sets over a ground set $V$ and $f:2^V \rightarrow \mathbb{R}$ is submodular. In this work we focus on a more general class of \emph{multivariate submodular optimization} (MVSO) problems:  $\min/\max~f (S_1,S_2,\ldots,S_k):  S_1 \uplus S_2 \uplus \cdots \uplus S_k \in \mathcal{F}$. Here we use $\uplus$ to denote union of disjoint sets and hence this model is attractive where resources are being allocated across  $k$ agents, who share a ``joint'' multivariate nonnegative objective $f(S_1,S_2,\ldots,S_k)$ that captures some type of submodularity (i.e. diminishing returns) property. We provide some explicit examples and potential applications for this new framework. For maximization, we show that practical algorithms such as accelerated greedy variants and distributed algorithms achieve good approximation guarantees for very general families (such as matroids and $p$-systems). For arbitrary families, we show that monotone (resp. nonmonotone) MVSO admits an $\alpha (1-1/e)$ (resp. $\alpha \cdot 0.385$) approximation whenever monotone (resp. nonmonotone) SO admits an $\alpha$-approximation over the multilinear formulation. This substantially expands the family of tractable models. On the minimization side we give essentially optimal approximations in terms of the curvature of $f$.","['McGill University', 'University of British Columbia']"
2019,Beyond Adaptive Submodularity: Approximation Guarantees of Greedy Policy with Adaptive Submodularity Ratio,"Kaito Fujii, Shinsaku Sakaue",https://icml.cc/Conferences/2019/Schedule?showEvent=3723,"We propose a new concept named adaptive submodularity ratio to study the greedy policy for sequential decision making. While the greedy policy is known to perform well for a wide variety of adaptive stochastic optimization problems in practice, its theoretical properties have been analyzed only for a limited class of problems. We narrow the gap between theory and practice by using adaptive submodularity ratio, which enables us to prove approximation guarantees of the greedy policy for a substantially wider class of problems. Examples of newly analyzed problems include important applications such as adaptive influence maximization and adaptive feature selection. Our adaptive submodularity ratio also provides bounds of adaptivity gaps. Experiments confirm that the greedy policy performs well with the applications being considered compared to standard heuristics.
","['University of Tokyo', 'NTT']"
2019,Approximating Orthogonal Matrices with Effective Givens Factorization,"Thomas Frerix, Joan Bruna",https://icml.cc/Conferences/2019/Schedule?showEvent=3791,"We analyze effective approximation of unitary matrices. In our formulation, a unitary matrix is represented as a product of rotations in two-dimensional subspaces, so-called Givens rotations. Instead of the quadratic dimension dependence when applying a dense matrix, applying such an approximation scales with the number factors, each of which can be implemented efficiently. Consequently, in settings where an approximation is once computed and then applied many times, such a representation becomes advantageous. Although effective Givens factorization is not possible for generic unitary operators, we show that minimizing a sparsity-inducing objective with a coordinate descent algorithm on the unitary group yields good factorizations for structured matrices. Canonical applications of such a setup are orthogonal basis transforms. We demonstrate numerical results of approximating the graph Fourier transform, which is the matrix obtained when diagonalizing a graph Laplacian.
","['Technical University of Munich', 'New York University']"
2019,New results on information theoretic clustering,"Ferdinando Cicalese, Eduardo Laber, Lucas Murtinho",https://icml.cc/Conferences/2019/Schedule?showEvent=3668,"We study the problem of optimizing the clustering of a set of vectors when the quality of the clustering is measured by the Entropy or the Gini impurity measure.
Our results contribute to  the state of the art both in terms of best known approximation guarantees and inapproximability bounds: (i) we give the first polynomial time algorithm for Entropy impurity based clustering with approximation guarantee independent of the number of vectors and  (ii) we show that 
the problem of clustering based on entropy impurity does not admit a PTAS. 
This also implies  an inapproximability result in information theoretic clustering 
for probability distributions closing a  problem left open in [Chaudhury and McGregor, COLT08] and [Ackermann et al., ECCC11]. We also report 
experiments with  a new clustering method that was designed on  top of the 
theoretical tools leading to the above results. These experiments suggest a practical applicability for our method, in particular, when the number of clusters 
is large.
","['University of Verona', 'PUC-RIO', 'PUC-RJ']"
2019,Improved Parallel Algorithms for Density-Based Network Clustering,"Mohsen Ghaffari, Silvio Lattanzi, Slobodan Mitrović",https://icml.cc/Conferences/2019/Schedule?showEvent=3626,"Clustering large-scale networks is  a central topic in unsupervised learning with many applications in machine learning and data mining. A classic approach to cluster a network is to identify regions of high edge density, which in the literature is captured by two fundamental problems: the densest subgraph and the $k$-core decomposition problems. We design massively parallel computation (MPC) algorithms for these problems that are considerably faster than prior work. In the case of $k$-core decomposition, our work improves exponentially on the algorithm provided by Esfandiari et al.~(ICML'18). Compared to the prior work on densest subgraph presented by Bahmani et al.~(VLDB'12, '14), our result requires quadratically fewer MPC rounds. We complement our analysis with an experimental scalability analysis of our techniques.
","['ETH Zurich', 'Google Zurich', 'MIT']"
2019,Submodular Observation Selection and Information Gathering for Quadratic Models,"Abolfazl Hashemi, Mahsa Ghasemi, Haris Vikalo, Ufuk Topcu",https://icml.cc/Conferences/2019/Schedule?showEvent=4291,"We study the problem of selecting most informative subset of a large observation set to enable accurate estimation of unknown parameters. This problem arises in a variety of settings in machine learning and signal processing including feature selection, phase retrieval, and target localization. Since for quadratic measurement models the moment matrix of the optimal estimator is generally unknown, majority of prior work resorts to approximation techniques such as linearization of the observation model to optimize the alphabetical optimality criteria of an approximate moment matrix. Conversely, by exploiting a connection to the classical Van Trees' inequality, we derive new alphabetical optimality criteria without distorting the relational structure of the observation model. We further show that under certain conditions on parameters of the problem these optimality criteria are monotone and (weak) submodular set functions. These results enable us to develop an efficient greedy observation selection algorithm uniquely tailored for quadratic models, and provide theoretical bounds on its achievable utility.
","['University of Texas at Austin', 'The University of Texas at Austin', 'University of Texas at Austin', 'University of Texas at Austin']"
2019,Submodular Cost Submodular Cover with an Approximate Oracle,"Victoria Crawford, Alan Kuhnle, My T Thai",https://icml.cc/Conferences/2019/Schedule?showEvent=3591,"In this work, we study the Submodular Cost Submodular Cover problem, which is to minimize the submodular cost required to ensure that the submodular benefit function exceeds a given threshold. Existing approximation ratios for the greedy algorithm assume a value oracle to the benefit function. However, access to a value oracle is not a realistic assumption for many applications of this problem, where the benefit function is difficult to compute. We present two incomparable approximation ratios for this problem with an approximate value oracle and demonstrate that the ratios take on empirically relevant values through a case study with the Influence Threshold problem in online social networks.
","['University of Florida', 'Florida State University', 'University of Florida']"
2019,"Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity","Ehsan Kazemi, Marko Mitrovic, Morteza Zadimoghaddam, Silvio Lattanzi, Amin Karbasi",https://icml.cc/Conferences/2019/Schedule?showEvent=3925,"Streaming algorithms are generally judged by the quality of their solution, memory footprint, and computational complexity. In this paper, we study the problem of maximizing a  monotone submodular function in the streaming setting with a cardinality constraint $k$. We first propose SIEVE-STREAMING++, which requires just one pass over the data, keeps only $O(k)$ elements and achieves the tight $\frac{1}{2}$-approximation guarantee. The best previously known streaming algorithms either achieve a suboptimal $\frac{1}{4}$-approximation with $\Theta(k)$ memory or the optimal $\frac{1}{2}$-approximation with $O(k\log k)$ memory. 

Next, we show that by buffering a small fraction of the stream and applying a careful filtering procedure, one can heavily reduce the number of adaptive computational rounds, thus substantially lowering the computational complexity of SIEVE-STREAMING++. We then generalize our results to the more challenging multi-source streaming setting. We show how one can achieve the tight $\frac{1}{2}$-approximation guarantee with $O(k)$ shared memory, while minimizing not only the rounds of computations but also the total number of communicated bits. Finally, we demonstrate the efficiency of our algorithms on real-world data summarization tasks for multi-source streams of tweets and of YouTube videos.","['Yale', 'Yale University', 'Google', 'Google Zurich', 'Yale']"
2019,Hiring Under Uncertainty,"Manish Purohit, Sreenivas Gollapudi, Manish Raghavan",https://icml.cc/Conferences/2019/Schedule?showEvent=4304,"In this paper we introduce the hiring under uncertainty problem to model the questions faced by hiring committees in large enterprises and universities alike. Given a set of $n$ eligible candidates, the decision maker needs to choose the sequence of candidates to make offers so as to hire the $k$ best candidates. However, candidates may choose to reject an offer (for instance, due to a competing offer) and the decision maker has a time limit by which all positions must be filled. Given an estimate of the probabilities of acceptance for each candidate, the hiring under uncertainty problem is to design a strategy of making offers so that the total expected value of all candidates hired by the time limit is maximized. We provide a 2-approximation algorithm for the setting where offers must be made in sequence, an 8-approximation when offers may be made in parallel, and a 10-approximation for the more general stochastic knapsack setting with finite probes.","['Google Research', 'Google Research', 'Cornell']"
2019,Position-aware Graph Neural Networks,"Jiaxuan You, Rex (Zhitao) Ying, Jure Leskovec",https://icml.cc/Conferences/2019/Schedule?showEvent=4000,"Learning node embeddings that capture a node's position within the broader graph structure is crucial for many prediction tasks on graphs. However, existing Graph Neural Network (GNN) architectures have limited power in capturing the position/location of a given node with respect to all other nodes of the graph. Here we propose Position-aware Graph Neural Networks (P-GNNs), a new class of GNNs for computing position-aware node embeddings. P-GNN first samples sets of anchor nodes, computes the distance of a given target node to each anchor-set, and then learns a non-linear distance-weighted aggregation scheme over the anchor-sets. This way P-GNNs can capture positions/locations of nodes with respect to the anchor nodes. P-GNNs have several advantages: they are inductive, scalable, and can incorporate node feature information. We apply P-GNNs to multiple prediction tasks including link prediction and community detection. We show that P-GNNs consistently outperform state of the art GNNs, with up to 66% improvement in terms of the ROC AUC score.
","['Stanford University', 'Stanford University', 'Stanford University']"
2019,Detecting Overlapping and Correlated Communities without Pure Nodes: Identifiability and Algorithm,"Kejun Huang, Xiao Fu",https://icml.cc/Conferences/2019/Schedule?showEvent=3926,"Many machine learning problems come in the form of networks with relational data between entities, and one of the key unsupervised learning tasks is to detect communities in such a network. We adopt the mixed-membership stochastic blockmodel as the underlying probabilistic model, and give conditions under which the memberships of a subset of nodes can be uniquely identified. Our method starts by constructing a second-order graph moment, which can be shown to converge to a specific product of the true parameters as the size of the network increases. To correctly recover the true membership parameters, we formulate an optimization problem using insights from convex geometry. We show that if the true memberships satisfy a so-called sufficiently scattered condition, then solving the proposed problem correctly identifies the ground truth. We also propose an efficient algorithm for detecting communities, which is significantly faster than prior work and with better convergence properties. Experiments on synthetic and real data justify the validity of the proposed learning framework for network data.
","['University of Florida', 'Oregon State University']"
2019,Learning Generative Models across Incomparable Spaces,"Charlotte Bunne, David Alvarez-Melis, Andreas Krause, Stefanie Jegelka",https://icml.cc/Conferences/2019/Schedule?showEvent=3687,"Generative Adversarial Networks have shown remarkable success in learning a distribution that faithfully recovers a reference distribution in its entirety. However, in some cases, we may want to only learn some aspects (e.g., cluster or manifold structure), while modifying others (e.g., style, orientation or dimension). In this work, we propose an approach to learn generative models across such incomparable spaces, and demonstrate how to steer the learned distribution towards target properties. A key component of our model is the Gromov-Wasserstein distance, a notion of discrepancy that compares distributions relationally rather than absolutely. While this framework subsumes current generative models in identically reproducing distributions, its inherent flexibility allows application to tasks in manifold learning, relational learning and cross-domain learning.
","['ETH Zürich', 'MIT', 'ETH Zurich', 'MIT']"
2019,Relational Pooling for Graph Representations,"Ryan Murphy, Balasubramaniam Srinivasan, Vinayak A Rao, Bruno Ribeiro",https://icml.cc/Conferences/2019/Schedule?showEvent=4298,"This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions.  Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.
","['Purdue University', 'Purdue University', 'Purdue University', 'Purdue University']"
2019,Disentangled Graph Convolutional Networks,"Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, wenwu zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3948,"    The formation of a real-world graph typically arises from the highly complex interaction of many latent factors.
    The existing deep learning methods for graph-structured data neglect the entanglement of the latent factors, rendering the learned representations non-robust and hardly explainable.
    However, learning representations that disentangle the latent factors poses great challenges and remains largely unexplored in the literature of graph neural networks.
    In this paper, we introduce the disentangled graph convolutional network (DisenGCN) to learn disentangled node representations.
    In particular, we propose a novel neighborhood routing mechanism, which is capable of dynamically identifying the latent factor that may have caused the edge between a node and one of its neighbors,  and accordingly assigning the neighbor to a channel that extracts and convolutes features specific to that factor.
    We theoretically prove the convergence properties of the routing mechanism.
    Empirical results show that our proposed model can achieve significant performance gains, especially when the data demonstrate the existence of many entangled factors.

","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2019,Open Vocabulary Learning on Source Code with a Graph-Structured Cache,"Milan Cvitkovic, Badal Singh, Anima Anandkumar",https://icml.cc/Conferences/2019/Schedule?showEvent=4178,"Machine learning models that take computer program source code as input typically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an open, rapidly changing vocabulary due to, e.g., the coinage of new variable and method names.  Reasoning over such a vocabulary is not something for which most NLP methods are designed.  We introduce a Graph-Structured Cache to address this problem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code.  We find that combining this graph-structured cache strategy with recent Graph-Neural-Network-based models for supervised learning on code improves the models' performance on a code completion task and a variable naming task --- with over 100% relative improvement on the latter --- at the cost of a moderate increase in computation time.
","['California Institute of Technology', 'Amazon Web Services', 'Caltech']"
2019,Learning Discrete Structures for Graph Neural Networks,"Luca Franceschi, Mathias Niepert, Massimiliano Pontil, Xiao He",https://icml.cc/Conferences/2019/Schedule?showEvent=4197,"Graph neural networks (GNNs) are a popular class of machine learning models that have been successfully applied to a range of problems. Their major advantage lies in their ability to explicitly incorporate a sparse and discrete dependency structure between data points. Unfortunately, GNNs can only be used when such a graph-structure is available. In practice, however, real-world graphs are often noisy and incomplete or might not be available at all. 
With this work, we propose to jointly learn the graph structure and the parameters of graph convolutional networks (GCNs) by approximately solving a bilevel program that learns a discrete probability distribution on the edges of the graph. 
This allows one to apply GCNs not only in scenarios where the given graph is incomplete or corrupted but also in those where a graph is not available.
We conduct a series of experiments that analyze the behavior of the proposed method and demonstrate that it outperforms related methods by a significant margin. 
","['Istituto Italiano di Tecnologia - University College London', 'NEC Laboratories Europe', 'Istituto Italiano di Tecnologia and University College London', 'NEC Laboratories Europe']"
2019,Compositional Fairness Constraints for Graph Embeddings,"Avishek Bose, William Hamilton",https://icml.cc/Conferences/2019/Schedule?showEvent=3936,"Learning high-quality node embeddings is a key building block for machine learning models that operate on graph data, such as social networks and recommender systems. However, existing graph embedding techniques are unable to cope with fairness constraints, e.g., ensuring that the learned representations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on graph embeddings. Our approach is {\em compositional}---meaning that it can flexibly accommodate different combinations of fairness constraints during inference. 
 For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. 
 Experiments on standard knowledge graph and recommender system benchmarks highlight the utility of our proposed framework.
","['McGill/Mila', 'McGill University']"
2019,A Recurrent Neural Cascade-based Model for Continuous-Time Diffusion,Sylvain Lamprier,https://icml.cc/Conferences/2019/Schedule?showEvent=3646,"Many works have been proposed in the literature to capture the dynamics of diffusion in networks. While some of them define graphical Markovian models to extract temporal relationships between node infections in networks, others consider diffusion episodes as sequences of infections via recurrent neural models. In this paper we propose a model at the crossroads of these two extremes, which embeds the history of diffusion in infected nodes as hidden continuous states. Depending on the trajectory followed by the content before reaching a given node, the distribution of influence probabilities may vary. However, content trajectories  are usually hidden in the data, which induces challenging learning problems. We propose a topological recurrent neural model which exhibits good experimental performances for diffusion modeling and prediction.
",['LIP6 - Sorbonne Universités']
2019,Stochastic Blockmodels meet Graph Neural Networks,"Nikhil Mehta, Lawrence Carin, Piyush Rai",https://icml.cc/Conferences/2019/Schedule?showEvent=3686,"Stochastic blockmodels (SBM) and their variants, $e.g.$, mixed-membership and overlapping stochastic blockmodels, are latent variable based generative models for graphs. They have proven to be successful for various tasks, such as discovering the community structure and link prediction on graph-structured data. Recently, graph neural networks, $e.g.$, graph convolutional networks, have also emerged as a promising approach to learn powerful representations (embeddings) for the nodes in the graph, by exploiting graph properties such as locality and invariance. In this work, we unify these two directions by developing a \emph{sparse} variational autoencoder for graphs, that retains the interpretability of SBMs, while also enjoying the excellent predictive performance of graph neural nets. Moreover, our framework is accompanied by a fast recognition model that enables fast inference of the node embeddings (which are of independent interest for inference in SBM and its variants). Although we develop this framework for a particular type of SBM, namely the \emph{overlapping} stochastic blockmodel, the proposed framework can be adapted readily for other types of SBMs. Experimental results on several benchmarks demonstrate encouraging results on link prediction while learning an interpretable latent structure that can be used for community discovery.","['Duke University', 'Duke', 'IIT Kanpur']"
2019,Distributed Learning with Sublinear Communication,"Jayadev Acharya, Christopher De Sa, Dylan Foster, Karthik Sridharan",https://icml.cc/Conferences/2019/Schedule?showEvent=3787,"In distributed statistical learning, $N$ samples are split across $m$ machines and a learner wishes to use minimal communication to learn as well as if the examples were on a single machine. This model has received substantial interest in machine learning due to its scalability and potential for parallel speedup. However, in high-dimensional settings, where the number examples is smaller than the number of features (`""dimension""), the speedup afforded by distributed learning may be overshadowed by the cost of communicating a single example. This paper investigates the following question: When is it possible to learn a $d$-dimensional model in the distributed setting with total communication sublinear in $d$? Starting with a negative result, we observe that for learning $\ell_1$-bounded or sparse linear models, no algorithm can obtain optimal error until communication is linear in dimension. Our main result is that by slightly relaxing the standard boundedness assumptions for linear models, we can obtain distributed algorithms that enjoy optimal error with communication \emph{logarithmic} in dimension. This result is based on a family of algorithms that combine mirror descent with randomized sparsification/quantization of iterates, and extends to the general stochastic convex optimization model.","['Cornell University', 'Cornell', 'MIT', 'Cornell University']"
2019,On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization,"Hao Yu, rong jin, Sen Yang",https://icml.cc/Conferences/2019/Schedule?showEvent=3779,"Recent developments on large-scale distributed machine learning applications, e.g., deep neural networks, benefit enormously from the advances in distributed non-convex optimization techniques, e.g., distributed Stochastic Gradient Descent (SGD).  A series of recent works study the linear speedup property of distributed SGD variants with reduced communication. The linear speedup property enables us to scale out the computing capability by adding more computing nodes into our system. The reduced communication complexity is desirable since communication overhead is often the performance bottleneck in distributed systems. Recently, momentum methods are more and more widely adopted by practitioners to train machine learning models since they can often converge faster and generalize better.  However,  it remains unclear whether any distributed momentum SGD possesses the same linear speedup property as distributed SGD and has reduced communication complexity.  This paper fills the gap by considering a distributed communication efficient momentum SGD method and proving its linear speedup property. 
","['Alibaba Group (US) Inc', 'alibaba group', '']"
2019,Stochastic Gradient Push for Distributed Deep Learning,"Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, Michael Rabbat",https://icml.cc/Conferences/2019/Schedule?showEvent=3899,"Distributed data-parallel algorithms aim to accelerate the training of deep neural networks by parallelizing the computation of large mini-batch gradient updates across multiple nodes. Approaches that synchronize nodes using exact distributed averaging (e.g., via AllReduce) are sensitive to stragglers and communication delays. The PushSum gossip algorithm is robust to these issues, but only performs approximate distributed averaging. This paper studies Stochastic Gradient Push (SGP), which combines PushSum with stochastic gradient updates. We prove that SGP converges to a stationary point of smooth, non-convex objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus. We empirically validate the performance of SGP on image classification (ResNet-50, ImageNet) and machine translation (Transformer, WMT'16 En-De) workloads.
","['McGill University/Facebook AI Research', 'The University of Edinburgh', 'Facebook FAIR', 'Facebook']"
2019,Collective Model Fusion for Multiple Black-Box Experts,"Minh Hoang, Nghia Hoang, Bryan Kian Hsiang Low, Carleton Kingsford",https://icml.cc/Conferences/2019/Schedule?showEvent=3952,"Model fusion is a fundamental problem in collec-tive machine learning (ML) where independentexperts with heterogeneous learning architecturesare required to combine expertise to improve pre-dictive  performance.   This  is  particularly  chal-lenging in information-sensitive domains whereexperts do not have access to each other’s internalarchitecture and local data.  This paper presentsthe first collective model fusion framework formultiple experts with heterogeneous black-box ar-chitectures. The proposed method will enable thisby addressing the key issues of how black-boxexperts interact to understand the predictive be-haviors of one another; how these understandingscan be represented and shared efficiently amongthemselves; and how the shared understandingscan be combined to generate high-quality consen-sus prediction. The performance of the resultingframework is analyzed theoretically and demon-strated empirically on several datasets.
","['Carnegie Mellon University', 'MIT-IBM Watson AI Lab, IBM Research', 'National University of Singapore', 'Carnegie Mellon University']"
2019,Trading Redundancy for Communication: Speeding up Distributed SGD for Non-convex Optimization,"Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Viveck Cadambe",https://icml.cc/Conferences/2019/Schedule?showEvent=4289,"Communication overhead is one of the key challenges that hinders the scalability of distributed optimization algorithms to train large neural networks. In recent years, there has been a great deal of research to alleviate communication cost by compressing the gradient vector or using local updates and periodic model averaging. In this paper, we advocate the use of redundancy towards communication-efficient distributed stochastic algorithms for non-convex optimization. In particular, we, both theoretically and practically, show that by properly infusing redundancy to the training data with model averaging, it is possible to significantly reduce the number of communication rounds. To be more precise, we show that redundancy reduces residual error in local averaging, thereby reaching the same level of accuracy with fewer rounds of communication as compared with previous algorithms. Empirical studies on CIFAR10, CIFAR100 and ImageNet datasets in a distributed environment complement our theoretical results; they show that our algorithms have additional beneficial aspects including tolerance to failures, as well as greater gradient diversity.
","['Pennsylvania State University', 'The Pennsylvania State University', 'Pennsylvania State University', 'Pennsylvania State University']"
2019,"Trimming the $\ell_1$ Regularizer: Statistical Analysis, Optimization, and Applications to Deep Learning","Jihun Yun, Peng Zheng, Eunho Yang, Aurelie Lozano, Aleksandr Aravkin",https://icml.cc/Conferences/2019/Schedule?showEvent=4087,"We study high-dimensional estimators with the trimmed $\ell_1$ penalty, which leaves the h largest parameter entries penalty-free. While optimization techniques for this nonconvex penalty have been studied, the statistical properties have not yet been analyzed. We present the first statistical analyses for M-estimation, and characterize support recovery, $\ell_\infty$ and $\ell_2$ error of the trimmed $\ell_1$ estimates as a function of the trimming parameter h. Our results show different regimes based on how h compares to the true support size. Our second contribution is a new algorithm for the trimmed regularization problem, which has the same theoretical convergence rate as difference of convex (DC) algorithms, but in practice is faster and finds lower objective values. Empirical evaluation of $\ell_1$ trimming for sparse linear regression and graphical model estimation indicate that trimmed $\ell_1$ can outperform vanilla $\ell_1$ and non-convex alternatives. Our last contribution is to show that the trimmed penalty is beneficial beyond M-estimation, and yields promising results for two deep learning tasks: input structures recovery and network sparsification.","['KAIST', 'University of Washington', 'KAIST,AITRICS', 'IBM', 'UW']"
2019,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"Vatsal Sharan, Kai Sheng Tai, Peter Bailis, Gregory Valiant",https://icml.cc/Conferences/2019/Schedule?showEvent=3747,"What learning algorithms can be run directly on compressively-sensed data? In this work, we consider the question of accurately and efficiently computing low-rank matrix or tensor factorizations given data compressed via random projections. We examine the approach of first performing factorization in the compressed domain, and then reconstructing the original high-dimensional factors from the recovered (compressed) factors. In both the matrix and tensor settings, we establish conditions under which this natural approach will provably recover the original factors. While it is well-known that random projections preserve a number of geometric properties of a dataset, our work can be viewed as showing that they can also preserve certain solutions of non-convex, NP-Hard problems like non-negative matrix factorization. We support these theoretical results with experiments on synthetic data and demonstrate the practical applicability of compressed factorization on real-world gene expression and EEG time series datasets.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
2019,Noisy Dual Principal Component Pursuit,"Tianyu Ding, Zhihui Zhu, Tianjiao Ding, Yunchen Yang, Daniel Robinson, Manolis Tsakiris, Rene Vidal",https://icml.cc/Conferences/2019/Schedule?showEvent=3955,"Dual Principal Component Pursuit (DPCP) is a recently proposed non-convex optimization based method for learning subspaces of high relative dimension from noiseless datasets contaminated by as many outliers as the square of the number of inliers. Experimentally, DPCP has proved to be robust to noise and outperform the popular RANSAC on 3D vision tasks such as road plane detection and relative poses estimation from three views. This paper extends the global optimality and convergence theory of DPCP to the case of data corrupted by noise, and further demonstrates its robustness using synthetic and real data. 
","['Johns Hopkins University', 'Johns Hopkins University', 'ShanghaiTech University', 'ShanghaiTech', 'Johns Hopkins University', 'ShanghaiTech University', 'Johns Hopkins University, USA']"
2019,Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling,"Shanshan Wu, Alexandros Dimakis, Sujay Sanghavi, Felix Xinnan Yu, Daniel Holtmann-Rice, Dmitry Storcheus, Afshin Rostamizadeh, Sanjiv Kumar",https://icml.cc/Conferences/2019/Schedule?showEvent=3980,"Linear encoding of sparse vectors is widely popular, but is commonly data-independent -- missing any possible extra (but a priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used $\ell_1$ decoder. The convex $\ell_1$ decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into $T$ projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets; our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification, and empirically show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches.","['University of Texas at Austin', 'UT Austin', 'UT Austin', 'Google AI', 'Google Inc', 'Google Research', 'Google', 'Google Research, NY']"
2019,Screening rules for Lasso with  non-convex Sparse Regularizers,"alain rakotomamonjy, Gilles Gasso, Joseph Salmon",https://icml.cc/Conferences/2019/Schedule?showEvent=4181,"Leveraging on the convexity of the Lasso problem, screening rules
help in accelerating solvers by discarding irrelevant variables, during the optimization process. However, because they provide better theoretical guarantees in identifying relevant   variables, several non-convex regularizers for the Lasso have been proposed in the literature.  This work is the first that introduces
a screening rule strategy into a non-convex Lasso solver. The approach we propose is based on a iterative majorization-minimization (MM) strategy that includes a screening rule in the  inner solver and a condition for propagating screened variables between iterations of MM. In addition to improve efficiency of solvers, we also provide guarantees that the inner solver is able to identify the zeros components of its critical point in finite time.  Our experimental analysis illustrates the significant computational gain  brought by the new screening rule compared to classical coordinate-descent  or proximal gradient descent methods.
","['Universite de Rouen Normandie / Criteo AI Lab', 'INSA Rouen', 'Université de Montpellier']"
2019,Monge blunts Bayes: Hardness Results for Adversarial Training,"Zac Cranko, Aditya Menon, Richard Nock, Cheng Soon Ong, Zhan Shi, Christian Walder",https://icml.cc/Conferences/2019/Schedule?showEvent=3589,"The last few years have seen a staggering number of 
empirical studies of the
robustness of neural networks in a model of adversarial
perturbations of their inputs. Most
rely on an adversary which carries out local
modifications within prescribed balls. None however has so far
questioned the broader picture: how to frame a \textit{resource-bounded} adversary so
that it can be \textit{severely detrimental} to learning,
a non-trivial problem which entails at a minimum the choice of loss and classifiers.
We suggest a formal answer for losses that satisfy the minimal statistical
requirement of being \textit{proper}. We pin down a simple sufficient property for any given class of adversaries to be detrimental to learning,
involving a central measure of ``harmfulness''
which generalizes
the well-known class of integral probability metrics.
A key feature of our result is that it holds for \textit{all} proper losses,
and for a popular subset of these, the optimisation of this central measure appears to be
\textit{independent of the loss}. When classifiers
are Lipschitz -- a now popular approach in adversarial training --, this
optimisation resorts to \textit{optimal transport} to make a
low-budget compression of class marginals. Toy experiments reveal a
finding recently separately observed: training against a sufficiently
budgeted adversary of this kind \textit{improves} generalization.
","['ANU', 'Google Research', 'Data61, The Australian National University and the University of Sydney', 'Data61 and ANU', 'University of Illinois at Chicago', 'Data61, the Australian National University']"
2019,Better generalization with less data using robust gradient descent,"Matthew J. Holland, Kazushi Ikeda",https://icml.cc/Conferences/2019/Schedule?showEvent=3604,"For learning tasks where the data (or losses) may be heavy-tailed, algorithms based on empirical risk minimization may require a substantial number of observations in order to perform well off-sample. In pursuit of stronger performance under weaker assumptions, we propose a technique which uses a cheap and robust iterative estimate of the risk gradient, which can be easily fed into any steepest descent procedure. Finite-sample risk bounds are provided under weak moment assumptions on the loss gradient. The algorithm is simple to implement, and empirical tests using simulations and real-world data illustrate that more efficient and reliable learning is possible without prior knowledge of the loss tails.
","['Osaka University', 'Nara Institute of Science and Technology']"
2019,Near optimal finite time identification of arbitrary linear dynamical systems,"Tuhin Sarkar, Alexander Rakhlin",https://icml.cc/Conferences/2019/Schedule?showEvent=3634,"We derive finite time error bounds for estimating general linear time-invariant (LTI) systems from a single observed trajectory using the method of least squares. We provide the first analysis of the general case when eigenvalues of the LTI system are arbitrarily distributed in three regimes:  stable, marginally stable, and explosive. Our analysis yields sharp upper bounds for each of these cases separately. We observe that although the underlying process behaves quite differently in each of these three regimes, the systematic analysis of a self--normalized martingale difference term helps bound identification error up to logarithmic factors of the lower bound. On the other hand, we demonstrate that the least squares solution may be statistically inconsistent under certain conditions even when the signal-to-noise ratio is high. 
","['MIT', 'MIT']"
2019,Lossless or Quantized Boosting with Integer Arithmetic,"Richard Nock, Robert C Williamson",https://icml.cc/Conferences/2019/Schedule?showEvent=3592,"In supervised learning, efficiency often starts with the choice of a good loss: support vector machines popularised Hinge loss, Adaboost popularised
the exponential loss, etc. Recent trends in machine learning have
highlighted the necessity for training routines to meet
tight requirements on communication, bandwidth, energy, operations,
encoding, among others. Fitting the often decades-old state of the art
training routines into these new constraints does not go without pain and uncertainty or
reduction in the original guarantees. 
Our
paper starts with the design of a new strictly proper canonical, twice differentiable loss called the
Q-loss. Importantly, its mirror update over
(arbitrary) rational inputs uses only integer arithmetics --
more precisely, the sole use of $+, -, /, \times, |.|$. We build a
learning algorithm which is able, under mild assumptions, to achieve a
lossless boosting-compliant training. We
give conditions for a quantization of its main memory footprint,
weights, to be done while keeping the whole algorithm boosting-compliant. Experiments
display that the algorithm can achieve a fast convergence
during the early boosting rounds compared to AdaBoost, even with a weight storage
that can be 30+ times smaller. Lastly, we show that the Bayes risk of the
Q-loss can be used as node splitting criterion for decision trees and
guarantees optimal boosting convergence.","['Data61, The Australian National University and the University of Sydney', 'ANU']"
2019,Orthogonal Random Forest for Causal Inference,"Miruna Oprescu, Vasilis Syrgkanis, Steven Wu",https://icml.cc/Conferences/2019/Schedule?showEvent=4078,"We propose the orthogonal random forest, an algorithm that combines Neyman-orthogonality to reduce sensitivity with respect to estimation error of nuisance parameters with generalized random forests (Athey et al., 2017)---a flexible non-parametric method for statistical estimation of conditional moment models using random forests. We provide a consistency rate and establish asymptotic normality for our estimator. We show that under mild assumptions on the consistency rate of the nuisance estimator, we can achieve the same error rate as an oracle with a priori knowledge of these nuisance parameters. We show that when the nuisance functions have a locally sparse parametrization, then a local ell_1-penalized regression achieves the required rate. We apply our method to estimate heterogeneous treatment effects from observational data with discrete treatments or continuous treatments, and we show that, unlike prior work, our method provably allows to control for a high-dimensional set of variables under standard sparsity conditions. We also provide a comprehensive empirical evaluation of our algorithm on both synthetic and real data.
","['Microsoft Research', 'Microsoft Research', 'University of Minnesota']"
2019,MONK --  Outlier-Robust Mean Embedding Estimation by Median-of-Means,"Matthieu Lerasle, Zoltan Szabo, Timothée Mathieu, Guillaume Lecue",https://icml.cc/Conferences/2019/Schedule?showEvent=3874,"Mean embeddings provide an extremely flexible and powerful tool in machine learning and statistics to represent probability distributions and define a semi-metric (MMD, maximum mean discrepancy; also called N-distance or energy distance), with numerous successful applications. The representation is constructed as the expectation of the feature map defined by a kernel. As a mean, its classical empirical estimator, however, can be arbitrary severely affected even by a single outlier in case of unbounded features. To the best of our knowledge, unfortunately even the consistency of the existing few techniques trying to alleviate this serious sensitivity bottleneck is unknown. In this paper, we show how the recently emerged principle of median-of-means can be used to design estimators for kernel mean embedding and MMD with excessive resistance properties to outliers, and optimal sub-Gaussian deviation bounds under mild assumptions.
","[""Laboratoire de Mathématiques d'Orsay, Univ. Paris-Sud; CNRS, Université Paris Saclay, France"", 'Ecole Polytechnique', ""Laboratoire de Mathématiques d'Orsay, Univ. Paris-Sud, France"", 'CREST']"
2019,The advantages of multiple classes for reducing overfitting from test set reuse,"Vitaly Feldman, Roy Frostig, Moritz Hardt",https://icml.cc/Conferences/2019/Schedule?showEvent=4321,"Excessive reuse of holdout data can lead to overfitting. However, there is little concrete evidence of significant overfitting due to holdout reuse in popular multiclass benchmarks today.

Known results show that, in the worst-case, revealing the accuracy of $k$ adaptively chosen classifiers on a data set of size $n$ allows to create a classifier with bias of $\Theta(\sqrt{k/n})$ for any binary prediction problem. We show a new upper bound of $\tilde O(\max\{\sqrt{k\log(n)/(mn)}, k/n\})$ on the worst-case bias that any attack can achieve in a prediction problem with $m$ classes. Moreover, we present an efficient attack that achieve a bias of $\Omega(\sqrt{k/(m^2 n)})$ and improves on previous work for the binary setting ($m=2$). We also present an inefficient attack that achieves a bias of $\tilde\Omega(k/n)$. Complementing our theoretical work, we give new practical attacks to stress-test multiclass benchmarks by aiming to create as large a bias as possible with a given number of queries. Our experiments show that the additional uncertainty of prediction with a large number of classes indeed mitigates the effect of our best attacks.","['Google Brain', 'Google Brain', 'Google Brain']"
2019,On the statistical rate of nonlinear recovery in generative models with heavy-tailed data,"Xiaohan Wei, Zhuoran Yang, Zhaoran Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=3990,"We consider estimating a high-dimensional vector from non-linear measurements where the unknown vector is represented by a generative model $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$ with $k\ll d$. Such a model poses structural priors on the unknown vector without having a dedicated basis, and in particular allows new and efficient approaches solving recovery problems with number of measurements far less than the ambient dimension of the vector. While progresses have been made recently regarding theoretical understandings on the linear Gaussian measurements, much less is known when the model is possibly misspecified and the measurements are non-Gaussian. 
In this paper, we make a step towards such a direction by considering the scenario where the measurements are non-Gaussian, subject to possibly unknown nonlinear transformations and the responses are heavy-tailed. We then propose new estimators via score functions based on the first and second order Stein's identity, and prove the sample size bound of 
$m=\mathcal{O}(k\varepsilon^{-2}\log(L/\varepsilon))$ achieving an $\varepsilon$ error in the form of exponential concentration inequalities. Furthermore, for the special case of multi-layer ReLU generative model, we improve the sample bound by a logarithm factor to $m=\mathcal{O}(k\varepsilon^{-2}\log(d))$, matching the state-of-art statistical rate in compressed sensing for estimating $k$-sparse vectors. 
On the technical side, we develop new chaining methods bounding heavy-tailed processes, which could be of independent interest.
","['University of Southern California', 'Princeton University', 'Northwestern U']"
2019,"Phase transition in PCA with missing data: Reduced signal-to-noise ratio, not sample size!","Niels Ipsen, Lars Kai Hansen",https://icml.cc/Conferences/2019/Schedule?showEvent=4014,"How does missing data affect our ability to learn signal structures? It has been shown that learning signal structure in terms of principal components is dependent on the ratio of sample size and dimensionality and that a critical number of observations is needed before learning starts (Biehl and Mietzner, 1993). Here we generalize this analysis to include missing data. Probabilistic principal component analysis is regularly used for estimating signal structures in datasets with missing data. Our analytic result suggest that the effect of  missing data  is to effectively reduce signal-to-noise ratio rather than - as generally believed - to reduce sample size. The theory predicts a phase transition in the learning curves and this is indeed found both in simulation data and in real datasets.
","['Technical University of Denmark', 'Technical University of Denmark']"
2019,On Medians of (Randomized) Pairwise Means,"Stephan Clemencon, Pierre Laforgue, Patrice Bertail",https://icml.cc/Conferences/2019/Schedule?showEvent=3669,"Tournament procedures, recently introduced in the literature, offer an appealing alternative, from a theoretical perspective at least, to the principle of Empirical Risk Minimization in machine learning. Statistical learning by Median-of-Means (MoM) basically consists in segmenting the training data into blocks of equal size and comparing the statistical performance of every pair of candidate decision rules on each data block: that with highest performance on the majority of the blocks is declared as the winner. In the context of nonparametric regression, functions having won all their duels have been shown to outperform empirical risk minimizers w.r.t. the mean squared error under minimal assumptions, while exhibiting robustness properties. It is the purpose of this paper to extend this approach, in order to address other learning problems in particular, for which the performance criterion takes the form of an expectation over pairs of observations rather than over one single observation, as may be the case in pairwise ranking, clustering or metric learning. Precisely, it is proved here that the bounds achieved by MoM are essentially conserved when the blocks are built by means of independent sampling without replacement schemes instead of a simple segmentation. These results are next extended to situations where the risk is related to a pairwise loss function and its empirical counterpart is of the form of a $U$-statistic. Beyond theoretical results guaranteeing the performance of the learning/estimation methods proposed, some numerical experiments provide empirical evidence of their relevance in practice.","['Telecom ParisTech', 'Télécom ParisTech', 'Université Paris Nanterre']"
2019,Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances,"Bugra Can, Mert Gurbuzbalaban, Lingjiong  Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=4211,"Momentum methods such as Polyak's heavy ball (HB) method, Nesterov's accelerated gradient (AG) as well as accelerated projected gradient (APG) method have been commonly used in machine learning practice, but their performance is quite sensitive to noise in the gradients. We study these methods under a first-order stochastic oracle model where noisy estimates of the gradients are available. For strongly convex problems, we show that the distribution of the iterates of AG converges with the accelerated $O(\sqrt{\kappa}\log(1/\varepsilon))$ linear rate to a ball of radius $\varepsilon$ centered at a unique invariant distribution in the 1-Wasserstein metric where $\kappa$ is the condition number as long as the noise variance is smaller than an explicit upper bound we can provide. Our analysis also certifies linear convergence rates as a function of the stepsize, momentum parameter and the noise variance; recovering the accelerated rates in the noiseless case and quantifying the level of noise that can be tolerated to achieve a given performance. To the best of our knowledge, these are the first linear convergence results for stochastic momentum methods under the stochastic oracle model. We also develop finer results for the special case of quadratic objectives, extend our results to the APG method and weakly convex functions showing accelerated rates when the noise magnitude is sufficiently small.","['Rutgers University', 'Rutgers University', 'Florida State University']"
2019,SGD without Replacement: Sharper Rates for General Smooth Convex Functions,"Dheeraj Nagaraj, Prateek Jain, Praneeth Netrapalli",https://icml.cc/Conferences/2019/Schedule?showEvent=3814,"	We study stochastic gradient descent {\em without replacement} (SGDo) for smooth convex functions. SGDo is widely observed to converge faster than true SGD where each sample is drawn independently {\em with replacement} (Bottou,2009) and hence, is more popular in practice. But it's convergence properties are not well understood as sampling without replacement leads to coupling between iterates and gradients. By using method of exchangeable pairs to bound Wasserstein distance, we provide the first non-asymptotic results for SGDo when applied to {\em general smooth, strongly-convex} functions. In particular, we show that SGDo converges at a rate of $O(1/K^2)$ while SGD is known to converge at $O(1/K)$ rate, where $K$ denotes the number of passes over data and is required to be {\em large enough}. Existing results for SGDo in this setting require additional {\em Hessian Lipschitz assumption} (Gurbuzbalaban et al, 2015; HaoChen and Sra 2018). For {\em small} $K$, we show SGDo can achieve same convergence rate as SGD for {\em general smooth strongly-convex} functions. Existing results in this setting require $K=1$ and hold only for generalized linear models (Shamir,2016). In addition, by careful analysis of the coupling, for both large and small $K$, we obtain better dependence on problem dependent parameters like condition number. ","['Massachusetts Institute of Technology', 'Microsoft Research', 'Microsoft Research']"
2019,On the Complexity of Approximating Wasserstein Barycenters,"Alexey Kroshnin, Nazarii Tupitsa, Darina Dvinskikh, Pavel Dvurechenskii, Alexander Gasnikov, Cesar Uribe",https://icml.cc/Conferences/2019/Schedule?showEvent=4071,"We study the complexity of approximating the Wasserstein barycenter of $m$ discrete measures, or histograms of size $n$, by contrasting two alternative approaches that use entropic regularization. The first approach is based on the Iterative Bregman Projections (IBP) algorithm for which our novel analysis gives a complexity bound proportional to ${mn^2}/{\varepsilon^2}$ to approximate the original non-regularized barycenter. 
On the other hand, using an approach based on accelerated gradient descent, we obtain a complexity proportional to~${mn^{2}}/{\varepsilon}$.
As a byproduct, we show that the regularization parameter in both approaches has to be proportional to $\varepsilon$, which causes instability of both algorithms when the desired accuracy is high. To overcome this issue, we propose a novel proximal-IBP algorithm, which can be seen as a proximal gradient method, which uses IBP on each iteration to make a proximal step.
We also consider the question of scalability of these algorithms using approaches from distributed optimization and show that the first algorithm can be implemented in a centralized distributed setting (master/slave), while the second one is amenable to a more general decentralized distributed setting with an arbitrary network topology.","['Institute for Information Transmission Problems', 'Institute for Information Transmission Problems', 'Weierstrass Institute for Applied Analysis and Stochastics', 'Weierstrass Institute', 'Moscow Institute of Physics and Technology', 'MIT']"
2019,Estimate Sequences for Variance-Reduced Stochastic Composite Optimization,"Andrei Kulunchakov, Julien Mairal",https://icml.cc/Conferences/2019/Schedule?showEvent=4062,"In this paper, we propose a unified view of gradient-based algorithms for stochastic convex composite optimization by extending the concept of estimate sequence introduced by Nesterov. This point of view covers the stochastic gradient descent method, variants of the approaches SAGA, SVRG, and has several advantages: (i) we provide a generic proof of convergence for the aforementioned methods; (ii) we show that this SVRG variant is adaptive to strong convexity; (iii) we naturally obtain new algorithms with the same guarantees; (iv) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corrupted by small random perturbations. Finally, we show that this viewpoint is useful to obtain new accelerated algorithms in the sense of Nesterov.
","['Inria', 'Inria']"
2019,A Dynamical Systems Perspective on Nesterov Acceleration,"Michael Muehlebach, Michael Jordan",https://icml.cc/Conferences/2019/Schedule?showEvent=3920,"We present a dynamical system framework for understanding Nesterov's accelerated gradient method. In contrast to earlier work, our derivation does not rely on a vanishing step size argument. We show that Nesterov acceleration arises from discretizing an ordinary differential equation with a semi-implicit Euler integration scheme. We analyze both the underlying differential equation as well as the discretization to obtain insights into the phenomenon of acceleration. The analysis suggests that a curvature-dependent damping term lies at the heart of the phenomenon. We further establish connections between the discretized and the continuous-time dynamics.
","['UC Berkeley', 'UC Berkeley']"
2019,Random Shuffling Beats SGD after Finite Epochs,"Jeff HaoChen, Suvrit Sra",https://icml.cc/Conferences/2019/Schedule?showEvent=4126,"  A long-standing problem in stochastic optimization is proving that \rsgd, the without-replacement version of \sgd, converges faster than the usual with-replacement \sgd. Building upon~\citep{gurbuzbalaban2015random}, we present the \emph{first} (to our knowledge) non-asymptotic results for this problem by proving that after a reasonable number of epochs \rsgd converges faster than \sgd. Specifically, we prove that for strongly convex, second-order smooth functions, the iterates of \rsgd converge to the optimal solution as $\mathcal{O}(\nicefrac{1}{T^2} + \nicefrac{n^3}{T^3})$, where $n$ is the number of components in the objective, and $T$ is number of iterations. This result implies that after $\mathcal{O}(\sqrt{n})$ epochs, \rsgd is \emph{strictly better} than \sgd (which converges as $\mathcal{O}(\nicefrac{1}{T})$). The key step toward showing this better dependence on $T$ is the introduction of $n$ into the bound; and as our analysis shows, in general a dependence on $n$ is unavoidable without further changes. To understand how \rsgd works in practice, we further explore two empirically useful settings: data sparsity and over-parameterization. For sparse data, \rsgd has the rate $\mathcal{O}\left(\frac{1}{T^2}\right)$, again strictly better than \sgd. Under a setting closely related to over-parameterization, \rsgd is shown to converge faster than \sgd after any \emph{arbitrary} number of iterations.  Finally, we extend the analysis of \rsgd to smooth non-convex and convex functions.
","['Tsinghua University', 'MIT']"
2019,First-Order Algorithms Converge Faster than $O(1/k)$ on Convex Problems,"Ching-pei Lee, Stephen Wright",https://icml.cc/Conferences/2019/Schedule?showEvent=3677,"  It is well known that both gradient descent and
  stochastic coordinate descent achieve a global convergence rate of
  $O(1/k)$ in the objective value, when applied to a scheme for
  minimizing a Lipschitz-continuously differentiable, unconstrained
  convex function.  In this work, we improve this rate to $o(1/k)$.
  We extend the result to proximal gradient and proximal coordinate
  descent on regularized problems to show similar $o(1/k)$ convergence
  rates. The result is tight in the sense that a rate of
  $O(1/k^{1+\epsilon})$ is not generally attainable for any
  $\epsilon>0$, for any of these methods.","['University of Wisconsin-Madison', 'University of Wisconsin-Madison']"
2019,Improved Convergence for $\ell_1$ and $\ell_\infty$ Regression via Iteratively Reweighted Least Squares,"Alina Ene, Adrian Vladu",https://icml.cc/Conferences/2019/Schedule?showEvent=3597,"The iteratively reweighted least squares method (IRLS) is a popular technique used in practice for solving regression problems. Various versions of this method have been proposed, but their theoretical analyses failed to capture the good practical performance. 

In this paper we propose a simple and natural version of IRLS for solving $\ell_\infty$ and $\ell_1$ regression, which provably converges to a $(1+\epsilon)$-approximate solution in $O(m^{1/3}\log(1/\epsilon)/\epsilon^{2/3} + \log m/\epsilon^2)$ iterations, where $m$ is the number of rows of the input matrix. Interestingly, this running time is independent of the conditioning of the input, and the dominant term of the running time depends sublinearly in $\epsilon^{-1}$, which is atypical for the optimization of non-smooth functions.

This improves upon the more complex algorithms of Chin et al. (ITCS '12), and Christiano et al. (STOC '11) by a factor of at least $1/\epsilon^2$, and yields a truly efficient natural algorithm for the slime mold dynamics (Straszak-Vishnoi, SODA '16, ITCS '16, ITCS '17).","['Boston University', 'Boston University']"
2019,Optimal Mini-Batch and Step Sizes for SAGA,"Nidham Gazagnadou, Robert Gower, Joseph Salmon",https://icml.cc/Conferences/2019/Schedule?showEvent=4142,"Recently it has been shown that the step sizes of a family of variance reduced gradient methods called the JacSketch methods depend on the expected smoothness constant. In particular, if this expected smoothness constant could be calculated a priori, then one could safely set much larger step sizes which would result in a much faster convergence rate. We fill in this gap, and provide simple closed form expressions for the expected smoothness constant and careful numerical experiments verifying these bounds. Using these bounds, and since the SAGA algorithm is part of this JacSketch family, we suggest a new standard practice for setting the step and mini-batch sizes for SAGA that are competitive with a numerical grid search. Furthermore, we can now show that the total complexity of the SAGA algorithm decreases linearly in the mini-batch size up to a pre-defined value: the optimal mini-batch size. This is a rare result in the stochastic variance reduced literature, only previously shown for the Katyusha algorithm. Finally we conjecture that this is the case for many other stochastic variance reduced methods and that our bounds and analysis of the expected smoothness constant is key to extending these results.
","['Télécom ParisTech', 'Telecom ParisTech', 'Université de Montpellier']"
2019,Differential Inclusions for Modeling Nonsmooth ADMM Variants: A Continuous Limit Theory,"Jessica E, Yuren Zhou, Chris Junchi Li, Qingyun Sun",https://icml.cc/Conferences/2019/Schedule?showEvent=3903,"Recently, there has been a great deal of research attention on understanding the convergence behavior of first-order methods. One line of this research focuses on analyzing the convergence behavior of first-order methods using tools from continuous dynamical systems such as ordinary differential equations and differential inclusions. These research results shed lights on better understanding first-order methods from a non-optimization point of view. The alternating direction method of multipliers (ADMM) is a widely used first-order method for solving optimization problems arising from machine learning and statistics, and it is important to investigate its behavior using these new techniques from dynamical systems. Existing works along this line have been mainly focusing on problems with smooth objective functions, which exclude many important applications that are traditionally solved by ADMM variants. In this paper, we analyze some well-known and widely used ADMM variants for nonsmooth optimization problems using tools of differential inclusions. In particular, we analyze the convergence behavior of linearized ADMM, gradient-based ADMM, generalized ADMM and accelerated generalized ADMM for nonsmooth problems and show their connections with dynamical systems. We anticipate that these results will provide new insights on understanding ADMM for solving nonsmooth problems.
","['Peking University', 'Duke University', 'Tencent AI Lab', 'Stanford University']"
2019,Distribution calibration for regression,"Hao Song, Tom Diethe, Meelis Kull, Peter Flach",https://icml.cc/Conferences/2019/Schedule?showEvent=4047,"We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted target value. 
We introduce the novel concept of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration.
We further propose a post-hoc approach to improving the predictions from previously trained regression models, using multi-output Gaussian Processes with a novel Beta link function.
The proposed method is experimentally verified on a set of common regression models and shows improvements for both distribution-level and quantile-level calibration.
","['University of Bristol', 'Amazon', 'University of Tartu', 'University of Bristol']"
2019,Graph Convolutional Gaussian Processes,"Ian Walker, Ben Glocker",https://icml.cc/Conferences/2019/Schedule?showEvent=4019,"We propose a novel Bayesian nonparametric method to learn translation-invariant relationships on non-Euclidean domains. The resulting graph convolutional Gaussian processes can be applied to problems in machine learning for which the input observations are functions with domains on general graphs. The structure of these models allows for high dimensional inputs while retaining expressibility, as is the case with convolutional neural networks. We present applications of graph convolutional Gaussian processes to images and triangular meshes, demonstrating their versatility and effectiveness, comparing favorably to existing methods, despite being relatively simple models.
","['Imperial College London', 'Imperial College London']"
2019,Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation,"Ahsan Alvi, Binxin Ru, Jan-Peter Calliess, Stephen Roberts, Michael A Osborne",https://icml.cc/Conferences/2019/Schedule?showEvent=4152,"Batch Bayesian optimisation (BO) has been successfully applied to hyperparameter tuning using parallel computing, but it is wasteful of resources: workers that complete jobs ahead of others are left idle. We address this problem by developing an approach, Penalising Locally for Asynchronous Bayesian Optimisation on K Workers (PLAyBOOK), for asynchronous parallel BO. We demonstrate empirically the efficacy of PLAyBOOK and its variants on synthetic tasks and a real-world problem. We undertake a comparison between synchronous and asynchronous BO, and show that asynchronous BO often outperforms synchronous batch BO in both wall-clock time and sample efficiency.
","['University of Oxford', 'University of Oxford', 'University of Oxford', 'University of Oxford', 'U Oxford']"
2019,GOODE: A Gaussian Off-The-Shelf Ordinary Differential Equation Solver,"David John, Vincent Heuveline, Michael Schober",https://icml.cc/Conferences/2019/Schedule?showEvent=4213,"There are two types of ordinary differential equations (ODEs):
initial value problems (IVPs) and boundary value problems (BVPs).
While many probabilistic numerical methods for the solution
of IVPs have been presented to-date, there exists no efficient probabilistic
general-purpose solver for nonlinear BVPs.
Our method based on iterated Gaussian process (GP) regression returns
a GP posterior over the solution of nonlinear ODEs,
which provides a meaningful error estimation via
its predictive posterior standard deviation.
Our solver is fast (typically of quadratic convergence rate)
and the theory of convergence can be transferred from 
prior non-probabilistic work. Our method performs on par with standard codes 
for an established benchmark of test problems.
","['Corporate Research, Robert Bosch GmbH', 'University Heidelberg', 'Bosch Center for Artificial Intelligence']"
2019,Overcoming Mean-Field Approximations in Recurrent Gaussian Process Models,"Alessandro Davide Ialongo, Mark van der Wilk, James Hensman, Carl E Rasmussen",https://icml.cc/Conferences/2019/Schedule?showEvent=4134,"We identify a new variational inference scheme for dynamical systems whose transition function is modelled by a Gaussian process. Inference in this setting has either employed computationally intensive MCMC methods, or relied on factorisations of the variational posterior. As we demonstrate in our experiments, the factorisation between latent system states and transition function can lead to a miscalibrated posterior and to learning unnecessarily large noise terms. We eliminate this factorisation by explicitly modelling the dependence between state trajectories and the low-rank representation of our Gaussian process posterior. Samples of the latent states can then be tractably generated by conditioning on this representation. The method we obtain gives better predictive performance and more calibrated estimates of the transition function, yet maintains the same time and space complexities as mean-field methods.
","['University of Cambirdge; Max Planck Tübingen', 'PROWLER.io', 'PROWLER.io', 'Cambridge University']"
2019,AReS and MaRS - Adversarial and MMD-Minimizing Regression for SDEs,"Gabriele Abbati, Philippe Wenk, Michael A Osborne, Andreas Krause, Bernhard Schölkopf, Stefan Bauer",https://icml.cc/Conferences/2019/Schedule?showEvent=3857,"Stochastic differential equations are an important modeling class in many disciplines. Consequently, there exist many methods relying on various discretization and numerical integration schemes. In this paper, we propose a novel, probabilistic model for estimating the drift and diffusion given noisy observations of the underlying stochastic system. Using state-of-the-art adversarial and moment matching inference techniques, we avoid the discretization schemes of classical approaches. This leads to significant improvements in parameter accuracy and robustness given random initial guesses. On four established benchmark systems, we compare the performance of our algorithms to state-of-the-art solutions based on extended Kalman filtering and Gaussian processes.
","['University of Oxford', 'ETH Zurich', 'U Oxford', 'ETH Zurich', 'Max Planck Institute for Intelligent Systems', 'MPI for Intelligent Systems']"
2019,End-to-End Probabilistic Inference for Nonstationary Audio Analysis,"William Wilkinson, Michael Riis Andersen, Joshua D. Reiss, Dan Stowell, Arno Solin",https://icml.cc/Conferences/2019/Schedule?showEvent=4030,"A typical audio signal processing pipeline includes multiple disjoint analysis stages, including calculation of a time-frequency representation followed by spectrogram-based feature analysis. We show how time-frequency analysis and nonnegative matrix factorisation can be jointly formulated as a spectral mixture Gaussian process model with nonstationary priors over the amplitude variance parameters. Further, we formulate this nonlinear model's state space representation, making it amenable to infinite-horizon Gaussian process regression with approximate inference via expectation propagation, which scales linearly in the number of time steps and quadratically in the state dimensionality. By doing so, we are able to process audio signals with hundreds of thousands of data points. We demonstrate, on various tasks with empirical data, how this inference scheme outperforms more standard techniques that rely on extended Kalman filtering.
","['Queen Mary University of London', 'Technical University of Denmark', 'Queen Mary University of London', 'Queen Mary University of London', 'Aalto University']"
2019,Deep Gaussian Processes with Importance-Weighted Variational Inference,"Hugh Salimbeni, Vincent Dutordoir, James Hensman, Marc P Deisenroth",https://icml.cc/Conferences/2019/Schedule?showEvent=4027,"Deep Gaussian processes (DGPs) can model complex marginal densities as well as complex mappings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DGP by incorporating uncorrelated variables to the model. Previous work in the DGP model has introduced noise additively, and used variational inference with a combination of sparse Gaussian processes and mean-field Gaussians for the approximate posterior. Additive noise attenuates the signal, and the Gaussian form of variational distribution may lead to an inaccurate posterior. We instead incorporate noisy variables as latent covariates, and propose a novel importance-weighted objective, which leverages analytic results and provides a mechanism to trade off computation for improved accuracy. Our results demonstrate that the importance-weighted objective works well in practice and consistently outperforms classical variational inference, especially for deeper models.
","['Imperial College', 'PROWLER.io', 'PROWLER.io', 'Imperial College London and PROWLER.io']"
2019,Automated Model Selection with Bayesian Quadrature,"Henry Chai, Jean-Francois Ton, Michael A Osborne, Roman Garnett",https://icml.cc/Conferences/2019/Schedule?showEvent=3784,"We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for computationally expensive models. Although previous research has shown that BQ offers sample efficiency superior to Monte Carlo in computing the evidence of an individual model, applying BQ directly to model comparison may waste computation producing an overly-accurate estimate for the evidence of a clearly poor model. We propose an automated and efficient algorithm for computing the most-relevant quantity for model selection: the posterior model probability. Our technique maximizes the mutual information between this quantity and observations of the models' likelihoods, yielding efficient sample acquisition across disparate model spaces when likelihood observations are limited. Our method produces more-accurate posterior estimates using fewer likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examples.
","['Washington University in St. Louis', 'University of Oxford', 'U Oxford', 'Washington University in St. Louis']"
2019,Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical Models with double power-law behavior,"Fadhel Ayed, Juho Lee, Francois Caron",https://icml.cc/Conferences/2019/Schedule?showEvent=4022,"Bayesian nonparametric approaches, in particular the Pitman-Yor process and the associated two-parameter Chinese Restaurant process, have been successfully used in applications where the data exhibit a power-law behavior. Examples include natural language processing, natural images or networks. There is also growing empirical evidence suggesting that some datasets exhibit a two-regime power-law behavior: one regime for small frequencies, and a second regime, with a different exponent, for high frequencies. In this paper, we introduce a class of completely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor process, we show that when completely random measures in this class are normalized to obtain random probability measures and associated random partitions, such partitions exhibit a double power-law behavior. We present two general constructions and discuss in particular two models within this class: the beta prime process (Broderick et al. (2015, 2018) and a novel process called generalized BFRY process. We derive efficient Markov chain Monte Carlo algorithms to estimate the parameters of these models. Finally, we show that the proposed models provide a better fit than the Pitman-Yor process on various datasets.
","['University of Oxford', 'AITRICS', 'Oxford']"
2019,DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures,"Andrew R Lawrence, Carl Henrik Ek, Neill Campbell",https://icml.cc/Conferences/2019/Schedule?showEvent=3566,"We present a non-parametric Bayesian latent variable model capable of learning dependency structures across dimensions in a multivariate setting. Our approach is based on flexible Gaussian process priors for the generative mappings and interchangeable Dirichlet process priors to learn the structure. The introduction of the Dirichlet process as a specific structural prior allows our model to circumvent issues associated with previous Gaussian process latent variable models. Inference is performed by deriving an efficient variational bound on the marginal log-likelihood of the model. We demonstrate the efficacy of our approach via analysis of discovered structure and superior quantitative performance on missing data imputation.
","['University of Bath', 'University of Bristol', 'University of Bath']"
2019,Random Function Priors for Correlation Modeling,"Aonan Zhang, John Paisley",https://icml.cc/Conferences/2019/Schedule?showEvent=3657,"The likelihood model of high dimensional data $X_n$ can often be expressed as $p(X_n|Z_n,\theta)$, where $\theta\mathrel{\mathop:}=(\theta_k)_{k\in[K]}$ is a collection of hidden features shared across objects, indexed by $n$, and $Z_n$ is a non-negative factor loading vector with $K$ entries where $Z_{nk}$ indicates the strength of $\theta_k$ used to express $X_n$. In this paper, we introduce random function priors for $Z_n$ for modeling correlations among its $K$ dimensions $Z_{n1}$ through $Z_{nK}$, which we call \textit{population random measure embedding} (PRME). Our model can be viewed as a generalized paintbox model~\cite{Broderick13} using random functions, and can be learned efficiently with neural networks via amortized variational inference. We derive our Bayesian nonparametric method by applying a representation theorem on separately exchangeable discrete random measures.","['Columbia University', 'Columbia University']"
2019,Variational Russian Roulette for Deep Bayesian Nonparametrics,"Kai Xu, Akash Srivastava, Charles Sutton",https://icml.cc/Conferences/2019/Schedule?showEvent=4118,"Bayesian nonparametric models provide a principled way to automatically adapt the complexity of a model to the amount of the data available, but computation in such models is difficult. Amortized variational approximations are appealing because of their computational efficiency, but current methods rely on a fixed finite truncation of the infinite model. This truncation level can be difficult to set, and also interacts poorly with amortized methods due to the over-pruning problem. Instead, we propose a new variational approximation, based on a method from statistical physics called Russian roulette sampling. This allows the variational distribution to adapt its complexity during inference, without relying on a fixed truncation level, and while still obtaining an unbiased estimate of the gradient of the original variational objective. We demonstrate this method on infinite sized variational auto-encoders using a Beta-Bernoulli (Indian buffet process) prior.
","['University of Edinburgh', 'MIT-IBM, University Of Edinburgh', 'Google']"
2019,Incorporating Grouping Information into Bayesian Decision Tree Ensembles,"JUNLIANG DU, Antonio Linero",https://icml.cc/Conferences/2019/Schedule?showEvent=4149,"We consider the problem of nonparametric regression in the high-dimensional setting in which $P \gg N$. We study the use of overlapping group structures to improve prediction and variable selection. These structures arise commonly when analyzing DNA microarray data, where genes can naturally be grouped according to genetic pathways. We incorporate overlapping group structure into a Bayesian additive regression trees model using a prior constructed so that, if a variable from some group is used to construct a split, this increases the probability that subsequent splits will use predictors from the same group. We refer to our model as an overlapping group Bayesian additive regression trees (OG-BART) model, and our prior on the splits an overlapping group Dirichlet (OG-Dirichlet) prior. Like the sparse group lasso, our prior encourages sparsity both within and between groups. We study the correlation structure of the prior, illustrate the proposed methodology on simulated data, and apply the methodology to gene expression data to learn which genetic pathways are predictive of breast cancer tumor metastasis. ","['Florida State University', 'Florida State University']"
2019,Variational Implicit Processes,"Chao Ma, Yingzhen Li, Jose Miguel Hernandez-Lobato",https://icml.cc/Conferences/2019/Schedule?showEvent=4123,"We introduce the implicit processes (IPs), a stochastic process that places implicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible implicit priors over \emph{functions}, with examples including data simulators, Bayesian neural networks and non-linear transformations of stochastic processes. A novel and efficient approximate inference algorithm for IPs, namely the variational implicit processes (VIPs), is derived using generalised wake-sleep updates.  This method returns simple update equations and allows scalable hyper-parameter learning with stochastic optimization. Experiments show that VIPs return better uncertainty estimates and lower errors over existing inference methods for challenging models such as Bayesian neural networks, and Gaussian processes. 
","['University of Cambridge', 'Microsoft Research Cambridge', 'University of Cambridge']"
2019,Discovering Latent Covariance Structures for Multiple Time Series,"Anh Tong, Jaesik Choi",https://icml.cc/Conferences/2019/Schedule?showEvent=4069,"Analyzing multivariate time series data is important to predict future events and changes of complex systems in finance, manufacturing, and administrative decisions. The expressiveness power of Gaussian Process (GP) regression methods has been significantly improved by compositional covariance structures. In this paper, we present a new GP model which naturally handles multiple time series by placing an Indian Buffet Process (IBP) prior on the presence of shared kernels. Our selective covariance structure decomposition allows exploiting shared parameters over a set of multiple, selected time series. We also investigate the well-definedness of the models when infinite latent components are introduced. We present a pragmatic search algorithm which explores a larger structure space efficiently. Experiments conducted on five real-world data sets demonstrate that our new model outperforms existing methods in term of structure discoveries and predictive performances.
","['Ulsan National Institute of Science and Technology', 'Ulsan National Institute of Science and Technology']"
2019,Scalable Training of Inference Networks for Gaussian-Process Models,"Jiaxin Shi, Mohammad Emtiyaz Khan, Jun Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3746,"Inference in Gaussian process (GP) models is computationally challenging for large data, and often difficult to approximate with a small number of inducing points. We explore an alternative approximation that employs stochastic inference networks for a flexible inference. Unfortunately, for such networks, minibatch training is difficult to be able to learn meaningful correlations over function outputs for a large dataset. We propose an algorithm that enables such training by tracking a stochastic, functional mirror-descent algorithm. At each iteration, this only requires considering a finite number of input locations, resulting in a scalable and easy-to-implement algorithm. Empirical results show comparable and, sometimes, superior performance to existing sparse variational GP methods.
","['Tsinghua University', 'RIKEN', 'Tsinghua University']"
2019,Bayesian Optimization Meets Bayesian Optimal Stopping,"Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, Patrick Jaillet ",https://icml.cc/Conferences/2019/Schedule?showEvent=3919,"Bayesian optimization (BO) is a popular paradigm for optimizing the hyperparameters of machine learning (ML) models due to its sample efficiency. Many ML models require running an iterative training procedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training process (e.g., validation accuracy after each epoch) can be exploited for improving the epoch efficiency of BO algorithms by early-stopping model training under hyperparameter settings that will end up under-performing and hence eliminating unnecessary training epochs. This paper proposes to unify BO (specifically, Gaussian process-upper confidence bound (GP-UCB)) with Bayesian optimal stopping (BO-BOS) to boost the epoch efficiency of BO. To achieve this, while GP-UCB is sample-efficient in the number of function evaluations, BOS complements it with epoch efficiency for each function evaluation by providing a principled optimal stopping mechanism for early stopping. BO-BOS preserves the (asymptotic) no-regret performance of GP-UCB using our specified choice of BOS parameters that is amenable to an elegant interpretation in terms of the exploration-exploitation trade-off. We empirically evaluate the performance of BO-BOS and demonstrate its generality in hyperparameter optimization of ML models and two other interesting applications.
","['National University of Singapore', 'National University of Singapore', 'National University of Singapore', 'MIT']"
2019,Learning interpretable continuous-time models of latent stochastic dynamical systems,"Lea Duncker, Gergo Bohner, Julien Boussard, Maneesh Sahani",https://icml.cc/Conferences/2019/Schedule?showEvent=4089,"We develop an approach to learn an interpretable semi-parametric model of a latent continuous-time stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear
stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function drawn from a Gaussian process (GP) conditioned on a set of learnt fixed points and corresponding local Jacobian matrices.  This form yields a
flexible nonparametric model of the dynamics, with a representation corresponding directly to the interpretable portraits routinely employed in the study of nonlinear dynamical systems.  The learning algorithm combines inference of continuous latent paths underlying observed data with a sparse variational description of the dynamical process. We demonstrate our approach on simulated data from different nonlinear dynamical systems.
","['Gatsby Unit, UCL', 'Gatsby Unit, UCL', 'Stanford university', 'Gatsby Unit, UCL']"
2019,A Tree-Based Method for Fast Repeated Sampling of Determinantal Point Processes,"Jennifer Gillenwater, Alex Kulesza, Zelda Mariet, Sergei Vassilvitskii",https://icml.cc/Conferences/2019/Schedule?showEvent=3607,"It is often desirable in recommender systems and other information retrieval applications to provide diverse results, and determinantal point processes (DPPs) have become a popular way to capture the trade-off between the quality of individual results and the diversity of the overall set. However, sampling from a DPP is inherently expensive: if the underlying collection contains N items, then generating each DPP sample requires time linear in N following a one-time preprocessing phase. Additionally, results often need to be personalized to a user, but standard approaches to personalization invalidate the preprocessing, making personalized samples especially expensive. In this work we address both of these shortcomings. First, we propose a new algorithm for generating DPP samples in time logarithmic in N, following a slightly more expensive preprocessing phase.  We then extend the algorithm to support arbitrary query-time feature weights, allowing us to generate samples customized to individual users while still retaining logarithmic runtime; experiments show our approach runs over 300 times faster than traditional DPP sampling on collections of 100,000 items for samples of size 10.
","['Google Research NYC', 'Google', 'MIT', 'Google']"
2019,Nonlinear Stein Variational Gradient Descent for Learning Diversified Mixture Models,"Dilin Wang, Qiang Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=3886,"Diversification has been shown to be a powerful mechanism for learning robust models in non-convex settings. A notable example is learning mixture models, in which enforcing diversity between the different mixture components allows us to prevent the model collapsing phenomenon and capture more patterns from the observed data. In this work, we present a variational approach for diversity-promoting learning, which leverages the entropy functional as a natural mechanism for enforcing diversity. We develop a simple and efficient functional gradient-based algorithm for optimizing the variational objective function, which provides a significant generalization of Stein variational gradient descent (SVGD). We test our method on various challenging real world problems, including deep embedded clustering and deep anomaly detection. Empirical results show that our method provides an effective mechanism for diversity-promoting learning, achieving substantial improvement over existing methods. 
","['UT Austin', 'UT Austin']"
2019,Understanding and Accelerating Particle-Based Variational Inference,"Chang Liu, Jingwei Zhuo, Pengyu Cheng, RUIYI (ROY) ZHANG, Jun Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3823,"Particle-based variational inference methods (ParVIs) have gained attention in the Bayesian inference literature, for their capacity to yield flexible and accurate approximations. We explore ParVIs from the perspective of Wasserstein gradient flows, and make both theoretical and practical contributions. We unify various finite-particle approximations that existing ParVIs use, and recognize that the approximation is essentially a compulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumptions and relations of existing ParVIs, and also inspires new ParVIs. We propose an acceleration framework and a principled bandwidth-selection method for general ParVIs; these are based on the developed theory and leverage the geometry of the Wasserstein space. Experimental results show the improved convergence by the acceleration framework and enhanced sample accuracy by the bandwidth-selection method.
","['Tsinghua University', 'Tsinghua University', 'Duke University', 'Duke University', 'Tsinghua University']"
2019,Efficient learning of smooth probability functions from Bernoulli tests with guarantees,"Paul Rolland, Ali Kavis, Alexander Niklaus Immer, Adish Singla, Volkan Cevher",https://icml.cc/Conferences/2019/Schedule?showEvent=4143,"We study the fundamental problem of learning an unknown, smooth probability function via point-wise Bernoulli tests. We provide a scalable algorithm for efficiently solving this problem with rigorous guarantees. In particular, we prove the convergence rate of our posterior update rule to the true probability function in L2-norm. Moreover, we allow the Bernoulli tests to depend on contextual features, and provide a modified inference engine with provable guarantees for this novel setting. Numerical results show that the empirical convergence rates match the theory, and illustrate the superiority of our approach in handling contextual features over the state-of-the-art.
","['Ecole Polytechnique Fédérale de Lausanne', 'EPFL', 'EPFL, RIKEN', 'Max Planck Institute (MPI-SWS)', 'EPFL']"
2019,The Variational Predictive Natural Gradient,"Da Tang, Rajesh Ranganath",https://icml.cc/Conferences/2019/Schedule?showEvent=3950,"Variational inference transforms posterior inference into parametric optimization thereby enabling the use of latent variable models where otherwise impractical. However, variational inference can be finicky when different variational parameters control variables that are strongly correlated under the model. Traditional natural gradients based on the variational approximation fail to correct for correlations when the approximation is not the true posterior. To address this, we construct a new natural gradient called the Variational Predictive Natural Gradient (VPNG). Unlike traditional natural gradients for variational inference, this natural gradient accounts for the relationship between model parameters and variational parameters. We demonstrate the insight with a simple example as well as the empirical value on a classification task, a deep generative model of images, and probabilistic matrix factorization for recommendation.
","['Columbia University', 'New York University']"
2019,Scalable Nonparametric Sampling from Multimodal Posteriors with the Posterior Bootstrap,"Edwin Fong, Simon Lyddon, Christopher Holmes",https://icml.cc/Conferences/2019/Schedule?showEvent=4150,"Increasingly complex datasets pose a number of challenges for Bayesian inference. Conventional posterior sampling based on Markov chain Monte Carlo can be too computationally intensive, is serial in nature and mixes poorly between posterior modes. Furthermore, all models are misspecified, which brings into question the validity of the conventional Bayesian update. We present a scalable Bayesian nonparametric learning routine that enables posterior sampling through the optimization of suitably randomized objective functions. A Dirichlet process prior on the unknown data distribution accounts for model misspecification, and admits an embarrassingly parallel posterior bootstrap algorithm that generates independent and exact samples from the nonparametric posterior distribution. Our method is particularly adept at sampling from multimodal posterior distributions via a random restart mechanism, and we demonstrate this on Gaussian mixture model and sparse logistic regression examples.
","['University of Oxford', 'University of Oxford', 'University of Oxford']"
2019,An Instability in Variational Inference for Topic Models,"Behrooz Ghorbani, Hamidreza Hakim Javadi, Andrea Montanari",https://icml.cc/Conferences/2019/Schedule?showEvent=3966,"Naive mean field variational methods are the state of-the-art approach to inference in topic modeling. We show that these methods suffer from an instability that can produce misleading conclusions. Namely, for certain regimes of the model parameters, variational inference outputs a non-trivial decomposition into topics. However -for the same parameter values- the data contain no actual information
about the true topic decomposition, and the output of the algorithm is uncorrelated with it. In particular, the estimated posterior mean is wrong, and estimated credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field approximations.
","['Stanford University', 'Rice University', 'Stanford University']"
2019,Bayesian Optimization of Composite Functions,"Raul Astudillo, Peter I Frazier",https://icml.cc/Conferences/2019/Schedule?showEvent=3963,"We consider optimization of composite objective functions, i.e., of the form $f(x)=g(h(x))$, where $h$ is a black-box derivative-free expensive-to-evaluate function with vector-valued outputs, and $g$ is a cheap-to-evaluate real-valued function. While these problems can be solved with standard Bayesian optimization, we propose a novel approach that exploits the composite structure of the objective function to substantially improve sampling efficiency. Our approach models $h$ using a multi-output Gaussian process and chooses where to sample using the expected improvement evaluated on the implied non-Gaussian posterior on $f$, which we call expected improvement for composite functions (EI-CF).  Although EI-CF cannot be computed in closed form, we provide a novel stochastic gradient estimator that allows its efficient maximization.  We also show that our approach is asymptotically consistent, i.e., that it recovers a globally optimal solution as sampling effort grows to infinity, generalizing previous convergence results for classical expected improvement.
Numerical experiments show that our approach dramatically outperforms standard Bayesian optimization benchmarks, reducing simple regret by several orders of magnitude.","['Cornell University', 'Cornell University / Uber']"
2019,The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions,"Raj Agrawal, Brian Trippe, Jonathan Huggins, Tamara Broderick",https://icml.cc/Conferences/2019/Schedule?showEvent=3785,"Discovering interaction effects on a response of interest is a fundamental problem faced in biology, medicine, economics, and many other scientific disciplines. In theory, Bayesian methods for discovering pairwise interactions enjoy many benefits such as coherent uncertainty quantification, the ability to incorporate background knowledge, and desirable shrinkage properties. In practice, however, Bayesian methods are often computationally intractable for even moderate- dimensional problems. Our key insight is that many hierarchical models of practical interest admit a Gaussian process representation such that rather than maintaining a posterior over all O(p^2) interactions, we need only maintain a vector of O(p) kernel hyper-parameters. This implicit representation allows us to run Markov chain Monte Carlo (MCMC) over model hyper-parameters in time and memory linear in p per iteration. We focus on sparsity-inducing models and show on datasets with a variety of covariate behaviors that our method: (1) reduces runtime by orders of magnitude over naive applications of MCMC, (2) provides lower Type I and Type II error relative to state-of-the-art LASSO-based approaches, and (3) offers improved computational scaling in high dimensions relative to existing Bayesian and LASSO-based approaches.
","['MIT', 'MIT', 'Harvard', 'MIT']"
2019,Quantile Stein Variational Gradient Descent for Batch  Bayesian Optimization,"Chengyue Gong, Jian Peng, Qiang Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=3796,"Batch Bayesian optimization has been shown to be an efficient and successful approach for black-box function optimization, especially when the evaluation of cost function is highly expensive but can be efficiently parallelized. In this paper, we introduce a novel variational framework for batch query optimization, based on the argument that the query batch should be selected to have both high diversity and good worst case performance. This motivates us to introduce a variational objective that combines a quantile-based risk measure (for worst case performance) and entropy regularization (for enforcing diversity). We derive a gradient-based particle-based algorithm for solving our quantile-based variational objective, which generalizes Stein variational gradient descent (SVGD). We evaluate our method on a number of real-world applications and show that it consistently outperforms other recent state-of-the-art batch Bayesian optimization methods. Extensive experimental results indicate that our method achieves better or comparable performance, compared to the existing methods.
","['university of texas at austin', 'UIUC', 'UT Austin']"
2019,Exploiting Worker Correlation for Label Aggregation in Crowdsourcing,"Yuan Li, Benjamin Rubinstein, Trevor Cohn",https://icml.cc/Conferences/2019/Schedule?showEvent=4013,"Crowdsourcing has emerged as a core component of data science pipelines. From collected noisy worker labels, aggregation models that incorporate worker reliability parameters aim to infer a latent true annotation. In this paper, we argue that existing crowdsourcing approaches do not sufficiently model worker correlations observed in practical settings; we propose in response an enhanced Bayesian classifier combination (EBCC) model, with inference based on a mean-field variational approach. An introduced mixture of intra-class reliabilities---connected to tensor decomposition and item clustering---induces inter-worker correlation. EBCC does not suffer the limitations of existing correlation models: intractable marginalisation of missing labels and poor scaling to large worker cohorts. Extensive empirical comparison on 17 real-world datasets sees EBCC achieving the highest mean accuracy across 10 benchmark crowdsourcing methods.
","['University of Melbourne', 'University\u200b of Melbourne', 'University of Melbourne']"
2019,Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems,"Ted Meeds, Geoffrey Roeder, Paul Grant, Andrew Phillips, Neil Dalchau",https://icml.cc/Conferences/2019/Schedule?showEvent=4101,"We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or ""white-box"" sub-components and neural network or ""black-box"" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called ``zero-shot"" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors.
","['Microsoft Research', 'Princeton University', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2019,A Multitask Multiple Kernel Learning Algorithm for Survival Analysis with Application to Cancer Biology,"Onur Dereli, Ceyda  Oğuz, Mehmet Gönen",https://icml.cc/Conferences/2019/Schedule?showEvent=4219,"Predictive performance of machine learning algorithms on related problems can be improved using multitask learning approaches. Rather than performing survival analysis on each data set to predict survival times of cancer patients, we developed a novel multitask approach based on multiple kernel learning (MKL). Our multitask MKL algorithm both works on multiple cancer data sets and integrates cancer-related pathways/gene sets into survival analysis. We tested our algorithm, which is named as Path2MSurv, on the Cancer Genome Atlas data sets analyzing gene expression profiles of 7,655 patients from 20 cancer types together with cancer-specific pathway/gene set collections. Path2MSurv obtained better or comparable predictive performance when benchmarked against random survival forest, survival support vector machine, and single-task variant of our algorithm. Path2MSurv has the ability to identify key pathways/gene sets in predicting survival times of patients from different cancer types.
","['Koç University', 'Koç University', 'Koç University']"
2019,Fast and Flexible Inference of Joint Distributions from their Marginals,"Charles Frogner, Tomaso Poggio",https://icml.cc/Conferences/2019/Schedule?showEvent=3600,"Across the social sciences and elsewhere, practitioners frequently have to reason about relationships between random variables, despite lacking joint observations of the variables. This is sometimes called an ""ecological"" inference; given samples from the marginal distributions of the variables, one attempts to infer their joint distribution. The problem is inherently ill-posed, yet only a few models have been proposed for bringing prior information into the problem, often relying on restrictive or unrealistic assumptions and lacking a unified approach. In this paper, we treat the inference problem generally and propose a unified class of models that encompasses some of those previously proposed while including many new ones. Previous work has relied on either relaxation or approximate inference via MCMC, with the latter known to mix prohibitively slowly for this type of problem. Here we instead give a single exact inference algorithm that works for the entire model class via an efficient fixed point iteration called Dykstra's method. We investigate empirically both the computational cost of our algorithm and the accuracy of the new models on real datasets, showing favorable performance in both cases and illustrating the impact of increased flexibility in modeling enabled by this work.
","['MIT', 'Massachusetts Institute of Technology']"
2019,Cognitive model priors for predicting human decisions,"Joshua C Peterson, David D Bourgin, Daniel Reichman, Thomas Griffiths, Stuart Russell",https://icml.cc/Conferences/2019/Schedule?showEvent=3762,"Human decision-making underlies all economic behavior. For the past four decades, human decision-making under uncertainty has continued to be explained by theoretical models based on prospect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have developed slowly, and robust, high-precision predictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these problems, it is currently unclear to what extent it can improve predictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive sample sizes to be accurately captured by off-the-shelf machine learning methods. To solve this problem, what is needed are machine learning models with appropriate inductive biases for capturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct ``cognitive model priors'' by pretraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models developed by cognitive psychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unprecedented state-of-the-art improvements on two benchmark datasets. Second, we present the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision problems. This dataset reveals the circumstances where cognitive model priors are useful, and provides a new standard for benchmarking prediction of human decisions under uncertainty.
","['Princeton University', 'UC Berkeley', 'UC Berkeley', 'Princeton University', 'UC Berkeley']"
2019,Conditioning by adaptive sampling for robust design,"David Brookes, Jennifer Listgarten",https://icml.cc/Conferences/2019/Schedule?showEvent=4261,"We present a method for design problems wherein the goal is to maximize or specify the value of one or more properties of interest (e.g. maximizing the fluorescence of a protein). We assume access to black box, stochastic ``oracle"" predictive functions, each of which maps from design space to a distribution over properties of interest. Because many state-of-the-art predictive models are known to suffer from pathologies, especially for data far from the training distribution, the problem becomes different from directly optimizing the oracles. Herein, we propose a method to solve this problem that uses model-based adaptive sampling to estimate a distribution over the design space, conditioned on the desired properties. 
","['University of California, Berkeley', 'University of California, Berkeley']"
2019,Direct Uncertainty Prediction for Medical Second Opinions,"Maithra Raghu, Katy Blumer, Rory sayres, Ziad Obermeyer, Bobby Kleinberg, Sendhil Mullainathan, Jon Kleinberg",https://icml.cc/Conferences/2019/Schedule?showEvent=3695,"The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application. 
","['Cornell University / Google Brain', 'Google', 'Google', 'UC Berkeley School of Public Health', 'Cornell', 'Harvard University', 'Cornell University']"
2019,Dynamic Measurement Scheduling for Event Forecasting using Deep RL,"Chun-Hao (Kingsley) Chang, Mingjie Mai, Anna Goldenberg",https://icml.cc/Conferences/2019/Schedule?showEvent=4215,"Imagine a patient in critical condition. What and when should be measured to forecast detrimental events, especially under the budget constraints? We answer this question by deep reinforcement learning (RL) that jointly minimizes the measurement cost and maximizes predictive gain, by scheduling strategically-timed measurements. We learn our policy to be dynamically dependent on the patient's health history. To scale our framework to exponentially large action space, we distribute our reward in a sequential setting that makes the learning easier. In our simulation, our policy outperforms heuristic-based scheduling with higher predictive gain and lower cost. In a real-world ICU mortality prediction task (MIMIC3), our policies reduce the total number of measurements by 31% or improve predictive gain by a factor of 3 as compared to physicians, under the off-policy policy evaluation.
","['University of Toronto', 'University of Toronto', 'University of Toronto']"
2019,Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization,"Hesham Mostafa, Xin Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=3692,"Modern deep neural networks are typically highly overparameterized.  Pruning techniques are able to remove a significant fraction of network parameters with little loss in accuracy.  Recently, techniques based on dynamic reallocation of non-zero parameters have emerged, allowing direct training of sparse networks without having to pre-train a large dense model.  Here we present a novel dynamic sparse reparameterization method that addresses the limitations of previous techniques such as high computational cost and the need for manual configuration of the number of free parameters allocated to each layer.  We evaluate the performance of dynamic reallocation methods in training deep convolutional networks and show that our method outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget, on par with accuracies obtained by iteratively pruning a pre-trained dense model.  We further investigated the mechanisms underlying the superior generalization performance of the resultant sparse networks.  We found that neither the structure, nor the initialization of the non-zero parameters were sufficient to explain the superior performance.  Rather, effective learning crucially depended on the continuous exploration of the sparse network structure space during training.  Our work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network.
","['Intel Corporation', 'Cerebras Systems']"
2019,DeepNose: Using artificial neural networks to represent the space of odorants,"Ngoc Tran, Daniel Kepple, Sergey Shuvaev, Alexei Koulakov",https://icml.cc/Conferences/2019/Schedule?showEvent=4175,"The olfactory system employs an ensemble of odorant receptors (ORs) to sense odorants and to derive olfactory percepts. We trained artificial neural networks to represent the chemical space of odorants and used this representation to predict human olfactory percepts. We hypothesized that ORs may be considered 3D convolutional filters that extract molecular features and, as such, can be trained using machine learning methods. First, we trained a convolutional autoencoder, called DeepNose, to deduce a low-dimensional representation of odorant molecules which were represented by their 3D spatial structure. Next, we tested the ability of DeepNose features in predicting physical properties and odorant percepts based on 3D molecular structure alone. We found that, despite the lack of human expertise, DeepNose features often outperformed molecular descriptors used in computational chemistry in predicting both physical properties and human perceptions. We propose that DeepNose network can extract {\it de novo} chemical features predictive of various bioactivities and can help understand the factors influencing the composition of ORs ensemble.
","['Cold Spring Harbor Laboratory', 'Cold Spring Harbor Laboratory', 'Cold Spring Harbor Laboratory', 'Cold Spring Harbor Laboratory']"
2019,Domain Agnostic Learning with Disentangled Representations,"Xingchao Peng, Zijun Huang, Ximeng Sun, Kate Saenko",https://icml.cc/Conferences/2019/Schedule?showEvent=3571,"Unsupervised model transfer has the potential to greatly improve the generalizability of deep models to novel domains. Yet the current literature assumes that the separation of target data into distinct domains is known a priori. In this paper, we propose the task of Domain-Agnostic Learning (DAL): How to transfer knowledge from a labeled source domain to unlabeled data from arbitrary target domains? To tackle this problem, we devise a novel Deep Adversarial Disentangled Autoencoder (DADA) capable of disentangling domain-specific features from class identity. We demonstrate experimentally that when the target domain labels are unknown, DADA leads to state-of-the-art performance on several image classification datasets.
","['Boston University', 'Columbia University', 'Boston University', 'Boston University']"
2019,Composing Value Functions in Reinforcement Learning,"Benjamin van Niekerk, Steven James, Adam Earle, Benjamin Rosman",https://icml.cc/Conferences/2019/Schedule?showEvent=3993,"An important property for lifelong-learning agents is the ability to combine existing skills to solve new unseen tasks. In general, however, it is unclear how to compose existing skills in a principled manner. Under the assumption of deterministic dynamics, we prove that optimal value function composition can be achieved in entropy-regularised reinforcement learning (RL), and extend this result to the standard RL setting. Composition is demonstrated in a high-dimensional video game, where an agent with an existing library of skills is immediately able to solve new tasks without the need for further learning.
","['University of the Witwatersrand', 'University of the Witwatersrand', 'University of the Witwatersrand', 'Council for Scientific and Industrial Research']"
2019,Fast Context Adaptation via Meta-Learning,"Luisa Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, Shimon Whiteson",https://icml.cc/Conferences/2019/Schedule?showEvent=4086,"We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight  weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.
","['University of Oxford', 'University of Amsterdam', 'University of Oxford', 'Microsoft', 'University of Oxford']"
2019,Provable Guarantees for Gradient-Based Meta-Learning,"Nina Balcan, Mikhail Khodak, Ameet Talwalkar",https://icml.cc/Conferences/2019/Schedule?showEvent=3887,"We study the problem of meta-learning through the lens of online convex optimization, developing a meta-algorithm bridging the gap between popular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the first to simultaneously satisfy good sample efficiency guarantees in the convex setting, with generalization bounds that improve with task-similarity, while also being computationally scalable to modern deep learning architectures and the many-task setting. Despite its simplicity, the algorithm matches, up to a constant factor, a lower bound on the performance of any such parameter-transfer method under natural task similarity assumptions. We use experiments in both convex and deep learning settings to verify and demonstrate the applicability of our theory.
","['Carnegie Mellon University', 'CMU', 'Carnegie Mellon University']"
2019,Towards Understanding Knowledge Distillation,"Mary Phuong, Christoph H. Lampert",https://icml.cc/Conferences/2019/Schedule?showEvent=3824,"Knowledge distillation, i.e., one classifier being trained on the outputs of another classifier, is an empirically very successful technique for knowledge transfer between classifiers. It has even been observed that classifiers learn much faster and more reliably if trained with the outputs of another classifier as soft labels, instead of from ground truth data. So far, however, there is no satisfactory theoretical explanation of this phenomenon.
In this work, we provide the first insights into the working mechanisms of distillation by studying the special case of linear and deep linear classifiers. Specifically, we prove a generalization bound that establishes fast convergence of the expected risk of a distillation-trained linear classifier. From the bound and its proof we extract three key factors that determine the success of distillation: 
* data geometry -- geometric properties of the data distribution, in particular class separation, has a direct influence on the convergence speed of the risk;
* optimization bias -- gradient descent optimization finds a very favorable minimum of the distillation objective; and
* strong monotonicity -- the expected risk of the student classifier always decreases when the size of the training set grows.
","['IST Austria', 'IST Austria']"
2019,Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers,"Hong Liu, Mingsheng Long, Jianmin Wang, Michael Jordan",https://icml.cc/Conferences/2019/Schedule?showEvent=3644,"Domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain. A mainstream approach is adversarial feature adaptation, which learns domain-invariant representations through aligning the feature distributions of both domains. However, a theoretical prerequisite of domain adaptation is the adaptability measured by the expected risk of an ideal joint hypothesis over the source and target domains. In this respect, adversarial feature adaptation may potentially deteriorate the adaptability, since it distorts the original feature distributions when suppressing domain-specific variations. To this end, we propose Transferable Adversarial Training (TAT) to enable the adaptation of deep classifiers. The approach generates transferable examples to fill in the gap between the source and target domains, and adversarially trains the deep classifiers to make consistent predictions over the transferable examples. Without learning domain-invariant representations at the expense of distorting the feature distributions, the adaptability in the theoretical learning bound is algorithmically guaranteed. A series of experiments validate that our approach advances the state of the arts on a variety of domain adaptation tasks in vision and NLP, including object recognition, learning from synthetic to real data, and sentiment classification.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'UC Berkeley']"
2019,Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation,"Xinyang Chen, Sinan Wang, Mingsheng Long, Jianmin Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=3734,"Adversarial domain adaptation has made remarkable advances in learning transferable representations for knowledge transfer across domains. While adversarial learning strengthens the feature transferability which the community focuses on, its impact on the feature discriminability has not been fully explored. In this paper, a series of experiments based on spectral analysis of the feature representations have been conducted, revealing an unexpected deterioration of the discriminability while learning transferable features adversarially. Our key finding is that the eigenvectors with the largest singular values will dominate the feature transferability. As a consequence, the transferability is enhanced at the expense of over penalization of other eigenvectors that embody rich structures crucial for discriminability. Towards this problem, we present Batch Spectral Penalization (BSP), a general approach to penalizing the largest singular values so that other eigenvectors can be relatively strengthened to boost the feature discriminability. Experiments show that the approach significantly improves upon representative adversarial domain adaptation methods to yield state of the art results.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2019,Learning-to-Learn Stochastic Gradient Descent with Biased Regularization,"Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, Massimiliano Pontil",https://icml.cc/Conferences/2019/Schedule?showEvent=4116,"We study the problem of learning-to-learn: infer- ring a learning algorithm that works well on a family of tasks sampled from an unknown distribution. As class of algorithms we consider Stochastic Gradient Descent (SGD) on the true risk regularized by the square euclidean distance from a bias vector. We present an average excess risk bound for such a learning algorithm that quantifies the potential benefit of using a bias vector with respect to the unbiased case. We then propose a novel meta-algorithm to estimate the bias term online from a sequence of observed tasks. The small memory footprint and low time complexity of our approach makes it appealing in practice while our theoretical analysis provides guarantees on the generalization properties of the meta-algorithm on new tasks. A key feature of our results is that, when the number of tasks grows and their vari- ance is relatively small, our learning-to-learn approach has a significant advantage over learning each task in isolation by standard SGD without a bias term. Numerical experiments demonstrate the effectiveness of our approach in practice.
","['IIT', 'Imperial College London', 'Istituto Italiano di Tecnologia - University College London', 'University College London']"
2019,BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,"Asa Cooper Stickland, Iain Murray",https://icml.cc/Conferences/2019/Schedule?showEvent=4070,"Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a \hbox{single} BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or `projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with $\approx$7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.","['University of Edinburgh', 'University of Edinburgh']"
2019,Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation,"Kaichao You, Ximei Wang, Mingsheng Long, Michael Jordan",https://icml.cc/Conferences/2019/Schedule?showEvent=3708,"Deep unsupervised domain adaptation (Deep UDA) methods successfully leverage rich labeled data in a source domain to boost the performance on related but unlabeled data in a target domain. However, algorithm comparison is cumbersome in Deep UDA due to the absence of accurate and standardized model selection method, posing an obstacle to further advances in the field. Existing model selection methods for Deep UDA are either highly biased, restricted, unstable, or even controversial (requiring labeled target data). To this end, we propose Deep Embedded Validation (DEV), which embeds adapted feature representation into the validation procedure to obtain unbiased estimation of the target risk with bounded variance. The variance is further reduced by the technique of control variate. The efficacy of the method has been justified both theoretically and empirically. 
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'UC Berkeley']"
2019,Active Embedding Search via Noisy Paired Comparisons,"Gregory Canal, Andy Massimino, Mark Davenport, Christopher Rozell",https://icml.cc/Conferences/2019/Schedule?showEvent=4212,"Suppose that we wish to estimate a user's preference vector $w$ from paired comparisons of the form ``does user $w$ prefer item $p$ or item $q$?,'' where both the user and items are embedded in a low-dimensional Euclidean space with distances that reflect user and item similarities. Such observations arise in numerous settings, including psychometrics and psychology experiments, search tasks, advertising, and recommender systems. In such tasks, queries can be extremely costly and subject to varying levels of response noise; thus, we aim to actively choose pairs that are most informative given the results of previous comparisons. We provide new theoretical insights into the benefits and challenges of greedy information maximization in this setting, and develop two novel strategies that maximize lower bounds on information gain and are simpler to analyze and compute respectively. We use simulated responses from a real-world dataset to validate our strategies through their similar performance to greedy information maximization, and their superior preference estimation over state-of-the-art selection methods as well as random queries.","['Georgia Institute of Technology', 'Gatech', 'Georgia Institute of Technology', 'Georgia Institute of Technology']"
2019,Fast Direct Search in an Optimally Compressed Continuous Target Space for Efficient Multi-Label Active Learning,"weishi shi, Qi Yu",https://icml.cc/Conferences/2019/Schedule?showEvent=3596,"Active learning for multi-label classification poses fundamental challenges given the complex label correlations and a potentially large and sparse label space. We propose a novel CS-BPCA process that integrates compressed sensing and Bayesian principal component analysis to perform a two-level label transformation, resulting in an optimally compressed continuous target space. Besides leveraging correlation and sparsity of a large label space for effective compression, an optimal compressing rate and the relative importance of the resultant targets are automatically determined through Bayesian inference. Furthermore, the orthogonality of the transformed space completely decouples the correlations among targets, which significantly simplifies multi-label sampling in the target space. We define a novel sampling function that leverages  a multi-output  Gaussian  Process  (MOGP). Gradient-free optimization strategies are developed to achieve fast online hyper-parameter learning and model retraining for active learning. Experimental results over multiple real-world datasets and comparison with competitive multi-label active learning models demonstrate the effectiveness of the proposed framework. 
","['Rochester Institute of Technology', 'Rochester Institute of Technology']"
2019,Myopic Posterior Sampling for Adaptive Goal Oriented Design of Experiments,"Kirthevasan Kandasamy, Willie Neiswanger, Reed Zhang, Akshay Krishnamurthy, Jeff Schneider, Barnabás Póczos",https://icml.cc/Conferences/2019/Schedule?showEvent=3616,"Bayesian methods for adaptive decision-making, such as Bayesian optimisation,
active learning, and active search have seen great success in relevant applications.
However, real world data collection tasks are more broad and complex, as we may need to
achieve a combination of the above goals and/or application specific goals.
In such scenarios, specialised methods have limited applicability.
In this work,
we design a new myopic strategy for a wide class of adaptive design of
experiment (DOE) problems, where we wish to collect data in order to fulfil a given goal.
Our approach, Myopic Posterior Sampling (MPS), 
which is inspired by the classical posterior sampling algorithm
for multi-armed bandits,
enables us to address a broad suite of DOE tasks where a practitioner may
incorporate domain expertise about the system and specify her
desired goal via a reward function.
Empirically, this general-purpose strategy is competitive with more
specialised methods in a wide array of synthetic and real world DOE tasks.
More importantly, it enables
addressing complex DOE goals where no existing method seems applicable.
On the theoretical side, we leverage ideas from adaptive submodularity and
reinforcement learning to derive conditions under which MPS achieves
sublinear regret against natural benchmark policies.
","['Carnegie Mellon University', 'CMU', 'Carnegie Mellon University', 'Microsoft Research', 'Uber/CMU', 'CMU']"
2019,Bayesian Generative Active Deep Learning,"Toan Tran, Thanh-Toan Do, Ian Reid, Gustavo Carneiro",https://icml.cc/Conferences/2019/Schedule?showEvent=3619,"Deep learning models have demonstrated outstanding performance in several problems, but their training process tends to require immense amounts of computational and human resources for training and labeling, constraining the types of problems that can be tackled.
Therefore, the design of effective training methods that require small labeled training sets is an important research direction that will allow a more effective use of resources.
Among current approaches designed to address this issue, two are particularly interesting: data augmentation and active learning.  
Data augmentation achieves this goal by artificially generating new training points, while active learning relies on the selection of the ``most informative'' subset of unlabeled training samples to be labelled by an oracle.
Although successful in practice, data augmentation can waste computational resources because it indiscriminately generates samples that are not guaranteed to be informative, and active learning selects a small subset of informative samples (from a large un-annotated set) that may be insufficient for the training process. 
In this paper, we propose a Bayesian generative active deep learning approach that combines active learning with data augmentation -- we provide theoretical and empirical evidence (MNIST, CIFAR-$\{10,100\}$, and SVHN) that our approach has more efficient training and better classification results than data augmentation and active learning.","['University of Adelaide', 'The University of Liverpool', '""University of Adelaide, Australia""', 'University of Adelaide']"
2019,Active Learning for Probabilistic Structured Prediction of Cuts and Matchings,"Sima Behpour, Anqi Liu, Brian Ziebart",https://icml.cc/Conferences/2019/Schedule?showEvent=3830,"Active learning methods, like uncertainty sampling, combined with probabilistic prediction techniques  have  achieved  success  in  various  problems like image classification and text classification.  For  more  complex  multivariate  prediction tasks, the relationships between labels play an important role in designing structured classifiers with better performance. However, computational time complexity limits prevalent probabilistic methods from effectively supporting active learning. Specifically, while non-probabilistic methods based on structured support vector ma-chines can be tractably applied to predicting cuts and bipartite matchings, conditional random fields are intractable for these structures. We propose an adversarial approach for active learning with structured prediction domains that is tractable for cuts  and  matching.  We  evaluate  this  approach algorithmically in two important structured prediction problems: multi-label classification and object tracking in videos. We demonstrate better accuracy and computational efficiency for our proposed method.
","['University of Pennsylvania', 'Caltech', 'University of Illinois at Chicago']"
2019,Active Learning with Disagreement Graphs,"Corinna Cortes, Giulia DeSalvo, Mehryar Mohri, Ningshan Zhang, Claudio Gentile",https://icml.cc/Conferences/2019/Schedule?showEvent=4190,"We present two novel enhancements of an online importance-weighted active learning algorithm IWAL, using the properties of disagreements among hypotheses. The first enhancement, IWALD, prunes the hypothesis set with a more aggressive strategy based on the disagreement graph. We show that IWAL-D improves the generalization performance and the label complexity of the original IWAL, and quantify the improvement in terms of the disagreement graph coefficient. The second enhancement, IZOOM, further improves IWAL-D by adaptively zooming into the current version space and thus reducing the best-in-class error. We show that IZOOM admits favorable theoretical guarantees with the changing hypothesis set. We report experimental results on multiple datasets and demonstrate that the proposed algorithms achieve better test performances than IWAL given the same amount of labeling budget.
","['Google Research', 'Google Research', 'Courant Institute and Google Research', 'New York University', 'INRIA and Google']"
2019,Multi-Frequency Vector Diffusion Maps,"Yifeng Fan, Zhizhen Zhao",https://icml.cc/Conferences/2019/Schedule?showEvent=3945,"We introduce multi-frequency vector diffusion maps (MFVDM), a new framework for organizing and analyzing high dimensional data sets. The new method is a mathematical and algorithmic generalization of vector diffusion maps (VDM) and other non-linear dimensionality reduction methods. The idea of MFVDM is to incorporates multiple unitary irreducible representations of the alignment group which introduces robustness to noise. We illustrate the efficacy of MFVDM on synthetic and cryo-EM image datasets, achieving better nearest neighbors search and alignment estimation than other baselines as VDM and diffusion maps (DM), especially on extremely noisy data.
","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana Champaign']"
2019,Co-manifold learning with missing data,"Gal Mishne, Eric Chi, Ronald Coifman",https://icml.cc/Conferences/2019/Schedule?showEvent=3946,"Representation learning is typically applied to only one mode of a data matrix, either its rows or columns. Yet in many applications, there is an underlying geometry to both the rows and the columns. We propose utilizing this coupled structure to perform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data setting. Our unsupervised approach consists of three components. We first solve a family of optimization problems to estimate a complete matrix at multiple scales of smoothness. We then use this collection of smooth matrix estimates to compute pairwise distances on the rows and columns based on a new multi-scale metric that implicitly introduces a coupling between the rows and the columns. Finally, we construct row and column representations from these multi-scale metrics.  We demonstrate that our approach outperforms competing methods in both data visualization and clustering.
","['Yale', 'North Carolina State University', 'Yale University']"
2019,Hybrid Models with Deep and Invertible Features,"Eric Nalisnick, Akihiro Matsukawa, Yee-Whye Teh, Dilan Gorur, Balaji Lakshminarayanan",https://icml.cc/Conferences/2019/Schedule?showEvent=4312,"We propose a neural hybrid model consisting of a linear model defined on a set of features computed by a deep, invertible transformation (i.e. a normalizing flow). An attractive property of our model is that both p(features),  the density of the features,  and  p(targets|features), the predictive distribution, can be computed exactly in a single feed-forward pass.   We show that our hybrid model, despite the invertibility constraints, achieves similar accuracy to purely predictive models.  Yet the generative component remains a good model of the input features despite the hybrid optimization objective.  This offers additional capabilities such as detection of out-of-distribution inputs and enabling semi-supervised learning.  The availability of the exact joint density p(targets, features) also allows us to compute many quantities readily, making our hybrid model a useful building block for downstream applications of probabilistic deep learning.
","['University of Cambridge & DeepMind', 'DeepMind', 'Oxford and DeepMind', '', 'Google DeepMind']"
2019,State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations,"Alex Lamb, Jonathan Binas, Anirudh Goyal, Sandeep Subramanian, Ioannis Mitliagkas, Yoshua Bengio, Michael Mozer",https://icml.cc/Conferences/2019/Schedule?showEvent=3949,"Machine learning promises methods that generalize well from finite labeled data. However, the brittleness of existing neural net approaches is revealed by notable failures, such as the existence of adversarial examples that are misclassified despite being nearly identical to a training example, or the inability of recurrent sequence-processing nets to stay on track without teacher forcing. We introduce a method, which we refer to as state reification, that involves modeling the distribution of hidden states over the training data and then projecting hidden states observed during testing toward this distribution. Our intuition is that if the network can remain in a familiar manifold of hidden space, subsequent layers of the net should be well trained to respond appropriately. We show that this state-reification method helps neural nets to generalize better, especially when labeled data are sparse, and also helps overcome the challenge of achieving robust generalization with adversarial training.
","['Universite de Montreal', 'Mila, Montreal', 'Université de Montréal', 'MILA', 'MILA, UdeM', 'Mila / U. Montreal', 'Google Research &            U. Colorado Boulder']"
2019,Variational Laplace Autoencoders,"Yookoon Park, Chris Kim, Gunhee Kim",https://icml.cc/Conferences/2019/Schedule?showEvent=3667,"Variational autoencoders employ an amortized inference model to approximate the posterior of latent variables. However, such amortized variational inference faces two challenges: (1) the limited posterior expressiveness of fully-factorized Gaussian assumption and (2) the amortization error of the inference model. We present a novel approach that addresses both challenges. First, we focus on ReLU networks with Gaussian output and illustrate their connection to probabilistic PCA. Building on this observation, we derive an iterative algorithm that finds the mode of the posterior and apply fullcovariance Gaussian posterior approximation centered on the mode. Subsequently, we present a general framework named Variational Laplace Autoencoders (VLAEs) for training deep generative models. Based on the Laplace approximation of the latent variable posterior, VLAEs enhance the expressiveness of the posterior while reducing the amortization error. Empirical results on MNIST, Omniglot, Fashion-MNIST, SVHN and CIFAR10 show that the proposed approach significantly outperforms other recent amortized or iterative methods on the ReLU networks.
","['Seoul National University', 'Seoul National University', 'Seoul National University']"
2019,Latent Normalizing Flows for Discrete Sequences,"Zachary Ziegler, Alexander Rush",https://icml.cc/Conferences/2019/Schedule?showEvent=4292,"Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.
","['Harvard University', 'Harvard University']"
2019,Multi-objective training of Generative Adversarial Networks with multiple discriminators,"Isabela Albuquerque, Joao Monteiro, Thang Doan, Breandan Considine, Tiago Falk, Ioannis Mitliagkas",https://icml.cc/Conferences/2019/Schedule?showEvent=4114,"Recent literature has demonstrated promising results for training Generative Adversarial Networks by employing a set of discriminators, in contrast to the traditional game involving one generator against a single adversary. Such methods perform single-objective optimization on some simple consolidation of the losses, e.g. an arithmetic average. In this work, we revisit the multiple-discriminator setting by framing the simultaneous minimization of losses provided by different models as a multi-objective optimization problem. Specifically, we evaluate the performance of multiple gradient descent and the hypervolume maximization algorithm on a number of different datasets. Moreover, we argue that the previously proposed methods and hypervolume maximization can all be seen as variations of multiple gradient descent in which the update direction can be computed efficiently. Our results indicate that hypervolume maximization presents a better compromise between sample quality and computational cost than previous methods.
","['Institut National de la Recherche Scientifique', 'Institut National de la Recherche Scientifique (INRS)', 'McGill University', 'Mila', 'INRS-EMT', 'MILA, UdeM']"
2019,Learning Discrete and Continuous Factors of Data via Alternating Disentanglement,"Yeonwoo Jeong, Hyun Oh Song",https://icml.cc/Conferences/2019/Schedule?showEvent=4023,"We address the problem of unsupervised disentanglement of discrete and continuous explanatory factors of data. We first show a simple procedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or perform importance sampling, via cascading the information flow in the beta-VAE framework. Furthermore, we propose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by employing a separate discrete inference procedure.
This leads to an interesting alternating minimization problem which switches between finding the most likely discrete configuration given the continuous factors and updating the variational encoder based on the computed discrete factors. Experiments show that the proposed method clearly disentangles discrete factors and significantly outperforms current disentanglement methods based on the disentanglement score and inference network classification score. The source code is available at https://github.com/snumllab/DisentanglementICML19.
","['Seoul National University', 'Seoul National University']"
2019,Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables,"Friso Kingma, Pieter Abbeel, Jonathan Ho",https://icml.cc/Conferences/2019/Schedule?showEvent=4305,"The bits-back argument suggests that latent variable models can be turned into lossless compression schemes. Translating the bits-back argument into efficient and practical lossless compression schemes for general latent variable models, however, is still an open problem. Bits-Back with Asymmetric Numeral Systems (BB-ANS), recently proposed by Townsend et al,. 2019, makes bits-back coding practically feasible for latent variable models with one latent layer, but it is inefficient for hierarchical latent variable models. In this paper we propose Bit-Swap, a new compression scheme that generalizes BB-ANS and achieves strictly better compression rates for hierarchical latent variable models with Markov chain structure. Through experiments we verify that Bit-Swap results in lossless compression rates that are empirically superior to existing techniques.
","['UC Berkeley', 'OpenAI / UC Berkeley', 'UC Berkeley']"
2019,Graphite: Iterative Generative Modeling of Graphs,"Aditya Grover, Aaron Zweig, Stefano Ermon",https://icml.cc/Conferences/2019/Schedule?showEvent=4008,"Graphs are a fundamental abstraction for modeling relational data. However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses statistical and computational challenges. In this work, we propose Graphite, an algorithmic framework for unsupervised learning of representations over nodes in large graphs using deep latent variable generative models. Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding. On a wide variety of synthetic and benchmark datasets, Graphite outperforms competing approaches for the tasks of density estimation, link prediction, and node classification. Finally, we derive a theoretical connection between message passing in graph neural networks and mean-field variational inference.
","['Stanford University', 'Stanford University', 'Stanford University']"
2019,MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets,"Pierre-Alexandre Mattei, Jes Frellsen",https://icml.cc/Conferences/2019/Schedule?showEvent=4140,"We consider the problem of handling missing data with deep latent variable models (DLVMs). First, we present a simple technique to train DLVMs when the training set contains missing-at-random data. Our approach, called MIWAE, is based on the importance-weighted autoencoder (IWAE), and maximises a potentially tight lower bound of the log-likelihood of the observed data. Compared to the original IWAE, our algorithm does not induce any additional computational overhead due to the missing data. We also develop Monte Carlo techniques for single and multiple imputation using a DLVM trained on an incomplete data set. We illustrate our approach by training a convolutional DLVM on incomplete static binarisations of MNIST. Moreover, on various continuous data sets, we show that MIWAE provides extremely accurate single imputations, and is highly competitive with state-of-the-art methods.
","['IT University Copenhagen', 'IT University of Copenhagen']"
2019,On Scalable and Efficient Computation of Large Scale Optimal Transport,"Yujia Xie, Minshuo Chen, Haoming Jiang, Tuo Zhao, Hongyuan Zha",https://icml.cc/Conferences/2019/Schedule?showEvent=3918,"Optimal Transport (OT) naturally arises in many machine learning applications, yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.
","['Georgia Institute of Technology', 'Georgia Tech', 'Georgia Tech', 'Georgia Institute of Technology', 'Georgia Institute of Technology']"
2019,Understanding and correcting pathologies in the training of learned optimizers,"Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, Jascha Sohl-Dickstein",https://icml.cc/Conferences/2019/Schedule?showEvent=3868,"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. Moreover, by training the optimizer against validation loss (as opposed to training loss), we are able to learn optimizers that train networks to generalize better than first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss.
","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']"
2019,Demystifying Dropout,"Hongchang Gao, Jian Pei, Heng Huang",https://icml.cc/Conferences/2019/Schedule?showEvent=4061,"Dropout is a popular technique to train large-scale deep neural networks to alleviate the overfitting problem. To disclose the underlying reasons for its gain, numerous works have tried to explain it from different perspectives. In this paper, unlike existing works, we explore it from a new perspective to provide new insight into this line of research. In detail, we disentangle the forward and backward pass of dropout. Then, we find that these two passes need different levels of noise to improve the generalization performance of deep neural networks. Based on this observation, we propose the augmented dropout which employs different dropping strategies in the forward and backward pass. Experimental results have verified the effectiveness of our proposed method. 
","['University of Pittsburgh', 'Simon Fraser University', 'University of Pittsburgh']"
2019,Ladder Capsule Network,"Taewon Jeong, Youngmin Lee, Heeyoung Kim",https://icml.cc/Conferences/2019/Schedule?showEvent=3735,"We propose a new architecture of the capsule network called the ladder capsule network, which has an alternative building block to the dynamic routing algorithm in the capsule network (Sabour et al., 2017). Motivated by the need for using only important capsules during training for robust performance, we first introduce a new layer called the pruning layer, which removes irrelevant capsules. Based on the selected capsules, we construct higher-level capsule outputs. Subsequently, to capture the part-whole spatial relationships, we introduce another new layer called the ladder layer, the outputs of which are regressed lower-level capsule outputs from higher-level capsules. Unlike the capsule network adopting the routing-by-agreement, the ladder capsule network uses backpropagation from a loss function to reconstruct the lower-level capsule outputs from higher-level capsules; thus, the ladder layer implements the reverse directional inference of the agreement/disagreement mechanism of the capsule network. The experiments on MNIST demonstrate that the ladder capsule network learns an equivariant representation and improves the capability to extrapolate or generalize to pose variations.
","['KAIST', 'KAIST', 'KAIST']"
2019,Unreproducible Research is Reproducible,"Xavier Bouthillier, César Laurent, Pascal Vincent",https://icml.cc/Conferences/2019/Schedule?showEvent=4177,"The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.
","['Mila - Université de Montréal', 'MILA', 'U Montreal']"
2019,Geometric Scattering for Graph Data Analysis,"Feng Gao, Guy Wolf, Matthew Hirn",https://icml.cc/Conferences/2019/Schedule?showEvent=3844,"We explore the generalization of scattering transforms from traditional (e.g., image or audio) signals to graph data, analogous to the generalization of ConvNets in geometric deep learning, and the utility of extracted graph features in graph data analysis. In particular, we focus on the capacity of these features to retain informative variability and relations in the data (e.g., between individual graphs, or in aggregate), while relating our construction to previous theoretical results that establish the stability of similar transforms to families of graph deformations. We demonstrate the application of our geometric scattering features in graph classification of social network data, and in data exploration of biochemistry data.
","['Michigan State University', 'Université de Montréal', 'Michigan State University']"
2019,Robust Inference via Generative Classifiers for Handling Noisy Labels,"Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, Jinwoo Shin",https://icml.cc/Conferences/2019/Schedule?showEvent=3968,"Large-scale datasets may contain significant proportions of noisy (incorrect) class labels, and it is well-known that modern deep neural networks (DNNs) poorly generalize from such noisy training datasets. To mitigate the issue, we propose a novel inference method, termed Robust Generative classifier (RoG), applicable to any discriminative (e.g., softmax) neural classifier pre-trained on noisy datasets. In particular, we induce a generative classifier on top of hidden feature spaces of the pre-trained DNNs, for obtaining a more robust decision boundary. By estimating the parameters of generative classifier using the minimum covariance determinant estimator, we significantly improve the classification accuracy with neither re-training of the deep model nor changing its architectures. With the assumption of Gaussian distribution for features, we prove that RoG generalizes better than baselines under noisy labels. Finally, we propose the ensemble version of RoG to improve its performance by investigating the layer-wise characteristics of DNNs. Our extensive experimental results demonstrate the superiority of RoG given different learning models optimized by several training techniques to handle diverse scenarios of noisy labels. 
","['KAIST', 'KAIST', 'University of Michigan', 'Google / U. Michigan', 'UIUC', 'KAIST, AITRICS']"
2019,LIT: Learned Intermediate Representation Training for Model Compression,"Animesh Koratana, Daniel Kang, Peter Bailis, Matei Zaharia",https://icml.cc/Conferences/2019/Schedule?showEvent=4318,"Researchers have proposed a range of model compression techniques to reduce the
computational and memory footprint of deep neural networks (DNNs). In
this work, we introduce Learned Intermediate representation
Training (LIT), a novel model compression technique that outperforms a
range of recent model compression techniques by leveraging the highly repetitive
structure of modern DNNs (e.g., ResNet). LIT uses a teacher DNN to train a
student DNN of reduced depth by leveraging two key ideas: 1) LIT directly
compares intermediate representations of the teacher and student model and 2)
LIT uses the intermediate representation from the teacher model's previous block
as input to the current student block during training, improving stability of
intermediate representations in the student network. We show that LIT can
substantially reduce network size without loss in accuracy on a range of DNN
architectures and datasets. For example, LIT can compress ResNet on CIFAR10 by
3.4$\times$ outperforming network slimming and FitNets. Furthermore, LIT can
compress, by depth, ResNeXt 5.5$\times$ on CIFAR10 (image classification), VDCNN
by 1.7$\times$ on Amazon Reviews (sentiment analysis), and StarGAN by
1.8$\times$ on CelebA (style transfer, i.e., GANs).","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford and Databricks']"
2019,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,"Nicholas Frosst, Nicolas Papernot, Geoffrey Hinton",https://icml.cc/Conferences/2019/Schedule?showEvent=4280,"We explore and expand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that maximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally improved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.
","['Google Brain', '', 'Google']"
2019,What is the Effect of Importance Weighting in Deep Learning?,"Jonathon Byrd, Zachary Lipton",https://icml.cc/Conferences/2019/Schedule?showEvent=4247,"Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. This work is inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, prompting us to ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts models early in training, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? Our experiments confirm these findings across 
a range of architectures and datasets.
","['Carnegie Mellon University', 'Carnegie Mellon University']"
2019,Similarity of Neural Network Representations Revisited,"Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton",https://icml.cc/Conferences/2019/Schedule?showEvent=4185,"Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.
","['Google Brain', 'Google Brain', 'Google / U. Michigan', 'Google']"
2019,Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations,"Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, Christopher Re",https://icml.cc/Conferences/2019/Schedule?showEvent=3573,"Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural prior they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to $1024$. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points---the first time a structured approach has done so---with 4X faster inference speed and 40X fewer parameters.","['Stanford', 'Stanford University', 'University at Buffalo', 'University at Buffalo, SUNY', 'Stanford']"
2019,"Distributed, Egocentric Representations of Graphs for Detecting Critical Structures","Ruo-Chun Tzeng, Shan-Hung (Brandon) Wu",https://icml.cc/Conferences/2019/Schedule?showEvent=3807,"We study the problem of detecting critical structures using a graph embedding model. Existing graph embedding models lack the ability to precisely detect critical structures that are specific to a task at the global scale. In this paper, we propose a novel graph embedding model, called the Ego-CNNs, that employs the ego-convolutions convolutions at each layer and stacks up layers using an ego-centric way to detects precise critical structures efficiently. An Ego-CNN can be jointly trained with a task model and help explain/discover knowledge for the task. We conduct extensive experiments and the results show that Ego-CNNs (1) can lead to comparable task performance as the state-of-the-art graph embedding models, (2) works nicely with CNN visualization techniques to illustrate the detected structures, and (3) is efficient and can incorporate with scale-free priors, which commonly occurs in social network datasets, to further improve the training efficiency.
","['Microsoft Inc.', 'National Tsing Hua University']"
2019,Breaking the Softmax Bottleneck via Learnable Monotonic Pointwise Non-linearities,"Octavian-Eugen Ganea, Sylvain Gelly, Gary Becigneul, Aliaksei Severyn",https://icml.cc/Conferences/2019/Schedule?showEvent=4329,"The Softmax function on top of a final linear layer is the de facto method to output probability distributions in neural networks. In many applications such as language models or text generation, this model has to produce distributions over large output vocabularies. Recently, this has been shown to have limited representational capacity due to its connection with the rank bottleneck in matrix factorization. However, little is known about the limitations of Linear-Softmax for quantities of practical interest such as cross entropy or mode estimation, a direction that we explore here. As an efficient and effective solution to alleviate this issue, we propose to learn parametric monotonic functions on top of the logits. We theoretically investigate the rank increasing capabilities of such monotonic functions. Empirically, our method improves in two different quality metrics over the traditional Linear-Softmax layer in synthetic and real language model experiments, adding little time or memory overhead, while being comparable to the more computationally expensive mixture of Softmaxes.
","['ETH Zurich', 'Google Brain', 'ETHZ', 'Google']"
2019,Multi-Object Representation Learning with Iterative Variational Inference,"Klaus Greff, Raphael Lopez Kaufman, Rishabh Kabra, Nicholas Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner",https://icml.cc/Conferences/2019/Schedule?showEvent=4327,"Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities.
Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step.
Instead, we argue for the importance of learning to segment and represent objects jointly.
We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations.
Our method learns -- without supervision -- to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. 
We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.
","['IDSIA', 'Deepmind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2019,Cross-Domain 3D Equivariant Image Embeddings,"Carlos Esteves, Avneesh Sud, Zhengyi Luo, Kostas Daniilidis, Ameesh Makadia",https://icml.cc/Conferences/2019/Schedule?showEvent=3729,"Spherical convolutional networks have been introduced recently as tools to learn powerful feature representations of 3D shapes. Spherical CNNs are equivariant to 3D rotations making them ideally suited to applications where 3D data may be observed in arbitrary orientations. In this paper we learn 2D image embeddings with a similar equivariant structure: embedding the image of a 3D object should commute with rotations of the object.  We introduce a cross-domain embedding from 2D images into a spherical CNN latent space. This embedding encodes images with 3D shape properties and is equivariant to 3D rotations of the observed object. The model is supervised only by target embeddings obtained from a spherical CNN pretrained for 3D shape classification. We show that learning a rich embedding for images with appropriate geometric structure is sufficient for tackling varied applications, such as relative pose estimation and novel view synthesis, without requiring additional task-specific supervision.
","['University of Pennsylvania', 'Google', 'University of Pennsylvania', 'University of Pennsylvania', 'Google Research']"
2019,Loss Landscapes of Regularized Linear Autoencoders,"Daniel Kunin, Jonathan Bloom, Aleksandrina Goeva, Cotton Seed",https://icml.cc/Conferences/2019/Schedule?showEvent=4260,"Autoencoders are a deep learning model for representation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subspace spanned by the top principal directions but cannot learn the principal directions themselves. In this paper, we prove that $L_2$-regularized LAEs are symmetric at all critical points and learn the principal directions as the left singular vectors of the decoder. We smoothly parameterize the critical manifold and relate the minima to the MAP estimate of probabilistic PCA. We illustrate these results empirically and consider implications for PCA algorithms, computational neuroscience, and the algebraic topology of learning.","['Stanford University', 'Broad Institute', 'Broad Institute of MIT and Harvard', 'Broad Institute of MIT and Harvard']"
2019,Hyperbolic Disk Embeddings for Directed Acyclic Graphs,"Ryota Suzuki, Ryusuke Takahama, Shun Onoda",https://icml.cc/Conferences/2019/Schedule?showEvent=4043,"Obtaining continuous representations of structural data such as directed acyclic graphs (DAGs) has gained attention in machine learning and artificial intelligence. However, embedding complex DAGs in which both ancestors and descendants of nodes are exponentially increasing is difficult. Tackling in this problem, we develop Disk Embeddings, which is a framework for embedding DAGs into quasi-metric spaces. Existing state-of-the-art methods, Order Embeddings and Hyperbolic Entailment Cones, are instances of Disk Embedding in Euclidean space and spheres respectively. Furthermore, we propose a novel method Hyperbolic Disk Embeddings to handle exponential growth of relations. The results of our experiments show that our Disk Embedding models outperform existing methods especially in complex DAGs other than trees.
","['LAPRAS Inc.', 'LAPRAS Inc.', 'scouty Inc.']"
2019,LatentGNN: Learning Efficient Non-local Relations for Visual Recognition,"Songyang Zhang, Xuming He, Shipeng Yan",https://icml.cc/Conferences/2019/Schedule?showEvent=3647,"Capturing long-range dependencies in feature representations is crucial for many visual recognition tasks. Despite recent successes of deep convolutional networks, it remains challenging to model non-local context relations between visual features. A promising strategy is to model the feature context by a fully-connected graph neural network (GNN), which augments traditional convolutional features with an estimated non-local context representation. However, most GNN-based approaches require computing a dense graph affinity matrix and hence have difficulty in scaling up to tackle complex real-world visual problems. In this work, we propose an efficient and yet flexible non-local relation representation based on a novel class of graph neural networks. Our key idea is to introduce a latent space to reduce the complexity of graph, which allows us to use a low-rank representation for the graph affinity matrix and to achieve a linear complexity in computation. Extensive experimental evaluations on three major visual recognition tasks show that our method outperforms the prior works with a large margin while maintaining a low computation cost.   
","['ShanghaiTech University', 'ShanghaiTech University', 'ShanghaiTech University']"
2019,Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness,"Raphael Suter, Djordje Miladinovic, Bernhard Schölkopf, Stefan Bauer",https://icml.cc/Conferences/2019/Schedule?showEvent=4055,"The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size. 
","['ETH Zurich', 'ETH Zurich', 'MPI for Intelligent Systems Tübingen, Germany', 'MPI for Intelligent Systems']"
2019,Lorentzian Distance Learning for Hyperbolic Representations,"Marc Law, Renjie Liao, Jake Snell, Richard Zemel",https://icml.cc/Conferences/2019/Schedule?showEvent=3914,"We introduce an approach to learn representations based on the Lorentzian distance in hyperbolic geometry. Hyperbolic geometry is especially suited to hierarchically-structured datasets, which are prevalent in the real world. Current hyperbolic representation learning methods compare examples with the Poincar\'e distance. They try to minimize the distance of each node in a hierarchy with its descendants while maximizing its distance with other nodes. This formulation produces node representations close to the centroid of their descendants. To obtain efficient and interpretable algorithms, we exploit the fact that the centroid w.r.t the squared Lorentzian distance can be written in closed-form. We show that the Euclidean norm of such a centroid decreases as the curvature of the hyperbolic space decreases. This property makes it appropriate to represent hierarchies where parent nodes minimize the distances to their descendants and have smaller Euclidean norm than their children. Our approach obtains state-of-the-art results in retrieval and classification tasks on different datasets.
","['University of Toronto', 'University of Toronto', 'University of Toronto', 'Vector Institute']"
2019,Batch Policy Learning under Constraints,"Hoang Le, Cameron Voloshin, Yisong Yue",https://icml.cc/Conferences/2019/Schedule?showEvent=3576,"When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. As part of off-policy learning, we propose a simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.
","['Caltech', 'Caltech', 'Caltech']"
2019,Quantifying Generalization in Reinforcement Learning,"Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, John Schulman",https://icml.cc/Conferences/2019/Schedule?showEvent=3742,"In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.
","['OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI']"
2019,Learning Latent Dynamics for Planning from Pixels,"Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson",https://icml.cc/Conferences/2019/Schedule?showEvent=3764,"Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.
","['Google Brain & University of Toronto', 'Google DeepMind', 'Google', 'University of Michigan', 'Google', 'Google / U. Michigan', 'Google Brain']"
2019,Projections for Approximate Policy Iteration Algorithms,"Riad Akrour, Joni Pajarinen, Jan Peters, Gerhard Neumann",https://icml.cc/Conferences/2019/Schedule?showEvent=4056,"Approximate policy iteration is a class of reinforcement learning (RL) algorithms where the policy is encoded using a function approximator and which has been especially prominent in RL with continuous action spaces. In this class of RL algorithms, ensuring increase of the policy return during policy update often requires to constrain the change in action distribution. Several approximations exist in the literature to solve this constrained policy update problem. In this paper, we propose to improve over such solutions by introducing a set of projections that transform the constrained problem into an unconstrained one which is then solved by standard gradient descent. Using these projections, we empirically demonstrate that our approach can improve the policy update solution and the control over exploration of existing approximate policy iteration algorithms.
","['TU Darmstadt', 'TU Darmstadt', 'TU Darmstadt + Max Planck Institute for Intelligent Systems', 'University of Lincoln']"
2019,Learning Structured Decision Problems with Unawareness,"Craig Innes, Alex Lascarides",https://icml.cc/Conferences/2019/Schedule?showEvent=3648,"Structured models of decision making often assume an agent is aware of all possible states and actions in advance. This assumption is sometimes untenable. In this paper, we learn Bayesian Decision Networks from both domain exploration and expert assertions in a way which guarantees convergence to optimal behaviour, even when the agent starts unaware of actions or belief variables that are critical to success. Our experiments show that our agent learns optimal behaviour on both small and large decision problems, and that allowing an agent to conserve information upon making new discoveries results in faster convergence.
","['University of Edinburgh', 'University of Edinburgh']"
2019,Calibrated Model-Based Deep Reinforcement Learning,"Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, Stefano Ermon",https://icml.cc/Conferences/2019/Schedule?showEvent=3997,"Estimates of predictive uncertainty are important for accurate model-based planning and reinforcement learning. However, predictive uncertainties --- especially ones derived from modern deep learning systems --- can be inaccurate and impose a bottleneck on performance. This paper explores which uncertainties are needed for model-based reinforcement learning and argues that ideal uncertainties should be calibrated, i.e. their probabilities should match empirical frequencies of predicted events. We describe a simple way to augment any model-based reinforcement learning agent with a calibrated model and show that doing so consistently improves planning, sample complexity, and exploration. On the \textsc{HalfCheetah} MuJoCo task, our system achieves state-of-the-art performance using 50\% fewer samples than the current leading approach. Our findings suggest that calibration can improve the performance of model-based reinforcement learning with minimal computational and implementation overhead.
","['Stanford Universtiy', 'Stanford University / Afresh', 'Stanford', 'Afresh Technologies', 'Afresh Technologies', 'Stanford University']"
2019,Reinforcement Learning in Configurable Continuous Environments,"Alberto Maria Metelli, Emanuele Ghelfi, Marcello Restelli",https://icml.cc/Conferences/2019/Schedule?showEvent=4037,"Configurable Markov Decision Processes (Conf-MDPs) have been recently introduced as an extension of the usual MDP model to account for the possibility of configuring the environment to improve the agent's performance. Currently, there is still no suitable algorithm to solve the learning problem for real-world Conf-MDPs. In this paper, we fill this gap by proposing a trust-region method, Relative Entropy Model Policy Search (REMPS), able to learn both the policy and the MDP configuration in continuous domains without requiring the knowledge of the true model of the environment. After introducing our approach and providing a finite-sample analysis, we empirically evaluate REMPS on both benchmark and realistic environments by comparing our results with those of the gradient methods.
","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano']"
2019,Target-Based Temporal-Difference Learning,"Donghwan Lee, Niao He",https://icml.cc/Conferences/2019/Schedule?showEvent=4092,"The use of target networks has been a popular and key component of recent deep Q-learning algorithms for reinforcement learning, yet little is known from the theory side. In this work, we introduce a new family of target-based temporal difference (TD) learning algorithms that maintain two separate learning parameters – the target variable and online variable. We propose three members in the family, the averaging TD, double TD, and periodic TD, where the target variable is updated through an averaging, symmetric, or periodic fashion, respectively, mirroring those techniques used in deep Q-learning practice. We establish asymptotic convergence analyses for both averaging TD and double TD and a finite sample
analysis for periodic TD. In addition, we provide some simulation results showing potentially superior convergence of these target-based TD algorithms compared to the standard TD-learning. While this work focuses on linear function approximation
and policy evaluation setting, we consider this as a meaningful step towards the theoretical understanding of deep Q-learning variants with target networks.
","['University of Illinois, Urbana-Champaign', 'UIUC']"
2019,Iterative Linearized Control: Stable Algorithms and Complexity Guarantees,"Vincent Roulet, Dmitriy Drusvyatskiy, Siddhartha Srinivasa, Zaid Harchaoui",https://icml.cc/Conferences/2019/Schedule?showEvent=4201,"We examine popular gradient-based algorithms for nonlinear control in the light of the modern complexity analysis of first-order optimization algorithms. 
The examination reveals that the complexity bounds can be clearly stated in terms of calls to a computational oracle related to dynamic programming 
and implementable by gradient back-propagation using machine learning software libraries such as PyTorch or TensorFlow. Finally, we propose a regularized Gauss-Newton algorithm enjoying worst-case complexity bounds and improved convergence behavior in practice. The software library based on PyTorch is publicly available. 
","['University of Washington', 'University of Washington', 'University of Washington', 'University of Washington']"
2019,Finding Options that Minimize Planning Time,"Yuu Jinnai, David Abel, David Hershkowitz, Michael L. Littman, George Konidaris",https://icml.cc/Conferences/2019/Schedule?showEvent=4275,"We formalize the problem of selecting the optimal set of options for planning as that of  computing the smallest set of options so that planning converges in less than a given maximum of value-iteration passes. We first show that the problem is  $\NP$-hard, even if the task is constrained to be deterministic---the first such complexity result for option discovery. We then present the first polynomial-time boundedly suboptimal approximation algorithm for this setting, and empirically evaluate it against both the optimal options and a representative collection of heuristic approaches in simple grid-based domains.","['Brown University', 'Brown University', 'Carnegie Mellon University', 'Brown University', 'Brown']"
2019,Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement,"Wouter Kool, Herke van Hoof, Max Welling",https://icml.cc/Conferences/2019/Schedule?showEvent=3878,"The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-$k$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.","['University of Amsterdam', 'University of Amsterdam', 'University of Amsterdam']"
2019,Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs,"Lingbing Guo, Zequn Sun, Wei Hu",https://icml.cc/Conferences/2019/Schedule?showEvent=3605,"We study the problem of knowledge graph (KG) embedding. A widely-established assumption to this problem is that similar entities are likely to have similar relational roles. However, existing related methods derive KG embeddings mainly based on triple-level learning, which lack the capability of capturing long-term relational dependencies of entities. Moreover, triple-level learning is insufficient for the propagation of semantic information among entities, especially for the case of cross-KG embedding. In this paper, we propose recurrent skipping networks (RSNs), which employ a skipping mechanism to bridge the gaps between entities. RSNs integrate recurrent neural networks (RNNs) with residual learning to efficiently capture the long-term relational dependencies within and between KGs. We design an end-to-end framework to support RSNs on different tasks. Our experimental results showed that RSNs outperformed state-of-the-art embedding-based methods for entity alignment and achieved competitive performance for KG completion.
","['Nanjing University', 'Nanjing University', 'Nanjing University']"
2019,Meta-Learning Neural Bloom Filters,"Jack Rae, Sergey Bartunov, Timothy Lillicrap",https://icml.cc/Conferences/2019/Schedule?showEvent=3846,"There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression. In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In applications where inputs arrive at high throughput, or are ephemeral, training a network from scratch is not practical. This motivates the need for few-shot neural data structures. In this paper we explore the learning of approximate set membership over a set of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which is able to achieve significant compression gains over classical Bloom Filters and existing memory-augmented neural networks.
","['DeepMind', 'DeepMind', 'Google DeepMind']"
2019,CoT: Cooperative Training for Generative Modeling of Discrete Data,"Sidi Lu, Lantao Yu, Siyuan Feng, Yaoming Zhu, Weinan Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3885,"In this paper, we study the generative models of sequential discrete data. To tackle the exposure bias problem inherent in maximum likelihood estimation (MLE), generative adversarial networks (GANs) are introduced to penalize the unrealistic generated samples. To exploit the supervision signal from the discriminator, most previous models leverage REINFORCE to address the non-differentiable problem of sequential discrete data. However, because of the unstable property of the training signal during the dynamic process of adversarial training, the effectiveness of REINFORCE, in this case, is hardly guaranteed. To deal with such a problem, we propose a novel approach called Cooperative Training (CoT) to improve the training of sequence generative models. CoT transforms the min-max game of GANs into a joint maximization framework and manages to explicitly estimate and optimize Jensen-Shannon divergence. Moreover, CoT works without the necessity of pre-training via MLE, which is crucial to the success of previous methods. In the experiments, compared to existing state-of-the-art methods, CoT shows superior or at least competitive performance on sample quality, diversity, as well as training stability. 
","['Shanghai Jiao Tong University', 'Stanford University', 'Apex Data & Knowledge Management Lab, Shanghai Jiao Tong University', 'Apex Data & Knowledge Management Lab, Shanghai Jiao Tong University', 'Apex Data & Knowledge Management Lab, Shanghai Jiao Tong University']"
2019,Non-Monotonic Sequential Text Generation,"Sean Welleck, Kiante Brantley, Hal Daume, Kyunghyun Cho",https://icml.cc/Conferences/2019/Schedule?showEvent=4053,"Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy's own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.
","['New York University', 'The University of Maryland College Park', 'Microsoft Research', 'New York University']"
2019,Insertion Transformer: Flexible Sequence Generation via Insertion Operations,"Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit",https://icml.cc/Conferences/2019/Schedule?showEvent=4221,"We present the Insertion Transformer, an iterative, partially autoregressive model for sequence generation based on insertion operations. Unlike typical autoregressive models which rely on a fixed, often left-to-right ordering of the output, our approach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This flexibility confers a number of advantages: for instance, not only can our model be trained to follow specific orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entropy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and partially autoregressive generation (simultaneous insertions at multiple locations). We validate our approach by analyzing its performance on the WMT 2014 English-German machine translation task under various settings for training and decoding. We find that the Insertion Transformer outperforms many prior non-autoregressive approaches to translation at comparable or better levels of parallelism, and successfully recovers the performance of the original Transformer while requiring only logarithmically many iterations during decoding.
","['UC Berkeley', 'Google Brain', 'Google Inc.', 'Google, Inc.']"
2019,Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models,"Eldan Cohen, Christopher Beck",https://icml.cc/Conferences/2019/Schedule?showEvent=4064,"Beam search is the most popular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can potentially lead to a sequence with a higher overall probability. However, work on a number of applications has found that the quality of the highest probability hypothesis found by beam search degrades with large beam widths. We perform an empirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disproportionately based on early, very low probability tokens that are followed by a sequence of tokens with higher (conditional) probability. We show that, empirically, such sequences are more likely to have a lower evaluation score than lower probability sequences without this pattern. Using the notion of search discrepancies from heuristic search, we hypothesize that large discrepancies are the cause of the performance degradation. We show that this hypothesis generalizes the previous ones in machine translation and image captioning. To validate our hypothesis, we show that constraining beam search to avoid large discrepancies eliminates the performance degradation.
","['University of Toronto', 'University of Toronto']"
2019,Trainable Decoding of Sets of Sequences for Neural Sequence Models,"Ashwin Kalyan, Peter Anderson, Stefan Lee, Dhruv Batra",https://icml.cc/Conferences/2019/Schedule?showEvent=4105,"Many sequence prediction tasks admit multiple correct outputs and so, it is often useful to decode a set of outputs that maximize some task-specific set-level metric. However, retooling standard sequence prediction procedures tailored towards predicting the single best output leads to the decoding of sets containing very similar sequences; failing to capture the variation in the output space. To address this, we propose $\nabla$BS, a trainable decoding procedure that outputs a set of sequences, highly valued according to the metric. Our method tightly integrates the training and decoding phases and further allows for the optimization of the task-specific metric addressing the shortcomings of standard sequence prediction. Further, we discuss the trade-offs of commonly used set-level metrics and motivate a new set-level metric that naturally evaluates the notion of ``capturing the variation in the output space''. Finally, we show results on the image captioning task and find that our model outperforms standard techniques and natural ablations.","['Georgia Tech', 'Georgia Tech', 'Georgia Institute of Technology', 'Georgia Institute of Technology / Facebook AI Research']"
2019,Learning to Generalize from Sparse and Underspecified Rewards,"Rishabh Agarwal, Chen Liang, Dale Schuurmans, Mohammad Norouzi",https://icml.cc/Conferences/2019/Schedule?showEvent=4332,"We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback.  Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental
success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy.  We
propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning.  The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms an alternative method for reward learning based on Bayesian Optimization, and achieves the state-of-the-art on
weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively.
","['Google Research, Brain Team', 'Google Brain', 'Google / University of Alberta', 'Google Brain']"
2019,Efficient Training of BERT by Progressively Stacking,"Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, Tie-Yan Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=3819,"Unsupervised pre-training is popularly used in natural language processing. By designing proper unsupervised prediction tasks, a deep neural network can be trained and shown to be effective in many downstream tasks. As the data is usually adequate, the model for pre-training is generally huge and contains millions of parameters. Therefore, the training efficiency becomes a critical issue even when using high-performance hardware. In this paper, we explore an efficient training method for the state-of-the-art bidirectional Transformer (BERT) model. By visualizing the self-attention distribution of different layers at different positions in a well-trained BERT model, we find that in most layers, the self-attention distribution will concentrate locally around its position and the start-of-sentence token. Motivating from this, we propose the stacking algorithm to transfer knowledge from a shallow model to a deep model; then we apply stacking progressively to accelerate BERT training. The experimental results showed that the models trained by our training strategy achieve similar performance to models trained from scratch, but our algorithm is much faster.
","['Peking University', 'Peking University', 'Peking University', 'Microsoft Research Asia', 'Peking University', 'Microsoft Research Asia']"
2019,Decentralized Exploration in Multi-Armed Bandits,"Raphaël Féraud, REDA ALAMI, Romain Laroche",https://icml.cc/Conferences/2019/Schedule?showEvent=3563,"We consider the decentralized exploration problem: a set of players collaborate to identify the best arm by asynchronously interacting with the same stochastic environment. The objective is to insure privacy in the best arm identification problem between asynchronous, collaborative, and thrifty players. In the context of a digital service, we advocate that this decentralized approach allows a good balance between conflicting interests: the providers optimize their services, while
protecting privacy of users and saving resources. We define the privacy level as the amount of information an adversary could infer by intercepting
all the messages concerning a single user. We provide a generic algorithm DECENTRALIZED ELIMINATION, which uses any best arm identification
algorithm as a subroutine. We prove that this algorithm insures privacy, with a low communication cost, and that in comparison to the lower bound of the best arm identification problem, its sample complexity suffers from a penalty depending on the inverse of the probability of the most frequent players. Then, thanks to the genericity of the approach, we extend the proposed algorithm to the non-stationary bandits. Finally, experiments illustrate and complete the analysis.
","['Orange Labs', 'Orange Labs - Paris Saclay University - INRIA', 'Microsoft Research']"
2019,Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback,"Chicheng Zhang, Alekh Agarwal, Hal Daume, John Langford, Sahand Negahban",https://icml.cc/Conferences/2019/Schedule?showEvent=3673,"We investigate the feasibility of learning from both fully-labeled supervised data and contextual bandit data. We specifically consider settings in which the underlying learning signal may be different between these two data sources. Theoretically, we state and prove no-regret algorithms for learning that is robust to divergences between the two sources. Empirically, we evaluate some of these algorithms on a large selection of datasets, showing that our approaches are feasible, and helpful in practice.
","['Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'YALE']"
2019,Exploiting structure of uncertainty for efficient matroid semi-bandits,"Pierre Perrault, Vianney Perchet, Michal Valko",https://icml.cc/Conferences/2019/Schedule?showEvent=3850,"We improve the efficiency of algorithms for stochastic combinatorial semi-bandits. In most interesting problems, state-of-the-art algorithms take advantage of structural properties of rewards, such as independence. However, while being minimax optimal in terms of regret, these algorithms are intractable. In our paper, we first reduce their implementation to a specific submodular maximization. Then, in case of matroid constraints, we design adapted approximation routines, thereby providing the first efficient algorithms that exploit the reward structure. In particular, we improve the state-of-the-art efficient gap-free regret bound by a factor sqrt(k), where k is the maximum action size. Finally, we show how our improvement translates to more general budgeted combinatorial semi-bandits.
","['Inria Lille - Nord Europe', 'ENS Paris Saclay & Criteo AI Lab', 'DeepMind']"
2019,PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits,"Arghya Roy Chaudhuri, Shivaram Kalyanakrishnan",https://icml.cc/Conferences/2019/Schedule?showEvent=3895,"We consider the problem of identifying any k out of the best m arms in an n-armed stochastic multi-armed bandit; framed in the PAC setting, this particular problem generalises both the problem of “best subset selection” (Kalyanakrishnan & Stone, 2010) and that of selecting “one out of the best m” arms (Roy Chaudhuri & Kalyanakrishnan, 2017). We present a lower bound on the worst-case sample complexity for general k, and a fully sequential PAC algorithm, LUCB-k-m, which is more sample-efficient on easy instances. Also, extending our analysis to infinite-armed bandits, we present a PAC algorithm that is independent of n, which identifies an arm from the best ρ fraction of arms using at most an additive poly-log number of samples than compared to the lower bound, thereby improving over Roy Chaudhuri & Kalyanakrishnan (2017) and Aziz et al. (2018). The problem of identifying k > 1 distinct arms from the best ρ fraction is not always well-defined; for a special class of this problem, we present lower and upper bounds. Finally, through a reduction, we establish a relation between upper bounds for the “one out of the best ρ” problem for infinite instances and the “one out of the best m” problem for finite instances. We conjecture that it is more efficient to solve “small” finite instances using the latter formulation, rather than going through the former.
","['Indian Institute of Technology Bombay', 'IIT Bombay']"
2019,Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model,"Gi-Soo Kim, Myunghee Cho Paik",https://icml.cc/Conferences/2019/Schedule?showEvent=3696,"Contextual multi-armed bandit (MAB) algorithms have been shown promising for maximizing cumulative rewards in sequential decision tasks such as news article recommendation systems, web page ad placement algorithms, and mobile health. However, most of the proposed contextual MAB algorithms assume linear relationships between the reward and the context of the action. This paper proposes a new contextual MAB algorithm for a relaxed, semiparametric reward model that supports nonstationarity. The proposed method is less restrictive, easier to implement and faster than two alternative algorithms that consider the same model, while achieving a tight regret upper bound. We prove that the high-probability upper bound of the regret incurred by the proposed algorithm has the same order as the Thompson sampling algorithm for linear reward models. The proposed and existing algorithms are evaluated via simulation and also applied to Yahoo! news article recommendation log data.
","['Seoul National University', 'Seoul National University']"
2019,Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning,"Jakob Foerster, Francis Song, Edward Hughes, Neil Burch, Iain Dunning, Shimon Whiteson, Matthew Botvinick, Michael Bowling",https://icml.cc/Conferences/2019/Schedule?showEvent=3726,"When observing the actions of others, humans make inferences about why they acted as they did, and what this implies about the world; humans also use the fact that their actions will be interpreted in this manner, allowing them to act informatively and thereby communicate efficiently with others. Although learning algorithms have recently achieved superhuman performance in a number of two-player, zero-sum games, scalable multi-agent reinforcement learning algorithms that can discover effective strategies and conventions in complex, partially observable settings have proven elusive. We present the \emph{Bayesian action decoder} (BAD), a new multi-agent learning method that uses an approximate Bayesian update to obtain a public belief that conditions on the actions taken by all agents in the environment. BAD introduces a new Markov decision process, the \emph{public belief MDP}, in which the action space consists of all deterministic partial policies, and exploits the fact that an agent acting only on this public belief state can still learn to use its private information if the action space is augmented to be over all partial policies mapping private information into environment actions. The Bayesian update is closely related to the \emph{theory of mind} reasoning that humans carry out when observing others' actions. We first validate BAD on a proof-of-principle two-step matrix game, where it outperforms policy gradient methods; we then evaluate BAD on the challenging, cooperative partial-information card game Hanabi, where, in the two-player setting, it surpasses all previously published learning and hand-coded approaches, establishing a new state of the art.
","['Facebook AI Research', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'University of Oxford', 'DeepMind', 'DeepMind']"
2019,TarMAC: Targeted Multi-Agent Communication,"Abhishek Das, Theophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Michael Rabbat, Joelle Pineau",https://icml.cc/Conferences/2019/Schedule?showEvent=3610,"We propose a targeted communication architecture for multi-agent reinforcement learning, where agents learn both \emph{what} messages to send and \emph{whom} to address them to while performing cooperative tasks in partially-observable environments. This targeting behavior is learnt solely from downstream task-specific reward without any communication supervision. We additionally augment this with a multi-round communication approach where agents coordinate via multiple rounds of communication before taking actions in the environment.
We evaluate our approach on a diverse set of cooperative multi-agent tasks, of varying difficulties, with varying number of agents, in a variety of environments ranging from 2D grid layouts of shapes and simulated traffic junctions to 3D indoor environments, and demonstrate the benefits of targeted and multi-round communication. Moreover, we show that the targeted communication strategies learned by agents are interpretable and intuitive.
Finally, we show that our architecture can be easily extended to mixed and competitive environments, leading to improved performance and sample complexity over recent state-of-the-art approaches.
","['Georgia Tech', 'Carnegie Mellon University', 'McGill University', 'Georgia Institute of Technology / Facebook AI Research', 'Georgia Tech & Facebook AI Research', 'Facebook', 'Facebook']"
2019,QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning,"Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, Yung Yi",https://icml.cc/Conferences/2019/Schedule?showEvent=3987,"We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate  QTRAN's superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively. 
","['KAIST', 'KAIST', 'KAIST', 'KAIST', 'KAIST']"
2019,Actor-Attention-Critic for Multi-Agent Reinforcement Learning,"Shariq Iqbal, Fei Sha",https://icml.cc/Conferences/2019/Schedule?showEvent=4199,"Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not provide global states, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.
","['University of Southern California', 'Google Research']"
2019,Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning,"Thinh Doan, Siva Maguluri, Justin Romberg",https://icml.cc/Conferences/2019/Schedule?showEvent=3727,"We study the policy evaluation problem in multi-agent reinforcement learning. In this problem, a group of agents works cooperatively to evaluate the value function for the global discounted accumulative reward problem, which is composed of local rewards observed by the agents. Over a series of time steps, the agents act, get rewarded, update their local estimate of the value function, then communicate with their neighbors.  The local update at each agent can be interpreted as a distributed consensus-based variant of the popular temporal difference learning algorithm TD(0). While distributed reinforcement learning algorithms have been presented in the literature, almost nothing is known about their convergence rate.  Our main contribution is providing a finite-time analysis for the convergence of the distributed TD(0) algorithm. We do this when the communication network between the agents is time-varying in general. We obtain an explicit upper bound on the rate of convergence of this algorithm as a function of the network topology and the discount factor. Our results mirror what we would expect from using distributed stochastic gradient descent for solving convex optimization problems. 
","['Georgia Institute of Technology', 'Georgia Tech', 'Georgia Tech']"
2019,Neural Network Attributions: A Causal Perspective,"Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, Vineeth N Balasubramanian",https://icml.cc/Conferences/2019/Schedule?showEvent=4003,"We propose a new attribution method for neural networks developed using ﬁrst principles of causality (to the best of our knowledge, the ﬁrst such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to compute the causal effect of each feature on the output is presented. With reasonable assumptions on the causal structure of the input data, we propose algorithms to efﬁciently compute the causal effects, as well as scale the approach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We report experimental results on both simulated and real datasets showcasing the promise and usefulness of the proposed algorithm.
","['Johns Hopkins University', 'IIT Hyderabad', 'Indian Institute of Technology, Hyderabad', 'Indian Institute of Technology, Hyderabad']"
2019,Towards a Deep and Unified Understanding of Deep Neural Models in NLP,"Chaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin Chen, Di He, Xing Xie",https://icml.cc/Conferences/2019/Schedule?showEvent=3999,"We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing
(NLP) models leverage information of input words. Our method advances existing explanation methods by addressing issues in coherency and generality. Explanations generated by using our method are consistent and faithful across different timestamps, layers, and models. We show how our method can be applied to four widely used models in NLP and explain their performances on three real-world benchmark datasets.
","['Shanghai Jiao Tong University', 'Microsoft Research Asia', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Peking University', 'Microsoft Research Asia']"
2019,Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation,"Marco Ancona, Cengiz Oztireli, Markus Gross",https://icml.cc/Conferences/2019/Schedule?showEvent=3901,"The problem of explaining the behavior of deep neural networks has recently gained a lot of attention. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley values as a unique way of assigning relevance scores such that certain desirable properties are satisfied. Unfortunately, the exact evaluation of Shapley values is prohibitively expensive, exponential in the number of input features. In this work, by leveraging recent results on uncertainty propagation, we propose a novel, polynomial-time approximation of Shapley values in deep neural networks. We show that our method produces significantly better approximations of Shapley values than existing state-of-the-art attribution methods.
","['ETH Zurich', 'Disney Research', 'ETH Zurich']"
2019,Functional Transparency for Structured Data: a Game-Theoretic Approach,"Guang-He Lee, Wengong Jin, David Alvarez-Melis, Tommi Jaakkola",https://icml.cc/Conferences/2019/Schedule?showEvent=4259,"We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted \emph{predictor} such as a neural network, and a set of \emph{witnesses} chosen from the desired transparent family. The goal of the witnesses is to highlight, locally, how well the predictor conforms to the chosen family of functions, while the predictor is trained to minimize the highlighted discrepancy. We emphasize that the predictor remains globally powerful as it is only encouraged to agree locally with locally adapted witnesses. We analyze the effect of the proposed approach, provide example formulations in the context of deep graph and sequence models, and empirically illustrate the idea in chemical property prediction, temporal modeling, and molecule representation learning.
","['MIT CSAIL', 'MIT', 'MIT', 'MIT']"
2019,Exploring interpretable LSTM neural networks over multi-variable data,"Tian Guo, Tao Lin, Nino Antulov-Fantulin",https://icml.cc/Conferences/2019/Schedule?showEvent=4209,"For recurrent neural networks trained on time series with target and exogenous variables, in addition to accurate prediction, it is also desired to provide interpretable insights into the data. 
In this paper, we explore the structure of LSTM recurrent neural networks to learn variable-wise hidden states, with the aim to capture different dynamics in multi-variable time series and distinguish the contribution of variables to the prediction.
With these variable-wise hidden states, a mixture attention mechanism is proposed to model the generative process of the target. Then we develop associated training methods to jointly learn network parameters, variable and temporal importance w.r.t the prediction of the target variable.
Extensive experiments on real datasets demonstrate enhanced prediction performance by capturing the dynamics of different variables.
Meanwhile, we evaluate the interpretation results both qualitatively and quantitatively.
It exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variable data.
","['ETH Zurich', 'EPFL', 'ETHZ']"
2019,TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing,"Augustus Odena, Catherine Olsson, David Andersen, Ian Goodfellow",https://icml.cc/Conferences/2019/Schedule?showEvent=4139,"Neural networks are difficult to interpret and debug. We introduce testing techniques for neural networks that
can discover errors occurring only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF)
methods for neural networks. In CGF, random mutations of inputs are guided by a coverage metric toward the
goal of satisfying user-specified constraints. We describe how approximate nearest neighbor (ANN) algorithms
can provide this coverage metric for neural networks. We then combine these methods with techniques for
property-based testing (PBT). In PBT, one asserts properties that a function should satisfy and the system
automatically generates tests exercising those properties. We then apply this system to practical goals including
(but not limited to) surfacing broken loss functions in popular GitHub repositories and making performance
improvements to TensorFlow. Finally, we release an open source library called TensorFuzz that implements the
described techniques.
","['Google Brain', 'Open Philanthropy Project', 'Google', 'Google Brain']"
2019,Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute,Tong Wang,https://icml.cc/Conferences/2019/Schedule?showEvent=3773,"This work addresses the situation where a black-box model with good predictive performance is chosen over its interpretable competitors, and we show interpretability is still achievable in this case. Our solution is to find an interpretable substitute on a subset of data where the black-box model is \emph{overkill} or nearly overkill while leaving the rest to the black-box.  This transparency is obtained at minimal cost or no cost of the predictive performance. Under this framework, we develop a Hybrid Rule Sets (HyRS)  model that uses decision rules to capture the subspace of data where the rules are as accurate or almost as accurate as the black-box provided. To train a HyRS, we devise an efficient search algorithm that iteratively finds the optimal model and exploits theoretically grounded strategies to reduce computation. Our framework is \emph{agnostic} to the black-box during training. Experiments on structured and text data show that HyRS obtains an effective trade-off between transparency and interpretability.
",['University of Iowa']
2019,State-Regularized Recurrent Neural Networks,"Cheng Wang, Mathias Niepert",https://icml.cc/Conferences/2019/Schedule?showEvent=3704,"Recurrent neural networks are a widely used class of neural architectures with two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We evaluate state-regularized RNNs on (1) regular languages for the purpose of automata extraction; (2) nonregular languages such as balanced parentheses, palindromes, and the copy task where external memory is required; and (3) real-word sequence learning tasks for sentiment analysis, visual object recognition, and language modeling. We show that state-regularization simplifies the extraction of finite state automata from the RNN's state transition dynamics; forces RNNs to operate more like automata with external memory and less like finite state machines; and makes RNNs more interpretable. 
","['NEC Laboratories Europe', 'NEC Laboratories Europe']"
2019,Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation,"Sahil Singla, Eric Wallace, Shi Feng, Soheil Feizi",https://icml.cc/Conferences/2019/Schedule?showEvent=3755,"Current saliency map interpretations for neural networks generally rely on two key assumptions. First, they use first-order approximations of the loss function, neglecting higher-order terms such as the loss curvature. Second, they evaluate each feature’s importance in isolation, ignoring feature interdependencies. This work studies the effect of relaxing these two assumptions. First, we characterize a closed-form formula for the input Hessian matrix of a deep ReLU network. Using this formula, we show that, for classification problems with many classes, if a prediction has high probability then including the Hessian term has a small impact on the interpretation. We prove this result by demonstrating that these conditions cause the Hessian matrix to be approximately rank one and its leading eigenvector to be almost parallel to the gradient of the loss. We empirically validate this theory by interpreting ImageNet classifiers. Second, we incorporate feature interdependencies by calculating the importance of group-features using a sparsity regularization term. We use an L0 − L1 relaxation technique along with proximal gradient descent to efficiently compute group-feature importance values. Our empirical results show that our method significantly improves deep learning interpretations.
","['University of Maryland', 'Allen Institute for Artificial Intelligence', 'University of Maryland', 'University of Maryland']"
2019,On the Connection Between Adversarial Robustness and Saliency Map Interpretability,"Christian Etmann, Sebastian Lunz, Peter Maass, Carola-Bibiane Schönlieb",https://icml.cc/Conferences/2019/Schedule?showEvent=4176,"Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behaviour by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the nonlinear nature of neural networks weakens the relation.
","['University of Bremen', 'University of Cambridge', 'University of Bremen', 'University of Cambridge']"
2019,Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem,"Alon Brutzkus, Amir Globerson",https://icml.cc/Conferences/2019/Schedule?showEvent=3884,"Empirical evidence suggests that neural networks with ReLU activations generalize better with over-parameterization. However, there is currently no theoretical analysis that explains this observation. In this work, we provide theoretical and empirical evidence that, in certain cases, overparameterized convolutional networks generalize better than small networks because of an interplay between weight clustering and feature exploration at initialization.  We demonstrate this theoretically for a 3-layer convolutional neural network with max-pooling, in a novel setting which extends the XOR problem. We show that this interplay implies that with overparamterization, gradient descent converges to global minima with better generalization performance compared to global minima of small networks. Empirically, we demonstrate these phenomena for a 3-layer convolutional neural network in the MNIST task.
","['Tel Aviv University', 'Tel Aviv University, Google']"
2019,On the Spectral Bias of Neural Networks,"Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, Aaron Courville",https://icml.cc/Conferences/2019/Schedule?showEvent=4226,"Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions -- i.e. functions that vary globally without local fluctuations -- which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.
","['University of Heidelberg', 'MILA', 'Montréal Institute for Learning Algorithms, Canada', 'Heidelberg University', 'University of Montreal', 'Heidelberg Collaboratory for Image Processing', 'Mila / U. Montreal', 'University of Montreal']"
2019,Recursive Sketches for Modular Deep Learning,"Badih Ghazi, Rina Panigrahy, Joshua R. Wang",https://icml.cc/Conferences/2019/Schedule?showEvent=3654,"We present a mechanism to compute a sketch (succinct summary) of how a complex modular deep network processes its inputs. The sketch summarizes essential information about the inputs and outputs of the network and can be used to quickly identify key components and summary statistics of the inputs. Furthermore, the sketch is recursive and can be unrolled to identify sub-components of these components and so forth, capturing a potentially complicated DAG structure. These sketches erase gracefully; even if we erase a fraction of the sketch at random, the remainder still retains the high-weight'' information present in the original sketch. The sketches can also be organized in a repository to implicitly form aknowledge graph''; it is possible to quickly retrieve sketches in the repository that are related to a sketch of interest; arranged in this fashion, the sketches can also be used to learn emerging concepts by looking for new clusters in sketch space. Finally, in the scenario where we want to learn a ground truth deep network, we show that augmenting input/output pairs with these sketches can theoretically make it easier to do so.
","['Google', 'Google', 'Google']"
2019,Zero-Shot Knowledge Distillation in Deep Networks,"Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, Venkatesh Babu Radhakrishnan, Anirban Chakraborty",https://icml.cc/Conferences/2019/Schedule?showEvent=3836,"Knowledge distillation deals with the problem of training a smaller model (\emph{Student}) from a high capacity source model (\emph{Teacher}) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the \emph{Student}. However, accessing the dataset on which the \emph{Teacher} has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the \emph{Student} from the \emph{Teacher}. Without even using any meta-data, we synthesize the \emph{Data Impressions} from the complex \emph{Teacher} model and utilize these as surrogates for the original training data samples to transfer its learning to \emph{Student} via knowledge distillation. We, therefore, dub our method ``Zero-Shot Knowledge Distillation"" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.
","['Indian Institute of Science', 'University of Edinburgh', 'University Of Lincoln', 'Indian Institute of Science', 'Indian Institute of Science']"
2019,A Convergence Theory for Deep Learning via Over-Parameterization,"Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song",https://icml.cc/Conferences/2019/Schedule?showEvent=4040,"Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper.
On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps ~ e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).
","['Microsoft Research AI', 'Stanford', 'UT-Austin']"
2019,A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks,"Umut Simsekli, Levent Sagun, Mert Gurbuzbalaban",https://icml.cc/Conferences/2019/Schedule?showEvent=3560,"The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed $\alpha$-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a L\'{e}vy motion. Such SDEs can incur `jumps', which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the $\alpha$-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.","['Telecom ParisTech', 'CEA', 'Rutgers University']"
2019,Approximation and non-parametric estimation of ResNet-type convolutional neural networks,"Kenta Oono, Taiji Suzuki",https://icml.cc/Conferences/2019/Schedule?showEvent=3724,"Convolutional neural networks (CNNs) have been shown to achieve optimal approximation and estimation error rates (in minimax sense) in several function classes. However, previous analyzed optimal CNNs are unrealistically wide and difficult to obtain via optimization due to sparse constraints in important function classes, including the H\""older class. We show a ResNet-type CNN can attain the minimax optimal error rates in these classes in more plausible situations -- it can be dense, and its width, channel size, and filter size are constant with respect to sample size. The key idea is that we can replicate the learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as long as the FNNs have \textit{block-sparse} structures. Our theory is general in a sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. As an application, we derive approximation and estimation error rates of the aformentioned type of CNNs for the Barron and H\""older classes with the same strategy.
","['University of Tokyo / Preferred Networks', 'The University of Tokyo / RIKEN']"
2019,Global Convergence of Block Coordinate Descent in Deep Learning,"Jinshan ZENG, Tsz Kit Lau, Shaobo Lin, Yuan Yao",https://icml.cc/Conferences/2019/Schedule?showEvent=3715,"Deep learning has aroused extensive attention due to its great empirical success. The efficiency of the block coordinate descent (BCD) methods has been recently demonstrated in deep neural network (DNN) training. However, theoretical studies on their convergence properties are limited due to the highly nonconvex nature of DNN training. In this paper, we aim at providing a general methodology for provable convergence guarantees for this type of methods. In particular, for most of the commonly used DNN training models involving both two- and three-splitting schemes, we establish the global convergence to a critical point at a rate of ${\cal O}(1/k)$, where $k$ is the number of iterations. The results extend to general loss functions which have Lipschitz continuous gradients and deep residual networks (ResNets). Our key development adds several new elements to the Kurdyka-Lojasiewicz inequality framework that enables us to carry out the global convergence analysis of BCD in the general scenario of deep learning. ","['Hongkong University of Science and Technology', 'Northwestern University', 'Wenzhou University', 'HongKong University of Science and Technology']"
2019,Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians,Vardan Papyan,https://icml.cc/Conferences/2019/Schedule?showEvent=3758,"We expose a structure in deep classifying neural networks in the derivative of the logits with respect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decomposed the Hessian into two components, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means possess an additive two-way structure that is the source of the outliers in the spectrum. This structure can be used to approximate the principal subspace of the Hessian using certain ""averaging"" operations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizes.
",['Stanford University']
2019,On the Limitations of Representing Functions on Sets,"Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, Michael A Osborne",https://icml.cc/Conferences/2019/Schedule?showEvent=4106,"Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the dimension of this latent space may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this is only of limited practical use. Motivated by this observation, we prove that an implementation of this model via continuous mappings (as provided by e.g. neural networks or Gaussian processes) actually imposes a constraint on the dimensionality of the latent space. Practical universal function representation for set inputs can only be achieved with a latent dimension at least the size of the maximum number of input elements.
","['University of Oxford', 'Oxford Robotics Insitute', 'University of Oxford', 'University of Oxford', 'U Oxford']"
2019,Probabilistic Neural Symbolic Models for Interpretable Visual Question Answering,"Shanmukha Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, Devi Parikh",https://icml.cc/Conferences/2019/Schedule?showEvent=3579,"We propose a new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instantiated in the context of visual question answering, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for VQA. Firstly, the programs generated by our model are more understandable while requiring less number of teaching examples. Secondly, we show that one can pose counterfactual scenarios to the model, to probe its beliefs on the programs that could lead to a specified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hypotheses, showing that the model gets better program (and answer) prediction accuracy even in the low data regime, and allows one to probe the coherence and consistency of reasoning performed.
","['Facebook AI Research', 'Georgia Tech', 'Georgia Institute of Technology', 'Facebook AI Research', 'Georgia Institute of Technology / Facebook AI Research', 'Georgia Tech & Facebook AI Research']"
2019,Nonparametric Bayesian Deep Networks with Local Competition,"Konstantinos Panousis, Sotirios Chatzis, Sergios Theodoridis",https://icml.cc/Conferences/2019/Schedule?showEvent=3879,"The aim of this work is to enable inference of deep networks that retain high accuracy for the least possible model complexity, with the latter deduced from the data during inference.  To this end, we revisit deep networks that comprise competing linear units, as opposed to nonlinear units that do not entail any form of (local) competition. In this context, our main technical innovation consists in an inferential setup that leverages solid arguments from Bayesian nonparametrics. We infer both the needed set of connections or locally competing sets of units, as well as the required floating-point precision for storing the network parameters. Specifically, we introduce auxiliary discrete latent variables representing which initial network components are actually needed for modeling the data at hand, and perform Bayesian inference over them by imposing appropriate stick-breaking priors.  As we experimentally show using benchmark datasets, our approach yields networks with less computational footprint than the state-of-the-art, and with no compromises in predictive accuracy.
","['National and Kapodistrian University of Athens', 'Cyprus University of Technology', 'National and Kapodistrian University of Athens']"
2019,Good Initializations of Variational Bayes for Deep Models,"Simone Rossi, Pietro Michiardi, Maurizio Filippone",https://icml.cc/Conferences/2019/Schedule?showEvent=3847,"Stochastic variational inference is an established way to carry out approximate Bayesian inference for deep models flexibly and at scale. While there have been effective proposals for good initializations for loss minimization in deep learning, far less attention has been devoted to the issue of initialization of stochastic variational inference. We address this by proposing a novel layer-wise initialization strategy based on Bayesian linear models. The proposed method is extensively validated on regression and classification tasks, including Bayesian Deep Nets and Conv Nets, showing faster and better convergence compared to alternatives inspired by the literature on initializations for loss minimization.
","['EURECOM', 'EURECOM', 'Eurecom']"
2019,Dropout as a Structured Shrinkage Prior,"Eric Nalisnick, Jose Miguel Hernandez-Lobato, Padhraic Smyth",https://icml.cc/Conferences/2019/Schedule?showEvent=3900,"Dropout regularization of deep neural networks has been a mysterious yet effective tool to prevent overfitting.  Explanations for its success range from the prevention of ""co-adapted"" weights to it being a form of cheap Bayesian inference.  We propose a novel framework for understanding multiplicative noise in neural networks, considering continuous distributions as well as Bernoulli noise (i.e. dropout).  We show that multiplicative noise induces structured shrinkage priors on a network's weights.  We derive the equivalence through reparametrization properties of scale mixtures and without invoking any approximations.  Given the equivalence, we then show that dropout's Monte Carlo training objective approximates marginal MAP estimation.  We leverage these insights to propose a novel shrinkage framework for resnets, terming the prior 'automatic depth determination' as it is the natural analog of automatic relevance determination for network depth.  Lastly, we investigate two inference strategies that improve upon the aforementioned MAP approximation in regression benchmarks.
","['University of Cambridge & DeepMind', 'University of Cambridge', 'UC Irvine']"
2019,ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables,"Mingzhang Yin, Yuguang Yue, Mingyuan Zhou",https://icml.cc/Conferences/2019/Schedule?showEvent=4166,"To address the challenge of backpropagating the gradient through categorical variables, we propose the augment-REINFORCE-swap-merge (ARSM) gradient estimator that is unbiased and has low variance. ARSM first uses variable augmentation, REINFORCE, and Rao-Blackwellization to re-express the gradient as an expectation under the Dirichlet distribution, then uses variable swapping to construct differently expressed but equivalent expectations, and finally shares common random numbers between these expectations to achieve significant variance reduction. Experimental results show ARSM closely resembles the performance of the true gradient for optimization in univariate settings; outperforms existing estimators by a large margin when applied to categorical variational auto-encoders; and provides a ""try-and-see self-critic"" variance reduction method for discrete-action policy gradient, which removes the need of estimating baselines by generating a random number of pseudo actions and estimating their action-value functions. 
","['University of Texas at Austin', 'University of Texas at Austin', 'University of Texas at Austin']"
2019,On Variational Bounds of Mutual Information,"Ben Poole, Sherjil Ozair, Aäron van den Oord, Alexander Alemi, George Tucker",https://icml.cc/Conferences/2019/Schedule?showEvent=4330,"Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning, but bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks. However, the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and representation learning.
","['Google Brain', 'University of Montreal', 'Google Deepmind', 'Google', 'Google Brain']"
2019,Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation,"Samuel Wiqvist, Pierre-Alexandre Mattei, Umberto Picchini, Jes Frellsen",https://icml.cc/Conferences/2019/Schedule?showEvent=4054,"We present a novel family of deep neural architectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial exchangeability properties of conditionally Markovian processes. Moreover, we show that any block-switch invariant function has a PEN-like representation. The DeepSets architecture is a special case of PEN and we can therefore also target fully exchangeable data. We employ PENs to learn summary statistics in approximate Bayesian computation (ABC). When comparing PENs to previous deep learning methods for learning summary statistics, our results are highly competitive, both considering time series and static models. Indeed, PENs provide more reliable posterior samples even when using less training data.
","['Lund University', 'IT University Copenhagen', 'Department of Mathematical Sciences, Chalmers University of Technology and the University of Gothenburg', 'IT University of Copenhagen']"
2019,Hierarchical Importance Weighted Autoencoders,"Chin-Wei Huang, Kris Sankaran, Eeshan Dhekane, Alexandre Lacoste, Aaron Courville",https://icml.cc/Conferences/2019/Schedule?showEvent=4323,"Importance weighted variational inference (Burda et al., 2015) uses multiple i.i.d. samples to have a tighter variational lower bound. We believe a joint proposal has the potential of reducing the number of redundant samples, and introduce a hierarchical structure to induce correlation. The hope is that the proposals would coordinate to make up for the error made by one another to reduce the variance of the importance estimator.  Theoretically, we analyze the condition under which convergence of the estimator variance can be connected to convergence of the lower bound. Empirically, we confirm that maximization of the lower bound does implicitly minimize variance.  Further analysis shows that this is a result of negative correlation induced by the proposed hierarchical meta sampling scheme, and performance of inference also improves when the number of samples increases.
","['MILA', 'Mila', 'MILA, Université de Montréal', 'Element AI', 'Université de Montréal']"
2019,Faster Attend-Infer-Repeat with Tractable Probabilistic Models,"Karl Stelzner, Robert Peharz, Kristian Kersting",https://icml.cc/Conferences/2019/Schedule?showEvent=4081,"The recent Attend-Infer-Repeat (AIR) framework marks a milestone in structured probabilistic modeling, as it tackles the challenging problem of unsupervised scene understanding via Bayesian inference. AIR expresses the composition of visual scenes from individual objects, and uses variational autoencoders to model the appearance of those objects. However, inference in the overall model is highly intractable, which hampers its learning speed and makes it prone to suboptimal solutions. In this paper, we show that the speed and robustness of learning in AIR can be considerably improved by replacing the intractable object representations with tractable probabilistic models. In particular, we opt for sum-product networks (SPNs), expressive deep probabilistic models with a rich set of tractable inference routines. The resulting model, called SuPAIR, learns an order of magnitude faster than AIR, treats object occlusions in a consistent manner, and allows for the inclusion of a background noise model, improving the robustness of Bayesian scene understanding.
","['TU Darmstadt', 'University of Cambridge', 'TU Darmstadt']"
2019,Understanding Priors in Bayesian Neural Networks at the Unit Level,"Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, Julyan Arbel",https://icml.cc/Conferences/2019/Schedule?showEvent=4258,"We investigate deep Bayesian neural networks with Gaussian priors on the weights and a class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian priors are well known to induce an L2,  ``weight decay'', regularization. Our results indicate a more intricate regularization effect at the level of the unit activations. Our main result  establishes that the induced prior distribution on the units before and after activation becomes increasingly heavy-tailed with the depth of the layer. 
We show that first layer units are Gaussian, second layer units are sub-exponential, and units in deeper layers are characterized by  sub-Weibull distributions. Our results  provide new theoretical insight on deep Bayesian neural networks, which we corroborate with simulation experiments. 
","['Inria', 'INRIA', 'Universidad de Granada', 'Inria Grenoble Rhone-Alpes']"
2019,Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning,"Dong Yin, Yudong Chen, Kannan Ramchandran, Peter Bartlett",https://icml.cc/Conferences/2019/Schedule?showEvent=3660,"We study robust distributed learning that involves minimizing a non-convex loss function with saddle points.
We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior, and in this setting, the Byzantine machines may create fake local minima near a saddle point that is far away from any true local minimum, even when robust gradient estimators are used.
We develop ByzantinePGD, a robust first-order algorithm that can provably escape saddle points and fake local minima, and converge to an approximate true local minimizer with low iteration complexity.
As a by-product, we give a simpler algorithm and analysis for escaping saddle points in the usual non-Byzantine setting.
We further discuss three robust gradient estimators that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering.
We characterize their performance in concrete statistical settings, and argue for their near-optimality in low and high dimensional regimes. 
","['UC Berkeley', 'Cornell University', 'UC Berkeley', 'UC Berkeley']"
2019,Stochastic Iterative Hard Thresholding for Graph-structured Sparsity Optimization,"Baojian Zhou, Feng Chen, Yiming Ying",https://icml.cc/Conferences/2019/Schedule?showEvent=3889,"Stochastic optimization algorithms update models with cheap per-iteration costs sequentially, which makes them amenable for large-scale data analysis. Such algorithms have been widely studied for structured sparse models where the sparsity information is very specific, e.g., convex sparsity-inducing norms or $\ell^0$-norm. However, these norms cannot be directly applied to the problem of complex (non-convex) graph-structured sparsity models, which have important application in disease outbreak and social networks, etc. In this paper, we propose a stochastic gradient-based method for solving graph-structured sparsity constraint problems, not restricted to the least square loss. We prove that our algorithm enjoys a linear convergence up to a constant error, which is competitive with the counterparts in the batch learning setting. We conduct extensive experiments to show the efficiency and effectiveness of the proposed algorithms. ","['University at Albany, SUNY', 'University at albany SUNY', 'SUNY Albany']"
2019,Neuron birth-death dynamics accelerates gradient descent and converges asymptotically,"Grant Rotskoff, Samy Jelassi, Joan Bruna, Eric Vanden-Eijnden",https://icml.cc/Conferences/2019/Schedule?showEvent=4286,"Neural networks with a large number of parameters admit a mean-field description, which has recently served as a theoretical explanation for the favorable training properties of models with a large number of parameters. In this regime, gradient descent obeys a deterministic partial differential equation (PDE) that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. In this work, we propose a non-local mass transport dynamics that leads to a modified PDE with the same minimizer. We implement this non-local dynamics as a stochastic neuronal birth/death process and we prove that it accelerates the rate of convergence in the mean-field limit. We subsequently realize this PDE with two classes of numerical schemes that converge to the mean-field equation, each of which can easily be implemented for neural networks with finite numbers of parameters. We illustrate our algorithms with two models to provide intuition for the mechanism through which convergence is accelerated.
","['New York University', 'Princeton University', 'New York University', 'New York University']"
2019,Width Provably Matters in Optimization for Deep Linear Neural Networks,"Simon Du, Wei Hu",https://icml.cc/Conferences/2019/Schedule?showEvent=4001,"We prove that for an $L$-layer fully-connected linear neural network, if the width of every hidden layer is $\widetilde{\Omega}\left(L \cdot r \cdot d_{out} \cdot \kappa^3 \right)$, where $r$ and $\kappa$ are the rank and the condition number of the input data, and $d_{out}$ is the output dimension, then gradient descent with Gaussian random initialization converges to a global minimum at a linear rate. The number of iterations to find an  $\epsilon$-suboptimal solution is $O(\kappa \log(\frac{1}{\epsilon}))$. Our polynomial upper bound on the total running time for wide deep linear networks and the  $\exp\left(\Omega\left(L\right)\right)$ lower bound for narrow deep linear neural networks [Shamir, 2018] together demonstrate that wide layers are necessary for optimizing deep models.","['Carnegie Mellon University', 'Princeton University']"
2019,Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?,"Samet Oymak, Mahdi Soltanolkotabi",https://icml.cc/Conferences/2019/Schedule?showEvent=3700,"Many modern learning tasks involve fitting nonlinear models which are trained in an overparameterized regime where the parameters of the model exceed the size of the training dataset. Due to this overparameterization, the training loss may have infinitely many global minima and it is critical to understand the properties of the solutions found by first-order optimization schemes such as (stochastic) gradient descent starting from different initializations. In this paper we demonstrate that when the loss has certain properties over a minimally small neighborhood of the initial point, first order methods such as (stochastic) gradient descent have a few intriguing properties: (1) the iterates converge at a geometric rate to a global optima even when the loss is nonconvex, (2) among all global optima of the loss the iterates converge to one with a near minimal distance to the initial point, (3) the iterates take a near direct route from the initial point to this global optimum. As part of our proof technique, we introduce a new potential function which captures the tradeoff between the loss function and the distance to the initial point as the iterations progress. The utility of our general theory is demonstrated for a variety of problem domains spanning low-rank matrix recovery to shallow neural network training.
","['University of California, Riverside', 'University of Southern California']"
2019,Power k-Means Clustering,"Jason Xu, Kenneth Lange",https://icml.cc/Conferences/2019/Schedule?showEvent=4117,"Clustering is a fundamental task in unsupervised machine learning. Lloyd's 1957 algorithm for k-means clustering remains one of the most widely used due to its speed and simplicity, but the greedy approach is sensitive to initialization and often falls short at a poor solution. This paper explores an alternative to Lloyd's algorithm that retains its simplicity and mitigates its tendency to get trapped by local minima. Called power k-means, our method embeds the k-means problem in a continuous class of similar, better behaved problems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the appealing descent property and low complexity of Lloyd's algorithm. Further, our method complements widely used seeding strategies, reaping marked improvements when used together as demonstrated on a suite of simulated and real data examples. 
","['Duke University', 'UCLA']"
2019,Distributed Learning over Unreliable Networks,"Chen Yu, Hanlin Tang, Cedric Renggli, Simon Kassing, Ankit Singla, Dan Alistarh, Ce Zhang, Ji Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=4184,"Most of today's distributed machine learning systems assume {\em reliable networks}: whenever two machines exchange information (e.g., gradients or models), the network should guarantee the delivery of the message. At the same time, recent work exhibits the impressive tolerance of machine learning algorithms to errors or noise arising from relaxed communication or synchronization. In this paper, we connect these two trends, and consider the following question: {\em Can we design machine learning systems that are tolerant to network unreliability during training?} With this motivation, we focus on a theoretical problem of independent interest---given a standard distributed parameter server architecture, if every communication between the worker and the server has a non-zero probability $p$ of being dropped, does there exist an algorithm that still converges, and at what speed? In the context of prior art, this problem can be phrased as {\em distributed learning over random topologies}. The technical contribution of this paper is a novel theoretical analysis proving that distributed learning over random topologies can achieve comparable convergence rate to centralized or distributed learning over reliable networks. Further, we prove that the influence of the packet drop rate diminishes with the growth of the number of parameter servers. We map this theoretical result onto a real-world scenario, training deep neural networks over an unreliable network layer, and conduct network simulation to validate the system improvement by allowing the networks to be unreliable.
","['University of Rochester', 'University of Rochester', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'IST Austria & ETH Zurich', 'ETH Zurich', 'Kwai Seattle AI lab, University of Rochester']"
2019,Escaping Saddle Points with Adaptive Gradient Methods,"Matthew Staib, Sashank Jakkam Reddi, Satyen Kale, Sanjiv Kumar, Suvrit Sra",https://icml.cc/Conferences/2019/Schedule?showEvent=4059,"Adaptive methods such as Adam and RMSProp are widely used in deep learning but are not well understood. In this paper, we seek a crisp, clean and precise characterization of their behavior in nonconvex settings. To this end, we first provide a novel view of adaptive methods as preconditioned SGD, where the preconditioner is estimated in an online manner. By studying the preconditioner on its own, we elucidate its purpose: it rescales the stochastic gradient noise to be isotropic near stationary points, which helps escape saddle points. Furthermore, we show that adaptive methods can efficiently estimate the aforementioned preconditioner. By gluing together these two components, we provide the first (to our knowledge) second-order convergence result for any adaptive method. The key insight from our analysis is that, compared to SGD, adaptive methods escape saddle points faster, and can converge faster overall to second-order stationary points.
","['MIT', 'Google', 'Google', 'Google Research, NY', 'MIT']"
2019,$\texttt{DoubleSqueeze}$: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression,"Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, Ji Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=4191,"A standard approach in large scale machine learning is distributed stochastic gradient training, which requires the computation of aggregated stochastic gradients over multiple nodes on a network. Communication is a major bottleneck in such applications, and in recent years, compressed stochastic gradient methods such as QSGD (quantized SGD) and sparse SGD have been proposed to reduce communication. It was also shown that error compensation can be combined with compression to achieve better convergence in a scheme that each node compresses its local stochastic gradient and broadcast the result to all other nodes over the network in a single pass. However, such a single pass broadcast approach is not realistic in many practical implementations. For example, under the popular parameter-server model for distributed learning, the worker nodes need to send the compressed local gradients to the parameter server, which performs the aggregation. The parameter server has to compress the aggregated stochastic gradient again before sending it back to the worker nodes. In this work, we provide a detailed analysis on this two-pass communication model, with error-compensated compression both on the worker nodes and on the parameter server. We show that the error-compensated stochastic gradient algorithm admits three very nice properties: 1) it is compatible with an \emph{arbitrary} compression technique; 2) it admits an improved convergence rate than the non error-compensated stochastic gradient method such as QSGD and sparse SGD; 3) it admits linear speedup with respect to the number of workers. The empirical study is also conducted to validate our theoretical results.
","['University of Rochester', 'University of Rochester', 'University of Rochester', 'Tecent AI Lab', 'Kwai Seattle AI lab, University of Rochester']"
2019,Model Function Based Conditional Gradient Method with Armijo-like Line Search,"Peter Ochs, Yura Malitsky",https://icml.cc/Conferences/2019/Schedule?showEvent=4029,"The Conditional Gradient Method is generalized to a class of non-smooth non-convex optimization problems with many applications in machine learning. The proposed algorithm iterates by minimizing so-called model functions over the constraint set. Complemented with an Armijo line search procedure, we prove that subsequences converge to a stationary point. The abstract framework of model functions provides great flexibility in the design of concrete algorithms. As special cases, for example, we develop an algorithm for additive composite problems and an algorithm for non-linear composite problems which leads to a Gauss-Newton-type algorithm. Both instances are novel in non-smooth non-convex optimization and come with numerous applications in machine learning. We perform an experiment on a non-linear robust regression problem and discuss the flexibility of the proposed framework in several matrix factorization formulations.
","['Saarland University', 'University of Göttingen']"
2019,Analogies Explained: Towards Understanding Word Embeddings,"Carl Allen, Timothy Hospedales",https://icml.cc/Conferences/2019/Schedule?showEvent=4051,"Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy ``woman is to queen as man is to king'' approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice.
We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of ``$w_x$ is to $w_y$''. From these concepts we prove existence of linear relationship between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.","['The University of Edinburgh', 'Samsung AI Centre / University of Edinburgh']"
2019,Parameter-Efficient Transfer Learning for NLP,"Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly",https://icml.cc/Conferences/2019/Schedule?showEvent=4119,"Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8\%$ of the performance of full fine-tuning, adding only $3.6\%$ parameters per task. By contrast, fine-tuning trains $100\%$ of the parameters per task.","['Google', 'Google', 'New York University', 'Google', 'Google Brain', 'Google', 'Google', 'Google Brain']"
2019,Efficient On-Device Models using Neural Projections,Sujith Ravi,https://icml.cc/Conferences/2019/Schedule?showEvent=3794,"Many applications involving visual and language understanding can be effectively solved using deep neural networks. Even though these techniques achieve state-of-the-art results, it is very challenging to apply them on devices with limited memory and computational capacity such as mobile phones, smart watches and IoT. We propose a neural projection approach for training compact on-device neural networks. We introduce ""projection"" networks that use locality-sensitive projections to generate compact binary representations and learn small neural networks with computationally efficient operations. We design a joint optimization framework where the projection network can be trained from scratch or leverage existing larger neural networks such as feed-forward NNs, CNNs or RNNs. The trained neural projection network can be directly used for inference on device at low memory and computation cost. We demonstrate the effectiveness of this as a general-purpose approach for significantly shrinking memory requirements of different types of neural networks while preserving good accuracy on multiple visual and text classification tasks.
",['Google Research']
2019,Deep Residual Output Layers for Neural Language Generation,"Nikolaos Pappas, James Henderson",https://icml.cc/Conferences/2019/Schedule?showEvent=4145,"Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse.  State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space.
","['Idiap Research Institute', 'IDIAP']"
2019,Improving Neural Language Modeling via Adversarial Training,"Dilin Wang, Chengyue Gong, Qiang Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=3888,"Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.65, respectively. 
When applied to machine translation,  our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.
","['UT Austin', 'university of texas at austin', 'UT Austin']"
2019,Mixture Models for Diverse Machine Translation: Tricks of the Trade,"Tianxiao Shen, Myle Ott, Michael Auli, Marc'Aurelio Ranzato",https://icml.cc/Conferences/2019/Schedule?showEvent=4319,"Mixture models trained via EM are among the simplest, most widely used and well understood latent variable models in the machine learning literature. Surprisingly, these models have been hardly explored in text generation applications such as machine translation. In principle, they provide a latent variable to control generation and produce a diverse set of hypotheses. In practice, however, mixture models are prone to degeneracies---often only one component gets trained or the latent variable is simply ignored. We find that disabling dropout noise in responsibility computation is critical to successful training. In addition, the design choices of parameterization, prior distribution, hard versus soft EM and online versus offline assignment can dramatically affect model performance. We develop an evaluation protocol to assess both quality and diversity of generations against multiple references, and provide an extensive empirical study of several mixture model variants. Our analysis shows that certain types of mixture models are more robust and offer the best trade-off between translation quality and diversity compared to variational models and diverse decoding approaches.\footnote{Code to reproduce the results in this paper is available at \url{https://github.com/pytorch/fairseq}}
","['MIT', 'Facebook AI Research', 'Facebook', 'Facebook']"
2019,MASS: Masked Sequence to Sequence Pre-training for Language Generation,"Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=3802,"Pre-training and fine-tuning, e.g., BERT~\citep{devlin2018bert}, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Especially, we achieve the state-of-the-art accuracy (30.02 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model~\citep{bahdanau2015neural}.
","['Nanjing University of Science and Technology', 'Microsoft Research', 'Microsoft Research Asia', 'Nanjing University of Science and Technology', 'Microsoft']"
2019,Humor in Word Embeddings: Cockamamie Gobbledegook for Nincompoops,"Limor Gultchin, Genevieve Patterson, Nancy Baym, Nathaniel Swinger, Adam Kalai",https://icml.cc/Conferences/2019/Schedule?showEvent=3915,"While humor is often thought to be beyond the reach of Natural Language Processing, we show that several aspects of single-word humor correlate with simple linear directions in Word Embeddings. In particular: (a) the word vectors capture multiple aspects discussed in humor theories from various disciplines; (b) each individual's sense of humor can be represented by a vector, which can predict differences in people's senses of humor on new, unrated, words; and (c) upon clustering humor ratings of multiple demographic groups, different humor preferences emerge across the different groups. Humor ratings are taken from the work of Engelthaler and Hills (2017) as well as from an original crowdsourcing study of 120,000 words. Our dataset further includes annotations for the theoretically-motivated humor features we identify.
","['University of Oxford', 'TRASH', 'Micr', 'Lexington High School', 'Microsoft Research']"
2019,MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization,"Eric Chu, Peter Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=3891,"Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder where the mean of the representations of the input reviews decodes to a reasonable summary-review. We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components. We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews. Finally, we collect a ground-truth evaluation dataset and show that our model outperforms a strong extractive baseline.
","['Massachusetts Institute of Technology', 'Google Brain']"
2019,CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network,"Tom Kenter, Vincent Wan, Chun-an Chan, Robert Clark, Jakub Vit",https://icml.cc/Conferences/2019/Schedule?showEvent=3864,"The prosodic aspects of speech signals produced by current text-to-speech systems are typically averaged over training material, and as such lack the variety and liveliness found in natural speech. To avoid monotony and averaged prosody contours, it is desirable to have a way of modeling the variation in the prosodic aspects of speech, so audio signals can be synthesized in multiple ways for a given text. We present a new, hierarchically structured conditional variational auto-encoder to generate prosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding representing the prosody of a sentence may be sampled from the variational layer to allow for prosodic variation. To efficiently capture the hierarchical nature of the linguistic input (words, syllables and phones), both the encoder and decoder parts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the respective rates. We show in our experiments that our dynamic hierarchical network outperforms a non-hierarchical state-of-the-art baseline, and, additionally, that prosody transfer across sentences is possible by employing the prosody embedding of one sentence to generate the speech signal of another.
","['Google UK', 'Google', 'Google', 'Google UK', 'University of West Bohemia']"
2019,COMIC: Multi-view Clustering Without Parameter Selection,"Xi Peng, Zhenyu Huang, Jiancheng Lv, Hongyuan Zhu, Joey Tianyi Zhou",https://icml.cc/Conferences/2019/Schedule?showEvent=3777,"In this paper, we study two challenges in clustering analysis, namely, how to cluster multi-view data and how to perform clustering without parameter selection on cluster size. To this end, we propose a novel objective function to project raw data into one space in which the projection embraces the geometric consistency (GC) and the cluster assignment consistency (CAC). To be specific, the GC aims to learn a connection graph from a projection space wherein the data points are connected if and only if they belong to the same cluster. The CAC aims to minimize the discrepancy of pairwise connection graphs induced from different views based on the view-consensus assumption, \textit{i.e.}, different views could produce the same cluster assignment structure as they are different portraits of the same object. Thanks to the view-consensus derived from the connection graph, our method could achieve promising  performance in learning view-specific representation and eliminating the heterogeneous gaps across different views. Furthermore, with the proposed objective, it could learn almost all parameters including the cluster number from data without labor-intensive parameter selection. Extensive experimental results show the promising performance achieved by our method on five datasets comparing with nine state-of-the-art multi-view clustering approaches. 
","['Sichuan University', 'Sichuan University', 'Sichuan University', 'Institute for Infocomm, Research Agency for Science, Technology and Research (A*STAR) Singapore', 'A*STAR']"
2019,The Wasserstein Transform,"Facundo Memoli, Zane Smith, Zhengchao Wan",https://icml.cc/Conferences/2019/Schedule?showEvent=3637,"We introduce the Wasserstein transform, a method for enhancing and denoising datasets defined on general metric spaces. The construction draws inspiration from Optimal Transportation ideas. We establish the stability of our method under data perturbation and, when the dataset is assumed to be Euclidean, we also exhibit a precise connection between the Wasserstein transform and the mean shift family of algorithms. We then use this connection to prove that mean shift also inherits stability under perturbations. We study the performance of the Wasserstein transform method on different datasets as a preprocessing step prior to clustering and classification tasks.
","['Ohio State University', 'University of Minnesota', 'The Ohio State University']"
2019,Sequential Facility Location: Approximate Submodularity and Greedy Algorithm,Ehsan Elhamifar,https://icml.cc/Conferences/2019/Schedule?showEvent=3609,"We develop and analyze a novel utility function and a fast optimization algorithm for subset selection in sequential data that incorporates the dynamic model of data. We propose a cardinality-constrained sequential facility location function that finds a fixed number of representatives, where the sequence of representatives is compatible with the dynamic model and well encodes the data. As maximizing this new objective function is NP-hard, we develop a fast greedy algorithm based on submodular maximization. Unlike the conventional facility location, the computation of the marginal gain in our case cannot be done by operations on each item independently. We exploit the sequential structure of the problem and develop an efficient dynamic programming-based algorithm that computes the marginal gain exactly. We investigate conditions on the dynamic model, under which our utility function is  ($\epsilon$-approximately) submodualr, hence, the greedy algorithm comes with performance guarantees. By experiments on synthetic data and the problem of procedure learning from instructional videos, we show that our framework significantly improves the computational time, achieves better objective function values and obtains more coherent summaries. ",['Northeastern University']
2019,Neural Collaborative Subspace Clustering,"Tong Zhang, Pan Ji, Mehrtash Harandi, Wenbing Huang, HONGDONG LI",https://icml.cc/Conferences/2019/Schedule?showEvent=3595,"We introduce the Neural Collaborative Subspace Clustering, a neural model that discovers clusters of data points drawn from a union of low-dimensional subspaces. In contrast to previous attempts, our model runs without the aid of spectral clustering. This makes our algorithm one of the kinds that can gracefully scale to large datasets. At its heart, our neural model benefits from a classifier which determines whether a pair of points lies on the same subspace or not. Essential to our model is the construction of two affinity matrices, one from the classifier and the other from a notion of subspace self-expressiveness, to supervise training in a collaborative scheme. We thoroughly assess and contrast the performance of our model against various state-of-the-art  clustering algorithms including deep subspace-based ones. 
","['The Australian National University', 'NEC Laboratories America', 'Monash University', 'Tencent AI Lab', 'Australian National University, Australia']"
2019,Unsupervised Deep Learning by Neighbourhood Discovery,"Jiabo Huang, Qi Dong, Shaogang Gong, Xiatian Zhu",https://icml.cc/Conferences/2019/Schedule?showEvent=3564,"Deep convolutional neural networks (CNNs) have demonstrated remarkable success in computer vision by supervisedly learning strong visual feature representations. However, training CNNs relies heavily on the availability of exhaustive training data annotations, limiting significantly their deployment and scalability in many application scenarios. In this work, we introduce a generic unsupervised deep learning approach to training deep models without the need for any manual label supervision. Specifically, we progressively discover sample anchored/centred neighbourhoods to reason and learn the underlying class decision boundaries iteratively and accumulatively. Every single neighbourhood is specially formulated so that all the member samples can share the same unseen class labels at high probability for facilitating the extraction of class discriminative feature representations during training. Experiments on image classification show the performance advantages of the proposed method over the state-of-the-art unsupervised learning models on six benchmarks including both coarse-grained and fine-grained object image categorisation. 
","['Queen Mary University of London', 'Queen Mary University of London', 'Queen Mary University of London', 'Vision Semantics Limited']"
2019,Autoregressive Energy Machines,"Conor Durkan, Charlie Nash",https://icml.cc/Conferences/2019/Schedule?showEvent=3871,"Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.
","['University of Edinburgh', 'The University of Edinburgh']"
2019,Greedy Orthogonal Pivoting Algorithm for Non-Negative Matrix Factorization,"Kai Zhang, Sheng Zhang, Jun Liu, Jun Wang, Jie Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3663,"Non-negative matrix factorization is a powerful tool for learning useful representations in the data and has been widely applied in many problems such as data mining and signal processing.  Orthogonal NMF, which  can improve the locality of decomposition, has drawn considerable interest in solving clustering problems in recent years. However, imposing simultaneous non-negative and orthogonal structure can be quite difficult, and so existing algorithms can only solve it approximately. To address this challenge, we propose an innovative procedure called Greedy Orthogonal Pivoting Algorithm (GOPA). The GOPA algorithm fully exploits the sparsity of non-negative orthogonal solutions to break the global problem into a series of local optimizations, in which an adaptive subset of coordinates are updated in a greedy, closed-form manner. The biggest advantage of GOPA is that it promotes exact orthogonality and provides solid empirical evidence that stronger orthogonality does contribute favorably to better  clustering performance. On the other hand, we further design randomized and parallel version of GOPA, which can further reduce the computational cost and improve accuracy, making it suitable for large data.
","['East China Normal University', 'Temple University', 'Infinia ML Inc.', 'Alibaba', 'Fudan University']"
2019,Noise2Self: Blind Denoising by Self-Supervision,"Joshua Batson, Loic Royer",https://icml.cc/Conferences/2019/Schedule?showEvent=4200,"We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions (``$\mathcal{J}$-invariant''), it is then possible to estimate the performance of a denoiser from noisy data alone. This allows us to calibrate $\mathcal{J}$-invariant versions of any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.","['Chan Zuckerberg Biohub', 'Chan Zuckerberg Biohub']"
2019,Learning Dependency Structures for Weak Supervision Models,"Paroma Varma, Frederic Sala, Ann He, Alexander J Ratner, Christopher Re",https://icml.cc/Conferences/2019/Schedule?showEvent=3601,"Labeling training data is a key bottleneck in the modern machine learning pipeline. Recent weak supervision approaches combine labels from multiple noisy sources by estimating their accuracies without access to ground truth labels; however, estimating the dependencies among these sources is a critical challenge. We focus on a robust PCA-based algorithm for learning these dependency structures, establish improved theoretical recovery rates, and outperform existing methods on various real-world tasks. Under certain conditions, we show that the amount of unlabeled data needed can scale sublinearly or even logarithmically with the number of sources m, improving over previous efforts that ignore the sparsity pattern in the dependency structure and scale linearly in m. We provide an information-theoretic lower bound on the minimum sample complexity of the weak supervision setting. Our method outperforms weak supervision approaches that assume conditionally-independent sources by up to 4.64 F1 points and previous structure learning approaches by up to 4.41 F1 points on real-world relation extraction and image classification tasks.
","['Stanford University', 'Stanford', 'Stanford University', 'Stanford University', 'Stanford']"
2019,Geometry and Symmetry in Short-and-Sparse Deconvolution,"Han-Wen Kuo, Yenson Lau, Yuqian Zhang, John Wright",https://icml.cc/Conferences/2019/Schedule?showEvent=4207,"We study the Short-and-Sparse (SaS) deconvolution problem of recovering a short signal a0 and a sparse signal x0 from their convolution. We propose a method based on nonconvex optimization, which under certain conditions recovers the target short and sparse signals, up to a signed shift symmetry which is intrinsic to this model. This symmetry plays a central role in shaping the optimization landscape for
deconvolution. We give a regional analysis, which characterizes this landscape geometrically, on a union of subspaces. Our geometric characterization holds when the length-p0 short signal a0 has shift coherence µ, and x0 follows a random sparsity model with sparsity rate θ ∈ [c1/p0, c2/(p0\sqrt{\mu}+\sqrt{p0})] / (log^2(p0)) . Based on this geometry, we give a provable method that successfully solves SaS deconvolution with high probability.
","['Columbia University', 'Columbia University', 'Columbia University', 'Columbia University, USA']"
2019,On Sparse Linear Regression in the Local Differential Privacy Model,"Di Wang, Jinhui Xu",https://icml.cc/Conferences/2019/Schedule?showEvent=3642,"In this paper, we study the sparse linear regression problem under the Local Differential Privacy (LDP) model. We first show that polynomial dependency on the dimensionality $p$ of the space is unavoidable for the estimation error in both non-interactive and sequential interactive local models, if the privacy of the whole dataset needs to be preserved.  Similar limitations also exist for other types of error measurements and in the relaxed local models. This indicates that differential privacy in high dimensional space is unlikely achievable for the problem. With the understanding of this limitation, we then present two algorithmic results. The first one is 
a sequential interactive LDP algorithm for the low dimensional sparse case, called Locally Differentially Private Iterative Hard Thresholding (LDP-IHT), which achieves a near optimal upper bound. This algorithm is actually rather general and can be used to solve quite a few other problems, such as (Local) DP-ERM with sparsity constraints and sparse regression with non-linear measurements.  The second one is for the restricted (high dimensional) case where only  the privacy  of the responses (labels) needs to be preserved. For this case, 
we show that the optimal rate of the error estimation can be made logarithmically depending on $p$ (i.e., $\log p$) in the local model, 
where an upper bound is obtained by a label-privacy version of LDP-IHT. Experiments on real world and synthetic datasets confirm our theoretical analysis. ","['State University of New York at Buffalo', 'SUNY Buffalo']"
2019,Differentially Private Empirical Risk Minimization with Non-convex Loss Functions,"Di Wang, Changyou Chen, Jinhui Xu",https://icml.cc/Conferences/2019/Schedule?showEvent=3618,"We study the problem of Empirical Risk Minimization (ERM) with (smooth) non-convex loss functions under the differential-privacy (DP) model. Existing approaches for this problem mainly adopt gradient norms to measure the error, which in general cannot guarantee the quality of the solution. To address this issue, 
we first study the expected excess empirical (or population) risk, which was primarily used as the utility to measure the quality for convex loss functions. Specifically, we show that
the excess empirical (or population) risk can be upper bounded by $\tilde{O}(\frac{d\log (1/\delta)}{\log n\epsilon^2})$ in the $(\epsilon, \delta)$-DP settings, where $n$ is the data size and $d$ is the dimensionality of the space. 
The $\frac{1}{\log n}$ term in the empirical risk bound can be further improved to $\frac{1}{n^{\Omega(1)}}$ (when $d$ is a constant) by a highly non-trivial analysis on the time-average error. 
To obtain more efficient solutions, we also consider the connection between achieving differential privacy and finding approximate local minimum. 
Particularly, we show that when the size $n$ is large enough, there are $(\epsilon, \delta)$-DP algorithms which can find an approximate local minimum of the empirical risk with high probability in both the constrained and non-constrained settings. 
These results indicate that one can escape saddle points privately.","['State University of New York at Buffalo', 'SUNY Buffalo', 'SUNY Buffalo']"
2019,Bounding User Contributions: A Bias-Variance Trade-off in Differential Privacy,"Kareem Amin, Alex Kulesza, andres munoz, Sergei Vassilvitskii",https://icml.cc/Conferences/2019/Schedule?showEvent=4220,"Differentially private learning algorithms protect individual participants in the training dataset by guaranteeing that their presence does not significantly change the resulting model. In order to make this promise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to protect them. While most existing analyses assume that the maximum contribution is known and fixed in advance—indeed, it is often assumed that each user contributes only a single example—we argue that in practice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end up adding excessive noise to protect a few outliers, even when the majority contribute only modestly. On the other hand, limiting users to small contributions keeps noise levels low at the cost of potentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this trade-off for an empirical risk minimization setting, showing that in general there is a “sweet spot” that depends on measurable properties of the dataset, but that there is also a concrete cost to privacy that cannot be avoided simply by collecting more data.
","['Google Research', 'Google', 'Google', 'Google']"
2019,Differentially Private Learning of Geometric Concepts,"Haim Kaplan, Yishay Mansour, Yossi Matias, Uri Stemmer",https://icml.cc/Conferences/2019/Schedule?showEvent=3863,"We present differentially private efficient algorithms for learning union of polygons in the plane (which are not necessarily convex). Our algorithms achieve $(\alpha,\beta)$-PAC learning and $(\epsilon,\delta)$-differential privacy using a sample of size $\tilde{O}\left(\frac{1}{\alpha\epsilon}k\log d\right)$, where the domain is $[d]\times[d]$ and $k$ is the number of edges in the union of polygons.","['Tel Aviv University and Google', 'Google and Tel Aviv University', 'Google', 'Ben-Gurion University']"
2019,Toward Controlling Discrimination in Online Ad Auctions,"L. Elisa Celis, Anay Mehrotra, Nisheeth Vishnoi",https://icml.cc/Conferences/2019/Schedule?showEvent=4311,"Online advertising platforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with respect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical and/or legal boundaries. To prevent this, we propose a constrained ad auction framework that maximizes the platform’s revenue conditioned on ensuring that the audience seeing an advertiser’s ad is distributed appropriately across sensitive types such as gender or race. Building upon Myerson’s classic work, we first present an optimal auction mechanism for a large class of fairness constraints. Finding the parameters of this optimal auction, however, turns out to be a non-convex problem. We show that this non-convex problem can be reformulated as a more structured non-convex problem with no saddle points or local-maxima; this allows us to develop a gradient-descent-based algorithm to solve it. Our empirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user types for each advertiser at a minor loss to the revenue of the platform, and a small change to the size of the audience each advertiser reaches.
","['Yale University', 'Indian Institute of Technology Kanpur', 'Yale University']"
2019,Learning Optimal Fair Policies,"Razieh Nabi, Daniel Malinsky, Ilya Shpitser",https://icml.cc/Conferences/2019/Schedule?showEvent=4266,"Systematic discriminatory biases present in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are put into practice as policy. Automated decision procedures and learning algorithms applied to such data may serve to perpetuate existing injustice or unfairness in our society. In this paper, we consider how to make optimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair dependence of both decisions and outcomes on sensitive features (e.g., variables that correspond to gender, race, disability, or other protected attributes). We use methods from causal inference and constrained optimization to learn optimal policies in a way that addresses multiple potential biases which afflict data analysis in sensitive contexts, extending the approach of Nabi & Shpitser (2018). Our proposal comes equipped with the theoretical guarantee that the chosen fair policy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our approach with both synthetic data and real criminal justice data. 
","['Johns Hopkins University', 'Johns Hopkins University', 'Johns Hopkins University']"
2019,Fairness-Aware Learning for Continuous Attributes and Treatments,"Jeremie Mary, Clément Calauzènes, Noureddine El Karoui",https://icml.cc/Conferences/2019/Schedule?showEvent=4130,"We address the problem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness metrics can be expressed as measures of (conditional) independence between variables, we propose to use the R\'enyi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exploit Witsenhausen's characterization of the R\'enyi correlation coefficient to propose a differentiable implementation linked to $f$-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a penalty that upper bounds this coefficient. Theses allows fairness to be extented to variables such as mixed ethnic groups or financial status without thresholds effects. 
This  penalty can be estimated on  mini-batches allowing to use deep nets. Experiments  show  favorable comparisons  to state of the art on binary variables and prove the ability to protect continuous ones","['CRITEO', 'Criteo AI Lab', 'Criteo AI Lab and UC, Berkeley']"
2019,Fairness risk measures,"Robert C Williamson, Aditya Menon",https://icml.cc/Conferences/2019/Schedule?showEvent=3728,"Ensuring that classifiers are non-discriminatory or fair with respect to a sensitive feature (e.g., race or gender) is a topical problem. Progress in this task requires fixing a definition of fairness, and there have been several proposals in this regard over the past few years. Several of these, however, assume either binary sensitive features (thus precluding categorical or real-valued sensitive groups), or result in non-convex objectives (thus adversely affecting the optimisation landscape). In this paper, we propose a new definition of fairness that generalises some existing proposals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the expected losses (or risks) across each subgroup induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a special case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).
","['ANU', 'Google']"
2019,Proportionally Fair Clustering,"Xingyu Chen, Brandon Fain, Liang Lyu, Kamesh Munagala",https://icml.cc/Conferences/2019/Schedule?showEvent=4228,"We extend the fair machine learning literature by considering the problem of proportional centroid clustering in a metric context. For clustering n points with k centers, we define fairness as proportionality to mean that any n/k points are entitled to form their own cluster if there is another center that is closer in distance for all n/k points. We seek clustering solutions to which there are no such justified complaints from any subsets of agents, without assuming any a priori notion of protected subsets. We present and analyze algorithms to efficiently compute, optimize, and audit proportional solutions. We conclude with an empirical examination of the tradeoff between proportional solutions and the k-means objective.
","['Duke University', 'Duke University', 'Duke University', 'Duke University']"
2019,Stable and Fair Classification,"Lingxiao Huang, Nisheeth Vishnoi",https://icml.cc/Conferences/2019/Schedule?showEvent=4314,"In a recent study, Friedler et al.  pointed out that several fair classification algorithms are not stable with respect to variations in the training set -- a crucial consideration in several applications. Motivated by their work, we study the problem of designing classification algorithms that are both fair and stable. We propose an extended framework based on fair classification algorithms that are formulated as optimization problems, by introducing a stability-focused regularization term. Theoretically, we prove an additional stability guarantee, that was lacking in fair classification algorithms, and also provide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization parameter in our framework. We assess the benefits of our approach empirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our empirical results show that our extended framework indeed improves the stability at only a slight sacrifice in accuracy.
","['EPFL', 'Yale University']"
2019,Flexibly Fair Representation Learning by Disentanglement,"Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, Richard Zemel",https://icml.cc/Conferences/2019/Schedule?showEvent=4254,"We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \emph{flexibly fair}, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---allows for the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.
","['University of Toronto', 'University of Toronto', 'Vector Institute and University of Toronto', 'University of Tübingen', 'Google Brain', 'University of Toronto', 'Vector Institute']"
2019, Fair Regression: Quantitative Definitions and Reduction-Based Algorithms,"Alekh Agarwal, Miroslav Dudik, Steven Wu",https://icml.cc/Conferences/2019/Schedule?showEvent=4075,"In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness--accuracy frontiers on several standard datasets.
","['Microsoft Research', 'Microsoft Research', 'University of Minnesota']"
2019,Fairness without Harm: Decoupled Classifiers with Preference Guarantees,"Berk Ustun, Yang Liu, David Parkes",https://icml.cc/Conferences/2019/Schedule?showEvent=3684,"In domains such as medicine, it can be acceptable for machine learning models to include {\em sensitive attributes} such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity, then it should be in the best interest of each group. Drawing on ethical principles such as beneficence (""do the best"") and non-maleficence (""do no harm""), we show how to use sensitive attributes to train decoupled classifiers that satisfy preference guarantees. These guarantees ensure the majority of individuals in each group prefer their assigned classifier to (i) a pooled model that ignores group membership (rationality), and (ii) the model assigned to any other group (envy-freeness). We introduce a  recursive  procedure that adaptively selects group attributes for decoupling, and present formal conditions to ensure preference guarantees in terms of generalization error. We validate the effectiveness of the procedure on real-world datasets, showing that it improves accuracy without violating  preference guarantees on test data.
","['Harvard University', 'UCSC', 'Harvard University']"
2019,Differentially Private Fair Learning,"Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed Sharifi-Malvajerdi, Jonathan Ullman",https://icml.cc/Conferences/2019/Schedule?showEvent=4111,"Motivated by settings in which predictive models may be required to be non-discriminatory with respect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential privacy. Our first algorithm is a private implementation of the equalized odds post-processing approach of (Hardt et al., 2016). This algorithm is appealingly simple, but must be able to use protected group membership explicitly at test time, which can be viewed as a form of “disparate treatment”. Our second algorithm is a differentially private version of the oracle-efficient in-processing approach of (Agarwal et al., 2018) which is more complex but need not have access to protected group membership at test time. We identify new tradeoffs between fairness, accuracy, and privacy that emerge only when requiring all three properties, and show that these tradeoffs can be milder if group membership may be used at test time. We conclude with a brief experimental evaluation.
","['Northeastern University', 'University of Pennsylvania', 'University of Pennsylvania', 'Northeastern University', 'University of Pennsylvania', 'University of Pennsylvania', 'Northeastern University']"
2019,Obtaining Fairness using Optimal Transport Theory,"Paula Gordaliza, Eustasio del Barrio, Gamboa Fabrice, Loubes Jean-Michel",https://icml.cc/Conferences/2019/Schedule?showEvent=4010,"In the fair classification setup, we recast the links between fairness and predictability in terms of probability metrics. We analyze repair methods based on mapping conditional distributions to the Wasserstein barycenter. We propose a Random Repair which yields a tradeoff between minimal information loss and a certain amount of fairness.
","['Institut de Mathématiques de Toulouse and IMUVA', 'IMUVA', 'Université Toulouse Paul Sabatier Institut de Mathématiques de Toulouse', 'Université Toulouse Paul Sabatier Institut de Mathématiques de Toulouse']"
2019,Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions,"Hao Wang, Berk Ustun, Flavio Calmon",https://icml.cc/Conferences/2019/Schedule?showEvent=3898,"When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance  disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy.
","['Harvard University', 'Harvard University', 'Harvard University']"
2019,On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning,"Hoda Heidari, Vedant Nanda, Krishna Gummadi",https://icml.cc/Conferences/2019/Schedule?showEvent=3998,"Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse impact of the algorithmic decisions today on the long-term welfare and prosperity of certain segments of the population. We take a broader perspective on algorithmic fairness. We propose an effort-based measure of fairness and present a data-driven framework for characterizing the long-term impact of algorithmic policies on reshaping the underlying population. Motivated by the psychological literature on social learning and the economic literature on equality of opportunity, we propose a micro-scale model of how individuals may respond to decision-making algorithms. We employ existing measures of segregation from sociology and economics to quantify the resulting macro- scale population-level change. Importantly, we observe that different models may shift the group- conditional distribution of qualifications in different directions. Our findings raise a number of important questions regarding the formalization of fairness for decision-making models.
","['ETHZ', 'MPI-SWS', 'MPI-SWS']"
2019,Making Decisions that Reduce Discriminatory Impacts,"Matt J. Kusner, Chris Russell, Joshua Loftus, Ricardo Silva",https://icml.cc/Conferences/2019/Schedule?showEvent=4198,"As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one aspect of this, namely the discriminatory prediction problem: How can we reduce discrimination in the predictions themselves? While an important question, solutions to this problem only apply in a restricted setting, as we have full control over the predictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key aspect of this challenge, the discriminatory impact problem: How can we reduce discrimination arising from the real-world impact of decisions? To address this, we describe causal methods that model the relevant parts of the real-world system in which the decisions are made. Unlike previous approaches, these  models not only allow us to map the causal pathway of a single decision, but also to model the effect of interference--how the impact on an individual depends on decisions made about other people. Often, the goal of decision policies is to maximize a beneficial impact overall. To reduce the discrimination of these benefits, we devise a constraint inspired by recent work in counterfactual fairness, and give an efficient procedure to solve the constrained optimization problem. We demonstrate our approach with an example: how to increase students taking college entrance exams in New York City public schools.
","['The Alan Turing Institute', 'The Alan Turing Institute/University of Surrey', 'New York University', 'University College London']"
2019,"Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications","Christopher Harshaw, Moran Feldman, Justin Ward, Amin Karbasi",https://icml.cc/Conferences/2019/Schedule?showEvent=4159,"It is generally believed that submodular functions--and the more general class of 
$\gamma$-weakly submodular functions--may only be optimized under the non-negativity assumption $f(S) \geq 0$. In this paper, we show that once the function is expressed as the difference $f = g - c$, where $g$ is monotone, non-negative, and $\gamma$-weakly submodular and $c$ is non-negative modular, then strong approximation guarantees may be obtained. We present an algorithm for maximizing $g - c$ under a $k$-cardinality constraint which produces a random feasible set $S$ such that $\mathbb{E}[g(S) -c(S)]  \geq  (1  -  e^{-\gamma}  - \epsilon) g(\opt) - c(\opt)$, 
whose running time is $O (\frac{n}{\epsilon} \log^2 \frac{1}{\epsilon})$, independent of $k$. We extend these results to the unconstrained setting by describing an algorithm with the same approximation guarantees and faster $O(n \frac{1}{\epsilon} \log\frac{1}{\epsilon})$ runtime. The main techniques underlying our algorithms are two-fold: the use of a surrogate objective which varies the relative importance between $g$ and $c$ throughout the algorithm, and a geometric sweep over possible $\gamma$ values. Our algorithmic guarantees are complemented by a hardness result showing that no polynomial-time algorithm which accesses $g$ through a value oracle can do better. We empirically demonstrate the success of our algorithms by applying them to experimental design on the Boston Housing dataset and directed vertex cover on the Email EU dataset.","['Yale University', 'The Open University of Israel', 'Queen Mary University of London', 'Yale']"
2019,Online Algorithms for Rent-Or-Buy with Expert Advice,"Sreenivas Gollapudi, Debmalya Panigrahi",https://icml.cc/Conferences/2019/Schedule?showEvent=4288,"We study the use of predictions by multiple experts (such as machine learning algorithms) to improve the performance of online algorithms. In particular, we consider the classical rent-or-buy problem (also called ski rental), and obtain algorithms that provably improve their performance over the adversarial scenario by using these predictions. We also prove matching lower bounds to show that our algorithms are the best possible, and perform experiments to empirically validate their performance in practice
","['Google Research', 'Duke University']"
2019,Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity,"Matthew Fahrbach, Vahab Mirrokni, Morteza Zadimoghaddam",https://icml.cc/Conferences/2019/Schedule?showEvent=3890,"Submodular maximization is a general optimization problem with a wide range of applications in machine learning (e.g., active learning, clustering, and feature selection). In large-scale optimization, the parallel running time of an algorithm is governed by its adaptivity, which measures the number of sequential rounds needed if the algorithm can execute polynomially-many independent oracle queries in parallel. While low adaptivity is ideal, it is not sufficient for an algorithm to be efficient in practice---there are many applications of distributed submodular optimization where the number of function evaluations becomes prohibitively expensive. Motivated by these applications, we study the adaptivity and query complexity of submodular maximization. In this paper, we give the first constant-factor approximation algorithm for maximizing a non-monotone submodular function subject to a cardinality constraint $k$ that runs in $O(\log(n))$ adaptive rounds and makes $O(n \log(k))$ oracle queries in expectation. In our empirical study, we use three real-world applications to compare our algorithm with several benchmarks for non-monotone submodular maximization. The results demonstrate that our algorithm finds competitive solutions using significantly fewer rounds and queries.","['Georgia Institute of Technology', 'Google Research', 'Google']"
2019,Categorical Feature Compression via Submodular Optimization,"Mohammad Hossein Bateni, Lin Chen, Hossein Esfandiari, Thomas Fu, Vahab Mirrokni, Afshin Rostamizadeh",https://icml.cc/Conferences/2019/Schedule?showEvent=4276,"In the era of big data, learning from categorical features with very large vocabularies (e.g., 28 million for the Criteo click prediction dataset) has become a practical challenge for machine learning researchers and practitioners.  We design a highly-scalable vocabulary compression algorithm that seeks to maximize the mutual information between the compressed categorical feature and the target binary labels and we furthermore show that its solution is guaranteed to be within a $1-1/e \approx 63\%$ factor of the global optimal solution. Although in some settings, entropy-based set functions are known to be submodular, this is not the case for the mutual information objective we consider (mutual information with respect to the target labels).  To address this, we introduce a novel re-parametrization of the mutual information objective, which we prove is submodular, and also design a data structure to query the submodular function in amortized $O(\log n )$ time (where $n$ is the input vocabulary size). Our complete algorithm is shown to operate in $O(n \log n )$ time. Additionally, we design a distributed implementation in which the query data structure is decomposed across $O(k)$ machines such that each machine only requires $O(\frac n k)$ space, while still preserving the approximation guarantee and using only logarithmic rounds of computation.  We also provide analysis of simple alternative heuristic compression methods to demonstrate they cannot achieve any approximation guarantee.  Using the large-scale Criteo learning task, we demonstrate better performance in retaining mutual information and also verify competitive learning performance compared to other baseline methods.","['Google Research', 'Yale University', 'Google Research', 'Google', 'Google Research', 'Google']"
2019,Multi-Frequency Phase Synchronization,"Tingran Gao, Zhizhen Zhao",https://icml.cc/Conferences/2019/Schedule?showEvent=3893,"We propose a novel formulation for phase synchronization---the statistical problem of jointly estimating alignment angles from noisy pairwise comparisons---as a nonconvex optimization problem that enforces consistency among the pairwise comparisons in multiple frequency channels. Inspired by harmonic retrieval in signal processing, we develop a simple yet efficient two-stage algorithm that leverages the
multi-frequency information. We demonstrate in theory and practice that the proposed algorithm significantly outperforms state-of-the-art phase synchronization algorithms, at a mild computational costs incurred by using the extra frequency channels. We also extend our algorithmic framework to general synchronization problems over compact Lie groups.
","['University of Chicago', 'University of Illinois at Urbana Champaign']"
2019,Faster Algorithms for Binary Matrix Factorization,"Ravi Kumar, Rina Panigrahy, Ali Rahimi, David Woodruff",https://icml.cc/Conferences/2019/Schedule?showEvent=4153,"We give faster approximation algorithms for well-studied variants of Binary Matrix Factorization (BMF), where we are given a binary $m \times n$ matrix $A$ and would like to find binary rank-$k$ matrices $U, V$ to minimize the Frobenius norm of $U \cdot V - A$.   In the first setting, $U \cdot V$ denotes multiplication over $\mathbb{Z}$, and we give a constant-factor approximation algorithm that runs in $2^{O(k^2 \log k)} \textrm{poly}(mn)$ time, improving upon the previous $\min(2^{2^k}, 2^n) \textrm{poly}(mn)$ time.  Our techniques generalize to minimizing $\|U \cdot V - A\|_p$ for $p \geq 1$, in $2^{O(k^{\lceil p/2 \rceil + 1}\log k)} \textrm{poly}(mn)$ time. For $p = 1$, this has a graph-theoretic consequence, namely, a $2^{O(k^2)} \poly(mn)$-time algorithm to approximate a graph as a union of disjoint bicliques.  In the second setting, $U \cdot V$ is over $\GF(2)$, and we give a bicriteria constant-factor approximation algorithm that runs in $2^{O(k^3)} \poly(mn)$ time
to find binary rank-$O(k \log m)$ matrices $U$, $V$ whose cost is as good as the best rank-$k$ approximation, improving upon $\min(2^{2^k}mn, \min(m,n)^{k^{O(1)}} \textrm{poly}(mn))$ time.","['Google', 'Google', 'Google', 'Carnegie Mellon University']"
2019,Guided evolutionary strategies: augmenting random search with surrogate gradients,"Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, Jascha Sohl-Dickstein",https://icml.cc/Conferences/2019/Schedule?showEvent=3975,"Many applications in machine learning require optimizing a function whose true gradient is unknown or computationally expensive, but where surrogate gradient information, directions that may be correlated with the true gradient, is cheaply available. For example, this occurs when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in reinforcement learning or training networks with discrete variables). We propose Guided Evolutionary Strategies (GES), a method for optimally using surrogate gradient directions to accelerate random search. GES defines a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients and estimates a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace and use this to derive a setting of the hyperparameters that works well across problems. We evaluate GES on several example problems, demonstrating an improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.
","['Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain']"
2019,Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces,"Johannes Kirschner, Mojmir Mutny, Nicole Hiller, Rasmus Ischebeck, Andreas Krause",https://icml.cc/Conferences/2019/Schedule?showEvent=4048,"Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.
","['ETH Zurich', 'ETH Zurich', 'PSI', 'PSI', 'ETH Zurich']"
2019,Semi-Cyclic Stochastic Gradient Descent,"Hubert Eichner, Tomer Koren, Brendan McMahan, Nati Srebro, Kunal Talwar",https://icml.cc/Conferences/2019/Schedule?showEvent=4282,"We consider convex SGD updates with a block-cyclic structure, i.e., where each cycle consists of a small number of blocks, each with many samples from a possibly different, block-specific, distribution.  This situation arises, e.g., in Federated Learning where the mobile devices available for updates at different times during the day have different characteristics. We show that such block-cyclic structure can significantly deteriorate the performance of SGD, but propose a simple  approach that allows prediction with the same guarantees as for i.i.d., non-cyclic, sampling.
","['Google', 'Google Brain', 'Google', 'Toyota Technological Institute at Chicago', 'Google']"
2019,Matrix-Free Preconditioning in Online Learning,"Ashok Cutkosky, Tamas Sarlos",https://icml.cc/Conferences/2019/Schedule?showEvent=3934,"We provide an online convex optimization algorithm with regret that interpolates between the regret of an algorithm using an optimal preconditioning matrix and one using a diagonal preconditioning matrix. Our regret bound is never worse than that obtained by diagonal preconditioning, and in certain setting even surpasses that of algorithms with full-matrix preconditioning. Importantly, our algorithm runs in the same time and space complexity as online gradient descent. Along the way we incorporate new techniques that mildly streamline and improve logarithmic factors in prior regret analyses. We conclude by benchmarking our algorithm on synthetic data and deep learning tasks.
","['Google', 'Google']"
2019,Online Convex Optimization in Adversarial Markov Decision Processes,"Aviv Rosenberg, Yishay Mansour",https://icml.cc/Conferences/2019/Schedule?showEvent=3679,"We consider online learning in episodic loop-free Markov decision processes (MDPs), where the  loss function can change arbitrarily between episodes, and the transition function is not known to the learner.
We show $\tilde{O}(L|X|\sqrt{|A|T})$ regret bound, where $T$ is the number of episodes, $X$ is the state space, $A$ is the action space, and $L$ is the length of each episode. 
Our online algorithm is implemented using entropic regularization methodology, which allows to extend the original adversarial MDP model to handle convex performance criteria (different ways to aggregate the losses of a single episode) , as well as improve previous regret bounds.","['Tel Aviv University', 'Google and Tel Aviv University']"
2019,Competing Against Nash Equilibria in Adversarially Changing Zero-Sum Games,"Adrian Rivera Cardoso, Jacob Abernethy, He Wang, Huan Xu",https://icml.cc/Conferences/2019/Schedule?showEvent=3866,"We study the problem of repeated play in a zero-sum game in which the payoff matrix may change, in a possibly adversarial fashion, on each round; we call these Online Matrix Games. Finding the Nash Equilibrium (NE) of a two player zero-sum game is core to many problems in statistics, optimization, and economics, and for a fixed game matrix this can be easily reduced to solving a linear program. But when the payoff matrix evolves over time our goal is to find a sequential algorithm that can compete with, in a certain sense, the NE of the long-term-averaged payoff matrix. We design an algorithm with small NE regret--that is, we ensure that the long-term payoff of both players is close to minimax optimum in hindsight. Our algorithm achieves near-optimal dependence with respect to the number of rounds and depends poly-logarithmically on the number of available actions of the players. Additionally, we show that the naive reduction, where each player simply minimizes its own regret, fails to achieve the stated objective regardless of which algorithm is used. Lastly, we consider the so-called bandit setting, where the feedback is significantly limited, and we provide an algorithm with small NE regret using one-point estimates of each payoff matrix.
","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Tech']"
2019,Online Learning with Sleeping Experts and Feedback Graphs,"Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, Scott Yang",https://icml.cc/Conferences/2019/Schedule?showEvent=4151,"We consider the scenario of online learning with sleeping experts, where not all experts are available at each round, and analyze the general framework of learning with feedback graphs, where the loss observations associated with each expert are characterized by a graph. A critical assumption in this framework is that the loss observations and the set of sleeping experts at each round are independent. We first extend the classical sleeping experts algorithm of Kleinberg et al. 2008 to the feedback graphs scenario, and prove matching upper and lower bounds for the sleeping regret of the resulting algorithm under the independence assumption. Our main contribution is then to relax this assumption, present a more general notion of sleeping regret, and derive a general algorithm with strong theoretical guarantees. We apply this new framework to the important scenario of online learning with abstention, where a learner can elect to abstain from making a prediction at the price of a certain cost. We empirically validate our algorithm against multiple online abstention algorithms on several real-world datasets, showing substantial performance improvements.
","['Google Research', 'Google Research', 'INRIA and Google', 'Courant Institute and Google Research', 'D. E. Shaw & Co.']"
2019,Incremental Randomized Sketching for Online Kernel Learning,"Xiao Zhang, Shizhong Liao",https://icml.cc/Conferences/2019/Schedule?showEvent=3922,"Randomized sketching has been used in offline kernel learning, but it cannot be applied directly to online kernel learning due to the lack of incremental maintenances for randomized sketches with regret guarantees. To address these issues, we propose a novel incremental randomized sketching approach for online kernel learning, which has efficient incremental maintenances with theoretical guarantees. We construct two incremental randomized sketches using the sparse transform matrix and the sampling matrix for kernel matrix approximation, update the incremental randomized sketches using rank-$1$ modifications, and construct an time-varying explicit feature mapping for online kernel learning. We prove that the proposed incremental randomized sketching is statistically unbiased for the matrix product approximation, obtains a $1 + \epsilon$ relative-error bound for the kernel matrix approximation, enjoys a sublinear regret bound for online kernel learning, and has constant time and space complexities at each round for incremental maintenances. Experimental results demonstrate that the incremental randomized sketching achieves a better learning performance in terms of accuracy and efficiency even in adversarial environments.","['Tianjin University', 'Tianjin University']"
2019,Adaptive Scale-Invariant Online Algorithms for Learning Linear Models,"Michal Kempka, Wojciech Kotlowski, Manfred K. Warmuth",https://icml.cc/Conferences/2019/Schedule?showEvent=4196,"We consider online learning with linear models, where the algorithm predicts on sequentially revealed instances (feature vectors), and is compared against the best linear function (comparator) in hindsight. Popular algorithms in this framework, such as Online Gradient Descent (OGD), have parameters (learning rates), which ideally should be tuned based on the scales of the features and the optimal comparator, but these quantities only become available at the end of the learning process. In this paper, we resolve the tuning problem by proposing online algorithms making predictions which are invariant under arbitrary rescaling of the features. The algorithms have no parameters to tune, do not require any prior knowledge on the scale of the instances or the comparator, and achieve regret bounds matching (up to a logarithmic factor) that of OGD with optimally tuned separate learning rates per dimension, while retaining comparable runtime performance.
","['Poznan University of Technology', 'Poznan University of Technology', 'UC Santa Cruz & Google Inc.']"
2019,Online Control with Adversarial Disturbances,"Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, Karan Singh",https://icml.cc/Conferences/2019/Schedule?showEvent=4313,"We study the control of linear dynamical systems with adversarial disturbances, as opposed to statistical noise. We present an efficient algorithm that achieves nearly-tight regret bounds in this setting. Our result generalizes upon previous work in two main aspects: the algorithm can accommodate adversarial noise in the dynamics, and can handle general convex costs.
","['Google AI Princeton', 'Princeton University', 'Princeton University', 'University of Washington', 'Princeton University']"
2019,Adversarial Online Learning with noise,"Alon Resler, Yishay Mansour",https://icml.cc/Conferences/2019/Schedule?showEvent=3633,"We present and study models of adversarial online learning where the feedback observed by the learner is noisy, and the feedback is either full information feedback or bandit feedback. Specifically, we consider binary losses xored with the noise, which is a Bernoulli random variable. We consider both a constant noise rate and a variable noise rate. Our main results are tight regret bounds for learning with noise in the adversarial online learning model.
","['Tel Aviv University', 'Google and Tel Aviv University']"
2019,Online Variance Reduction with Mixtures,"Zalán Borsos, Sebastian Curi, Yehuda Levy, Andreas Krause",https://icml.cc/Conferences/2019/Schedule?showEvent=3739,"Adaptive importance sampling for stochastic optimization is a promising approach that offers improved convergence through variance reduction. In this work, we propose a new framework for variance reduction that enables the use of mixtures over predefined sampling distributions, which can naturally encode prior knowledge about the data. While these sampling distributions are fixed, the mixture weights are adapted during the optimization process. We propose VRM, a novel and efficient adaptive scheme that asymptotically recovers the best mixture weights in hindsight and can also accommodate sampling distributions over sets of points. We empirically demonstrate the versatility of VRM in a range of applications.
","['ETH Zurich', 'ETH', 'ETH Zurich', 'ETH Zurich']"
2019,Bandit Multiclass Linear Classification: Efficient Algorithms for the Separable Case,"Alina Beygelzimer, David Pal, Balazs Szorenyi, Devanathan Thiruvenkatachari, Chen-Yu Wei, Chicheng Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=3672,"We study the problem of efficient online multiclass linear classification with
bandit feedback, where all examples belong to one of $K$ classes and lie in the
$d$-dimensional Euclidean space. Previous works have left open the challenge of
designing efficient algorithms with finite mistake bounds when the data is
linearly separable by a margin $\gamma$. In this work, we take a first step
towards this problem. We consider two notions of linear separability:
strong and weak.

1. Under the strong linear separability condition, we design an efficient
algorithm that achieves a near-optimal mistake bound of
$O\left(\frac{K}{\gamma^2} \right)$.

2. Under the more challenging weak linear separability condition, we design
an efficient algorithm with a mistake bound of $2^{\widetilde{O}(\min(K \log^2
\frac{1}{\gamma}, \sqrt{\frac{1}{\gamma}} \log K))}$. Our algorithm
is based on kernel Perceptron, which is inspired by the work
of Klivans & Servedio (2008) on improperly learning intersection of halfspaces.","['Yahoo Research', 'Expedia', 'Yahoo Research', 'New York University', 'University of Southern California', 'Microsoft Research']"
2019,Learning Linear-Quadratic Regulators Efficiently with only $\sqrt{T}$ Regret,"Alon Cohen, Tomer Koren, Yishay Mansour",https://icml.cc/Conferences/2019/Schedule?showEvent=3803,"We present the first computationally-efficient algorithm with $\widetilde{O}(\sqrt{T})$ regret for learning in Linear Quadratic Control systems with unknown dynamics.
By that, we resolve an open question of Abbasi-Yadkori and Szepesvari (2011) and Dean,Mania, Matni, Recht, and Tu (2018).","['Technion and Google', 'Google Brain', 'Google and Tel Aviv University']"
2019,Learning from Delayed Outcomes via Proxies with Applications to Recommender Systems,"Timothy Mann, Sven Gowal, András György, Huiyi Hu, Ray Jiang, Balaji Lakshminarayanan, Prav Srinivasan",https://icml.cc/Conferences/2019/Schedule?showEvent=3921,"Predicting delayed outcomes is an important problem in recommender systems (e.g., if customers will finish reading an ebook). We formalize the problem as an adversarial, delayed online learning problem and consider how a proxy for the delayed outcome (e.g., if customers read a third of the book in 24 hours) can help minimize regret, even though the proxy is not available when making a prediction. Motivated by our regret analysis, we propose two neural network architectures: Factored Forecaster (FF) which is ideal if the proxy is informative of the outcome in hindsight, and Residual Factored Forecaster (RFF)  that is robust to a non-informative proxy. Experiments on two real-world datasets for predicting human behavior show that RFF outperforms both FF and a direct forecaster that does not make use of the proxy. Our results suggest that exploiting proxies by factorization is a promising way to mitigate the impact of long delays in human-behavior prediction tasks.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Google Deepmind', 'Google DeepMind', 'DeepMind']"
2019,Adaptive Regret of Convex and Smooth Functions,"Lijun Zhang, Tie-Yan Liu, Zhi-Hua Zhou",https://icml.cc/Conferences/2019/Schedule?showEvent=3815,"We investigate online convex optimization in changing environments, and choose the adaptive regret as the performance measure. The goal is to achieve a small regret over every interval so that the comparator is allowed to change over time. Different from previous works that only utilize the convexity condition, this paper further exploits smoothness to improve the adaptive regret. To this end, we develop novel adaptive algorithms for convex and smooth functions, and establish problem-dependent regret bounds over any interval. Our regret bounds are comparable to existing results in the worst case, and become much tighter when the comparator has a small loss.
","['Nanjing University', 'Microsoft', 'Nanjing University']"
2019,Online Adaptive Principal Component Analysis and Its extensions,"Jianjun Yuan, Andrew Lamperski",https://icml.cc/Conferences/2019/Schedule?showEvent=3705,"We propose algorithms for online principal component analysis (PCA)
and variance minimization for adaptive settings.
Previous literature has focused on upper bounding the static adversarial regret,
whose comparator is the optimal fixed action in hindsight.
However, static regret is not an appropriate metric when the underlying environment is changing.
Instead, we adopt the adaptive regret metric from the previous literature 
and propose online adaptive algorithms for PCA and variance minimization, 
that have sub-linear adaptive regret guarantees.
We demonstrate both theoretically and experimentally that
the proposed algorithms can adapt to the changing environments.
","['University of Minnesota', 'University of Minnesota']"
2019,POLITEX: Regret Bounds for Policy Iteration using Expert Prediction,"Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba Szepesvari, Gellért Weisz",https://icml.cc/Conferences/2019/Schedule?showEvent=3753,"We present POLITEX (POLicy ITeration with EXpert advice), a variant of policy iteration where each policy is a Boltzmann distribution over the sum of action-value function estimates of the previous policies, and analyze its regret in continuing RL problems.  We assume that the value function error after running a policy for $\tau$ time steps scales as $\eps(\tau) = \eps_0 + O(\sqrt{d/\tau})$, where $\eps_0$ is the worst-case approximation error and $d$ is the number of features in a compressed representation of the state-action space. We establish that this condition is satisfied by the LSPE algorithm under certain assumptions on the MDP and policies. Under the error assumption, we show that the regret of POLITEX  in uniformly mixing MDPs scales  as  $O(d^{1/2}T^{3/4} + \eps_0T)$, where $O(\cdot)$ hides logarithmic terms and problem-dependent constants. Thus, we provide the first regret bound for a fully practical model-free method which only scales in the number of features, and not in the size of the underlying MDP. Experiments on a queuing problem confirm that POLITEX is competitive with some of its alternatives, while preliminary results on Ms Pacman (one of the standard Atari benchmark problems) confirm the viability of POLITEX beyond linear function approximation.","['Adobe Research', '""University of California, Berkeley""', 'UC Berkeley', 'Google', 'DeepMind/University of Alberta', 'DeepMind']"
2019,"Anytime Online-to-Batch, Optimism and Acceleration",Ashok Cutkosky,https://icml.cc/Conferences/2019/Schedule?showEvent=4147,"A standard way to obtain convergence guarantees in stochastic convex optimization is to run an online learning algorithm and then output the average of its iterates: the actual iterates of the online learning algorithm do not come with individual guarantees. We close this gap by introducing a black-box modification to any online learning algorithm whose iterates converge to the optimum in stochastic scenarios. We then consider the case of smooth losses, and show that combining our approach with optimistic online learning algorithms immediately yields a fast convergence rate of $O(L/T^{3/2}+\sigma/\sqrt{T})$ on $L$-smooth problems with $\sigma^2$ variance in the gradients. Finally, we provide a reduction that converts any adaptive online algorithm into one that obtains the optimal accelerated rate of $\tilde O(L/T^2 + \sigma/\sqrt{T})$, while still maintaining $\tilde O(1/\sqrt{T})$ convergence in the non-smooth setting. Importantly, our algorithms adapt to $L$ and $\sigma$ automatically: they do not need to know either to obtain these rates.",['Google']
2019,Cautious Regret Minimization: Online Optimization with Long-Term Budget Constraints,"Nikolaos Liakopoulos, Apostolos Destounis, Georgios Paschos, Thrasyvoulos Spyropoulos, Panayotis Mertikopoulos",https://icml.cc/Conferences/2019/Schedule?showEvent=4210,"We study a class of online convex optimization problems with long-term budget constraints that arise naturally as reliability guarantees or total consumption constraints. In this general setting, prior work by Mannor et al. (2009) has shown that achieving no regret is impossible if the functions defining the agent's budget are chosen by an adversary. To overcome this obstacle, we refine the agent's regret metric by introducing the notion of a ""K-benchmark"", i.e., a comparator which meets the problem's allotted budget over any window of length K. The impossibility analysis of Mannor et al. (2009) is recovered when K=T; however, for K=o(T), we show that it is possible to minimize regret while still meeting the problem's long-term budget constraints. We achieve this via an online learning policy based on Cautious Online Lagrangiant Descent (COLD) for which we derive explicit bounds, in terms of both the incurred regret and the residual budget violations.
","['Huawei Paris Research Center', 'Mathematical and Algorithmic Sciences Lab,  France Research Center, Huawei Technologies Co. Ltd.', 'Huawei ', 'EURECOM', 'CNRS']"
2019,Optimal Kronecker-Sum Approximation of Real Time Recurrent Learning,"Frederik Benzing, Marcelo Matheus Gauy, Asier Mujika, Anders Martinsson, Angelika Steger",https://icml.cc/Conferences/2019/Schedule?showEvent=3880,"One of the central goals of Recurrent Neural Networks (RNNs) is to learn long-term dependencies in sequential data. 
Nevertheless, the most popular training method, Truncated Backpropagation through Time (TBPTT), categorically forbids learning dependencies beyond the truncation horizon.
In contrast, the online training algorithm Real Time Recurrent Learning (RTRL) provides untruncated gradients, with the disadvantage of impractically large computational costs. 
Recently published approaches reduce these costs by providing noisy approximations of RTRL. 
We present a new approximation algorithm of RTRL, Optimal Kronecker-Sum Approximation (OK).
We prove that OK is optimal for a class of approximations of RTRL, which includes all approaches published so far. 
Additionally, we show that OK has empirically negligible noise: Unlike previous algorithms it matches TBPTT in a real world task (character-level Penn TreeBank) and can exploit online parameter updates to outperform TBPTT in a synthetic 
string memorization task.
Code available at GitHub.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2019,Adaptive Sensor Placement for Continuous Spaces,"James A. Grant, Alexis Boukouvalas, Ryan-Rhys Griffiths, David Leslie, Sattar Vakili, Enrique Munoz De Cote",https://icml.cc/Conferences/2019/Schedule?showEvent=4082,We consider the problem of adaptively placing sensors along an interval to detect stochastically-generated events. We present a new formulation of the problem as a continuum-armed bandit problem with feedback in the form of partial observations of realisations of an inhomogeneous Poisson process. We design a solution method by combining Thompson sampling with nonparametric inference via increasingly granular Bayesian histograms and derive an $\tilde{O}(T^{2/3})$ bound on the Bayesian regret in $T$ rounds. This is coupled with the design of an efficent optimisation approach to select actions in polynomial time. In simulations we demonstrate our approach to have substantially lower and less variable regret than competitor algorithms.,"['Lancaster University', 'PROWLER.io', 'University of Cambridge', 'Lancaster University', 'Prowler.io', 'PROWLER.io']"
2019,Scale-free adaptive planning for deterministic dynamics & discounted rewards,"Peter Bartlett, Victor Gabillon, Jennifer Healey, Michal Valko",https://icml.cc/Conferences/2019/Schedule?showEvent=4144,"We address the problem of planning in an environment with deterministic dynamics and stochastic discounted rewards under a limited numerical budget where the ranges of both rewards and noise are unknown.  We introduce PlaTypOOS, an adaptive, robust, and efficient alternative to the OLOP (open-loop optimistic planning) algorithm.  Whereas OLOP requires a priori knowledge of the ranges of both rewards and noise, PlaTypOOS dynamically adapts its behavior to both.  This allows PlaTypOOS to be immune to two vulnerabilities of OLOP: failure when given underestimated ranges of noise and rewards and inefficiency when these are overestimated. PlaTypOOS additionally adapts to the global smoothness of the value function. PlaTypOOS acts in a provably more efficient manner vs. OLOP when OLOP is given an overestimated reward and show that in the case of no noise, PlaTypOOS learns exponentially faster.
","['UC Berkeley', 'Huawei', 'Adobe', 'DeepMind']"
2019,Communication-Constrained Inference and the Role of Shared Randomness,"Jayadev Acharya, Clément Canonne, Himanshu Tyagi",https://icml.cc/Conferences/2019/Schedule?showEvent=3569,"A central server needs to perform statistical inference based on samples that are distributed over multiple users  who can each send a message of limited length to the center. We study  problems of distribution learning and identity testing in this distributed inference setting and examine the role of shared randomness as a resource. We propose a general purpose \textit{simulate-and-infer} strategy that  uses only private-coin communication protocols and is sample-optimal for distribution learning. This general strategy turns out to be sample-optimal even for distribution testing among private-coin protocols. Interestingly, we propose a public-coin protocol that outperforms simulate-and-infer for distribution testing and is, in fact, sample-optimal. Underlying our public-coin protocol is a random hash that when applied to the samples minimally contracts the chi-squared distance of their distribution from the uniform distribution. 
","['Cornell University', 'Stanford University', 'IISC']"
2019,Learning and Data Selection in Big Datasets,"Hossein Shokri Ghadikolaei, Hadi Ghauch, Inst. of Technology Carlo Fischione, Mikael Skoglund",https://icml.cc/Conferences/2019/Schedule?showEvent=3733,"Finding a dataset of minimal cardinality to characterize the optimal parameters of a model is of paramount importance in machine learning and distributed optimization over a network. This paper investigates the compressibility of large datasets. More specifically, we propose a framework that jointly learns the input-output mapping
as well as the most representative samples of the dataset (sufficient dataset). Our analytical results show that the cardinality of the sufficient dataset increases sub-linearly with respect to the original dataset size. Numerical evaluations of real
datasets reveal a large compressibility, up to 95%, without a noticeable drop in the learnability performance, measured by the generalization error.
","['KTH Royal Institute of Technology', 'Royal Institute of Technology, KTH', 'Royal Inst. of Technology, KTH', 'KTH Royal Institute of Technology']"
2019,Sublinear quantum algorithms for training linear and kernel-based classifiers,"Tongyang Li, Shouvanik Chakrabarti, Xiaodi Wu",https://icml.cc/Conferences/2019/Schedule?showEvent=3977,"We investigate quantum algorithms for classification, a fundamental problem in machine learning, with provable guarantees. Given $n$ $d$-dimensional data points, the state-of-the-art (and optimal) classical algorithm for training classifiers with constant margin by Clarkson et al. runs in $\tilde{O}(n +d)$, which is also optimal in its input/output model. We design sublinear quantum algorithms for the same task running in $\tilde{O}(\sqrt{n} +\sqrt{d})$, a quadratic improvement in both $n$ and $d$. Moreover, our algorithms use the standard quantization of the classical input and generate the same classical output, suggesting minimal overheads when used as subroutines for end-to-end applications. We also demonstrate a tight lower bound (up to poly-log factors) and discuss the possibility of implementation on near-term quantum machines.","['University of Maryland', 'University of Maryland College Park', 'University of Maryland']"
2019,Agnostic Federated Learning,"Mehryar Mohri, Gary Sivek, Ananda Suresh",https://icml.cc/Conferences/2019/Schedule?showEvent=4100,"A key learning scenario in large-scale applications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we propose a new framework of agnostic federated learning, where the centralized model is optimized for any target distribution formed by a mixture of the client distributions. We further show that this   framework naturally yields a notion of fairness. We present data-dependent Rademacher complexity guarantees for learning with this objective, which guide the definition of an algorithm for  agnostic federated learning. We also give a fast stochastic optimization algorithm for solving the corresponding optimization  problem, for which we prove convergence bounds, assuming a convex loss function and a convex hypothesis set. We further empirically demonstrate the benefits of our approach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud computing, domain adaptation, drifting, and other contexts where the training and test distributions do not coincide.
","['Courant Institute and Google Research', 'Google', 'Google']"
2019,Discovering Conditionally Salient Features with Statistical Guarantees,"Jaime Roquero Gimenez, James Zou",https://icml.cc/Conferences/2019/Schedule?showEvent=4238,"The goal of feature selection is to identify important features that are relevant to explain a outcome variable. Most of the work in this domain has focused on identifying \emph{globally} relevant features, which are features that are related to the outcome using evidence across the entire dataset. We study a more fine-grained statistical problem: \emph{conditional feature selection}, where a feature may be relevant depending on the values of the other features. For example in genetic association studies, variant $A$ could be associated with the phenotype in the entire dataset, but conditioned on variant $B$ being present it might be independent of the phenotype. In this sense, variant $A$ is globally relevant, but conditioned on $B$ it is  no longer locally relevant in that region of the feature space.  We present a generalization of the knockoff procedure that performs \emph{conditional feature selection} while controlling a generalization of the false discovery rate (FDR) to the conditional setting. By exploiting the feature/response model-free framework of the knockoffs, the quality of the statistical FDR guarantee is not degraded even when we perform conditional feature selections. We implement this method and present an algorithm that automatically partitions the feature space such that it enhances the differences between selected sets in different regions, and validate the statistical theoretical results with experiments.","['Stanford University', 'Stanford University']"
2019,A Theoretical Analysis of Contrastive Unsupervised Representation Learning,"Nikunj Umesh Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, Hrishikesh Khandeparkar",https://icml.cc/Conferences/2019/Schedule?showEvent=4241,"Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically similar""  data points andnegative samples,"" the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term {\em contrastive learning} for such algorithms and presents a theoretical framework for analyzing them by introducing {\em latent classes} and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.
","['Princeton University', 'Princeton University', ' Princeton University and Institute for Advanced Study', 'CMU', 'Princeton University']"
2019,The information-theoretic value of unlabeled data in semi-supervised learning,"Alexander Golovnev, David Pal, Balazs Szorenyi",https://icml.cc/Conferences/2019/Schedule?showEvent=3652,"We quantify the separation between the numbers of labeled examples required to
learn in two settings: Settings with and without the knowledge of
the distribution of the unlabeled data. More specifically, we prove a separation
by $\Theta(\log n)$ multiplicative factor for the class of projections over
the Boolean hypercube of dimension $n$. We prove that there is no separation
for the class of all functions on domain of any size. Learning with the knowledge of the distribution (a.k.a. fixed-distribution
learning) can be viewed as an idealized scenario of semi-supervised learning
where the number of unlabeled data points is so great that the unlabeled
distribution is known exactly. For this reason, we call the separation the
value of unlabeled data.","['Harvard', 'Expedia', 'Yahoo Research']"
2019,Unsupervised Label Noise Modeling and Loss Correction,"Eric Arazo, Diego Ortego, Paul Albert, Noel O'Connor, Kevin McGuinness",https://icml.cc/Conferences/2019/Schedule?showEvent=3738,"Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. This suggests using a suitable two-component mixture model as an unsupervised generative model of sample loss values during training to allow online estimation of the probability that a sample is mislabelled. Specifically, we propose a beta mixture to estimate this probability and correct the loss by relying on the network prediction (the so-called bootstrapping loss). We further adapt mixup augmentation to drive our approach a step further. 
Experiments on CIFAR-10/100 and TinyImageNet demonstrate a robustness to label noise that substantially outperforms recent state-of-the-art. Source code is available at https://git.io/fjsvE and Appendix at https://arxiv.org/abs/1904.11238.
","['Insight Centre for Data Analytics (DCU)', 'Insight Centre for Data Analytics (DCU)', 'Insight Centre for Data Analytics (DCU)', 'Dublin City University (DCU)', 'Insight Centre for Data Analytics']"
2019,Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment,"Yifan Wu, Ezra Winston, Divyansh  Kaushik, Zachary Lipton",https://icml.cc/Conferences/2019/Schedule?showEvent=3752,"Domain adaptation addresses the common situation in which the target distribution generating our test data differs from the source distribution generating our training data. While absent assumptions, domain adaptation is impossible, strict conditions, e.g. covariate or label shift, enable principled algorithms. Recently-proposed domain-adversarial approaches consist of aligning source and target encodings, an approach often motivated as minimizing two (of three) terms in a theoretical bound on target error. Unfortunately, this minimization can cause arbitrary increases in the third term, a problem guaranteed to arise under shifting label distributions. We propose asymmetrically-relaxed distribution alignment, a new approach that overcomes some limitations of standard domain-adversarial algorithms. Moreover, we characterize precise assumptions under which our algorithm is theoretically principled and demonstrate empirical benefits on both synthetic and real datasets.
","['Carnegie Mellon University', 'CMU MLD', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2019,Pareto Optimal Streaming Unsupervised Classification,"Soumya Basu, Steven Gutstein, Brent  Lance, Sanjay Shakkottai",https://icml.cc/Conferences/2019/Schedule?showEvent=3912,"We study an online and streaming unsupervised classification system. Our setting consists of a collection of classifiers (with unknown confusion matrices) each of which can classify one sample per unit time, and which are accessed by a stream of unlabeled samples. Each sample is dispatched to one or more classifiers, and depending on the labels collected from these classifiers, may be sent to other classifiers to collect additional labels. The labels are continually aggregated. Once the aggregated label has high enough accuracy (a pre-specified threshold for accuracy) or the sample is sent to all the classifiers, the now labeled sample is ejected from the system. For any given pre-specified threshold for accuracy, the objective is to sustain the maximum possible rate of arrival of new samples, such that the number of samples in memory does not grow unbounded. In this paper, we characterize the Pareto-optimal region of accuracy and arrival rate, and develop an algorithm that can operate at any point within this region. Our algorithm uses queueing-based routing and scheduling approaches combined with novel online tensor decomposition method to learn the hidden parameters, to Pareto-optimality guarantees. We finally verify our theoretical results through simulations on two ensembles formed using AlexNet, VGG, and ResNet deep image classifiers.
","['University of Texas at Austin', 'ARL', 'Army Research Laboratory ', 'University of Texas at Austin']"
2019,Geometric Losses for Distributional Learning,"Arthur Mensch, Mathieu Blondel, Gabriel Peyré",https://icml.cc/Conferences/2019/Schedule?showEvent=4133,"Building upon recent advances in entropy-regularized optimal transport, and upon Fenchel duality between measures and continuous functions, we propose a generalization of the logistic loss that incorporates a metric or cost between classes. Unlike previous attempts to use optimal transport distances for learning, our loss results in unconstrained convex objective functions, supports infinite (or very large) class spaces, and naturally defines a geometric generalization of the softmax operator. The geometric properties of this loss make it suitable for predicting sparse and singular distributions, for instance supported on curves or hyper-surfaces. We study the theoretical properties of our loss and showcase its effectiveness on two applications: ordinal regression and drawing generation.
","['ENS', 'NTT', 'CNRS and ENS']"
2019,"Classification from Positive, Unlabeled and Biased Negative Data","Yu-Guan Hsieh, Gang Niu, Masashi Sugiyama",https://icml.cc/Conferences/2019/Schedule?showEvent=3625,"In binary classification, there are situations where negative (N) data are too diverse to be fully labeled and we often resort to positive-unlabeled (PU) learning in these scenarios. However, collecting a non-representative N set that contains only a small portion of all possible N data can often be much easier in practice. This paper studies a novel classification framework which incorporates such biased N (bN) data in PU learning. We provide a method based on empirical risk minimization to address this PUbN classification problem. Our approach can be regarded as a novel example-weighting algorithm, with the weight of each example computed through a preliminary step that draws inspiration from PU learning. We also derive an estimation error bound for the proposed method. Experimental results demonstrate the effectiveness of our algorithm in not only PUbN learning scenarios but also ordinary PU learning scenarios on several benchmark datasets.
","['École normale supérieure', 'RIKEN', 'RIKEN / The University of Tokyo']"
2019,Complementary-Label Learning for Arbitrary Losses and Models,"Takashi Ishida, Gang Niu, Aditya Menon, Masashi Sugiyama",https://icml.cc/Conferences/2019/Schedule?showEvent=4046,"In contrast to the standard classification paradigm where the true class is given to each training pattern, complementary-label learning only uses training patterns each equipped with a complementary label, which only specifies one of the classes that the pattern does not belong to. The goal of this paper is to derive a novel framework of complementary-label learning with an unbiased estimator of the classification risk, for arbitrary losses and models---all existing methods have failed to achieve this goal. Not only is this beneficial for the learning stage, it also makes model/hyper-parameter selection (through cross-validation) possible without the need of any ordinarily labeled validation data, while using any linear/non-linear models or convex/non-convex loss functions. We further improve the risk estimator by a non-negative correction and gradient ascent trick, and demonstrate its superiority through experiments.
","['The University of Tokyo / RIKEN', 'RIKEN', 'Australian National University', 'RIKEN / The University of Tokyo']"
2019,Learning to Infer Program Sketches,"Maxwell Nye, Luke Hewitt, Josh Tenenbaum, Armando Solar-Lezama",https://icml.cc/Conferences/2019/Schedule?showEvent=4065,"Our goal is to build systems which write code automatically from the kinds of specifications humans can most easily provide, such as examples and natural language instruction. The key idea of this work is that a flexible combination of pattern recognition and explicit reasoning can be used to solve these complex programming problems. We propose a method for dynamically integrating these types of information. Our novel intermediate representation and training algorithm allow a program synthesis system to learn, without direct supervision, when to rely on pattern recognition and when to perform symbolic search. Our model matches the memorization and generalization performance of neural synthesis and symbolic search, respectively, and achieves state-of-the-art performance on a dataset of simple English description-to-code programming problems.
","['MIT', 'Massachusetts Institute of Technology', 'MIT', 'MIT']"
2019,Hierarchically Structured Meta-learning,"Huaxiu Yao, Ying WEI, Junzhou Huang, Zhenhui (Jessie) Li",https://icml.cc/Conferences/2019/Schedule?showEvent=3627,"In order to learn quickly with few samples, meta-learning utilizes prior knowledge learned from previous tasks. However, a critical challenge in meta-learning is task uncertainty and heterogeneity, which can not be handled via globally sharing knowledge among tasks. In this paper, based on gradient-based meta-learning, we propose a hierarchically structured meta-learning (HSML) algorithm that explicitly tailors the transferable knowledge to different clusters of tasks. Inspired by the way human beings organize knowledge, we resort to a hierarchical task clustering structure to cluster tasks. As a result, the proposed approach not only addresses the challenge via the knowledge customization to different clusters of tasks, but also preserves knowledge generalization among a cluster of similar tasks. To tackle the changing of task relationship, in addition, we extend the hierarchical structure to a continual learning environment. The experimental results show that our approach can achieve state-of-the-art performance in both toy-regression and few-shot image classification problems.
","['Pennsylvania State University', 'Tencent AI Lab', 'University of Texas at Arlington / Tencent AI Lab', 'Penn State University']"
2019,Bridging Theory and Algorithm for Domain Adaptation,"Yuchen Zhang, Tianle Liu, Mingsheng Long, Michael Jordan",https://icml.cc/Conferences/2019/Schedule?showEvent=3630,"This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adaptation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'UC Berkeley']"
2019,Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,"Shani Gamrian, Yoav Goldberg",https://icml.cc/Conferences/2019/Schedule?showEvent=3859,"Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning---the common transfer learning paradigm---fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in \url{https://youtu.be/4mnkzYyXMn4} and \url{https://youtu.be/KCGTrQi6Ogo}.
","['Bar Ilan University', '']"
2019,Learning What and Where to Transfer,"Yunhun Jang, Hankook Lee, Sung Ju Hwang, Jinwoo Shin",https://icml.cc/Conferences/2019/Schedule?showEvent=3985,"As the application of deep learning has expanded to real-world problems with insufficient volume of training data, transfer learning recently has gained much attention as means of improving the performance in such small-data regime. However, when existing methods are applied between heterogeneous architectures and tasks, it becomes more important to manage their detailed configurations and often requires exhaustive tuning on them for the desired performance. To address the issue, we propose a novel transfer learning approach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we propose an efficient training scheme to learn meta-networks that decide (a) which pairs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer approach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme significantly outperforms the prior baselines that find “what and where to transfer” in a hand-crafted manner.
","['OMNIOUS', 'KAIST', 'KAIST, AITRICS', 'KAIST, AITRICS']"
2019,DBSCAN++: Towards fast and scalable density clustering,"Jennifer Jang, Heinrich Jiang",https://icml.cc/Conferences/2019/Schedule?showEvent=3978,"DBSCAN is a classical density-based clustering procedure with tremendous practical relevance. However, DBSCAN implicitly needs to compute the empirical density for each sample point, leading to a quadratic  worst-case  time  complexity, which is too slow on large datasets. We propose DBSCAN++, a simple modification of DBSCAN which only requires computing the densities for a chosen subset of points. We show empirically that, compared to traditional DBSCAN, DBSCAN++ can provide not only competitive performance but also added robustness in the bandwidth hyperparameter while taking a fraction of the runtime. We also present statistical consistency guarantees showing the trade-off between computational cost and estimation rates.  Surprisingly, up to a certain point, we can enjoy the same estimation rates while lowering computational cost, showing that DBSCAN++ is a sub-quadratic algorithm that attains minimax optimal rates for level-set estimation, a quality that may be of independent interest.
","['Uber', 'Google']"
2019,Concrete Autoencoders: Differentiable Feature Selection and Reconstruction,"Muhammed Fatih Balın, Abubakar Abid, James Zou",https://icml.cc/Conferences/2019/Schedule?showEvent=4217,"We introduce the concrete autoencoder, an end-to-end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the input data from the selected features. Our method is unsupervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training phase, the temperature of the concrete selector layer is gradually decreased, which encourages a user-specified number of discrete features to be learned; during test time, the selected features can be used with the decoder network to reconstruct the remaining input features.  We evaluate concrete autoencoders on a variety of datasets, where they significantly outperform state-of-the-art methods for feature selection and data reconstruction. In particular, on a large-scale gene expression dataset, the concrete autoencoder selects a small subset of genes whose expression levels can be used to impute the expression levels of the remaining genes; in doing so, it improves on the current widely-used expert-curated L1000 landmark genes, potentially reducing measurement costs by 20%.  The concrete autoencoder can be implemented by adding just a few lines of code to a standard autoencoder, and the code for the algorithm and experiments is publicly available.
","['Bogazici ', 'Stanford', 'Stanford University']"
2019,Gromov-Wasserstein Learning for Graph Matching and Node Embedding,"Hongteng Xu, Dixin Luo, Hongyuan Zha, Lawrence Carin",https://icml.cc/Conferences/2019/Schedule?showEvent=3845,"A novel Gromov-Wasserstein learning framework is proposed to jointly match (align) graphs and learn embedding vectors for the associated graph nodes. 
Using Gromov-Wasserstein discrepancy, we measure the dissimilarity between two graphs and find their correspondence, according to the learned optimal transport. 
The node embeddings associated with the two graphs are learned under the guidance of the optimal transport, the distance of which not only reflects the topological structure of each graph but also yields the correspondence across the graphs. 
These two learning steps are mutually-beneficial, and are unified here by minimizing the Gromov-Wasserstein discrepancy with structural regularizers. This framework leads to an optimization problem that is solved by a proximal point method.
We apply the proposed method to matching problems in real-world networks, and demonstrate its superior performance compared to alternative approaches.
","['InfiniaML, Inc.', 'Duke University', 'Georgia Institute of Technology', 'Duke']"
2019,Spectral Clustering of Signed Graphs via Matrix Power Means,"Pedro Mercado, Francesco Tudisco, Matthias Hein",https://icml.cc/Conferences/2019/Schedule?showEvent=3699,"Signed graphs encode positive (attractive) and negative (repulsive) relations between nodes. We extend spectral clustering to signed graphs  via the one-parameter family of Signed Power Mean Laplacians, defined as the matrix power mean of normalized standard and signless Laplacians of positive and negative edges. We provide a thorough analysis of the proposed approach in the setting of a general Stochastic Block Model that includes models such as the Labeled Stochastic Block Model and the Censored Block Model. We show that in expectation the signed power mean Laplacian captures the  ground truth clusters under reasonable settings where state-of-the-art approaches fail. Moreover, we prove that the eigenvalues and  eigenvector of the signed power mean Laplacian concentrate around their expectation under reasonable conditions in the general Stochastic Block Model. Extensive experiments on random graphs and real world datasets confirm the theoretically predicted behaviour of the signed power mean Laplacian and show that it compares favourably with state-of-the-art methods. 
","['Saarland University / University of Tübingen', 'University of Strathclyde', 'University of Tübingen']"
2019,Coresets for Ordered Weighted Clustering,"Vladimir Braverman, Shaofeng Jiang, Robert Krauthgamer, Xuan Wu",https://icml.cc/Conferences/2019/Schedule?showEvent=3750,"We design coresets for Ordered k-Median, a generalization of classical clustering problems such as k-Median and k-Center. Its objective function is defined via the Ordered Weighted Averaging (OWA) paradigm of Yager (1988), where data points are weighted according to a predefined weight vector, but in order of their contribution to the objective (distance from the centers). A powerful data-reduction technique, called a coreset, is to summarize a point set $X$ in $\mathbb{R}^d$ into a small (weighted) point set $X'$, such that for every set of $k$ potential centers, the objective value of the coreset $X'$ approximates that of $X$ within factor $1\pm \epsilon$. When there are multiple objectives (weights), the above standard coreset might have limited usefulness, whereas in a \emph{simultaneous} coreset, the above approximation holds for all weights (in addition to all centers). Our main result is a construction of a simultaneous coreset of size $O_{\epsilon, d}(k^2 \log^2 |X|)$ for Ordered k-Median. We validate our algorithm on a real geographical data set, and we find our coreset leads to a massive speedup of clustering computations, while maintaining high accuracy for a range of weights. ","['Johns Hopkins University', 'Weizmann Institute of Science', 'Weizmann Institute of Science', 'Johns Hopkins University']"
2019,Fair k-Center Clustering for Data Summarization,"Matthäus Kleindessner, Pranjal Awasthi, Jamie Morgenstern",https://icml.cc/Conferences/2019/Schedule?showEvent=4242,"In data summarization we want to choose $k$ prototypes in order to summarize a data set. We study a setting where the data set comprises several demographic groups and we are restricted to choose $k_i$ prototypes belonging to group $i$. A common approach to the problem without the fairness constraint is to optimize a centroid-based clustering objective such as $k$-center. A natural extension then is to incorporate the fairness constraint into the clustering problem. Existing algorithms for doing so run in time super-quadratic in the size of the data set, which is in contrast to the standard $k$-center problem being approximable in linear time. In this paper, we resolve this gap by providing a simple approximation algorithm for the $k$-center problem under the fairness constraint with running time linear in the size of the data set and $k$. If the number of demographic groups is small, the approximation guarantee of our algorithm only incurs a constant-factor overhead.","['Rutgers University', 'Rutgers University', 'Georgia Institute of Technology']"
2019,A Better k-means++ Algorithm via Local Search,"Silvio Lattanzi, Christian Sohler",https://icml.cc/Conferences/2019/Schedule?showEvent=4015,"In this paper, we develop a new variant of k-means++ seeding that in expectation achieves a constant approximation guarantee. We obtain this result by a simple combination of k-means++ sampling with a local search strategy.
We evaluate our algorithm empirically and show that it also improves the quality of a solution in practice.
","['Google Zurich', 'Google']"
2019,Kernel Normalized Cut: a Theoretical Revisit,"Yoshikazu Terada, Michio Yamamoto",https://icml.cc/Conferences/2019/Schedule?showEvent=4044,"In this paper, we study the theoretical properties of clustering based on the kernel normalized cut. Our first contribution is to derive a nonasymptotic upper bound on the expected distortion rate of the kernel normalized cut. From this result, we show that the solution of the kernel normalized cut converges to that of the population-level weighted k-means clustering on a certain reproducing kernel Hilbert space (RKHS). Our second contribution is the discover of the interesting fact that the population-level weighted k-means clustering in the RKHS is equivalent to the population-level normalized cut. Combining these results, we can see that the kernel normalized cut converges to the population-level normalized cut. The criterion of the population-level normalized cut can be considered as an indivisibility of the population distribution, and this criterion plays an important role in the theoretical analysis of spectral clustering in Schiebinger et al. (2015). We believe that our results will provide deep insights into the behavior of both normalized cut and spectral clustering.
","['Osaka University / RIKEN', 'Okayama University / RIKEN']"
2019,Guarantees for Spectral Clustering with Fairness Constraints,"Matthäus Kleindessner, Samira Samadi, Pranjal Awasthi, Jamie Morgenstern",https://icml.cc/Conferences/2019/Schedule?showEvent=4249,"Given the widespread popularity of spectral clustering (SC) for partitioning graph data, we study a version of constrained SC in which we try to incorporate the fairness notion proposed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demographic group is approximately proportionally represented in each cluster. To this end, we develop variants of both normalized and unnormalized constrained SC and show that they help find fairer clusterings on both synthetic and real data. We also provide a rigorous theoretical analysis of our algorithms on a natural variant of the stochastic block model, where $h$ groups have strong inter-group connectivity, but also exhibit a “natural” clustering structure which is fair. We prove that our algorithms can recover this fair clustering with high probability.","['Rutgers University', 'Georgia Tech', 'Rutgers University', 'Georgia Institute of Technology']"
2019,Supervised Hierarchical Clustering with Exponential Linkage,"Nishant Yadav, Ari Kobren, Nicholas Monath, Andrew McCallum",https://icml.cc/Conferences/2019/Schedule?showEvent=4257,"In supervised clustering, standard techniques for learning a pairwise dissimilarity function often suffer from a discrepancy between the training and clustering objectives, leading to poor cluster quality. Rectifying this discrepancy necessitates matching the procedure for training the  dissimilarity function to the clustering algorithm. In this paper, we introduce a method for training the dissimilarity function in a way that is tightly coupled with hierarchical clustering, in particular single linkage. However, the appropriate clustering algorithm for a given dataset is often unknown. Thus we introduce an approach to supervised hierarchical clustering that smoothly interpolates between single, average, and complete linkage, and we give a training procedure that simultaneously learns a linkage function and a dissimilarity function. We accomplish this with a novel Exponential Linkage function that has a learnable parameter that controls the interpolation. In experiments on four datasets, our joint training procedure consistently matches or outperforms the next best training procedure/linkage function pair and gives up to 8 points improvement in dendrogram purity over discrepant pairs.
","['University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'UMass Amherst']"
2019,Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions,"Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, Fabian-Robert Stöter",https://icml.cc/Conferences/2019/Schedule?showEvent=3561,"By building upon the recent theory that established the connection between implicit generative modeling (IGM) and optimal transport, in this study, we propose a novel parameter-free algorithm for learning the underlying distributions of complicated datasets and sampling from them. The proposed algorithm is based on a functional optimization problem, which aims at finding a measure that is close to the data distribution as much as possible and also expressive enough for generative modeling purposes. We formulate the problem as a gradient flow in the space of probability measures. The connections between gradient flows and stochastic differential equations let us develop a computationally efficient algorithm for solving the optimization problem. We provide formal theoretical analysis where we prove finite-time error guarantees for the proposed algorithm. To the best of our knowledge, the proposed algorithm is the first nonparametric IGM algorithm with explicit theoretical guarantees. Our experimental results support our theory and show that our algorithm is able to successfully capture the structure of different types of data distributions.
","['Inria', 'Telecom ParisTech', 'IMPAN', 'ENS', 'Inria']"
2019,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,"Thanh Huy Nguyen, Umut Simsekli, Gaël RICHARD",https://icml.cc/Conferences/2019/Schedule?showEvent=3559,"Recent studies on diffusion-based sampling methods have shown that Langevin Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and rigorous theoretical guarantees have been proven for both asymptotic and finite-time regimes. Algorithmically, LMC-based algorithms resemble the well-known gradient descent (GD) algorithm, where the GD recursion is perturbed by an additive Gaussian noise whose variance has a particular form. Fractional Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the Gaussian noise is replaced by a heavy-tailed α-stable noise. As opposed to its Gaussian counterpart, these heavy-tailed perturbations can incur large jumps and it has been empirically demonstrated that the choice of α-stable noise can provide several advantages in modern machine learning problems, both in optimization and sampling contexts. However, as opposed to LMC, only asymptotic convergence properties of FLMC have been yet established. In this study, we analyze the non-asymptotic behavior of FLMC for non-convex optimization and prove finite-time bounds for its expected suboptimality. Our results show that the weak-error of FLMC increases faster than LMC, which suggests using smaller step-sizes in FLMC. We finally extend our results to the case where the exact gradients are replaced by stochastic gradients and show that similar results hold in this setting as well.
","['Telecom ParisTech', 'Telecom ParisTech', 'Télécom ParisTech']"
2019,Unifying Orthogonal Monte Carlo Methods,"Krzysztof Choromanski, Mark Rowland, Wenyu Chen, Adrian Weller",https://icml.cc/Conferences/2019/Schedule?showEvent=3580,"Many machine learning methods making use of Monte Carlo sampling in vector spaces have been shown to be improved by conditioning samples to be mutually orthogonal. Exact orthogonal coupling of samples is computationally intensive, hence approximate methods have been of great interest. In this paper, we present a unifying perspective of many approximate methods by considering Givens transformations, propose new approximate methods based on this framework, and demonstrate the ﬁrst statistical guarantees for families of approximate methods in kernel approximation. We provide extensive empirical evaluations with guidance for practitioners.
","['Google Brain Robotics', 'University of Cambridge', 'MIT', 'University of Cambridge, Alan Turing Institute']"
2019,Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits,"Martin Zhang, James Zou, David Tse",https://icml.cc/Conferences/2019/Schedule?showEvent=3590,"Monte Carlo (MC) permutation test is considered the gold standard for statistical hypothesis testing, especially when standard parametric assumptions are not clear or likely to fail. However, in modern data science settings where a large number of hypothesis tests need to be performed simultaneously, it is rarely used due to its prohibitive computational cost. In genome-wide association studies, for example, the number of hypothesis tests $m$ is around $10^6$ while the number of MC samples $n$ for each test could be greater than $10^8$, totaling more than $nm$=$10^{14}$ samples. In this paper, we propose  \texttt{A}daptive \texttt{M}C multiple \texttt{T}esting (\texttt{AMT}) to estimate MC p-values and control false discovery rate in multiple testing. The algorithm outputs the same result as the standard full MC approach with high probability while requiring only $\tilde{O}(\sqrt{n}m)$ samples. This sample complexity is shown to be optimal. On a Parkinson GWAS dataset, the algorithm reduces the running time from 2 months for full MC to an hour. The \texttt{AMT} algorithm is derived based on the theory of multi-armed bandits.","['Stanford University', 'Stanford', 'Stanford University']"
2019,Metropolis-Hastings Generative Adversarial Networks,"Ryan Turner, Jane Hung, Eric Frank, Yunus Saatchi, Jason Yosinski",https://icml.cc/Conferences/2019/Schedule?showEvent=3693,"We introduce the Metropolis-Hastings generative adversarial network (MH-GAN), which combines aspects of Markov chain Monte Carlo and GANs. The MH-GAN draws samples from the distribution implicitly defined by a GAN's discriminator-generator pair, as opposed to standard GANs which draw samples from the distribution defined only by the generator. It uses the discriminator from GAN training to build a wrapper around the generator for improved sampling. With a perfect discriminator, this wrapped generator samples from the true distribution on the data exactly even when the generator is imperfect. We demonstrate the benefits of the improved generator on multiple benchmark datasets, including CIFAR-10 and CelebA, using the DCGAN, WGAN, and progressive GAN.
","['Uber AI Labs', 'Uber', 'Uber AI Labs', 'Uber AI Labs', 'Uber Labs']"
2019,Scalable Metropolis-Hastings for Exact Bayesian Inference with Large Datasets,"Rob Cornish, Paul Vanetti, Alexandre Bouchard-Côté, George Deligiannidis, Arnaud Doucet",https://icml.cc/Conferences/2019/Schedule?showEvent=4131,"Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metropolis-Hastings is too computationally intensive to handle large datasets, since the cost per step usually scales like $O(n)$ in the number of data points $n$. We propose the Scalable Metropolis-Hastings (SMH) kernel that only requires processing on average $O(1)$ or even $O(1/\sqrt{n})$ data points per step. This scheme is based on a combination of factorized acceptance probabilities, procedures for fast simulation of Bernoulli processes, and control variate ideas. Contrary to many MCMC subsampling schemes such as fixed step-size Stochastic Gradient Langevin Dynamics, our approach is exact insofar as the invariant distribution is the true posterior and not an approximation to it. We characterise the performance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by empirical results that demonstrate overall performance benefits over standard Metropolis-Hastings and various subsampling algorithms.","['Oxford', 'Oxford', 'UBC', 'Oxford', 'Oxford University']"
2019,Replica Conditional Sequential Monte Carlo,"Alex Shestopaloff, Arnaud Doucet",https://icml.cc/Conferences/2019/Schedule?showEvent=3766,"We propose a Markov chain Monte Carlo (MCMC) scheme to perform state inference in non-linear non-Gaussian state-space models. Current state-of-the-art methods to address this problem rely on particle MCMC techniques and its variants, such as the iterated conditional Sequential Monte Carlo (cSMC) scheme, which uses a Sequential Monte Carlo (SMC) type proposal within MCMC. A deficiency of standard SMC proposals is that they only use observations up to time $t$ to propose states at time $t$ when an entire observation sequence is available. More sophisticated SMC based on lookahead techniques could be used but they can be difficult to put in practice. We propose here replica cSMC where we build SMC proposals for one replica using information from the entire observation sequence by conditioning on the states of the other replicas. This approach is easily parallelizable and we demonstrate its excellent empirical performance when compared to the standard iterated cSMC scheme at fixed computational complexity.
","['The Alan Turing Institute / University of Edinburgh', 'Oxford University']"
2019,A Polynomial Time MCMC Method for  Sampling from Continuous Determinantal Point Processes,"Alireza Rezaei, Shayan Oveis Gharan",https://icml.cc/Conferences/2019/Schedule?showEvent=3982,"We study the Gibbs sampling algorithm for discrete and continuous $k$-determinantal point processes. We show that in both cases, the spectral gap of the chain is bounded by a polynomial of $k$ and it is   independent of the  size of the domain.
As an immediate corollary, we obtain sublinear time algorithms for sampling from discrete $k$-DPPs given access to polynomially many processors. 
In the continuous setting, our result leads to the first class of rigorously analyzed efficient algorithms to generate random samples of continuous $k$-DPPs.
We achieve this by showing that the Gibbs sampler for a large family of continuous $k$-DPPs can be simulated efficiently when the spectrum is not concentrated on the top $k$ eigenvalues.","['University of Washington', 'University of Washington']"
2019,Adaptive Antithetic Sampling for Variance Reduction,"Hongyu Ren, Shengjia Zhao, Stefano Ermon",https://icml.cc/Conferences/2019/Schedule?showEvent=4125,"Variance reduction is crucial in stochastic estimation and optimization problems. Antithetic sampling reduces the variance of a Monte Carlo estimator by drawing correlated, rather than independent, samples. However, designing an effective correlation structure is challenging and application specific, thus limiting the practical applicability of these methods. In this paper, we propose a general-purpose adaptive antithetic sampling framework. We provide gradient-based and gradient-free methods to train the samplers such that they reduce variance while ensuring that the underlying Monte Carlo estimator is provably unbiased. We demonstrate the effectiveness of our approach on Bayesian inference and generative model training, where it reduces variance and improves task performance with little computational overhead.
","['Stanford University', 'Stanford University', 'Stanford University']"
2019,Accelerated Flow for Probability Distributions,"Amirhossein Taghvaei, Prashant Mehta",https://icml.cc/Conferences/2019/Schedule?showEvent=3689,"This paper presents a methodology and numerical algorithms for constructing accelerated gradient flows on the space of probability distributions. In particular, we extend the recent variational formulation of accelerated methods in (Wibisono et al., 2016) from vector valued variables to probability distributions. The variational problem  is modeled as a mean-field optimal control problem. A quantitative estimate on the asymptotic convergence rate is provided based on a Lyapunov function construction, when the objective functional is displacement convex.  An important special case is considered where the objective functional is the relative entropy. For this case, two numerical approximations are presented to implement the Hamilton's equations as a system of N interacting particles. The algorithm is numerically illustrated and compared with the MCMC and Hamiltonian MCMC algorithms. 
","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-CHampaign']"
2019,Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel $k$-means Clustering,"Taisuke Yasuda, David Woodruff, Manuel Fernandez",https://icml.cc/Conferences/2019/Schedule?showEvent=4309,"Kernel methods generalize machine learning algorithms that only depend on the pairwise inner products of the dataset by replacing inner products with kernel evaluations, a function that passes input points through a nonlinear feature map before taking the inner product in a higher dimensional space. In this work, we present nearly tight lower bounds on the number of kernel evaluations required to approximately solve kernel ridge regression (KRR) and kernel $k$-means clustering (KKMC) on $n$ input points. For KRR, our bound for relative error approximation the argmin of the objective function is $\Omega(nd_{\mathrm{eff}}^\lambda/\varepsilon)$ where $d_{\mathrm{eff}}^\lambda$ is the effective statistical dimension, tight up to a $\log(d_{\mathrm{eff}}^\lambda/\varepsilon)$ factor. For KKMC, our bound for finding a $k$-clustering achieving a relative error approximation of the objective function is $\Omega(nk/\varepsilon)$, tight up to a $\log(k/\varepsilon)$ factor. Our KRR result resolves a variant of an open question of El Alaoui and Mahoney, asking whether the effective statistical dimension is a lower bound on the sampling complexity or not. Furthermore, for the important input distribution case of mixtures of Gaussians, we provide algorithms that bypass the above lower bounds.","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2019,Dimensionality Reduction for Tukey Regression,"Kenneth  Clarkson, Ruosong Wang, David Woodruff",https://icml.cc/Conferences/2019/Schedule?showEvent=3896,"We give the first dimensionality reduction methods for the overconstrained Tukey regression problem. The Tukey loss function $\|y\|_M = \sum_i M(y_i)$ has $M(y_i) \approx |y_i|^p$ for residual errors $y_i$ smaller than a prescribed threshold $\tau$, but $M(y_i)$ becomes constant for errors $|y_i| > \tau$. Our results depend on a new structural result, proven constructively, showing that for any $d$-dimensional subspace $L \subset \mathbb{R}^n$, there is a fixed bounded-size subset of coordinates containing, for every $y \in L$, all the large coordinates, with respect to the Tukey loss function, of $y$. Our methods reduce a given Tukey regression problem to a smaller weighted version, whose solution is a provably good approximate solution to the original problem. Our reductions are fast, simple and easy to implement, and we give empirical results demonstrating their practicality, using existing heuristic solvers for the small versions. We also give exponential-time algorithms giving provably good solutions, and hardness results suggesting that a significant speedup in the worst case is unlikely. ","['IBM Research', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2019,Efficient Full-Matrix Adaptive Regularization,"Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, Yi Zhang",https://icml.cc/Conferences/2019/Schedule?showEvent=4302,"Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of machine learning problems, full-matrix preconditioning methods are prohibitively expensive. We show how to modify full-matrix adaptive regularization in order to make it practical and effective. We also provide a novel theoretical analysis for adaptive regularization in {\em non-convex} optimization settings. The core of our algorithm, termed GGT, consists of the efficient computation of the inverse square root of a low-rank matrix. Our preliminary experiments show improved iteration-wise convergence rates across synthetic tasks and standard deep learning benchmarks, and that the more carefully-preconditioned steps sometimes lead to a better solution.
","['Google AI Princeton', 'Princeton University', 'Google Research', 'Princeton University', 'Princeton University', 'Princeton University', 'Princeton University']"
2019,Breaking the gridlock in Mixture-of-Experts: Consistent and Efficient Algorithms,"Ashok Vardhan Makkuva, Pramod Viswanath, Sreeram Kannan, Sewoong Oh",https://icml.cc/Conferences/2019/Schedule?showEvent=3718,"Mixture-of-Experts (MoE) is a widely popular model for ensemble learning and is a basic building block of highly successful modern neural networks as well as a component in Gated Recurrent Units (GRU) and Attention networks. However, present algorithms for learning MoE, including the EM algorithm and gradient descent, are known to get stuck in local optima. From a theoretical viewpoint, finding an efficient and provably consistent algorithm to learn the parameters remains a long standing open problem for more than two decades. In this paper, we introduce the first algorithm that learns the true parameters of a MoE model for a wide class of non-linearities with global consistency guarantees. While existing algorithms jointly or iteratively estimate the expert parameters and the gating parameters in the MoE, we propose a novel algorithm that breaks the deadlock and can directly estimate the expert parameters by sensing its echo in a  carefully designed cross-moment tensor between the inputs and the output. Once the experts are known, the recovery of gating parameters still requires an EM algorithm; however, we show that the EM algorithm for this simplified problem, unlike the joint EM algorithm, converges to the true parameters. We empirically validate our algorithm on both the synthetic and real data sets in a variety of settings, and show superior performance to standard baselines.
","['UIUC', 'UIUC', 'University of Washington', 'University of Washington']"
2019,Efficient Nonconvex Regularized Tensor Completion with Structure-aware Proximal Iterations,"Quanming Yao, James Kwok, Bo Han",https://icml.cc/Conferences/2019/Schedule?showEvent=3790,"Nonconvex regularizers have been successfully used in low-rank matrix learning. In this paper, we extend this to the more challenging problem of low-rank tensor completion. Based on the proximal average algorithm,
we develop an efficient solver that avoids expensive tensor folding and unfolding. A special ``sparse plus low-rank"" structure, which is essential for fast computation of individual proximal steps,  is maintained
throughout the iterations. We also incorporate adaptive momentum to further speed up empirical convergence. Convergence results to critical points are provided under smoothness and Kurdyka-Lojasiewicz conditions. Experimental results on a number of synthetic and real-world data sets show that the proposed algorithm is more efficient in both time and space, and is also more accurate than existing approaches.
","['4Paradigm', 'Hong Kong University of Science and Technology', 'RIKEN-AIP']"
2019,Robust Estimation of Tree Structured Gaussian Graphical Models,"Ashish Katiyar, Jessica Hoffmann, Constantine Caramanis",https://icml.cc/Conferences/2019/Schedule?showEvent=3767,"Consider jointly Gaussian random variables whose conditional independence structure is specified by a graphical model. If we observe realizations of the variables, we can compute the covariance matrix, and it is well known that the support of the inverse covariance matrix corresponds to the edges of the graphical model. Instead, suppose we only have noisy observations. If the noise at each node is independent, we can compute the sum of the covariance matrix and an unknown diagonal. The inverse of this sum is (in general) dense. We ask: can the original independence structure be recovered? We address this question for tree structured graphical models. We prove that this problem is unidentifiable, but show that this unidentifiability is limited to a small class of candidate trees. We further present additional constraints under which the problem is identifiable. Finally, we provide an O(n^3) algorithm to find this equivalence class of trees.
","['The University of Texas at Austin', 'University of Texas at Austin', 'University of Texas']"
2019,Spectral Approximate Inference,"Sejun Park, Eunho Yang, Se-Young Yun, Jinwoo Shin",https://icml.cc/Conferences/2019/Schedule?showEvent=3958,"Given a graphical model (GM), computing its partition function is the most essential inference task, but it is computationally intractable in general. To address the issue, iterative approximation algorithms exploring certain local structure/consistency of GM have been investigated as popular choices in practice. However, due to their local/iterative nature, they often output poor approximations or even do not converge, e.g., in low-temperature regimes (hard instances of large parameters). To overcome the limitation, we propose a novel approach utilizing the global spectral feature of GM. Our contribution is two-fold: (a) we first propose a fully polynomial-time approximation scheme (FPTAS) for approximating the partition function of GM  associating with a low-rank coupling matrix; (b) for general high-rank GMs, we design a spectral mean-field scheme utilizing (a) as a subroutine, where it approximates a high-rank GM into a product of rank-1 GMs for an efficient approximation of the partition function. The proposed algorithm is more robust in its running time and accuracy than prior methods, i.e., neither suffers from the convergence issue nor depends on hard local structures, as demonstrated in our experiments.
","['KAIST', 'KAIST,AITRICS', 'KAIST', 'KAIST, AITRICS']"
2019,Partially Linear Additive Gaussian Graphical Models,"Sinong Geng, Minhao Yan, Mladen Kolar, Sanmi Koyejo",https://icml.cc/Conferences/2019/Schedule?showEvent=4083,"We propose a partially linear additive Gaussian graphical model (PLA-GGM) for the estimation of associations between random variables distorted by observed confounders. Model parameters are estimated  using an $L_1$-regularized maximal pseudo-profile likelihood estimator (MaPPLE) for which we prove a $\sqrt{n}$-sparsistency. Importantly, our approach avoids parametric constraints on the effects of confounders on the estimated graphical model structure. Empirically, the PLA-GGM is applied to both synthetic and real-world datasets, demonstrating superior performance compared to competing methods.","['Princeton University', 'Cornell University', 'University of Chicago Booth School of Business', 'Illinois / Google']"
2019,DAG-GNN: DAG Structure Learning with Graph Neural Networks,"Yue Yu, Jie Chen, Tian Gao, Mo Yu",https://icml.cc/Conferences/2019/Schedule?showEvent=4301,"Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \url{https://github.com/fishmoon1234/DAG-GNN}.
","['Lehigh University', 'IBM Research', 'IBM Research', 'IBM T. J. Watson']"
2019,Random Walks on Hypergraphs with Edge-Dependent Vertex Weights,"Uthsav Chitra, Benjamin Raphael",https://icml.cc/Conferences/2019/Schedule?showEvent=4240,"Hypergraphs are used in machine learning to model higher-order relationships in data. While spectral methods for graphs are well-established, spectral theory for hypergraphs remains an active area of research. In this paper, we use random walks to develop a spectral theory for hypergraphs with edge-dependent vertex weights: hypergraphs where every vertex v has a weight $\gamma_e(v)$ for each incident hyperedge e that describes the contribution of v to the hyperedge e. We derive a random walk-based hypergraph Laplacian, and bound the mixing time of random walks on such hypergraphs. Moreover, we give conditions under which random walks on such hypergraphs are equivalent to random walks on graphs. As a corollary, we show that current machine learning methods that rely on Laplacians derived from random walks on hypergraphs with edge-independent vertex weights do not utilize higher-order relationships in the data. Finally, we demonstrate the advantages of hypergraphs with edge-dependent vertex weights on ranking applications using real-world datasets.","['Princeton University', 'Princeton University']"
2019,Doubly Robust Joint Learning for Recommendation on Data Missing Not at Random,"Xiaojie Wang, Rui Zhang, Yu Sun, Jianzhong Qi",https://icml.cc/Conferences/2019/Schedule?showEvent=3717,"In recommender systems, usually the ratings of a user to most items are missing and a critical problem is that the missing ratings are often missing not at random (MNAR) in reality.
It is widely acknowledged that MNAR ratings make it difficult to accurately predict the ratings and unbiasedly estimate the performance of rating prediction.
Recent approaches use imputed errors to recover the prediction errors for missing ratings, or weight observed ratings with the propensities of being observed.
These approaches can still be severely biased in performance estimation or suffer from the variance of the propensities. 
To overcome these limitations, we first propose an estimator that integrates the imputed errors and propensities in a doubly robust way to obtain unbiased performance estimation and alleviate the effect of the propensity variance.
To achieve good performance guarantees, based on this estimator, we propose joint learning of rating prediction and error imputation, which outperforms the state-of-the-art approaches on four real-world datasets.
","['University of Melbourne', '"" University of Melbourne, Australia""', 'Twitter Inc.', 'The University of Melbourne']"
2019,Linear-Complexity Data-Parallel Earth Mover's Distance Approximations,"Kubilay Atasu, Thomas Mittelholzer",https://icml.cc/Conferences/2019/Schedule?showEvent=3568,"The Earth Mover's Distance (EMD) is a state-of-the art metric for comparing discrete probability distributions, but its high distinguishability comes at a high cost in computational complexity. Even though linear-complexity approximation algorithms have been proposed to improve its scalability, these algorithms are either limited to vector spaces with only a few dimensions or they become ineffective when the degree of overlap between the probability distributions is high. We propose novel approximation algorithms that overcome both of these limitations, yet still achieve linear time complexity. All our algorithms are data parallel, and thus, we take advantage of massively parallel computing engines, such as Graphics Processing Units (GPUs). On the popular text-based 20 Newsgroups dataset, the new algorithms are four orders of magnitude faster than a multi-threaded CPU implementation of Word Mover's Distance and match its nearest-neighbors-search accuracy. On MNIST images, the new algorithms are four orders of magnitude faster than a GPU implementation of the Sinkhorn's algorithm while offering a slightly higher nearest-neighbors-search accuracy.
","['IBM Research - Zurich', 'HSR Univ. Applied Sciences, Rapperswil, Switzerland']"
2019,Model Comparison for Semantic Grouping,"Francisco Vargas, Kamen Brestnichki, Nils Hammerla",https://icml.cc/Conferences/2019/Schedule?showEvent=3714,"We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate the task of semantic similarity as a model comparison task in which we contrast a generative model which jointly models two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear assumptions about how the embeddings of words are generated. We apply model comparison that utilises information criteria to address some of the shortcomings of Bayesian model comparison, whilst still penalising model complexity. We achieve competitive results by applying the proposed framework with an appropriate choice of likelihood on the STS datasets.
","['Babylon Health', 'Babylon Health', 'Babylon Health']"
2019,RaFM: Rank-Aware Factorization Machines,"Xiaoshuang Chen, Yin Zheng, Jiaxing Wang, Wenye Ma, Junzhou Huang",https://icml.cc/Conferences/2019/Schedule?showEvent=3709,"Fatorization machines (FM) are a popular model class to learn pairwise interactions by a low-rank approximation. Different from existing FM-based approaches which use a fixed rank for all features, this paper proposes a Rank-Aware FM (RaFM) model which adopts pairwise interactions from embeddings with different ranks. The proposed model achieves a better performance on real-world datasets where different features have significantly varying frequencies of occurrences. Moreover, we prove that the RaFM model can be stored, evaluated, and trained as efficiently as one single FM, and under some reasonable conditions it can be even significantly more efficient than FM. RaFM improves the performance of FMs in both regression tasks and classification tasks while incurring less computational burden, therefore also has attractive potential in industrial applications.
","['Tsinghua Univerisity', 'Tencent AI Lab', 'Institute of Automation, Chinese Academy of Sciences', 'Tencent', 'University of Texas at Arlington / Tencent AI Lab']"
2019,CAB: Continuous Adaptive Blending for Policy Evaluation and Learning,"Yi Su, Lequn Wang, Michele Santacatterina, Thorsten Joachims",https://icml.cc/Conferences/2019/Schedule?showEvent=4208,"The ability to perform offline A/B-testing and off-policy learning using logged contextual bandit feedback is highly desirable in a broad range of applications, including recommender systems, search engines, ad placement, and personalized health care. Both offline A/B-testing and off-policy learning require a counterfactual estimator that evaluates how some new policy would have performed, if it had been used instead of the logging policy. In this paper, we identify a family of counterfactual estimators which subsumes most such estimators proposed to date. Our analysis of this family identifies a new estimator - called Continuous Adaptive Blending (CAB) - which enjoys many advantageous theoretical and practical properties. In particular, it can be substantially less biased than clipped Inverse Propensity Score (IPS) weighting and the Direct Method, and it can have less variance than Doubly Robust and IPS estimators. In addition, it is sub-differentiable such that it can be used for learning, unlike the SWITCH estimator. Experimental results show that CAB provides excellent evaluation accuracy and outperforms other counterfactual estimators in terms of learning performance.
","['Cornell University', 'Cornell University', 'TRIPODS Center of Data Science - Cornell University', 'Cornell']"
2019,MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement,"Szu-Wei Fu, Chien-Feng Liao, Yu Tsao, Shou-De Lin",https://icml.cc/Conferences/2019/Schedule?showEvent=3820,"Adversarial loss in a conditional generative adversarial network (GAN) is not designed to directly optimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with improved metric scores. To overcome this issue, we propose a novel MetricGAN approach with an aim to optimize the generator with respect to one or multiple evaluation metrics. Moreover, based on MetricGAN, the metric scores of the generated data can also be arbitrarily specified by users. We tested the proposed MetricGAN on a speech enhancement task, which is particularly suitable to verify the proposed approach because there are multiple metrics measuring different aspects of speech signals. Moreover, these metrics are generally complex and could not be fully optimized by Lp or conventional adversarial losses.
","['National Taiwan University', 'Academia Sinica', 'Academia Sinica', 'National Taiwan University']"
2019,Neural Separation of Observed and Unobserved Distributions,"Tavi Halperin, Ariel Ephrat, Yedid Hoshen",https://icml.cc/Conferences/2019/Schedule?showEvent=3603,"Separating mixed distributions is a long standing challenge for machine learning and signal processing. Most current methods either rely on making strong assumptions on the source distributions or rely on having training samples of each source in the mixture. In this work, we introduce a new method---Neural Egg Separation---to tackle the scenario of extracting a signal from an unobserved distribution additively mixed with a signal from an observed distribution. Our method iteratively learns to separate the known distribution from progressively finer estimates of the unknown distribution. In some settings, Neural Egg Separation is initialization sensitive, we therefore introduce Latent Mixture Masking which ensures a good initialization. Extensive experiments on audio and image separation tasks show that our method outperforms current methods that use the same level of supervision, and often achieves similar performance to full supervision.
","['Hebrew University of Jerusalem', 'HUJI', 'Hebrew University of Jerusalem and Facebook AI Research']"
2019,Almost Unsupervised Text to Speech and Automatic Speech Recognition,"Yi Ren, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu",https://icml.cc/Conferences/2019/Schedule?showEvent=3849,"Text to speech (TTS) and automatic speech recognition (ASR) are two dual tasks in speech processing and both achieve impressive performance thanks to the recent advance in deep learning and large amount of aligned speech and text data. However, the lack of aligned data poses a major practical problem for TTS and ASR on low-resource languages. In this paper, by leveraging the dual nature of the two tasks, we propose an almost unsupervised learning method that only leverages few hundreds of paired data and extra unpaired data for TTS and ASR. Our method consists of the following components: (1) denoising auto-encoder, which reconstructs speech and text sequences respectively to develop the capability of language modeling both in speech and text domain; (2) dual transformation, where the TTS model transforms the text $y$ into speech $\hat{x}$, and the ASR model leverages the transformed pair $(\hat{x},y)$ for training, and vice versa, to boost the accuracy of the two tasks; (3) bidirectional sequence modeling, which address the error propagation problem especially in the long speech and text sequence when training with few paired data; (4) a unified model structure, which combines all the above components for TTS and ASR based on Transformer model. Our method achieves 99.84\% in terms of word level intelligible rate and 2.68 MOS for TTS, and 11.7\% PER for ASR on LJSpeech dataset, by leveraging only 200 paired speech and text data (about 20 minutes audio), together with extra unpaired speech and text data. ","['Zhejiang University', 'Microsoft Research', 'Microsoft Research Asia', 'Microsoft', 'Zhejiang University', 'Microsoft']"
2019,AutoVC:  Zero-Shot Voice Style Transfer with Only Autoencoder Loss,"Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, Mark Hasegawa-Johnson",https://icml.cc/Conferences/2019/Schedule?showEvent=4214,"Despite the progress in voice conversion, many-to-many voice conversion trained on non-parallel data, as well as zero-shot voice conversion, remains under-explored. Deep style transfer algorithms, generative adversarial networks (GAN) in particular, are being applied as new solutions in this field. However, GAN training is very sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on self-reconstruction loss. Based on this scheme, we proposed AutoVC, which achieves state-of-the-art results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.
","['UIUC', 'IBM-MIT Research Lab', 'MIT-IBM Watson AI Lab', 'University of Illinois at Urbana-Champaign', 'University of Illinois']"
2019,A fully differentiable beam search decoder,"Ronan Collobert, Awni Hannun, Gabriel Synnaeve",https://icml.cc/Conferences/2019/Schedule?showEvent=4328,"We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms can successfully train an acoustic model from the final transcription, while implicitly learning a language model.  Instead, we show that it is possible to discriminatively train an acoustic  model jointly with an \emph{explicit} and possibly pre-trained language model.
","['Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research']"
2019,Scaling Up Ordinal Embedding: A Landmark Approach,"Jesse Anderton, Javed Aslam",https://icml.cc/Conferences/2019/Schedule?showEvent=4058,"Ordinal Embedding is the problem of placing n objects into R^d to satisfy constraints like ""object a is closer to b than to c."" It can accommodate data that embeddings from features or distances cannot, but is a more difficult problem. We propose a novel landmark-based method as a partial solution. At small to medium scales, we present a novel combination of existing methods with some new theoretical justification. For very large values of n optimizing over an entire embedding breaks down, so we propose a novel method which first embeds a subset of m << n objects and then embeds the remaining objects independently and in parallel. We prove a distance error bound for our method in terms of m and that it has O(dn log m) time complexity, and show empirically that it is able to produce high quality embeddings in a fraction of the time needed for any published method.
","['Northeastern University', 'Northeastern University']"
2019,Learning to select for a predefined ranking,"Aleksei Ustimenko, Aleksandr Vorobev, Gleb Gusev, Pavel Serdyukov",https://icml.cc/Conferences/2019/Schedule?showEvent=3797,"In this paper, we formulate a novel problem of learning to select a set of items maximizing the quality of their ordered list, where the order is predefined by some explicit rule. Unlike the classic information retrieval problem, in our setting, the predefined order of items in the list may not correspond to their quality in general. For example, this is a dominant scenario in personalized news and social media feeds, where items are ordered by publication time in a user interface. We propose new theoretically grounded algorithms based on direct optimization of the resulting list quality. Our offline and online experiments with a large-scale product search engine demonstrate the overwhelming advantage of our methods over the baselines in terms of all key quality metrics.
","['Yandex', 'Yandex', 'Yandex', 'Yandex']"
2019,Mallows ranking models: maximum likelihood estimate and regeneration,Wenpin Tang,https://icml.cc/Conferences/2019/Schedule?showEvent=3913,"This paper is concerned with various Mallows ranking models. We study the statistical properties of the MLE of Mallows' $\phi$ model. We also make connections of various Mallows ranking models, encompassing recent progress in mathematics. Motivated by the infinite top-$t$ ranking model, we propose an algorithm to select the model size $t$ automatically. The key idea relies on the renewal property of such an infinite random permutation. Our algorithm shows good performance on several data sets.",['UCLA']
2019,Fast and Stable Maximum Likelihood Estimation for Incomplete Multinomial Models,"Chenyang ZHANG, Guosheng Yin",https://icml.cc/Conferences/2019/Schedule?showEvent=4002,"We propose a fixed-point iteration approach to the maximum likelihood estimation for the incomplete multinomial model, which provides a unified framework for ranking data analysis. Incomplete observations typically fall in a subset of categories, and thus cannot be distinguished as belonging to a unique category. We develop a minorization--maximization (MM) type of algorithm, which requires relatively fewer iterations and shorter time to achieve convergence. Under such a general framework, incomplete multinomial models can be reformulated to include several well-known ranking models as special cases, such as the Bradley--Terry, Plackett--Luce models and their variants. The simple form of iteratively updating equations in our algorithm involves only basic matrix operations, which makes it efficient and easy to implement with large data. Experimental results show that our algorithm runs faster than existing methods on synthetic data and real data.
","['University of Hong Kong', 'University of Hong Kong']"
2019,Fast Algorithm for Generalized Multinomial Models with Ranking Data,"Jiaqi Gu, Guosheng Yin",https://icml.cc/Conferences/2019/Schedule?showEvent=4007,"We develop a framework of generalized multinomial models, which includes both the popular Plackett--Luce model and Bradley--Terry model as special cases. From a theoretical perspective, we prove that the maximum likelihood estimator (MLE) under generalized multinomial models corresponds to the stationary distribution of an inhomogeneous Markov chain uniquely. Based on this property, we propose an iterative algorithm that is easy to implement and interpret, and is guaranteed to converge. Numerical experiments on synthetic data and real data demonstrate the advantages of our Markov chain based algorithm over existing ones. Our algorithm converges to the MLE with fewer iterations and at a faster convergence rate. The new algorithm is readily applicable to problems such as page ranking or sports ranking data.
","['The University of Hong Kong', 'University of Hong Kong']"
2019,Graph Resistance and Learning from Pairwise Comparisons,"Julien Hendrickx, Alex Olshevsky, Venkatesh Saligrama",https://icml.cc/Conferences/2019/Schedule?showEvent=4120,"We consider the problem of learning the qualities of a collection of items by performing noisy comparisons among them. Following the standard paradigm, we assume there is a fixed ``comparison graph'' and every neighboring pair of items in this graph is compared k times according to the Bradley-Terry-Luce model (where the probability than an item wins a comparison is proportional the item quality). We are interested in how the relative error in quality estimation scales with the comparison graph in the regime where k is large. We show that, asymptotically, the relevant graph-theoretic quantity is the square root of the resistance of the comparison graph. Specifically, we provide an algorithm with relative error decay that scales with the square root of the graph resistance, and provide a lower bound showing that (up to log factors) a better scaling is impossible. The performance guarantee of our algorithm, both in terms of the graph and the skewness of the item quality distribution, significantly outperforms earlier results. 
","['University of Catholique de Louvain', 'Boston University', 'Boston University']"
2019,Learning Context-dependent Label Permutations for Multi-label Classification,"Jinseok Nam, Young-Bum Kim, Eneldo Loza Mencia, Sunghyun Park, Ruhi Sarikaya, Johannes Fürnkranz",https://icml.cc/Conferences/2019/Schedule?showEvent=4041,"A key problem in multi-label classification is to utilize dependencies among the labels. Chaining classifiers are a simple technique for addressing this problem but current algorithms all assume a fixed, static label ordering. In this work, we propose a multi-label classification approach which allows to choose a dynamic, context-dependent label ordering. Our proposed approach consists of two sub-components: a simple EM-like algorithm which bootstraps the learned model, and a more elaborate approach based on reinforcement learning. Our experiments on three public multi-label classification benchmarks show that our proposed dynamic label ordering approach based on reinforcement learning outperforms recurrent neural networks with fixed label ordering across both bipartition and ranking measures on all the three datasets. As a result, we obtain a powerful sequence prediction-based algorithm for multi-label classification, which is able to efficiently and explicitly exploit label dependencies.
","['Amazon', 'Amazon', 'TU Darmstadt', 'Aamzon', 'Amazon', 'TU Darmstadt']"
2019,Discovering Context Effects from Raw Choice Data,"Arjun Seshadri, Alexander Peysakhovich, Johan Ugander",https://icml.cc/Conferences/2019/Schedule?showEvent=4284,"Many applications in preference learning assume that decisions come from the maximization of a stable utility function. Yet a large experimental literature shows that individual choices and judgements can be affected by ``irrelevant'' aspects of the context in which they are made. An important class of such contexts is the composition of the choice set. In this work, our goal is to discover such choice set effects from raw choice data. We introduce an extension of the Multinomial Logit (MNL) model, called the context dependent random utility model (CDM), which allows for a particular class of choice set effects. We show that the CDM can be thought of as a second-order approximation to a general choice system, can be inferred optimally using maximum likelihood and, importantly, is easily interpretable. We apply the CDM to both real and simulated choice data to perform principled exploratory analyses for the presence of choice set effects.
","['Stanford University', 'Facebook', 'Stanford University']"
2019,"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","Rohin Shah, Noah Gundotra, Pieter Abbeel, Anca Dragan",https://icml.cc/Conferences/2019/Schedule?showEvent=4315,"Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test -- rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.
","['UC Berkeley', 'University of California, Berkeley', 'OpenAI / UC Berkeley', 'EECS Department, University of California, Berkeley']"
2019,Learning Distance for Sequences by Learning a Ground Metric,"Bing Su, Ying Wu",https://icml.cc/Conferences/2019/Schedule?showEvent=3624,"Learning distances that operate directly on multi-dimensional sequences is challenging because such distances are structural by nature and the vectors in sequences are not independent. Generally, distances for sequences heavily depend on the ground metric between the vectors in sequences. We propose to learn the distance for sequences through learning a ground Mahalanobis metric for the vectors in sequences. The learning samples are sequences of vectors for which how the ground metric between vectors induces the overall distance is given, and the objective is that the distance induced by the learned ground metric produces large values for sequences from different classes and small values for those from the same class. We formulate the metric as a parameter of the distance, bring closer each sequence to an associated virtual sequence w.r.t. the distance to reduce the number of constraints, and develop a general iterative solution for any ground-metric-based sequence distance. Experiments on several sequence datasets demonstrate the effectiveness and efficiency of our method.
","['Institute of Software, Chinese Academy of Sciences', 'Northwestern University']"
2020,Distance Metric Learning with Joint Representation Diversification,"Xu Chu, Yang Lin, Yasha Wang, Xiting Wang, Hailong Yu, Xin Gao, Qi Tong",https://icml.cc/Conferences/2020/Schedule?showEvent=6438,"Distance metric learning (DML) is to learn a representation space equipped with a metric, such that similar examples are closer than dissimilar examples concerning the metric. The recent success of DNNs motivates many DML losses that encourage the intra-class compactness and inter-class separability. The trade-off between inter-class compactness and inter-class separability shapes the DML representation space by determining how much information of the original inputs to retain. In this paper, we propose a Distance Metric Learning with Joint Representation Diversification (JRD) that allows a better balancing point between intra-class compactness and inter-class separability. Specifically, we propose a Joint Representation Similarity regularizer that captures different abstract levels of invariant features and diversifies the joint distributions of representations across multiple layers. Experiments on three deep DML benchmark datasets demonstrate the effectiveness of the proposed approach.
","['Peking University', 'Peking University', 'Peking University', 'Microsoft Research Asia', 'Peking University', 'Peking University', 'Peking University']"
2020,Generating Programmatic Referring Expressions via Program Synthesis,"Jiani Huang, Calvin Smith, Osbert Bastani, Rishabh Singh, Aws Albarghouthi, Mayur Naik",https://icml.cc/Conferences/2020/Schedule?showEvent=5940,"Incorporating symbolic reasoning into machine learning algorithms is a promising approach to improve performance on learning tasks that require logical reasoning. We study the problem of generating a programmatic variant of referring expressions that we call referring relational programs. In particular, given a symbolic representation of an image and a target object in that image, the goal is to generate a relational program that uniquely identifies the target object in terms of its attributes and its relations to other objects in the image. We propose a neurosymbolic program synthesis algorithm that combines a policy neural network with enumerative search to generate such relational programs. The policy neural network employs a program interpreter that provides immediate feedback on the consequences of the decisions made by the policy, and also takes into account the uncertainty in the symbolic representation of the image. We evaluate our algorithm on challenging benchmarks based on the CLEVR dataset, and demonstrate that our approach significantly outperforms several baselines.
","['University of Pennsylvania', 'University of Wisconsin at Madison', 'University of Pennsylvania', 'Google Brain', 'University of Wisconsin-Madison', 'University of Pennsylvania']"
2020,An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm,"Christopher DeCarolis, Mukul A Ram, Seyed  Esmaeili, Yu-Xiang Wang, Furong Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=6240,"We provide an end-to-end differentially private spectral algorithm for learning LDA, based on matrix/tensor decompositions, and establish theoretical guarantees on utility/consistency of the estimated model parameters. We represent the spectral algorithm as a computational graph. Noise can be injected along the edges of this graph to obtain differential privacy. We identify subsets of edges, named ``configurations'', such that adding noise to all edges in such a subset guarantees differential privacy of the end-to-end spectral algorithm. We characterize the sensitivity of the edges with respect to the input and thus estimate the amount of noise to be added to each edge for any required privacy level. We then characterize the utility loss  for each configuration as a function of injected noise.  Overall, by combining the sensitivity and utility characterization, we obtain an end-to-end differentially private spectral algorithm for LDA and identify which configurations outperform others under specific regimes. We are the first to achieve utility guarantees under a required level of differential privacy for learning in LDA. We additionally show that our method systematically outperforms differentially private variational inference.
","['University of Maryland', 'University of Maryland', 'University of Maryland, College Park', 'UC Santa Barbara', 'University of Maryland College Park']"
2020,"Semismooth Newton Algorithm for Efficient Projections onto $\ell_{1, \infty}$-norm Ball","Dejun Chu, Changshui Zhang, Shiliang Sun, Qing Tao",https://icml.cc/Conferences/2020/Schedule?showEvent=5958,"Structured sparsity-inducing $\ell_{1, \infty}$-norm, as a generalization of the classical $\ell_1$-norm, plays an important role in jointly sparse models which select or remove simultaneously all the variables forming a group. However, its resulting problem is more difficult to solve than the conventional $\ell_1$-norm constrained problem. In this paper, we propose an efficient algorithm for Euclidean projection onto $\ell_{1, \infty}$-norm ball. We tackle the projection problem via semismooth Newton algorithm to solve the system of semismooth equations. Meanwhile, exploiting the structure of Jacobian matrix via LU decomposition yields an equivalent algorithm which is proved to terminate after a finite number  of iterations. Empirical studies demonstrate that our proposed algorithm outperforms the existing state-of-the-art solver and is promising for the optimization of learning problems with $\ell_{1, \infty}$-norm ball constraint.","['Tsinghua University', 'Tsinghua University', 'East China Normal University', 'Army Academy of Artillery and Air Defense']"
2020,LTF: A Label Transformation Framework for Correcting Label Shift,"Jiaxian Guo, Mingming Gong, Tongliang Liu, Kun Zhang, Dacheng Tao",https://icml.cc/Conferences/2020/Schedule?showEvent=5962,"Distribution shift is a major obstacle to the deployment of current deep learning models on real-world problems. Let $Y$ be the class label and $X$ the features. We focus on one type of distribution shift, \textit{ label shift}, where the label marginal distribution $P_Y$ changes but the conditional distribution $P_{X|Y}$ does not. Most existing methods estimate the density ratio between the source- and target-domain label distributions by density matching. However, these methods are either computationally infeasible for large-scale data or restricted to shift correction for discrete labels. In this paper, we propose an end-to-end Label Transformation Framework (LTF) for correcting label shift, which implicitly models the shift of $P_Y$ and the conditional distribution $P_{X|Y}$ using neural networks. Thanks to the flexibility of deep networks, our framework can handle continuous, discrete, and even multi-dimensional labels in a unified way and is scalable to large data. Moreover, for high dimensional $X$, such as images, we find that the redundant information in $X$ severely degrades the estimation accuracy. To remedy this issue, we propose to match the distribution implied by our generative model and the target-domain distribution in a low-dimensional feature space that discards information irrelevant to $Y$. Both theoretical and empirical studies demonstrate the superiority of our method over previous approaches.  ","['The University of Sydney', 'University of Melbourne', 'The University of Sydney', 'Carnegie Mellon University', 'The University of Sydney']"
2020,A Simple Framework for Contrastive Learning of Visual Representations,"Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",https://icml.cc/Conferences/2020/Schedule?showEvent=6762,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.
","['Google Brain', 'Google Brain', 'Google Research, Brain Team', 'Google']"
2020,Variational Bayesian Quantization,"Yibo Yang, Robert Bamler, Stephan Mandt",https://icml.cc/Conferences/2020/Schedule?showEvent=6764,"We propose a novel algorithm for quantizing continuous latent representations in trained models. Our approach applies to deep probabilistic models, such as variational autoencoders (VAEs), and enables both data and model compression. Unlike current end-to-end neural compression methods that cater the model to a fixed quantization scheme, our algorithm separates model design and training from quantization. Consequently, our algorithm enables ``plug-and-play'' compression at variable rate-distortion trade-off, using a single trained model.  Our algorithm can be seen as a novel extension of arithmetic coding to the continuous domain, and uses adaptive quantization accuracy based on estimates of posterior uncertainty. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single standard VAE. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method.
","['University of California, Irivine', 'University of California at Irvine', 'University of California, Irivine']"
2020,Problems with Shapley-value-based explanations as feature importance measures,"Indra Kumar, Suresh Venkatasubramanian, Carlos  Scheidegger, Sorelle Friedler",https://icml.cc/Conferences/2020/Schedule?showEvent=6519,"Game-theoretic formulations of feature importance have become popular as a way to ""explain"" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.
","['University of Utah', 'University of Utah, USA', 'The University of Arizona', 'Haverford College']"
2020,Efficient nonparametric statistical inference on population feature importance using Shapley values,"Brian Williamson, Jean Feng",https://icml.cc/Conferences/2020/Schedule?showEvent=6281,"The true population-level importance of a variable in a prediction task provides useful knowledge about the underlying data-generating mechanism and can help in deciding which measurements to collect in subsequent experiments. Valid statistical inference on this importance is a key component in understanding the population of interest. We present a computationally efficient procedure for estimating and obtaining valid statistical inference on the \textbf{S}hapley \textbf{P}opulation \textbf{V}ariable \textbf{I}mportance \textbf{M}easure (SPVIM). Although the computational complexity of the true SPVIM scales exponentially with the number of variables, we propose an estimator based on randomly sampling only $\Theta(n)$ feature subsets given $n$ observations. We prove that our estimator converges at an asymptotically optimal rate. Moreover, by deriving the asymptotic distribution of our estimator, we construct valid confidence intervals and hypothesis tests. Our procedure has good finite-sample performance in simulations, and for an in-hospital mortality prediction task produces similar variable importance estimates when different machine learning algorithms are applied.","['Fred Hutchinson Cancer Research Center', 'University of Washington']"
2020,Confidence-Aware Learning for Deep Neural Networks,"Jooyoung Moon, Jihyo Kim, Younghak Shin, Sangheum Hwang",https://icml.cc/Conferences/2020/Schedule?showEvent=6730,"Despite the power of deep neural networks for a wide range of tasks, an overconfident prediction issue has limited their practical use in many safety-critical applications. Many recent works have been proposed to mitigate this issue, but most of them require either additional computational costs in training and/or inference phases or customized architectures to output confidence estimates separately. In this paper, we propose a method of training deep neural networks with a novel loss function, named Correctness Ranking Loss, which regularizes class probabilities explicitly to be better confidence estimates in terms of ordinal ranking according to confidence. The proposed method is easy to implement and can be applied to the existing architectures without any modification. Also, it has almost the same computational costs for training as conventional deep classifiers and outputs reliable predictions by a single inference. Extensive experimental results on classification benchmark datasets indicate that the proposed method helps networks to produce well-ranked confidence estimates. We also demonstrate that it is effective for the tasks closely related to confidence estimation, out-of-distribution detection and active learning.
","['Seoul National University of Science and Technology', 'Seoul National University of Science and Technology', 'Mokpo National University', 'Seoul National University of Science and Technology']"
2020,Optimizing for the Future in Non-Stationary MDPs,"Yash Chandak, Georgios Theocharous, Shiv Shankar, Martha White, Sridhar Mahadevan, Philip Thomas",https://icml.cc/Conferences/2020/Schedule?showEvent=6316,"Most reinforcement learning methods are based upon the key assumption that the transition dynamics and reward functions are fixed, that is, the underlying Markov decision process is stationary. However, in many real-world applications, this assumption is violated, and using existing algorithms may result in a performance lag. To proactively search for a good future policy, we present a policy gradient algorithm that maximizes a forecast of future performance. This forecast is obtained by fitting a curve to the counter-factual estimates of policy performance over time, without explicitly modeling the underlying non-stationarity. The resulting algorithm amounts to a non-uniform reweighting of past data, and we observe that minimizing performance over some of the data from past episodes can be beneficial when searching for a policy that maximizes future performance. We show that our algorithm,  called Prognosticator, is more robust to non-stationarity than two online adaptation techniques, on three simulated problems motivated by real-world applications.
","['University of Massachusetts Amherst', 'Adobe Research', 'University of Massachusetts', 'University of Alberta', 'Adobe Research', 'University of Massachusetts Amherst']"
2020,Evaluating the Performance of Reinforcement Learning Algorithms,"Scott Jordan, Yash Chandak, Daniel Cohen, Mengxue Zhang, Philip Thomas",https://icml.cc/Conferences/2020/Schedule?showEvent=6301,"Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks.
","['University of Massachusetts', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'umass Amherst ', 'University of Massachusetts Amherst']"
2020,Random Hypervolume Scalarizations for Provable Multi-Objective Black Box Optimization,"Richard Zhang, Daniel Golovin",https://icml.cc/Conferences/2020/Schedule?showEvent=6583,"Single-objective black box optimization (also known as zeroth-order
optimization) is the process of minimizing a scalar objective $f(x)$, given evaluations at adaptively chosen inputs $x$. In this paper, we
consider multi-objective optimization, where $f(x)$ outputs a vector of
possibly competing objectives and the goal is to converge to the Pareto frontier. Quantitatively, we wish to maximize the standard \emph{hypervolume indicator} metric, which measures the dominated hypervolume of the entire set of chosen inputs. In this paper, we introduce a novel scalarization function, which we term the \emph{hypervolume scalarization}, and show that drawing random scalarizations from an appropriately chosen distribution can be used to efficiently approximate the \emph{hypervolume indicator} metric. We utilize this connection to show that Bayesian optimization with our scalarization via common acquisition functions, such as Thompson Sampling or Upper Confidence Bound, provably converges to the whole Pareto frontier by deriving tight \emph{hypervolume regret} bounds on the order of $\widetilde{O}(\sqrt{T})$. Furthermore, we highlight the general utility of our scalarization framework by showing that any provably convergent single-objective optimization process can be converted to a multi-objective optimization process with provable convergence guarantees. ","['Google Brain', 'Google, Inc.']"
2020,Deep Reasoning Networks for Unsupervised Pattern De-mixing with Constraint Reasoning,"Di Chen, Yiwei Bai, Wenting Zhao, Sebastian Ament, John Gregoire, Carla Gomes",https://icml.cc/Conferences/2020/Schedule?showEvent=5868,"We introduce Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with constraint reasoning for solving pattern de-mixing problems, typically in an unsupervised or very-weakly-supervised setting. DRNets exploit problem structure and prior knowledge by tightly combining constraint reasoning with stochastic-gradient-based neural network optimization. Our motivating task is from materials discovery and concerns inferring crystal structures of materials from X-ray diffraction data (Crystal-Structure-Phase-Mapping). Given the complexity of its underlying scientific domain, we start by introducing DRNets on an analogous but much simpler task: de-mixing overlapping hand-written Sudokus (Multi-MNIST-Sudoku). On Multi-MNIST-Sudoku, DRNets almost perfectly recovered the mixed Sudokus' digits, with 100\% digit accuracy, outperforming the supervised state-of-the-art MNIST de-mixing models. On Crystal-Structure-Phase-Mapping, DRNets significantly outperform the state of the art and experts' capabilities, recovering more precise and physically meaningful crystal structures.
","['Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Caltech', 'Cornell University']"
2020,Explaining Groups of Points in Low-Dimensional Representations,"Gregory Plumb, Jonathan Terhorst, Sriram Sankararaman, Ameet Talwalkar",https://icml.cc/Conferences/2020/Schedule?showEvent=6119,"A common workflow in data exploration is to learn a low-dimensional representation of the data, identify groups of points in that representation, and examine the differences between the groups to determine what they represent. We treat this workflow as an interpretable machine learning problem by leveraging the model that learned the low-dimensional representation to help identify the key differences between the groups. To solve this problem, we introduce a new type of explanation, a Global Counterfactual Explanation (GCE), and our algorithm, Transitive Global Translations (TGT), for computing GCEs. TGT identifies the differences between each pair of groups using compressed sensing but constrains those pairwise differences to be consistent among all of the groups. Empirically, we demonstrate that TGT is able to identify explanations that accurately explain the model while being relatively sparse, and that these explanations match real patterns in the data.
","['Carnegie Mellon University', 'U-M LSA', 'UCLA', 'Carnegie Mellon University']"
2020,From Importance Sampling to Doubly Robust Policy Gradient,"Jiawei Huang, Nan Jiang",https://icml.cc/Conferences/2020/Schedule?showEvent=5801,"We show that on-policy policy gradient (PG) and its variance reduction variants can be derived by taking finite-difference of function evaluations supplied by estimators from the importance sampling (IS) family for off-policy evaluation (OPE). Starting from the doubly robust (DR) estimator (Jiang & Li, 2016), we provide a simple derivation of a very general and flexible form of PG, which subsumes the state-of-the-art variance reduction technique (Cheng et al., 2019) as its special case and immediately hints at further variance reduction opportunities overlooked by existing literature. We analyze the variance of the new DR-PG estimator, compare it to existing methods as well as the Cramer-Rao lower bound of policy gradient, and empirically show its effectiveness.
","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']"
2020,All in the Exponential Family: Bregman Duality in Thermodynamic Variational Inference,"Rob Brekelmans, Vaden Masrani, Frank Wood, Greg Ver Steeg, Aram Galstyan",https://icml.cc/Conferences/2020/Schedule?showEvent=6234,"The recently proposed Thermodynamic Variational Objective (TVO) leverages thermodynamic integration to provide a family of variational inference objectives, which both tighten and generalize the ubiquitous Evidence Lower Bound (ELBO).  However, the tightness of TVO bounds was not previously known, an expensive grid search was used to choose a ``schedule'' of intermediate distributions, and model learning suffered with ostensibly tighter bounds.  In this work, we propose an exponential family interpretation of the geometric mixture curve underlying the TVO and various path sampling methods, which allows us to characterize the gap in TVO likelihood bounds as a sum of KL divergences.  We propose to choose intermediate distributions using equal spacing in the moment parameters of our exponential family, which matches grid search performance and allows the schedule to adaptively update over the course of training.  Finally, we derive a doubly reparameterized gradient estimator which improves model learning and allows the TVO to benefit from more refined bounds.  To further contextualize our contributions, we provide a unified framework for understanding thermodynamic integration and the TVO using Taylor series remainders.
","['University of Southern California', 'University of British Columbia', 'University of British Columbia', 'University of Southern California', 'USC Information Sciences Institute']"
2020,Taylor Expansion Policy Optimization,"Yunhao Tang, Michal Valko, Remi Munos",https://icml.cc/Conferences/2020/Schedule?showEvent=5902,"In this work, we investigate the application of Taylor expansions in reinforcement learning. In particular, we propose Taylor Expansion Policy Optimization, a policy optimization formalism that generalizes prior work as a first-order special case. We also show that Taylor expansions intimately relate to off-policy evaluation. Finally, we show that this new formulation entails modifications which  improve the performance of several state-of-the-art distributed algorithms.
","['Columbia University', 'DeepMind', 'DeepMind']"
2020,Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach,"Martin Mladenov, Elliot Creager, Omer Ben-Porat, Kevin Swersky, Richard Zemel, Craig Boutilier",https://icml.cc/Conferences/2020/Schedule?showEvent=6304,"Most recommender systems (RS) research assumes that a user's utility can be maximized independently of the utility of the other agents (e.g., other users, content providers). In realistic settings, this is often not true -- the dynamics of an RS ecosystem couple the long-term utility of all agents. In this work, we explore settings in which content providers cannot remain viable unless they receive a certain level of user engagement. We formulate this problem as one of equilibrium selection in the induced dynamical system, and show that it can be solved as an optimal constrained matching problem. Our model ensures the system reaches an equilibrium with maximal social welfare supported by a sufficiently diverse set of viable providers. We demonstrate that even in a simple, stylized dynamical RS model, the standard myopic approach to recommendation - always matching a user to the best provider - performs poorly. We develop several scalable techniques to solve the matching problem, and also draw connections to various notions of user regret and fairness, arguing that these outcomes are fairer in a utilitarian sense.
","['Google', 'University of Toronto', 'Technion--Israel Institute of Technology', 'Google Brain', 'Vector Institute', 'Google']"
2020,Fair Learning with Private Demographic Data,"Hussein Mozannar, Mesrob Ohannessian, Nati Srebro",https://icml.cc/Conferences/2020/Schedule?showEvent=6499,"Sensitive attributes such as race are rarely available to learners in real world settings as their collection is often restricted by laws and regulations. We give a scheme that allows individuals to release their sensitive information privately while still allowing any downstream entity to learn non-discriminatory predictors. We show how to adapt non-discriminatory learners to work with privatized protected attributes giving theoretical guarantees on performance. Finally, we highlight how the methodology could apply to learning fair predictors in settings where protected attributes are only available for a subset of the data.
","['Massachusetts Institute of Technology', 'University of Illinois at Chicago', 'Toyota Technological Institute at Chicago']"
2020,PENNI: Pruned Kernel Sharing for Efficient CNN Inference,"Shiyu Li, Edward Hanson, Hai Li, Yiran Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6232,"Although state-of-the-art (SOTA) CNNs achieve outstanding performance on various tasks, their high computation demand and massive number of parameters make it difficult to deploy these SOTA CNNs onto resource-constrained devices. Previous works on CNN acceleration utilize low-rank approximation of the original convolution layers to reduce computation cost. However, these methods are very difficult to conduct upon sparse models, which limits execution speedup since redundancies within the CNN model are not fully exploited. We argue that kernel granularity decomposition can be conducted with low-rank assumption while exploiting the redundancy within the remaining compact coefficients. Based on this observation, we propose PENNI, a CNN model compression framework that is able to achieve model compactness and hardware efficiency simultaneously by (1) implementing kernel sharing in convolution layers via a small number of basis kernels and (2) alternately adjusting bases and coefficients with sparse constraints. Experiments show that we can prune 97% parameters and 92% FLOPs on ResNet18 CIFAR10 with no accuracy loss, and achieve a 44% reduction in run-time memory consumption and a 53% reduction in inference latency.
","['Duke University', 'Duke University', 'Duke University', 'Duke University']"
2020,Private Reinforcement Learning with PAC and Regret Guarantees,"Giuseppe Vietri, Borja de Balle Pigem, Akshay Krishnamurthy, Steven Wu",https://icml.cc/Conferences/2020/Schedule?showEvent=6152,"Motivated by high-stakes decision-making domains like personalized
medicine where user information is inherently sensitive, we design
privacy preserving exploration policies for episodic reinforcement
learning (RL). We first provide a meaningful privacy formulation using
the notion of joint differential privacy (JDP)--a strong variant of
differential privacy for settings where each user receives their own
sets of output (e.g., policy recommendations). We then develop a
private optimism-based learning algorithm that simultaneously achieves
strong PAC and regret bounds, and enjoys a JDP guarantee. Our
algorithm only pays for a moderate privacy cost on exploration: in
comparison to the non-private bounds, the privacy parameter only
appears in lower-order terms.  Finally, we present lower bounds on
sample complexity and regret for reinforcement learning subject to
JDP.
","['University of Minnesota', 'Amazon Research', 'Microsoft Research', 'University of Minnesota']"
2020,Zeno++: Robust Fully Asynchronous SGD,"Cong Xie, Sanmi Koyejo, Indranil Gupta",https://icml.cc/Conferences/2020/Schedule?showEvent=6052,"We propose Zeno++, a new robust asynchronous Stochastic Gradient Descent(SGD) procedure, intended to  tolerate Byzantine failures of  workers. In contrast to previous work, Zeno++ removes several unrealistic restrictions on worker-server communication, now allowing for fully asynchronous updates from anonymous workers, for arbitrarily stale worker updates, and for the possibility of an unbounded number of Byzantine workers. The key idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. We prove the convergence of Zeno++ for non-convex problems under Byzantine failures. Experimental results show that Zeno++ outperforms existing Byzantine-tolerant asynchronous SGD algorithms.
","['UIUC', 'Illinois / Google', 'UIUC']"
2020,Faster Graph Embeddings via Coarsening,"Matthew Fahrbach, Gramoz Goranci, Richard Peng, Sushant Sachdeva, Chi Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6630,"Graph embeddings are a ubiquitous tool for machine learning tasks, such as node classification and link prediction, on graph-structured data. However, computing the embeddings for large-scale graphs is prohibitively inefficient even if we are interested only in a small subset of relevant vertices. To address this, we present an efficient graph coarsening approach, based on Schur complements, for computing the embedding of the relevant vertices. We prove that these embeddings are preserved exactly by the Schur complement graph that is obtained via Gaussian elimination on the non-relevant vertices. As computing Schur complements is expensive, we give a nearly-linear time algorithm that generates a coarsened graph on the relevant vertices that provably matches the Schur complement in expectation in each iteration. Our experiments involving prediction tasks on graphs demonstrate that computing embeddings on the coarsened graph, rather than the entire graph, leads to significant time savings without sacrificing accuracy.
","['Georgia Institute of Technology', 'University of Toronto', 'Georgia Tech / MSR Redmond', 'University of Toronto', 'Microsoft Research']"
2020,"Expert Learning through Generalized Inverse Multiobjective Optimization: Models, Insights, and Algorithms","Chaosheng Dong, Bo Zeng",https://icml.cc/Conferences/2020/Schedule?showEvent=5887,"We consider a new unsupervised learning task of inferring parameters of a multiobjective decision making model, based on a set of observed decisions from the human expert. This setting is important in applications (such as the task of portfolio management) where it may be difficult to obtain the human expert's intrinsic decision making model. We formulate such a learning problem as an inverse multiobjective optimization problem (IMOP) and propose its first sophisticated model with statistical guarantees. Then, we reveal several fundamental connections between IMOP, K-means clustering, and manifold learning. Leveraging these critical insights and connections, we propose two algorithms to solve IMOP through manifold learning and clustering. Numerical results confirm the effectiveness of our model and the computational efficacy of algorithms.
","['University of Pittsburgh', 'University of Pittsburgh']"
2020,AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks,"Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin, Zhangyang Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6553,"The compression of Generative Adversarial Networks (GANs) has lately drawn attention, due to the increasing demand for deploying GANs into mobile devices for numerous applications such as image translation, enhancement and editing. However, compared to the substantial efforts to compressing other deep models, the research on compressing GANs (usually the generators) remains at its infancy stage. Existing GAN compression algorithms are limited to handling specific GAN architectures and losses. Inspired by the recent success of AutoML in deep compression, we introduce AutoML to GAN compression and develop an AutoGAN-Distiller (AGD) framework. Starting with a specifically designed efficient search space, AGD performs an end-to-end discovery for new efficient generators, given the target computational resource constraints. The search is guided by the original GAN model via knowledge distillation, therefore fulfilling the compression. AGD is fully automatic, standalone (i.e., needing no trained discriminators), and generically applicable to various GAN models. We evaluate AGD in two representative GAN tasks: image translation and super resolution. Without bells and whistles, AGD yields remarkably lightweight yet more competitive compressed models, that largely outperform existing alternatives. Our codes and pretrained models are available at: https://github.com/TAMU-VITA/AGD.
","['Rice University', 'Texas A&M University', 'Texas A&M University', 'Rice University', 'Rice University', 'University of Texas at Austin']"
2020,Accelerated Stochastic Gradient-free and Projection-free Methods,"Feihu Huang, Lue Tao, Songcan Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6452,"In the paper, we propose a class of accelerated stochastic gradient-free and projection-free (a.k.a., zeroth-order Frank-Wolfe) methods
to solve the constrained stochastic and finite-sum nonconvex optimization.
Specifically, we propose an accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW) method based on the variance reduced technique of SPIDER/SpiderBoost and a novel momentum accelerated technique.
Moreover, under some mild conditions, we prove that the Acc-SZOFW has the function query complexity of $O(d\sqrt{n}\epsilon^{-2})$ for finding an $\epsilon$-stationary point in the finite-sum problem,
which improves the exiting best result by a factor of $O(\sqrt{n}\epsilon^{-2})$,
and has the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem, which improves the exiting best result by a factor of $O(\epsilon^{-1})$.
To relax the large batches required in the Acc-SZOFW, we further propose a novel accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW*) based on 
a new variance reduced technique of STORM, which still
reaches the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem without relying on any large batches. 
In particular, we present an accelerated framework of the Frank-Wolfe methods based on the proposed momentum accelerated technique.
The extensive experimental results on black-box adversarial attack and robust black-box classification demonstrate the efficiency of our algorithms.","['Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics', 'Nanjing University of Aeronautics and Astronautics']"
2020,Multigrid Neural Memory,"Tri Huynh, Michael Maire, Matthew Walter",https://icml.cc/Conferences/2020/Schedule?showEvent=6632,"We introduce a novel approach that endows neural networks with emergent, long-term, large-scale memory. Distinct from strategies that connect neural networks to external memory banks via intricately crafted controllers and hand-designed attentional mechanisms, our memory is internal, distributed, co-located alongside computation, and implicitly addressed, while being drastically simpler than prior efforts. Architecting networks with multigrid structure and connectivity, while distributing memory cells alongside computation throughout this topology, we observe the emergence of coherent memory subsystems. Our hierarchical spatial organization, parameterized convolutionally, permits efficient instantiation of large-capacity memories, while multigrid topology provides short internal routing pathways, allowing convolutional networks to efficiently approximate the behavior of fully connected networks.  Such networks have an implicit capacity for internal attention; augmented with memory, they learn to read and write specific memory locations in a dynamic data-dependent manner. We demonstrate these capabilities on synthetic exploration and mapping tasks, where our network is able to self-organize and retain long-term memory for trajectories of thousands of time steps. On tasks decoupled from any notion of spatial geometry: sorting, associative recall, and question answering, our design functions as a truly generic memory and yields excellent results.
","['The University of Chicago', 'University of Chicago', 'Toyota Technological Institute at Chicago']"
2020,Quadratically Regularized Subgradient Methods for Weakly Convex Optimization with Weakly Convex Constraints,"Runchao Ma, Qihang Lin, Tianbao Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6159,"Optimization models with non-convex constraints arise in many tasks in machine learning, e.g., learning with fairness constraints or Neyman-Pearson classification with non-convex loss. Although many efficient methods have been developed with theoretical convergence guarantees for non-convex unconstrained problems, it remains a challenge to design provably efficient algorithms for problems with non-convex functional constraints. This paper proposes a class of subgradient methods for constrained optimization where the objective function and the constraint functions are weakly convex and nonsmooth. Our methods solve a sequence of strongly convex subproblems, where a quadratic regularization term is added to both the objective function and each constraint function. Each subproblem can be solved by various algorithms for strongly convex optimization. Under a uniform Slater’s condition, we establish the computation complexities of our methods for finding a nearly stationary point.
","['University of Iowa', 'University of Iowa', 'The University of Iowa']"
2020,Asynchronous Coagent Networks,"James Kostas, Chris Nota, Philip Thomas",https://icml.cc/Conferences/2020/Schedule?showEvent=5899,"Coagent policy gradient algorithms (CPGAs) are reinforcement learning algorithms for training a class of stochastic neural networks called coagent networks. In this work, we prove that CPGAs converge to locally optimal policies. Additionally, we extend prior theory to encompass asynchronous and recurrent coagent networks. These extensions facilitate the straightforward design and analysis of hierarchical reinforcement learning algorithms like the option-critic, and eliminate the need for complex derivations of customized learning rules for these algorithms.
","['University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst']"
2020,Layered Sampling for Robust Optimization Problems,"Hu Ding, Zixiu Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=5905,"In real world,  our datasets often contain outliers. 
Most existing algorithms for handling outliers take high time complexities ({\em e.g.} quadratic or cubic complexity).
{\em Coreset} is a popular approach for compressing data so as to speed up the optimization algorithms. However, the current coreset methods cannot be easily extended to handle the case with outliers. 
In this paper, we propose a new variant of coreset technique,  {\em layered sampling}, to deal with two fundamental robust optimization problems: {\em $k$-median/means clustering with outliers} and {\em linear regression with outliers}. This new coreset method is in particular suitable to speed up the iterative algorithms (which often improve the solution within a local range) for those robust optimization problems. 
","['University of Science and Technology of China', 'University of Science and Technology of China']"
2020,Approximation Guarantees of Local Search Algorithms via Localizability of Set Functions,Kaito Fujii,https://icml.cc/Conferences/2020/Schedule?showEvent=5917,"This paper proposes a new framework for providing approximation guarantees of local search algorithms. Local search is a basic algorithm design technique and is widely used for various combinatorial optimization problems. To analyze local search algorithms for set function maximization, we propose a new notion called \textit{localizability} of set functions, which measures how effective local improvement is. Moreover, we provide approximation guarantees of standard local search algorithms under various combinatorial constraints in terms of localizability. The main application of our framework is sparse optimization, for which we show that restricted strong concavity and restricted smoothness of the objective function imply localizability, and further develop accelerated versions of local search algorithms. We conduct experiments in sparse regression and structure learning of graphical models to confirm the practical efficiency of the proposed local search algorithms.
",['National Institute of Informatics']
2020,Q-value Path Decomposition for Deep Multiagent Reinforcement Learning,"Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu, Changjie Fan, Zhongyu Wei",https://icml.cc/Conferences/2020/Schedule?showEvent=6337,"Recently, deep multiagent reinforcement learning (MARL) has become a highly active research area as many real-world problems can be inherently viewed as multiagent systems. A particularly interesting and widely applicable class of problems is the partially observable cooperative multiagent setting, in which a team of agents learns to coordinate their behaviors conditioning on their private observations and commonly shared global reward signals. One natural solution is to resort to the centralized training and decentralized execution paradigm and during centralized training, one key challenge is the multiagent credit assignment: how to allocate the global rewards for individual agent policies for better coordination towards maximizing system-level's benefits. In this paper, we propose a new method called Q-value Path Decomposition (QPD) to decompose the system's global Q-values into individual agents' Q-values. Unlike previous works which restrict the representation relation of the individual Q-values and the global one, we leverage the integrated gradient attribution technique into deep MARL to directly decompose global Q-values along trajectory paths to assign credits for agents. We evaluate QPD on the challenging StarCraft II micromanagement tasks and show that QPD achieves the state-of-the-art performance in both homogeneous and heterogeneous multiagent scenarios compared with existing cooperative MARL algorithms.
","['Tianjin University', 'Tianjin University', 'Tencent', 'Tianjin University', 'NetEase Fuxi AI Lab', 'NetEase Fuxi AI Lab', 'Netease', 'Fudan University']"
2020,Laplacian Regularized Few-Shot Learning,"Imtiaz Ziko, Jose Dolz, Eric Granger, Ismail Ben Ayed",https://icml.cc/Conferences/2020/Schedule?showEvent=6480,"We propose a transductive Laplacian-regularized inference for few-shot tasks. Given any feature embedding learned from the base classes, we minimize a quadratic binary-assignment function containing two terms: (1) a unary term assigning query samples to the nearest class prototype, and (2) a pairwise Laplacian term encouraging nearby query samples to have consistent label assignments. Our transductive inference does not re-train the base model, and can be viewed as a graph clustering of the query set, subject to supervision constraints from the support set. We derive a computationally efficient bound optimizer of a relaxation of our function, which computes independent (parallel) updates for each query sample, while guaranteeing convergence. Following a simple cross-entropy training on the base classes, and without complex meta-learning strategies, we conducted comprehensive experiments over five few-shot learning benchmarks. Our LaplacianShot consistently outperforms state-of-the-art methods by significant margins across different models, settings, and data sets. Furthermore, our transductive inference is very fast, with computational times that are close to inductive inference, and can be used for large-scale few-shot tasks. 
","['ETS Montreal', 'ETS Montreal', 'ETS Montreal', 'ETS Montreal']"
2020,Fair Generative Modeling via Weak Supervision,"Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, Stefano Ermon",https://icml.cc/Conferences/2020/Schedule?showEvent=6697,"Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning.
We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled reference dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and reference datasets for learning; and 2) data generation close in distribution to the reference dataset at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by an average of up to 34.6% over baselines for comparable image generation using generative adversarial networks.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
2020,Stochastic Optimization for Non-convex Inf-Projection Problems,"Yan Yan, Yi Xu, Lijun Zhang, Wang Xiaoyu, Tianbao Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6354,"In this paper, we study a family of non-convex and possibly non-smooth inf-projection minimization problems, where the target objective function is equal to minimization of a joint function over another variable. This problem include difference of convex (DC) functions  and a family of bi-convex functions as special cases. We develop stochastic algorithms and establish their first-order convergence for finding a (nearly) stationary solution of the target non-convex function under different conditions of the component functions. To the best of our knowledge, this is the first work that comprehensively studies stochastic optimization of non-convex inf-projection minimization problems with provable convergence guarantee. Our algorithms enable efficient stochastic optimization of a family of non-decomposable DC functions and a family of bi-convex functions. To demonstrate the power of the proposed algorithms we consider an important application in variance-based regularization. Experiments verify the effectiveness of our inf-projection based formulation and the proposed stochastic algorithm in comparison with previous stochastic algorithms based on the min-max formulation for achieving the same effect.
","['the University of Iowa', 'Alibaba Group (U.S.) Inc.', 'Nanjing University', 'The Chinese University of Hong Kong (Shenzhen)', 'The University of Iowa']"
2020,LEEP: A New Measure to Evaluate Transferability of Learned Representations,"Cuong Nguyen, Tal Hassner, Matthias W Seeger, Cedric Archambeau",https://icml.cc/Conferences/2020/Schedule?showEvent=6289,"We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.
","['Amazon Web Services', 'Open University of Israel', 'Amazon', 'Amazon Web Services']"
2020,Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data,"Lan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yufeng Li, Zhi-Hua Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=6319,"	Deep semi-supervised learning (SSL) has been recently shown very effectively. However, its performance is seriously decreased when the class distribution is mismatched, among which a common situation is that unlabeled data contains some classes not seen in the labeled data. Efforts on this issue remain to be limited. This paper proposes a simple and effective safe deep SSL method to alleviate the harm caused by it. In theory, the result learned from the new method is never worse than learning from merely labeled data, and it is theoretically guaranteed that its generalization approaches the optimal in the order $O(\sqrt{d\ln(n)/n})$, even faster than the convergence rate in supervised learning associated with massive parameters. In the experiment of benchmark data, unlike the existing deep SSL methods which are no longer as good as supervised learning in 40\% of unseen-class unlabeled data, the new method can still achieve performance gain in more than 60\% of unseen-class unlabeled data. Moreover, the proposal is suitable for many deep SSL algorithms and can be easily extended to handle other cases of class distribution mismatch.","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University']"
2020,Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks Using PAC-Bayesian Analysis,"Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6348,"The notion of flat minima has gained attention as a key metric of the generalization ability of deep learning models. However, current definitions of flatness are known to be sensitive to parameter rescaling. While some previous studies have proposed to rescale flatness metrics using parameter scales to avoid the scale dependence, the normalized metrics lose the direct theoretical connections between flat minima and generalization. In this paper, we first provide generalization error bounds using existing normalized flatness measures. Using the analysis, we then propose a novel normalized flatness metric. The proposed metric enjoys both direct theoretical connections and better empirical correlation to generalization error.
","['Preferred Networks, Inc', 'University of Tokyo / RIKEN', 'RIKEN / The University of Tokyo']"
2020,NGBoost: Natural Gradient Boosting for Probabilistic Prediction,"Tony Duan, Anand Avati, Daisy Ding, Khanh K. Thai, Sanjay Basu, Andrew Ng, Alejandro Schuler",https://icml.cc/Conferences/2020/Schedule?showEvent=6336,"We present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic prediction via gradient boosting. Typical regression models return a point estimate, conditional on covariates, but probabilistic regression models output a full probability distribution over the outcome space, conditional on the covariates. This allows for predictive uncertainty estimation - crucial in applications like healthcare and weather forecasting. NGBoost generalizes gradient boosting to probabilistic regression by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm. Furthermore, we show how the Natural Gradient is required to correct the training dynamics of our multiparameter boosting approach. NGBoost can be used with any base learner, any family of distributions with continuous parameters, and any scoring rule. NGBoost matches or exceeds the performance of existing methods for probabilistic prediction while offering additional benefits in flexibility, scalability, and usability. An open-source implementation is available at github.com/stanfordmlgroup/ngboost.
","['Microsoft Research', 'Stanford University', 'Stanford University', 'Kaiser Permanente Division of Research', 'Harvard Medical School', 'Stanford U.', 'Unlearn.ai']"
2020,Dynamic Knapsack Optimization Towards Efficient Multi-Channel Sequential Advertising,"Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao, Shan Chen, Rongquan Bai, Mingzhou Xie, Miao Xu, Zhenzhe Zheng, Chuan Yu, HAN LI, Jian Xu, Kun Gai",https://icml.cc/Conferences/2020/Schedule?showEvent=6409,"In E-commerce, advertising is essential for merchants to reach their target users. The typical objective is to maximize the advertiser's cumulative revenue over a period of time under a budget constraint. In real applications, an advertisement (ad) usually needs to be exposed to the same user multiple times until the user finally contributes revenue (e.g., places an order). However, existing advertising systems mainly focus on the immediate revenue with single ad exposures, ignoring the contribution of each exposure to the final conversion, thus usually falls into suboptimal solutions. In this paper, we formulate the sequential advertising strategy optimization as a dynamic knapsack problem. We propose a theoretically guaranteed bilevel optimization framework, which significantly reduces the solution space of the original optimization space while ensuring the solution quality. To improve the exploration efficiency of reinforcement learning, we also devise an effective action space reduction approach. Extensive offline and online experiments show the superior performance of our approaches over state-of-the-art baselines in terms of cumulative revenue.
","['College of Intelligence and Computing, Tianjin University', 'Alibaba Group', 'Tianjin University', 'Department of Automation, Tsinghua University', 'Alibaba Group', 'Tianjin University', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Shanghai Jiao Tong University', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba group']"
2020,Online Learning for Active Cache Synchronization,"Andrey Kolobov, Sebastien Bubeck, Julian Zimmert",https://icml.cc/Conferences/2020/Schedule?showEvent=6074,"Existing multi-armed bandit (MAB) models make two implicit assumptions: an arm generates a payoff only when it is played, and the agent observes every payoff that is generated. This paper introduces synchronization bandits, a MAB variant where all arms generate costs at all times, but the agent observes an arm's instantaneous cost only when the arm is played. Synchronization MABs are inspired by online caching scenarios such as Web crawling, where an arm corresponds to a cached item and playing the arm means downloading its fresh copy from a server. We present MirrorSync, an online learning algorithm for synchronization bandits, establish an adversarial regret of $O(T^{2/3})$ for it, and show how to make it practical.","['Microsoft Research', 'Microsoft Research', 'Google Research']"
2020,Minimax Rate for Learning From Pairwise Comparisons in the BTL Model,"Julien Hendrickx, Alex Olshevsky, Venkatesh Saligrama",https://icml.cc/Conferences/2020/Schedule?showEvent=6147,"We consider the problem of learning the qualities w1, ... , wn of a collection of items by performing noisy comparisons among them. We assume there is a fixed ``comparison graph'' and every neighboring pair of items is compared k times. We will study the popular Bradley-Terry-Luce model,  where the probability that item i wins a  comparison against j equals wi/(wi + wj).  We are interested in how the expected error in estimating the vector w = (w1, ... , w_n) behaves in the regime when the number of comparisons k is large.
Our contribution is the determination of the minimax rate up to a constant factor. We   show that this rate is achieved by a simple algorithm based on weighted least squares, with weights determined from the empirical outcomes of the comparisons. This algorithm can be implemented  in nearly linear time in the total number of comparisons.
","['University of Catholique de Louvain', 'Boston University', 'Boston University']"
2020,Full Law Identification in Graphical Models of Missing Data: Completeness Results,"Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser",https://icml.cc/Conferences/2020/Schedule?showEvent=5983,"Missing data has the potential to affect analyses conducted in all fields of scientific study including healthcare, economics, and the social sciences. Several approaches to unbiased inference in the presence of non-ignorable missingness rely on the specification of the target distribution and its missingness process as a probability distribution that factorizes with respect to a directed acyclic graph. In this paper, we address the longstanding question of the characterization of models that are identifiable within this class of missing data distributions. We provide the first completeness result in this field of study -- necessary and sufficient graphical conditions under which, the full data distribution can be recovered from the observed data distribution. We then simultaneously address issues that may arise due to the presence of both missing data and unmeasured confounding, by extending these graphical conditions and proofs of completeness, to settings where some variables are not just missing, but completely unobserved.
","['Johns Hopkins University', 'Johns Hopkins University', 'Johns Hopkins University']"
2020,DeltaGrad: Rapid retraining of machine learning models,"Yinjun Wu, Edgar Dobriban, Susan B Davidson",https://icml.cc/Conferences/2020/Schedule?showEvent=5915,"Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.
","['university of pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']"
2020,Bayesian Optimisation over Multiple Continuous and Categorical Inputs,"Binxin Ru, Ahsan Alvi, Vu Nguyen, Michael A Osborne, Stephen Roberts",https://icml.cc/Conferences/2020/Schedule?showEvent=6248,"Efficient optimisation of black-box problems that comprise both continuous and categorical inputs is important, yet poses significant challenges. Current approaches, like one-hot encoding, severely increase the dimension of the search space, while separate modelling of category-specific data is sample-inefficient. Both frameworks are not scalable to practical applications involving multiple categorical variables, each with multiple possible values. We propose a new approach, Continuous and Categorical Bayesian Optimisation (CoCaBO), which combines the strengths of multi-armed bandits and Bayesian optimisation to select values for both categorical and continuous inputs. We model this mixed-type space using a Gaussian Process kernel, designed to allow sharing of information across multiple categorical variables; this allows CoCaBO to leverage all available data efficiently. We extend our method to the batch setting and propose an efficient selection procedure that dynamically balances exploration and exploitation whilst encouraging batch diversity. 
We demonstrate empirically that our method outperforms existing approaches on both synthetic and real-world optimisation tasks with continuous and categorical inputs.
","['University of Oxford', 'McKinsey & Company', 'University of Oxford', 'U Oxford', 'University of Oxford']"
2020,On the Noisy Gradient Descent that Generalizes as SGD,"Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, Zhanxing Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=6307,"The gradient noise of SGD is considered to play a central role in the observed strong generalization abilities of deep learning. While past studies confirm that the magnitude and the covariance structure of gradient noise are critical for regularization, it remains unclear whether or not the class of noise distributions is important. In this work we provide negative results by showing that noises in classes different from the SGD noise can also effectively regularize gradient descent. Our finding is based on a novel observation on the structure of the SGD noise: it is the multiplication of the gradient matrix and a sampling noise that arises from the mini-batch sampling procedure. Moreover, the sampling noises unify two kinds of gradient regularizing noises that belong to the Gaussian class: the one using (scaled) Fisher as covariance and the one using the gradient covariance of SGD as covariance. Finally, thanks to the flexibility of choosing noise class, an algorithm is proposed to perform noisy gradient descent that generalizes well, the variant of which even benefits large batch SGD training without hurting generalization.
","['Johns Hopkins University', 'Missouri S&T', 'Baidu Research', 'Styling AI', 'Johns Hopkins University', 'Peking University']"
2020,Improving the Gating Mechanism of Recurrent Neural Networks,"Albert Gu, Caglar Gulcehre, Thomas Paine, Matthew Hoffman, Razvan Pascanu",https://icml.cc/Conferences/2020/Schedule?showEvent=6679,"Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate easily through depth or time. However, their saturation property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono-initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.
","['Stanford University', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2020,On Implicit Regularization in $\beta$-VAEs,"Abhishek Kumar, Ben Poole",https://icml.cc/Conferences/2020/Schedule?showEvent=6804,"While the impact of variational inference (VI) on posterior inference in a fixed generative model is well-characterized, its role in regularizing a learned generative model when used in variational autoencoders (VAEs) is poorly understood. 
We study the regularizing effects of variational distributions on learning in generative models from two perspectives. First, we analyze the role that the choice of variational family plays in imparting uniqueness to the learned model by restricting the set of optimal generative models. Second, we study the regularization effect of the variational family on the local geometry of the decoding model. This analysis uncovers the regularizer implicit in the $\beta$-VAE objective, and leads to an approximation consisting of a deterministic autoencoding objective plus analytic regularizers that depend on the Hessian or Jacobian of the decoding model, unifying VAEs with recent heuristics proposed for training regularized autoencoders. We empirically verify these findings, observing that the proposed deterministic objective exhibits similar behavior to the $\beta$-VAE in terms of objective value and sample quality.","['Google Brain', 'Google Brain']"
2020,Individual Fairness for k-Clustering,"Sepideh Mahabadi, Ali Vakilian",https://icml.cc/Conferences/2020/Schedule?showEvent=6504,"We give a local search based algorithm for $k$-median and $k$-means (and more generally for any $k$-clustering with $\ell_p$ norm cost function) from the perspective of individual fairness. 
More precisely, for a point $x$ in a point set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively, if a set of $k$ random points are chosen from $P$ as centers, every point $x\in P$ expects to have a center within radius $r(x)$. An individually fair clustering provides such a guarantee for every point $x\in P$. This notion of fairness was introduced in [Jung et al., 2019] where they showed how to get an approximately feasible $k$-clustering with respect to this fairness condition.

In this work, we show how to get an approximately \emph{optimal} such fair $k$-clustering. The $k$-median ($k$-means) cost of our solution is within a constant factor of the cost of an optimal fair $k$-clustering, and our solution approximately satisfies the fairness condition (also within a constant factor). ","['Toyota Technological Institute at Chicago', 'University of Wisconsin-Madison']"
2020,Efficient Domain Generalization via Common-Specific Low-Rank Decomposition,"Vihari Piratla, Praneeth Netrapalli, Sunita Sarawagi",https://icml.cc/Conferences/2020/Schedule?showEvent=6528,"Domain generalization refers to the task of training a model which generalizes to new domains that are not seen during training. We present CSD (Common Specific Decomposition), for this setting, which jointly learns a common component (which generalizes to new domains) and a domain specific component (which overfits on training domains). The domain specific components are discarded after training and only the common component is retained. The algorithm is extremely simple and involves only modifying the final linear classification layer of any given neural network architecture. We present a principled analysis to understand existing approaches, provide identifiability results of CSD, and study the effect of low-rank on domain generalization. We show that CSD either matches or beats state of the art approaches for domain generalization based on domain erasure,  domain perturbed data augmentation, and meta-learning. Further diagnostics on rotated MNIST, where domains are interpretable, confirm the hypothesis that CSD successfully disentangles common and domain specific components and hence leads to better domain generalization; moreover, our code and dataset are publicly available at the following URL: \url{https://github.com/vihari/csd}.
","['IIT Bombay', 'Microsoft Research', 'IIT Bombay']"
2020,Maximum-and-Concatenation Networks,"Xingyu Xie, Hao Kong, Jianlong Wu, Wayne Zhang, Guangcan Liu, Zhouchen Lin",https://icml.cc/Conferences/2020/Schedule?showEvent=5891,"While successful in many fields, deep neural networks (DNNs) still suffer from some open problems such as bad local minima and unsatisfactory generalization performance. 
In this work, we propose a novel architecture called Maximum-and-Concatenation Networks (MCN) to try eliminating bad local minima and improving generalization ability as well.
Remarkably, we prove that MCN has a very nice property; that is, every local minimum of an (l+1)-layer MCN can be better than, at least as good as, the global minima of the network consisting of its first l layers. 
In other words, by increasing the network depth, MCN can autonomously improve its local minima's goodness, what is more, it is easy to plug MCN into an existing deep model to make it also have this property.
Finally, under mild conditions, we show that MCN can approximate certain continuous function arbitrarily well with high efficiency; that is, the covering number of MCN is much smaller than most existing DNNs such as deep ReLU. 
Based on this, we further provide a tight generalization bound to guarantee the inference ability of MCN when dealing with testing samples. 
","['Peking University', 'Peking University', 'Peking University', 'SenseTime Research', 'Nanjing University of Information Science and Technology', 'Peking University']"
2020,Streaming k-Submodular Maximization under Noise subject to Size Constraint,"Lan N. Nguyen, My T. Thai",https://icml.cc/Conferences/2020/Schedule?showEvent=5845,"Maximizing on k-submodular functions subject to size constraint has received extensive attention recently. In this paper, we investigate a more realistic scenario of this problem that (1) obtaining exact evaluation of an objective function is impractical, instead, its noisy version is acquired; and (2) algorithms are required to take only one single pass over dataset, producing solutions in a timely manner. We propose two novel streaming algorithms, namely DStream and RStream, with their theoretical performance guarantees. We further demonstrate the efficiency of our algorithms in two application, showing that our algorithms can return comparative results to state-of-the-art non-streaming methods while using a much fewer number of queries.
","['University of Florida', 'University of Florida']"
2020,Min-Max Optimization without Gradients: Convergence and Applications to Black-Box Evasion and Poisoning Attacks,"Sijia Liu, Songtao Lu, Xiangyi Chen, Yao Feng, Kaidi Xu, Abdullah Al-Dujaili, Mingyi Hong, Una-May O'Reilly",https://icml.cc/Conferences/2020/Schedule?showEvent=5979,"In this paper, we study the problem of constrained min-max optimization in a black-box setting, where the desired optimizer cannot access the gradients of the objective function but may query its values. We present a principled optimization framework, integrating a zeroth-order (ZO) gradient estimator with an alternating projected stochastic gradient descent-ascent method, where the former only requires a small number of function queries and the later needs just one-step descent/ascent update. We show that the proposed framework, referred to as ZO-Min-Max, has a sublinear convergence rate under mild conditions and scales gracefully with problem size. We also explore a promising connection between black-box min-max optimization and black-box evasion and poisoning attacks in adversarial machine learning (ML). Our empirical evaluations on these use cases demonstrate the effectiveness of our approach and its scalability to dimensions that prohibit using recent black-box solvers.
","['MIT-IBM Watson AI Lab', 'IBM Research', 'University of Minnesota', 'Tsinghua University', 'Northeastern University', 'CSAIL', 'University of Minnesota', 'MIT']"
2020,On the Global Optimality of Model-Agnostic Meta-Learning,"Lingxiao Wang, Qi Cai, Zhuoran Yang, Zhaoran Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6057,"Model-agnostic meta-learning (MAML) formulates meta-learning as a bilevel optimization problem, where the inner level solves each subtask based on a shared prior, while the outer level searches for the optimal shared prior by optimizing its aggregated performance over all the subtasks.  Despite its empirical success, MAML remains less understood in theory, especially in terms of its global optimality, due to the nonconvexity of the meta-objective (the outer-level objective). To bridge such a gap between theory and practice, we characterize the optimality gap of the stationary points attained by MAML for both reinforcement learning and supervised learning, where the inner-level and outer-level problems are solved via first-order optimization methods. In particular, our characterization connects the optimality gap of such stationary points with (i) the functional geometry of inner-level objectives and (ii) the representation power of function approximators, including linear models and neural networks. To the best of our knowledge, our analysis establishes the global optimality of MAML with  nonconvex meta-objectives for the first time.
","['Northwestern University', 'Northwestern University', 'Princeton University', 'Northwestern U']"
2020,Student Specialization in Deep Rectified Networks With Finite Width and Input Dimension,Yuandong Tian,https://icml.cc/Conferences/2020/Schedule?showEvent=6344,"We consider a deep ReLU / Leaky ReLU student network trained from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). The student network is \emph{over-realized}: at each layer $l$, the number $n_l$ of student nodes is more than that ($m_l$) of teacher. Under mild conditions on dataset and teacher network, we prove that when the gradient is small at every data sample, each teacher node is \emph{specialized} by at least one student node \emph{at the lowest layer}. For two-layer network, such specialization can be achieved by training on any dataset of \emph{polynomial} size $\mathcal{O}( K^{5/2} d^3 \epsilon^{-1})$. until the gradient magnitude drops to $\mathcal{O}(\epsilon/K^{3/2}\sqrt{d})$. Here $d$ is the input dimension, $K = m_1 + n_1$ is the total number of neurons in the lowest layer of teacher and student. Note that we require a specific form of data augmentation and the sample complexity includes the additional data generated from augmentation. To our best knowledge, we are the first to give polynomial sample complexity for student specialization of training two-layer (Leaky) ReLU networks with finite depth and width in teacher-student setting, and finite complexity for the lowest layer specialization in multi-layer case, without parametric assumption of the input (like Gaussian). Our theory suggests that teacher nodes with large fan-out weights get specialized first when the gradient is still large, while others are specialized with small gradient, which suggests inductive bias in training. This shapes the stage of training as empirically observed in multiple previous works. Experiments on synthetic and CIFAR10 verify our findings. The code is released in \url{https://github.com/facebookresearch/luckmatters}. ",['Facebook AI Research']
2020,Reverse-engineering deep ReLU networks,"David Rolnick, Konrad Kording",https://icml.cc/Conferences/2020/Schedule?showEvent=5765,"The output of a neural network depends on its architecture and weights in a highly nonlinear way, and it is often assumed that a network's parameters cannot be recovered from its output. Here, we prove that, in fact, it is frequently possible to reconstruct the architecture, weights, and biases of a deep ReLU network by observing only its output. We leverage the fact that every ReLU network defines a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism.
","['University of Pennsylvania', 'Upenn']"
2020,Nested Subspace Arrangement for Representation of Relational Data,"Nozomi Hata, Shizuo Kaji, Akihiro Yoshida, Katsuki Fujisawa",https://icml.cc/Conferences/2020/Schedule?showEvent=6106,"Studies of acquiring appropriate continuous representations of a discrete objects such as graph and knowledge based data have been conducted by many researches in the field of machine learning.
In this paper, we introduce Nested SubSpace arrangement (NSS arrangement), a comprehensive framework for representation learning.
We show that existing embedding techniques can be regarded as a member of NSS arrangement.
Based on the concept of the NSS arrangement, we implemented Disk-ANChor ARrangement (DANCAR), a representation learning method specializing to reproduce general graphs.
Numerical experiments have shown that DANCAR has successfully embedded WordNet in ${\mathbb R}^{20}$ with the F1 score of 0.993 in the reconstruction task.
DANCAR is also suitable for visualization to understand the characteristics of graph.","['Kyushu University', 'Kyushu University', 'Kyushu University', 'Kyushu University']"
2020,Combinatorial Pure Exploration for Dueling Bandit,"Wei Chen, Yihan Du, Longbo Huang, Haoyu Zhao",https://icml.cc/Conferences/2020/Schedule?showEvent=6272,"In this paper, we study combinatorial pure exploration for dueling bandits (CPE-DB): we have multiple candidates for multiple positions as modeled by a bipartite graph, and in each round we sample a duel of two candidates on one position and observe who wins in the duel, with the goal of finding the best candidate-position matching with high probability after multiple rounds of samples. CPE-DB is an adaptation of the original combinatorial pure exploration for multi-armed bandit (CPE-MAB) problem to the dueling bandit setting. We consider both the Borda winner and the Condorcet winner cases. For Borda winner, we establish a reduction of the problem to the original CPE-MAB setting and design PAC and exact algorithms that achieve both the sample complexity similar to that in the CPE-MAB setting (which is nearly optimal for a subclass of problems)  and polynomial running time per round. For Condorcet winner, we first design a fully polynomial time approximation scheme (FPTAS) for the offline problem of finding the Condorcet winner with known winning probabilities, and then use the FPTAS as an oracle to design a novel pure exploration algorithm CAR-Cond with sample complexity analysis. CAR-Cond is the first algorithm with polynomial running time per round for identifying the Condorcet winner in CPE-DB.
","['Microsoft', 'IIIS, Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2020,Tensor denoising and completion based on ordinal observations,"Chanwoo Lee, Miaoyan Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=5861,"Higher-order tensors arise frequently in applications such as neuroimaging, recommendation system, and social network analysis. We consider the problem of low-rank tensor estimation from possibly incomplete, ordinal-valued observations. Two related problems are studied, one on tensor denoising and another on tensor completion. We propose a multi-linear cumulative link model, develop a rank-constrained M-estimator, and obtain theoretical accuracy guarantees. Our mean squared error bound enjoys a faster convergence rate than previous results, and we show that the proposed estimator is minimax optimal under the class of low-rank models. Furthermore, the procedure developed serves as an efficient completion method which guarantees consistent recovery of an order-K (d,...,d)-dimensional low-rank tensor using only O(Kd) noisy, quantized observations. We demonstrate the outperformance of our approach over previous methods on the tasks of clustering and collaborative filtering. 
","['University of Wisconsin - Madison', 'University of Wisconsin - Madison']"
2020,Stochastic Regret Minimization in Extensive-Form Games,"Gabriele Farina, Christian Kroer, Tuomas Sandholm",https://icml.cc/Conferences/2020/Schedule?showEvent=6605,"Monte-Carlo counterfactual regret minimization (MCCFR) is the state-of-the-art algorithm for solving sequential games that are too large for full tree traversals. It works by using gradient estimates that can be computed via sampling. However, stochastic methods for sequential games have not been investigated extensively beyond MCCFR. In this paper we develop a new framework for developing stochastic regret minimization methods. This framework allows us to use any regret-minimization algorithm, coupled with any gradient estimator. The MCCFR algorithm can be analyzed as a special case of our framework, and this analysis leads to significantly stronger theoretical guarantees on convergence, while simultaneously yielding a simplified proof. Our framework allows us to instantiate several new stochastic methods for solving sequential games. We show extensive experiments on five games, where some variants of our methods outperform MCCFR.
","['Carnegie Mellon University', 'Columbia University', 'Carnegie Mellon University']"
2020,Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling,"Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Richard Zemel",https://icml.cc/Conferences/2020/Schedule?showEvent=6649,"We present a new method for evaluating and training unnormalized density models. Our approach only requires access to the gradient of the unnormalized model’s log-density. We  estimate the Stein discrepancy between the data density p(x) and the model density q(x) based on a vector function of the data.  We parameterize this function with a neural network and fit its parameters to maximize this discrepancy.  This yields a novel goodness-of-fit test which outperforms existing methods on high dimensional data. Furthermore, optimizing q(x) to minimize this discrepancy produces a novel method for training unnormalized models. This training method can fit large unnormalized models faster than existing approaches. The ability to both learn and compare models is a unique feature of the proposed method.
","['University of Toronto', 'University of Toronto', 'Apple Inc.', 'University of Toronto', 'Vector Institute']"
2020,"Uncertainty quantification for nonconvex tensor completion: Confidence intervals, heteroscedasticity and optimality","Changxiao Cai, H. Vincent Poor, Yuxin Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6646,"We study the distribution and uncertainty of nonconvex optimization for noisy tensor completion --- the problem of estimating a low-rank tensor given incomplete and corrupted observations of its entries. Focusing on a two-stage nonconvex estimation algorithm, we characterize the distribution of this estimator down to fine scales. This distributional theory in turn allows one to construct valid and short confidence intervals for both the unseen tensor entries and its underlying tensor factors. The proposed inferential procedure enjoys several important features: (1) it is fully adaptive to noise heteroscedasticity, and (2) it is data-driven and adapts automatically to unknown noise distributions. Furthermore, our findings unveil the statistical optimality of nonconvex tensor completion: it attains un-improvable estimation accuracy --- including both the rates and the pre-constants --- under i.i.d. Gaussian noise.
","['Princeton University', 'Princeton University', 'Princeton University']"
2020,Amortized Population Gibbs Samplers with Neural Sufficient Statistics,"Hao Wu, Heiko Zimmermann, Eli Sennesh, Tuan Anh Le, Jan-Willem van de Meent",https://icml.cc/Conferences/2020/Schedule?showEvent=6715,"We develop amortized population Gibbs (APG) samplers, a class of scalable methods that frame structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers can be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods.
","['Northeastern University', 'Northeastern University', 'Northeastern University', 'MIT', 'Northeastern University']"
2020,Recurrent Hierarchical Topic-Guided RNN for Language Generation,"Dandan Guo, Bo Chen, Ruiying Lu, Mingyuan Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=6788,"To simultaneously capture syntax and global semantics from a text corpus, we propose a new larger-context recurrent neural network (RNN) based language model, which extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation. Moving beyond a conventional RNN-based language model that ignores long-range word dependencies and sentence order, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence topic dependencies. For inference, we develop a hybrid of stochastic-gradient Markov chain Monte Carlo and recurrent autoencoding variational Bayes. Experimental results on a variety of real-world text corpora demonstrate that the proposed model not only outperforms larger-context RNN-based language models, but also learns interpretable recurrent multilayer topics and generates diverse sentences and paragraphs that are syntactically correct and semantically coherent. 
","['National Laboratory of Radar Signal Processing, Xidian University', 'School of Electronic Engineering, Xidian University', 'xidian university', 'University of Texas at Austin']"
2020,Interpolation between Residual and Non-Residual Networks,"Zonghan Yang, Yang Liu, Chenglong Bao, Zuoqiang Shi",https://icml.cc/Conferences/2020/Schedule?showEvent=6829,"Although ordinary differential equations (ODEs) provide insights for designing network architectures, its relationship with the non-residual convolutional neural networks (CNNs) is still unclear. In this paper, we present a novel ODE model by adding a damping term. It can be shown that the proposed model can recover both a ResNet and a CNN by adjusting an interpolation coefficient. Therefore, the damped ODE model provides a unified framework for the interpretation of residual and non-residual networks. The Lyapunov analysis reveals better stability of the proposed model, and thus yields robustness improvement of the learned networks. Experiments on a number of image classification benchmarks show that the proposed model substantially improves the accuracy of ResNet and ResNeXt over the perturbed inputs from both stochastic noise and adversarial attack methods. Moreover, the loss landscape analysis demonstrates the improved robustness of our method along the attack direction.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2020,Collaborative Machine Learning with Incentive-Aware Model Rewards,"Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, Bryan Kian Hsiang Low",https://icml.cc/Conferences/2020/Schedule?showEvent=6831,"Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's reward based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.
","['National University of Singapore', 'National University of Singapore', 'NUS', 'National University of Singapore']"
2020,Towards Understanding the Dynamics of the First-Order Adversaries,"Zhun Deng, Hangfeng He, Jiaoyang Huang, Weijie Su",https://icml.cc/Conferences/2020/Schedule?showEvent=5966,"An acknowledged weakness of neural networks is their vulnerability to adversarial perturbations to the inputs. To improve the robustness of these models, one of the most popular defense mechanisms is to alternatively maximize the loss over the constrained perturbations (or called adversaries) on the inputs using projected gradient ascent and minimize over weights. In this paper, we analyze the dynamics of the maximization step towards understanding the experimentally observed effectiveness of this defense mechanism. Specifically, we investigate the landscape of the adversaries for a two-layer neural network with a quadratic loss. Our main result proves that projected gradient ascent finds a local maximum of this non-concave problem in a polynomial number of iterations with high probability.  To our knowledge, this is the first work that provides a convergence analysis of the first-order adversaries. Moreover, our analysis demonstrates that, in the initial phase of adversarial training, the scale of the inputs matters in the sense that a smaller input scale leads to faster convergence of adversarial training and a ``more regular'' landscape. Finally, we show that these theoretical findings are in excellent agreement with a series of experiments.
","['Harvard', 'University of Pennsylvania', 'Institute of Advanced Study', 'University of Pennsylvania']"
2020,Provably Efficient Exploration in Policy Optimization,"Qi Cai, Zhuoran Yang, Chi Jin, Zhaoran Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=5993,"While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper proposes an \underline{O}ptimistic variant of the \underline{P}roximal \underline{P}olicy \underline{O}ptimization algorithm (OPPO), which follows an ``optimistic version'' of the policy gradient direction. 
This paper proves that, in the problem of episodic Markov decision process with unknown transition and full-information feedback of adversarial reward, 
OPPO achieves an $\tilde{O}(\sqrt{|\cS|^2|\cA|H^3 T})$ regret. Here $|\cS|$ is the size of the state space, $|\cA|$ is the size of the action space, $H$ is the episode horizon, and $T$ is the total number of steps. To the best of our knowledge, OPPO is the first provably efficient policy optimization algorithm that explores.","['Northwestern University', 'Princeton University', 'Princeton University', 'Northwestern U']"
2020,Scalable Nearest Neighbor Search for Optimal Transport,"Arturs Backurs, Yihe Dong, Piotr Indyk, Ilya Razenshteyn, Tal Wagner",https://icml.cc/Conferences/2020/Schedule?showEvent=6078,"The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly popular similarity measure for rich data domains, such as images or text documents.  This raises the necessity for fast nearest neighbor search algorithms according to this distance, which poses a substantial computational bottleneck on massive datasets.
In this work we introduce Flowtree, a fast and accurate approximation algorithm for the Wasserstein-1 distance. We formally analyze its approximation factor and running time.  We perform extensive experimental evaluation of nearest neighbor search algorithms in the W_1 distance on real-world dataset.  Our results show that compared to previous state of the art, Flowtree achieves up to 7.4 times faster running time.
","['TTIC', 'Microsoft', 'MIT', 'Microsoft Research Redmond', 'MIT']"
2020,Concentration bounds for CVaR estimation: The cases of light-tailed and heavy-tailed distributions,"Prashanth L.A., Krishna Jagannathan, Ravi Kolla",https://icml.cc/Conferences/2020/Schedule?showEvent=6104,"Conditional Value-at-Risk (CVaR) is a widely used risk metric in applications such as finance. We derive concentration bounds for CVaR estimates, considering separately the cases of sub-Gaussian, light-tailed and heavy-tailed distributions. For the sub-Gaussian and light-tailed cases, we use a classical CVaR estimator based on the empirical distribution constructed from the samples. For heavy-tailed random variables, we assume a mild `bounded moment' condition, and derive a concentration bound for a truncation-based estimator. Our concentration bounds exhibit exponential decay in the sample size, and are tighter than those available in the literature for the above distribution classes. To demonstrate the applicability of our concentration results, we consider the CVaR optimization problem in a multi-armed bandit setting. Specifically, we address the best CVaR-arm identification problem under a fixed budget. Using our CVaR concentration results, we derive an upper-bound on the probability of incorrect arm identification. 
","['IIT Madras', 'Indian Institute of Technology Madras', 'ABInBev']"
2020,Simple and Deep Graph Convolutional Networks,"Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, Yaliang Li",https://icml.cc/Conferences/2020/Schedule?showEvent=6111,"Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data.  Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks.  We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\em Initial residual} and {\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks.  
","['Renmin University of China', 'Renmin University of China', 'Fudan University', '""Data Analytics and Intelligence Lab, Alibaba Group""', 'Alibaba Group']"
2020,Self-Concordant Analysis of Frank-Wolfe Algorithms,"Pavel Dvurechenskii, Petr Ostroukhov, Kamil Safin, Shimrit Shtern, Mathias Staudigl",https://icml.cc/Conferences/2020/Schedule?showEvent=6125,"Projection-free optimization via different variants of the Frank-Wolfe (FW), a.k.a. Conditional Gradient method has become one of the cornerstones in optimization for machine learning since in many cases the linear minimization oracle is much cheaper to implement than projections and some sparsity needs to be preserved. In a number of applications, e.g. Poisson inverse problems or quantum state tomography, the loss is given by a self-concordant (SC) function having unbounded curvature, implying absence of theoretical guarantees for the existing FW methods. We use the theory of SC functions to provide a new adaptive step size for FW methods and prove global convergence rate O(1/k) after k iterations.
If the problem admits a stronger local linear minimization oracle, we construct a novel FW method with linear convergence rate for SC functions.
","['Weierstrass Institute', 'Moscow Institute of Physics and Technology', 'Moscow Institute of Physics and Technology', 'Technion - Israel Institute of Technology', 'Maastricht University']"
2020,Distributionally Robust Policy Evaluation and Learning in Offline Contextual Bandits,"Nian Si, Fan Zhang, Zhengyuan Zhou, Jose Blanchet",https://icml.cc/Conferences/2020/Schedule?showEvent=6197,"Policy learning using historical observational data is an important problem that has found widespread applications. However, existing literature rests on the crucial assumption that the future environment where the learned policy will be deployed is the same as the past environment that has generated the data–an assumption that is often false or too coarse an approximation. In this paper, we lift this assumption and aim to learn a distributionally robust policy with bandit observational data. We propose a novel learning algorithm that is able to learn a robust policy to adversarial perturbations and unknown covariate shifts. We first present a policy evaluation procedure in the ambiguous environment and also give a heuristic algorithm to solve the distributionally robust policy learning problems efficiently. Additionally, we provide extensive simulations to demonstrate the robustness of our policy.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
2020,What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?,"Chi Jin, Praneeth Netrapalli, Michael Jordan",https://icml.cc/Conferences/2020/Schedule?showEvent=6298,"Minimax optimization has found extensive applications in modern machine learning, in settings such as generative adversarial networks (GANs), adversarial training and multi-agent reinforcement learning. As most of these applications involve continuous nonconvex-nonconcave formulations, a very basic question arises---``what is a proper definition of local optima?''
Most previous work answers this question using classical notions of equilibria from simultaneous games, where the min-player and the max-player act simultaneously. In contrast, most applications in machine learning, including GANs and adversarial training, correspond to sequential games, where the order of which player acts first is crucial (since minimax is in general not equal to maximin due to the nonconvex-nonconcave nature of the problems). The main contribution of this paper is to propose a proper mathematical definition of local optimality for this sequential setting---local minimax, as well as to present its properties and existence results. Finally, we establish a strong connection to a basic local search algorithm---gradient descent ascent (GDA): under mild conditions, all stable limit points of GDA are exactly local minimax points up to some degenerate points.
","['Princeton University', 'Microsoft Research', 'UC Berkeley']"
2020,Searching to Exploit Memorization Effect in Learning with Noisy Labels,"QUANMING YAO, Hansi Yang, Bo Han, Gang Niu, James Kwok",https://icml.cc/Conferences/2020/Schedule?showEvent=6326,"Sample selection approaches are popular in robust learning from noisy labels. However, how to properly control the selection process so that deep networks can benefit from the memorization effect is a hard problem. In this paper, motivated by the success of automated machine learning (AutoML), we model this issue as a function approximation problem. 
Specifically, we design a domain-specific search space based on general patterns of the memorization effect and propose a novel Newton algorithm to solve the bi-level optimization problem efficiently.  We further provide a theoretical analysis of the algorithm, which ensures a good approximation to critical points. Experiments are performed on both benchmark and real-world data sets. Results demonstrate that the proposed method is much better than the state-of-the-art noisy-label-learning approaches, and also much more efficient than existing AutoML algorithms.
","['4Paradigm', 'Tsinghua', 'HKBU / RIKEN', 'RIKEN', 'Hong Kong University of Science and Technology']"
2020,Fast and Private Submodular and $k$-Submodular Functions Maximization with Matroid Constraints,"Akbar Rafiey, Yuichi Yoshida",https://icml.cc/Conferences/2020/Schedule?showEvent=6365,"The problem of maximizing nonnegative monotone submodular functions under a certain constraint has been intensively studied in the last decade, and a wide range of efficient approximation algorithms have been developed for this problem. Many machine learning problems, including data summarization and influence maximization, can be naturally modeled as the problem of maximizing monotone submodular functions. However, when such applications involve sensitive data about individuals, their privacy concerns should be addressed. In this paper, we study the problem of maximizing monotone submodular functions subject to matroid constraints in the framework of differential privacy. We provide $(1-\frac{1}{\mathrm{e}})$-approximation algorithm which improves upon the previous results in terms of approximation guarantee. This is done with an almost cubic number of function evaluations in our algorithm.

Moreover, we study $k$-submodularity, a natural generalization of submodularity. We give the first $\frac{1}{2}$-approximation algorithm that preserves differential privacy for maximizing monotone $k$-submodular functions subject to matroid constraints. The approximation ratio is asymptotically tight and is obtained with an almost linear number of function evaluations.","['Simon Fraser University', 'National Institute of Informatics']"
2020,Streaming Submodular Maximization under a k-Set System Constraint,"Ran Haba, Ehsan Kazemi, Moran Feldman, Amin Karbasi",https://icml.cc/Conferences/2020/Schedule?showEvent=5937,"In this paper, we propose a novel framework that converts streaming algorithms for monotone submodular maximization into streaming algorithms for non-monotone submodular maximization. This reduction readily leads to the currently tightest deterministic approximation ratio for submodular maximization subject to a $k$-matchoid constraint. Moreover, we propose the first streaming algorithm for monotone submodular maximization subject to $k$-extendible and $k$-set system constraints. Together with our proposed reduction, we obtain $O(k\log k)$ and $O(k^2\log k)$ approximation ratio for submodular maximization subject to the above constraints, respectively. We extensively evaluate the empirical performance of our algorithm against the existing work in a series of experiments including finding the maximum independent set in randomly generated graphs, maximizing linear functions over social networks, movie recommendation,  Yelp location summarization, and Twitter data summarization. ","['Open University of Israel', 'Google', 'University of Haifa', 'Yale']"
2020,Customizing ML Predictions for Online Algorithms,"Keerti Anand, Rong Ge, Debmalya Panigrahi",https://icml.cc/Conferences/2020/Schedule?showEvent=6621,"A popular line of recent research incorporates ML advice in the design of online algorithms to improve their performance in typical instances. These papers treat the ML algorithm as a black-box, and redesign online algorithms to take advantage of ML predictions. In this paper, we ask the complementary question: can we redesign ML algorithms to provide better predictions for online algorithms? We explore this question in the context of the classic rent-or-buy problem, and show that incorporating optimization benchmarks in ML loss functions leads to significantly better performance, while maintaining a worst-case adversarial result when the advice is completely wrong. We support this finding both through theoretical bounds and numerical simulations.
","['Duke University', 'Duke University', 'Duke University']"
2020,FetchSGD: Communication-Efficient Federated Learning with Sketching,"Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman, Joseph E Gonzalez, Raman Arora",https://icml.cc/Conferences/2020/Schedule?showEvent=6724,"Existing approaches to federated learning suffer from a communication bottleneck as well as convergence issues due to sparse client participation.
In this paper we introduce a novel algorithm, called FetchSGD, to overcome these challenges.
FetchSGD compresses model updates using a Count Sketch, and then takes advantage of the mergeability of sketches to combine model updates from many workers.
A key insight in the design of FetchSGD is that, because the Count Sketch is linear, momentum and error accumulation can both be carried out within the sketch.
This allows the algorithm to move momentum and error accumulation from clients to the central aggregator, overcoming the challenges of sparse client participation while still achieving high compression rates and good convergence.
We prove that FetchSGD has favorable convergence guarantees, and we demonstrate its empirical effectiveness by training two residual networks and a transformer model.
","['UC Berkeley', 'UC Berkeley', 'Johns Hopkins University', 'Amazon', 'UC Berkeley', 'Johns Hopkins University', 'UC Berkeley', 'Johns Hopkins University']"
2020,FedBoost: A Communication-Efficient Algorithm for Federated Learning,"Jenny Hamer, Mehryar Mohri, Ananda Theertha Suresh",https://icml.cc/Conferences/2020/Schedule?showEvent=6733,"Communication cost is often a bottleneck in federated learning and other client-based distributed learning scenarios. To overcome this, several gradient compression and model compression algorithms have been proposed. In this work, we propose an alternative approach whereby an ensemble of pre-trained base predictors is trained via federated learning. This method allows for training a model which may otherwise surpass the communication bandwidth and storage capacity of the clients to be learned with on-device data through federated learning. Motivated by language modeling, we prove the optimality of ensemble methods for density estimation for standard empirical risk minimization and agnostic risk minimization. We provide communication-efficient ensemble algorithms for federated learning, where per-round communication cost is independent of the size of the ensemble. Furthermore, unlike works on gradient compression, our proposed approach reduces the communication cost of both server-to-client and client-to-server communication.
","['Google Research', 'Google Research and Courant Institute of Mathematical Sciences', 'Google Research']"
2020,Data Amplification: Instance-Optimal Property Estimation ,"Yi Hao, Alon Orlitsky",https://icml.cc/Conferences/2020/Schedule?showEvent=6805,"The best-known and most commonly used technique for distribution-property estimation uses a plug-in estimator, with empirical frequency replacing the underlying distribution. We present novel linear-time-computable estimators that significantly ``amplify'' the effective amount of data available. For a large variety of distribution properties including four of the most popular ones and for every underlying distribution, they achieve the accuracy that the empirical-frequency plug-in estimators would attain using a logarithmic-factor more samples. Specifically, for Shannon entropy and a broad class of Lipschitz properties including the $L_1$ distance to a fixed distribution, the new estimators use $n$ samples to achieve the accuracy attained by the empirical estimators with $n\log n$ samples. For support-size and coverage, the new estimators use $n$ samples to achieve the performance of empirical frequency with sample size $n$ times the logarithm of the property value. Significantly strengthening the traditional min-max formulation, these results hold not only for the worst distributions, but for each and every underlying distribution. Furthermore, the logarithmic amplification factors are optimal. Experiments on a wide variety of distributions show that the new estimators outperform the previous state-of-the-art estimators designed for each specific property.  ","['University of California, San Diego', 'UCSD']"
2020,ControlVAE: Controllable Variational Autoencoder,"Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang, Tarek Abdelzaher",https://icml.cc/Conferences/2020/Schedule?showEvent=6510,"Variational Autoencoders (VAE) and their variants have been widely used in a variety of applications, such as dialog generation, image generation and disentangled representation learning. However, the existing VAE models may suffer from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller, inspired by automatic control theory, with the basic VAE to improve the performance of resulting generative models. Specifically, we design a new non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically tune the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. The framework is evaluated using three applications; namely, language modeling, disentangled representation learning, and image generation. The results show that ControlVAE can achieve much better reconstruction quality than the competitive methods for the comparable disentanglement performance. For language modelling, it not only averts the KL-vanishing, but also improves the diversity of generated text. Finally, we also demonstrate that ControlVAE improves the reconstruction quality for image generation compared to the original VAE.
","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'AWS AI', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'Alibaba Group', 'University of Illinois at Urbana-Champaign']"
2020,Two Routes to Scalable Credit Assignment without Weight Symmetry,"Daniel Kunin, Aran Nayebi, Javier Sagastuy-Brena, Surya Ganguli, Jonathan Bloom, Daniel Yamins",https://icml.cc/Conferences/2020/Schedule?showEvent=6706,"The neural plausibility of backpropagation has long been disputed, primarily for its use of non-local weight transport --- the biologically dubious requirement that one neuron instantaneously measure the synaptic weights of another. Until recently, attempts to create local learning rules that avoid weight transport have typically failed in the large-scale learning scenarios where backpropagation shines, e.g. ImageNet categorization with deep convolutional networks. Here, we investigate a recently proposed local learning rule that yields competitive performance with backpropagation and find that it is highly sensitive to metaparameter choices, requiring laborious tuning that does not transfer across network architecture. Our analysis indicates the underlying mathematical reason for this instability, allowing us to identify a more robust local learning rule that better transfers without metaparameter tuning. Nonetheless, we find a performance and stability gap between this local rule and backpropagation that widens with increasing model depth. We then investigate several non-local learning rules that relax the need for instantaneous weight transport into a more biologically-plausible ""weight estimation"" process, showing that these rules match state-of-the-art performance on deep networks and operate effectively in the presence of noisy updates. Taken together, our results suggest two routes towards the discovery of neural implementations for credit assignment without weight symmetry: further improvement of local rules so that they perform consistently across architectures and the identification of biological implementations for non-local learning mechanisms.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford', 'Cellarity', 'Stanford University']"
2020,On the Generalization Effects of Linear Transformations in Data Augmentation,"Sen Wu, Hongyang Zhang, Gregory Valiant, Christopher Re",https://icml.cc/Conferences/2020/Schedule?showEvent=6089,"Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations which preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations which mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how \textit{uncertain} the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms RandAugment by 1.24\% on CIFAR-100 using Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA Adversarial AutoAugment on CIFAR datasets.
","['Stanford University', 'University of Pennsylvania', 'Stanford University', 'Stanford']"
2020,Deep Divergence Learning,"Kubra Cilingir, Rachel Manzelli, Brian Kulis",https://icml.cc/Conferences/2020/Schedule?showEvent=6500,"Classical linear metric learning methods have recently been extended along two distinct lines: deep metric learning methods for learning embeddings of the data using neural networks, and Bregman divergence learning approaches for extending learning Euclidean distances to more general divergence measures such as divergences over distributions.  In this paper, we introduce deep Bregman divergences, which are based on learning and parameterizing functional Bregman divergences using neural networks, and which unify and extend these existing lines of work.  We show in particular how deep metric learning formulations, kernel metric learning, Mahalanobis metric learning, and moment-matching functions for comparing distributions arise as special cases of these divergences in the symmetric setting.  We then describe a deep learning framework for learning general functional Bregman divergences, and show in experiments that this method yields superior performance on benchmark datasets as compared to existing deep metric learning approaches.  We also discuss novel applications, including a semi-supervised distributional clustering problem, and a new loss function for unsupervised data generation.
","['Boston University', 'Boston University', 'Boston University']"
2020,Generative Flows with Matrix Exponential,"Changyi Xiao, Ligang Liu",https://icml.cc/Conferences/2020/Schedule?showEvent=6123,"Generative flows models enjoy the properties of tractable exact likelihood and efficient sampling, which are composed of a sequence of invertible functions. In this paper, we incorporate matrix exponential into generative flows. Matrix exponential is a map from matrices to invertible matrices, this property is suitable for generative flows. Based on matrix exponential, we propose matrix exponential coupling layers that are a general case of affine coupling layers and matrix exponential invertible 1 x 1 convolutions that do not collapse during training. And we modify the networks architecture to make training stable and significantly speed up the training process. Our experiments show that our model achieves great performance on density estimation amongst generative flows models. 
","['University of Science and Technology of China', 'University of Science and Technology of China']"
2020,MoNet3D: Towards Accurate Monocular 3D Object Localization in Real Time,"XICHUAN ZHOU, YiCong Peng, Chunqiao Long, Fengbo Ren, Cong Shi",https://icml.cc/Conferences/2020/Schedule?showEvent=5865,"Monocular multi-object detection and localization in 3D space has been proven to be a challenging task. The MoNet3D algorithm is a novel and effective framework that can predict the 3D position of each object in a monocular image, and draw a 3D bounding box on each object. The MoNet3D method incorporates the prior knowledge of spatial geometric correlation of neighboring objects into the deep neural network training process, in order to improve the accuracy of 3D object localization. Experiments over the KITTI data set show that the accuracy of predicting the depth and horizontal coordinate of the object in 3D space can reach 96.25% and 94.74%,  respectively. Meanwhile, the method can realize the real-time image processing capability of 27.85 FPS. Our code is publicly available at https://github.com/CQUlearningsystemgroup/YicongPeng
","['Chongqing University', 'Chongqing University', 'Chongqing University', 'Arizona State University', 'Chongqing University']"
2020,Training Binary Neural Networks through Learning with Noisy Supervision,"Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, Chang Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=5791,"This paper formalizes the binarization operations over neural networks from a learning perspective. In contrast to classical hand crafted rules (\eg hard thresholding) to binarize full-precision neurons, we propose to learn a mapping from full-precision neurons to the target binary ones. Each individual weight entry will not be binarized independently. Instead, they are taken as a whole to accomplish the binarization, just as they work together in generating convolution features. To help the training of the binarization mapping, the full-precision neurons after taking sign operations is regarded as some auxiliary supervision signal, which is noisy but still has valuable guidance.  An unbiased estimator is therefore introduced to mitigate the influence of the supervision noise. Experimental results on benchmark datasets indicate that the proposed binarization technique attains consistent improvements over baselines.
","['Noah’s Ark Lab, Huawei Technologies', ""Noah's Ark Lab, Huawei Technologies."", 'Huawei Technologies', ""Huawei Noah's Ark Lab"", 'CAS', 'University of Sydney']"
2020,"Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study","Tanner Fiez, Benjamin Chasnov, Lillian Ratliff",https://icml.cc/Conferences/2020/Schedule?showEvent=6403,"Contemporary work on learning in continuous games has commonly overlooked the hierarchical decision-making structure present in machine learning problems formulated as games, instead treating them as simultaneous play games and adopting the Nash equilibrium solution concept. We deviate from this paradigm and provide a comprehensive study of learning in Stackelberg games. This work provides insights into the optimization landscape of zero-sum games by establishing connections between Nash and Stackelberg equilibria along with the limit points of simultaneous gradient descent. We derive novel gradient-based learning dynamics emulating the natural structure of a Stackelberg game using the implicit function theorem and provide convergence analysis for deterministic and stochastic updates for zero-sum and general-sum games. Notably, in zero-sum games using deterministic updates, we show the only critical points the dynamics converge to are Stackelberg equilibria and provide a local convergence rate. Empirically, our learning dynamics mitigate rotational behavior and exhibit benefits for training generative adversarial networks compared to simultaneous gradient descent. 
","['University of Washington', 'University of Washington', 'University of Washington']"
2020,What Can Learned Intrinsic Rewards Capture?,"Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van Hasselt, David Silver, Satinder Singh",https://icml.cc/Conferences/2020/Schedule?showEvent=6150,"The objective of a reinforcement learning agent is to behave so as to maximise the sum of a suitable scalar function of state: the reward. These rewards are typically given and immutable. In this paper, we instead consider the proposition that the reward function itself can be a good locus of learned knowledge. To investigate this, we propose a scalable meta-gradient framework for learning useful intrinsic reward functions across multiple lifetimes of experience. Through several proof-of-concept experiments, we show that it is feasible to learn and capture knowledge about long-term exploration and exploitation into a reward function. Furthermore, we show that unlike policy transfer methods that capture how'' the agent should behave, the learned reward functions can generalise to other kinds of agents and to changes in the dynamics of the environment by capturingwhat'' the agent should strive to do.
","['University of Michigan', 'DeepMind', 'Deep Mind', 'DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind']"
2020,Learning with Multiple Complementary Labels,"LEI FENG, Takuo Kaneko, Bo Han, Gang Niu, Bo An, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6084,"A complementary label (CL) simply indicates an incorrect class of an example, but learning with CLs results in multi-class classifiers that can predict the correct class. Unfortunately, the problem setting only allows a single CL for each example, which notably limits its potential since our labelers may easily identify multiple CLs (MCLs) to one example. In this paper, we propose a novel problem setting to allow MCLs for each example and two ways for learning with MCLs. In the first way, we design two wrappers that decompose MCLs into many single CLs, so that we could use any method for learning with CLs. However, the supervision information that MCLs hold is conceptually diluted after decomposition. Thus, in the second way, we derive an unbiased risk estimator; minimizing it processes each set of MCLs as a whole and possesses an estimation error bound. We further improve the second way into minimizing properly chosen upper bounds. Experiments show that the former way works well for learning with MCLs but the latter is even better.
","['Nanyang Technological University', 'The University of Tokyo', 'HKBU / RIKEN', 'RIKEN', 'Nanyang Technological University', 'RIKEN / The University of Tokyo']"
2020,Semiparametric Nonlinear Bipartite Graph Representation Learning with Provable Guarantees,"Sen Na, Yuwei Luo, Zhuoran Yang, Zhaoran Wang, Mladen Kolar",https://icml.cc/Conferences/2020/Schedule?showEvent=6181,"Graph representation learning is a ubiquitous task in machine learning where the goal is to embed each vertex into a low-dimensional vector space. We consider the bipartite graph and formalize its representation learning problem as a statistical estimation problem of parameters in a semiparametric exponential family distribution: the bipartite graph is assumed to be generated by a semiparametric exponential family distribution, whose parametric component is given by the proximity of outputs of two one-layer neural networks that take high-dimensional features as inputs, while nonparametric (nuisance) component is the base measure.  In this setting, the representation learning problem is equivalent to recovering the weight matrices, and the main challenges of estimation arise from the nonlinearity of activation functions and the nonparametric nuisance component of the distribution. To overcome these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and show that the proposed objective is strongly convex in a neighborhood around the ground truth, so that a gradient descent-based method achieves linear convergence rate. Moreover, we prove that the sample complexity of the problem is linear in dimensions (up to logarithmic factors), which is consistent with parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, which is validated in extensive experiments.
","['The University of Chicago', 'The University of Chicago', 'Princeton University', 'Northwestern U', 'University of Chicago Booth School of Business']"
2020,Learning from Irregularly-Sampled Time Series: A Missing Data Perspective,"Steven Cheng-Xian Li, Benjamin M Marlin",https://icml.cc/Conferences/2020/Schedule?showEvent=6300,"Irregularly-sampled time series occur in many domains including healthcare. They can be challenging to model because they do not naturally yield a fixed-dimensional representation as required by many standard machine learning models. In this paper, we consider irregular sampling from the perspective of missing data. We model observed irregularly-sampled time series data as a sequence of index-value pairs sampled from a continuous but unobserved function. We introduce an encoder-decoder framework for learning from such generic indexed sequences. We propose learning methods for this framework based on variational autoencoders and generative adversarial networks. For continuous irregularly-sampled time series, we introduce continuous convolutional layers that can efficiently interface with existing neural network architectures. Experiments show that our models are able to achieve competitive or better classification results on irregularly-sampled multivariate time series compared to recent RNN models while offering significantly faster training times.
","['University of Massachusetts Amherst', 'University of Massachusetts Amherst']"
2020,The Effect of Natural Distribution Shift on Question Answering Models,"John Miller, Karl Krauth, Benjamin Recht, Ludwig Schmidt",https://icml.cc/Conferences/2020/Schedule?showEvent=6341,"We build four new test sets for the Stanford Question Answering Dataset (SQuAD)
and evaluate the ability of question-answering systems to generalize to new
data. Our first test set is from the original Wikipedia domain and measures
the extent to which existing systems overfit the original test set. Despite
several years of heavy test set re-use, we find no evidence of adaptive
overfitting.  The remaining three test sets are constructed from New York
Times articles, Reddit posts, and Amazon product reviews and measure
robustness to natural distribution shifts. Across a broad range of models,
we observe average performance drops of 3.8, 14.0, and
17.4 F1 points, respectively. In contrast, a strong human baseline
matches or exceeds the performance of SQuAD models on the original domain
and exhibits little to no drop in new domains. Taken together, our results
confirm the surprising resilience of the holdout method and emphasize the
need to move towards evaluation metrics that incorporate robustness to natural
distribution shifts.
","['University of California, Berkeley', 'UC Berkeley', 'Berkeley', 'University of California, Berkeley']"
2020,A Free-Energy Principle for Representation Learning,"Yansong Gao, Pratik Chaudhari",https://icml.cc/Conferences/2020/Schedule?showEvent=6400,"This paper employs a formal connection of machine learning with thermodynamics to characterize the quality of learnt representations for transfer learning. We discuss how information-theoretic functionals such as rate, distortion and classification loss of a model lie on a convex, so-called equilibrium surface. We prescribe dynamical processes to traverse this surface under constraints, e.g., an iso-classification process that trades off rate and distortion to keep the classification loss unchanged. We demonstrate how this process can be used for transferring representations from a source dataset to a target dataset while keeping the classification loss constant. Experimental validation of the theoretical results is provided on standard image-classification datasets.
","['University of Pennsylvania', 'University of Pennsylvania']"
2020,Manifold Identification for Ultimately Communication-Efficient Distributed Optimization,"Yu-Sheng Li, Wei-Lin Chiang, Ching-pei Lee",https://icml.cc/Conferences/2020/Schedule?showEvent=6360,"This work proposes a progressive manifold identification approach for
distributed optimization with sound theoretical justifications to
greatly reduce both the rounds of communication and the bytes
communicated per round for partly-smooth regularized problems such as
the L1- and group-LASSO-regularized ones.
Our two-stage method first uses an inexact proximal quasi-Newton
method to iteratively identify a sequence of low-dimensional manifolds
in which the final solution would lie, and restricts the model update
within the current manifold to gradually lower the order of the
per-round communication cost from the problem dimension to the
dimension of the manifold that contains a solution and makes the
problem within it smooth.
After identifying this manifold, we take superlinear-convergent
truncated semismooth Newton steps computed by preconditioned conjugate
gradient to largely reduce the communication rounds by improving the
convergence rate from the existing linear or sublinear ones to a
superlinear rate.
Experiments show that our method can be two orders of magnitude better in
the communication cost and an order of magnitude faster in the running time
than state of the art.
","['National Taiwan University', 'National Taiwan University', 'National University of Singapore']"
2020,Contrastive Multi-View Representation Learning on Graphs,"Kaveh Hassani, Amir Hosein Khasahmadi",https://icml.cc/Conferences/2020/Schedule?showEvent=6085,"We introduce a self-supervised approach for learning node and graph level 
representations by contrasting structural views of graphs. We show that unlike 
visual representation learning, increasing the number of views to more than two or 
contrasting multi-scale encodings do not improve performance, and the best 
performance is achieved by contrasting encodings from first-order neighbors and 
a graph diffusion. We achieve new state-of-the-art results in self-supervised 
learning on 8 out of 8 node and graph classification benchmarks under the linear 
evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) 
classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 
5.5% and 2.4% relative improvements over previous state-of-the-art. When 
compared to supervised baselines, our approach outperforms them in 4 out of 8 
benchmarks.
","['Autodesk', 'University of Toronto']"
2020,Differentiating through the Fréchet Mean,"Aaron Lou, Isay Katsman, Qingxuan Jiang, Serge Belongie, Ser Nam Lim, Christopher De Sa",https://icml.cc/Conferences/2020/Schedule?showEvent=6073,"Recent advances in deep representation learning on Riemannian manifolds extend classical deep learning operations to better capture the geometry of the manifold. One possible extension is the Fréchet mean, the generalization of the Euclidean mean; however, it has been difficult to apply because it lacks a closed form with an easily computable derivative. In this paper, we show how to differentiate through the Fréchet mean for arbitrary Riemannian manifolds. Then, focusing on hyperbolic space, we derive explicit gradient expressions and a fast, accurate, and hyperparameter-free Fréchet mean solver. This fully integrates the Fréchet mean into the hyperbolic neural network pipeline. To demonstrate this integration, we present two case studies. First, we apply our Fréchet mean to the existing Hyperbolic Graph Convolutional Network, replacing its projected aggregation to obtain state-of-the-art results on datasets with high hyperbolicity. Second, to demonstrate the Fréchet mean's capacity to generalize Euclidean neural network operations, we develop a hyperbolic batch normalization method that gives an improvement parallel to the one observed in the Euclidean setting.
","['Cornell University', 'Cornell University', 'Cornell University', 'Cornell University', 'Facebook', 'Cornell']"
2020,Loss Function Search for Face Recognition,"Xiaobo Wang, Shuo Wang, Cheng Chi, Shifeng Zhang, Tao Mei",https://icml.cc/Conferences/2020/Schedule?showEvent=5802,"In face recognition, designing margin-based (\textit{e.g.}, angular, additive, additive angular margins) softmax loss functions plays an important role to learn discriminative features. However, these hand-crafted heuristic methods may be sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.
","['JD AI Research', 'JD AI Research', 'University of Chinese Academy of Sciences', 'CBSR, NLPR, CASIA', 'AI Research of JD.com']"
2020,Characterizing Distribution Equivalence and Structure Learning for Cyclic and Acyclic Directed Graphs,"AmirEmad Ghassami, Alan Yang, Negar Kiyavash, Kun Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6462,"The main approach to defining equivalence among acyclic directed causal graphical models is based on the conditional independence relationships in the distributions that the causal models can generate, in terms of the Markov equivalence. However, it is known that when cycles are allowed in the causal structure, conditional independence may not be a suitable notion for equivalence of two structures, as it does not reflect all the information in the distribution that is useful for identification of the underlying structure. In this paper, we present a general, unified notion of equivalence for linear Gaussian causal directed graphical models, whether they are cyclic or acyclic. In our proposed definition of equivalence, two structures are equivalent if they can generate the same set of data distributions. We also propose a weaker notion of equivalence called quasi-equivalence, which we show is the extent of identifiability from observational data. We propose analytic as well as graphical methods for characterizing the equivalence of two structures. Additionally, we propose a score-based method for learning the structure from observational data, which successfully deals with both acyclic and cyclic structures.
","['UIUC', 'University of Illinois at Urbana-Champaign', 'École Polytechnique Fédérale de Lausanne', 'Carnegie Mellon University']"
2020,On Second-Order Group Influence Functions for Black-Box Predictions,"Samyadeep Basu, Xuchen You, Soheil Feizi",https://icml.cc/Conferences/2020/Schedule?showEvent=6464,"With the rapid adoption of machine learning systems in sensitive applications, there is an increasing need to make black-box models explainable. Often we want to identify an influential group of training samples in a particular test prediction for a given machine learning model. Existing influence functions tackle this problem by using first-order approximations of the effect of removing a sample from the training set on model parameters. To compute the influence of a group of training samples (rather than an individual point) in model predictions, the change in optimal model parameters after removing that group from the training set can be large. Thus, in such cases, the first-order approximation can be loose. In this paper, we address this issue and propose second-order influence functions for identifying influential groups in test-time predictions. For linear models, across different sizes and types of groups, we show that using the proposed second-order influence function improves the correlation between the computed influence values and the ground truth ones. We also show that second-order influence functions could be used with optimization techniques to improve the selection of the most influential group for a test-sample.
","['UMD', 'University of Maryland', 'University of Maryland']"
2020,Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting,"Niccolo Dalmasso, Rafael Izbicki, Ann Lee",https://icml.cc/Conferences/2020/Schedule?showEvent=6469,"Parameter estimation, statistical tests and conﬁdence sets are the cornerstones of classical statistics that allow scientists to make inferences about the underlying process that generated the observed data. A key question is whether one can still construct hypothesis tests and conﬁdence sets with proper coverage and high power in a so-called likelihood-free inference (LFI) setting; that is, a setting where the likelihood is not explicitly known but one can forward-simulate observable data according to a stochastic model. In this paper, we present ACORE (Approximate Computation via Odds Ratio Estimation), a frequentist approach to LFI that ﬁrst formulates the classical likelihood ratio test (LRT) as a parametrized classiﬁcation problem, and then uses the equivalence of tests and conﬁdence sets to build conﬁdence regions for parameters of interest. We also present a goodness-of-ﬁt procedure for checking whether the constructed tests and conﬁdence regions are valid. ACORE is based on the key observation that the LRT statistic, the rejection probability of the test, and the coverage of the conﬁdence set are conditional distribution functions which often vary smoothly as a function of the parameters of interest. Hence, instead of relying solely on samples simulated at ﬁxed parameter settings (as is the convention in standard Monte Carlo solutions), one can leverage machine learning tools and data simulated in the neighborhood of a parameter to improve estimates of quantities of interest. We demonstrate the efﬁcacy of ACORE with both theoretical and empirical results. Our implementation is available on Github.
","['Carnegie Mellon University', 'UFSCar', 'Carnegie Mellon University']"
2020,Adversarial Learning Guarantees for Linear Hypotheses and Neural Networks,"Pranjal Awasthi, Natalie Frank, Mehryar Mohri",https://icml.cc/Conferences/2020/Schedule?showEvent=6477,"  Adversarial or test time robustness measures the susceptibility of a
  classifier to perturbations to the test input. While there has been
  a flurry of recent work on designing defenses against such
  perturbations, the theory of adversarial robustness is not well
  understood. In order to make progress on this, we focus on the
  problem of understanding generalization in adversarial settings, via
  the lens of Rademacher complexity. We give upper and lower bounds for the adversarial empirical
  Rademacher complexity of linear hypotheses with adversarial
  perturbations measured in $l_r$-norm for an arbitrary $r \geq
  1$. 
  We then extend our analysis to provide Rademacher complexity lower and
  upper bounds for a single ReLU unit. Finally, we give adversarial
  Rademacher complexity bounds for feed-forward neural networks with
  one hidden layer. ","['Rutgers University', 'NYU', 'Google Research and Courant Institute of Mathematical Sciences']"
2020,Fast Learning of Graph Neural Networks with Guaranteed Generalizability: One-hidden-layer Case,"shuai zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong",https://icml.cc/Conferences/2020/Schedule?showEvent=6527,"Although graph neural networks (GNNs) have made great progress recently on learning from graph-structured data in practice, their theoretical guarantee on generalizability remains elusive in the literature. In this paper, we provide a theoretically-grounded generalizability analysis of GNNs with one hidden layer for both regression and binary classification problems. Under the assumption that there exists a ground-truth GNN model (with zero generalization error), the objective of GNN learning is to estimate the ground-truth GNN parameters from the training data. To achieve this objective, we propose a learning algorithm that is built on tensor initialization and accelerated gradient descent. We then show that the proposed learning algorithm converges to the ground-truth GNN model for the regression problem, and to a model sufficiently close to the ground-truth for the binary classification problem. Moreover, for both cases, the convergence rate of the proposed learning algorithm is proven to be linear and faster than the vanilla gradient descent algorithm. We further explore the relationship between the sample complexity of GNNs and their underlying graph properties. Lastly, we provide numerical experiments to demonstrate the validity of our analysis and the effectiveness of the proposed learning algorithm for GNNs.
","['Rensselaer Polytechnic Institute', 'Rensselaer Polytechnic Institute', 'MIT-IBM Watson AI Lab', 'IBM Research AI', 'IBM Thomas J. Watson Research Center']"
2020,Improving Generative Imagination in Object-Centric World Models,"Zhixuan Lin, Yi-Fu Wu, Skand Peri, Bofeng Fu, Jindong Jiang, Sungjin Ahn",https://icml.cc/Conferences/2020/Schedule?showEvent=6582,"The remarkable recent advances in object-centric generative world models raise a few questions. First, while many of the recent achievements are indispensable for making a general and versatile world model, it is quite unclear how these ingredients can be integrated into a unified framework. Second, despite using generative objectives, abilities for object detection and tracking are mainly investigated, leaving the crucial ability of temporal imagination largely under question. Third, a few key abilities for more faithful temporal imagination such as multimodal uncertainty and situation-awareness are missing. In this paper, we introduce Generative Structured World Models (G-SWM). The G-SWM achieves the versatile world modeling not only by unifying the key properties of previous models in a principled framework but also by achieving two crucial new abilities, multimodal uncertainty and situation-awareness. Our thorough investigation on the temporal generation ability in comparison to the previous models demonstrates that G-SWM achieves the versatility with the best or comparable performance for all experiment settings including a few complex settings that have not been tested before. https://sites.google.com/view/gswm
","['Zhejiang University', 'Rutgers University', 'Rutgers University, New Jersey', 'Tianjin University', 'Rutgers University', 'Rutgers University']"
2020,Familywise Error Rate Control by Interactive Unmasking,"Boyan Duan, Aaditya Ramdas, Larry Wasserman",https://icml.cc/Conferences/2020/Schedule?showEvent=6595,"We propose a method for multiple hypothesis testing with familywise error rate (FWER) control, called the i-FWER test. Most testing methods are predefined algorithms that do not allow modifications after observing the data. However, in practice, analysts tend to choose a promising algorithm after observing the data; unfortunately, this violates the validity of the conclusion. The i-FWER test allows much flexibility: a human (or a computer program acting on the human's behalf) may adaptively guide the algorithm in a data-dependent manner. We prove that our test controls FWER if the analysts adhere to a particular protocol of masking and unmasking. We demonstrate via numerical experiments the power of our test under structured non-nulls, and then explore new forms of masking.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2020,Evaluating Lossy Compression Rates of Deep Generative Models,"Sicong Huang, Alireza Makhzani, Yanshuai Cao, Roger Grosse",https://icml.cc/Conferences/2020/Schedule?showEvent=6602,"The field of deep generative modeling has succeeded in producing astonishingly realistic-seeming images and audio, but quantitative evaluation remains a challenge. Log-likelihood is an appealing metric due to its grounding in statistics and information theory, but it can be challenging to estimate for implicit generative models, and scalar-valued metrics give an incomplete picture of a model's quality. In this work, we propose to use rate distortion (RD) curves to evaluate and compare deep generative models. While estimating RD curves is seemingly even more computationally demanding than log-likelihood estimation, we show that we can approximate the entire RD curve using nearly the same computations as were previously used to achieve a single log-likelihood estimate. We evaluate lossy compression rates of VAEs, GANs, and adversarial autoencoders (AAEs) on the MNIST and CIFAR10 datasets. Measuring the entire RD curve gives a more complete picture than scalar-valued metrics, and we arrive at a number of insights not obtainable from log-likelihoods alone.
","['University of Toronto', 'University of Toronto', 'Borealis AI', 'University of Toronto and Vector Institute']"
2020,Causal Strategic Linear Regression,"Yonadav Shavit, Benjamin Edelman, Brian Axelrod",https://icml.cc/Conferences/2020/Schedule?showEvent=6654,"In many predictive decision-making scenarios, such as credit scoring and academic testing, a decision-maker must construct a model that accounts for agents' incentives to ``game'' by changing their features to receive better decisions. Whereas the strategic classification literature has previously assumed that agents' outcomes are not causally dependent on their features (and thus strategic behavior is a form of lying), we join concurrent work in modeling agents' outcomes as a function of their changeable attributes. Our work introduces the realizable linear regression setting, and is the first to incorporate a crucial phenomenon: when agents act to change observable features, they may as a side effect perturb hidden features that causally affect their true outcomes. As our main contribution, we provide the efficient algorithms for optimizing three distinct decision-making objectives: accurately predicting agents' post-gaming outcomes (prediction risk minimization), incentivizing agents to improve these outcomes (agent outcome maximization), and estimating the coefficients of the true underlying model (parameter estimation). 
Our algorithms circumvent the hardness result of Miller et al. (2020) by allowing the decision maker to test a sequence of decision rules and observe agents' responses, in effect performing causal interventions by varying the chosen rule.
","['Harvard University', 'Harvard University', 'Stanford']"
2020,Randomized Smoothing of All Shapes and Sizes,"Greg Yang, Tony Duan, J. Edward Hu, Hadi Salman, Ilya Razenshteyn, Jerry Li",https://icml.cc/Conferences/2020/Schedule?showEvent=6327,"Randomized smoothing is the current state-of-the-art defense with provable robustness against $\ell_2$ adversarial attacks. Many works have devised new randomized smoothing schemes for other metrics, such as $\ell_1$ or $\ell_\infty$; however, substantial effort was needed to derive such new guarantees. This begs the question: can we find a general theory for randomized smoothing?

We propose a novel framework for devising and analyzing randomized smoothing schemes, and validate its effectiveness in practice. Our theoretical contributions are: (1) we show that for an appropriate notion of ""optimal"", the optimal smoothing distributions for any ""nice"" norms have level sets given by the norm's *Wulff Crystal*; (2) we propose two novel and complementary methods for deriving provably robust radii for any smoothing distribution; and, (3) we show fundamental limits to current randomized smoothing techniques via the theory of *Banach space cotypes*. By combining (1) and (2), we significantly improve the state-of-the-art certified accuracy in $\ell_1$ on standard datasets. Meanwhile, we show using (3) that with only label statistics under random input perturbations, randomized smoothing cannot achieve nontrivial certified accuracy against perturbations of $\ell_p$-norm $\Omega(\min(1, d^{\frac{1}{p} - \frac{1}{2}}))$, when the input dimension $d$ is large. We provide code in github.com/tonyduan/rs4a.","['Microsoft Research', 'Microsoft Research', 'Microsoft Research AI', 'Microsoft Research', 'Microsoft Research, Redmond', 'Microsoft']"
2020,Mutual Transfer Learning for Massive Data,"Ching-Wei Cheng, Xingye Qiao, Guang Cheng",https://icml.cc/Conferences/2020/Schedule?showEvent=6661,"In the transfer learning problem, the target and the source data domains are typically known. In this article, we study a new paradigm called mutual transfer learning where among many heterogeneous data domains, every data domain could potentially be the target of interest, and it could also be a useful source to help the learning in other data domains. However, it is important to note that given a target not every data domain can be a successful source; only data sets that are similar enough to be thought as from the same population can be useful sources for each other. Under this mutual learnability assumption, a confidence distribution fusion approach is proposed to recover the mutual learnability relation in the transfer learning regime. Our proposed method achieves the same oracle statistical inferential accuracy as if the true learnability structure were known. It can be implemented in an efficient parallel fashion to deal with large-scale data. Simulated and real examples are analyzed to illustrate the usefulness of the proposed method.
","['Purdue University', 'Binghamton University', 'Purdue University']"
2020,The Buckley-Osthus model and the block preferential attachment model: statistical analysis and application,"Wenpin Tang, Xin Guo, Fengmin Tang",https://icml.cc/Conferences/2020/Schedule?showEvent=6665,"This paper is concerned with statistical estimation of two preferential attachment models: the Buckley-Osthus model and the block preferential attachment model. We prove that the maximum likelihood estimates for both models are consistent. We perform simulation studies to corroborate our theoretical findings. We also apply both models to study the evolution of a real-world network. A list of open problems are presented.
","['UC Berkeley', 'University of California, Berkeley', 'UCLA']"
2020,On Variational Learning of Controllable Representations for Text without Supervision,"Peng Xu, Jackie Chi Kit Cheung, Yanshuai Cao",https://icml.cc/Conferences/2020/Schedule?showEvent=6704,"The variational autoencoder (VAE) can learn the manifold of natural images on certain datasets, as evidenced by meaningful interpolating or extrapolating in the continuous latent space. However, on discrete data such as text, it is unclear if unsupervised learning can discover similar latent space that allows controllable manipulation. In this work, we find that sequence VAEs trained on text fail to properly decode when the latent codes are manipulated, because the modified codes often land in holes or vacant regions in the aggregated posterior latent space, where the decoding network fails to generalize. Both as a validation of the explanation and as a fix to the problem, we propose to constrain the posterior mean to a learned probability simplex, and performs manipulation within this simplex. Our proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, our method outperforms unsupervised baselines and strong supervised approaches on text style transfer. On automatic evaluation metrics used in text style transfer, even with the decoding network trained from scratch, our method achieves comparable results with state-of-the-art supervised approaches leveraging large-scale pre-trained models for generation. Furthermore, it is capable of performing more flexible fine-grained control over text generation than existing methods.
","['Borealis AI', 'McGill University / Mila', 'Borealis AI']"
2020,GradientDICE: Rethinking Generalized Offline Estimation of Stationary Values,"Shangtong Zhang, Bo Liu, Shimon Whiteson",https://icml.cc/Conferences/2020/Schedule?showEvent=5836,"We present GradientDICE for estimating the density ratio between the state distribution of the target policy and the sampling distribution in off-policy reinforcement learning.
GradientDICE fixes several problems of GenDICE (Zhang et al., 2020), the current state-of-the-art for estimating such density ratios. 
Namely, the optimization problem in GenDICE is not a convex-concave saddle-point problem once nonlinearity in optimization variable parameterization is introduced to ensure positivity, 
so primal-dual algorithms are not guaranteed to find the desired solution. 
However, such nonlinearity is essential to ensure the consistency of GenDICE even with a tabular representation.
This is a fundamental contradiction,
resulting from GenDICE's original formulation of the optimization problem.
In GradientDICE, we optimize a different objective from GenDICE
by using the Perron-Frobenius theorem and eliminating GenDICE's use of divergence,
such that nonlinearity in parameterization is not necessary for GradientDICE, 
which is provably convergent under linear function approximation.
","['University of Oxford', 'Auburn University', 'University of Oxford']"
2020,Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources,"Yun Yun Tsai, Pin-Yu Chen, Tsung-Yi Ho",https://icml.cc/Conferences/2020/Schedule?showEvent=6376,"Current transfer learning methods are mainly based on finetuning a pretrained model with target-domain data. Motivated by the techniques from adversarial machine learning (ML) that are capable of manipulating the model prediction via data perturbations, in this paper we propose a novel approach, black-box adversarial reprogramming (BAR), that repurposes a well-trained black-box ML model (e.g., a prediction API or a proprietary software) for solving different ML tasks, especially in the scenario with scarce data and constrained resources. The rationale lies in exploiting high-performance but unknown ML models to gain learning capability for transfer learning. Using zeroth order optimization and multi-label mapping techniques, BAR can reprogram a black-box ML model solely based on its input-output responses without knowing the model architecture or changing any parameter. More importantly, in the limited medical data setting, on autism spectrum disorder classification, diabetic retinopathy detection, and melanoma detection tasks, BAR outperforms state-of-the-art methods and yields comparable performance to the vanilla adversarial reprogramming method requiring complete knowledge of the target ML model. BAR also outperforms baseline transfer learning approaches by a significant margin, demonstrating cost-effective means and new insights for transfer learning.
","['National Tsing Hua University', 'IBM Research AI', 'National Tsing Hua University']"
2020,AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"Esteban Real, Chen Liang, David So, Quoc Le",https://icml.cc/Conferences/2020/Schedule?showEvent=5987,"Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.
","['Google Research', 'Google Brain', 'Google Brain', 'Google Brain']"
2020,Detecting Out-of-Distribution Examples with Gram Matrices,"Chandramouli Shama Sastry, Sageev Oore",https://icml.cc/Conferences/2020/Schedule?showEvent=6703,"When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions; detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and predicted class. We find that characterizing activity patterns by Gram matrices and identifying anomalies in Gram matrix values can yield high OOD detection rates. We identify anomalies in the Gram matrices by simply comparing each value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and neither requires access to OOD data for fine-tuning hyperparameters, nor does it require OOD access for inferring parameters. We empirically demonstrate applicability across a variety of architectures and vision datasets and, for the important and surprisingly hard task of detecting far out-of-distribution examples, it generally performs better than or equal to state-of-the-art OOD detection methods (including those that do assume access to OOD examples).
","['Dalhousie University/Vector Institute', 'Dalhousie University and Vector Institute']"
2020,Learning Fair Policies in Multi-Objective (Deep) Reinforcement Learning with Average and Discounted Rewards,"Umer Siddique, Paul Weng, Matthieu Zimmer",https://icml.cc/Conferences/2020/Schedule?showEvent=6725,"As the operations of autonomous systems generally affect simultaneously several users, it is crucial that their designs account for fairness considerations.
In contrast to standard (deep) reinforcement learning (RL), we investigate the problem of learning a policy that treats its users equitably.
In this paper, we formulate this novel RL problem, in which an objective function, which encodes a notion of fairness that we formally define, is optimized.
For this problem, we provide a theoretical discussion where we examine the case of discounted rewards and that of average rewards.
During this analysis, we notably derive a new result in the standard RL setting, which is of independent interest:
it states a novel bound on the approximation error with respect to the optimal average reward of that of a policy optimal for the discounted reward.
Since learning with discounted rewards is generally easier, this discussion further justifies finding a fair policy for the average reward by learning a fair policy for the discounted reward.
Thus, we describe how several classic deep RL algorithms can be adapted to our fair optimization problem, and we validate our approach with extensive experiments in three different domains.
","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'UM-SJTU JI']"
2020,DROCC: Deep Robust One-Class Classification,"Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, Prateek Jain",https://icml.cc/Conferences/2020/Schedule?showEvent=6819,"Classical approaches for one-class problems such as one-class SVM and isolation forest require careful feature engineering when applied to structured domains like images. State-of-the-art methods aim to leverage deep learning to learn appropriate features via two main approaches. The first approach based on predicting transformations (Golan & El-Yaniv, 2018; Hendrycks et al., 2019a) while successful in some domains, crucially depends on an appropriate domain-specific set of transformations that are hard to obtain in general. The second approach of minimizing a classical one-class loss on the learned final layer representations, e.g., DeepSVDD (Ruff et al., 2018) suffers from the fundamental drawback of representation collapse. In this work, we propose Deep Robust One Class Classification (DROCC) that is both applicable to most standard domains without requiring any side-information and robust to representation collapse. DROCC is based on the assumption that the points from the class of interest lie on a well-sampled, locally linear low dimensional manifold. Empirical evaluation demonstrates that DROCC is highly effective in two different one-class problem settings and on a range of real-world datasets across different domains: tabular data, images (CIFAR and ImageNet), audio, and time-series, offering up to 20% increase in accuracy over the state-of-the-art in anomaly detection.
","['Microsoft research', 'Stanford', 'MILA / NIT Karnataka, Surathkal', 'Microsoft Research', 'Microsoft Research']"
2020,An EM Approach to Non-autoregressive Conditional Sequence Generation,"Zhiqing Sun, Yiming Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6203,"Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency.  Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation.  This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.
","['Carnegie Mellon University', 'Carnegie Mellon University']"
2020,Distributed Online Optimization over a Heterogeneous Network,"Nima Eshraghi, Ben Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6727,"In distributed online optimization over a computing network with heterogeneous nodes, slow nodes can adversely affect the progress of fast nodes, leading to drastic slowdown of the overall convergence process. To address this issue, we consider a new algorithm termed Distributed Any-Batch Mirror Descent (DABMD), which is based on distributed Mirror Descent but uses a fixed per-round computing time to limit the waiting by fast nodes to receive information updates from slow nodes. DABMD is characterized by varying minibatch sizes across nodes. It is applicable to a broader range of problems compared with existing distributed online optimization methods such as those based on dual averaging, and it accommodates time-varying network topology. We study two versions of DABMD, depending on whether the computing nodes average their primal variables via single or multiple consensus iterations. We show that both versions provide strong theoretical performance guarantee, by deriving upperbounds on their expected dynamic regret, which capture the variability in minibatch sizes. Our experimental results show substantial reduction in cost and acceleration in convergence compared with the known best alternative.
","['University of Toronto', 'University of Toronto']"
2020,Adversarial Neural Pruning with Latent Vulnerability Suppression,"Divyam Madaan, Jinwoo Shin, Sung Ju Hwang",https://icml.cc/Conferences/2020/Schedule?showEvent=5877,"Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \emph{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \emph{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \emph{Adversarial Neural Pruning with Vulnerability Suppression (ANP-VS)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.
","['KAIST', 'KAIST', 'KAIST, AITRICS']"
2020,Invariant Risk Minimization Games,"Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, Amit Dhurandhar",https://icml.cc/Conferences/2020/Schedule?showEvent=6221,"The standard risk minimization paradigm of machine learning is brittle when operating in environments whose test distributions are different from the training distribution due to spurious correlations. Training on data from many environments and finding invariant predictors reduces the effect of spurious features by concentrating models on features that have a causal relationship with the outcome. In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a simple training algorithm that uses best response dynamics and, in our experiments, yields similar or better empirical accuracy with much lower variance than the challenging bi-level optimization problem of Arjovsky et al. (2019). One key theoretical contribution is showing that the set of Nash equilibria for the proposed game are equivalent to the set of invariant predictors for any finite number of environments, even with nonlinear classifiers and transformations. As a result, our method also retains the generalization guarantees to a large set of environments shown in Arjovsky et al. (2019). The proposed algorithm adds to the collection of successful game-theoretic machine learning algorithms such as generative adversarial networks.
","['IBM Research', 'IBM Research NY', 'IBM Research AI', 'IBM Research']"
2020,Neural Clustering Processes,"Ari Pakman, Yueqi Wang, Catalin Mitelut, JinHyung Lee, Department of Statistics Liam Paninski",https://icml.cc/Conferences/2020/Schedule?showEvent=6425,"Probabilistic clustering models (or equivalently, mixture models) are basic building blocks in countless statistical models and involve latent random variables over discrete spaces. For these models, posterior inference methods can be inaccurate and/or very slow. In this work we introduce deep network architectures trained with labeled samples from any generative model of  clustered datasets. At test time, the networks generate approximate posterior samples of cluster labels for any new dataset of arbitrary size. We develop two complementary approaches to this task, requiring  either O(N) or O(K) network forward passes per dataset, where N is the dataset size and  K the number of clusters. Unlike previous approaches, our methods sample the labels of all the data points from a well-defined posterior, and can learn nonparametric Bayesian posteriors since they do not limit the number of mixture components. As a scientific application, we present a novel approach to neural spike sorting for high-density multielectrode arrays. 
","['Columbia University', 'Google, Columbia University', 'Columbia University', 'Columbia University', 'Department of Statistics, Columbia University']"
2020,Associative Memory in Iterated Overparameterized Sigmoid Autoencoders,"Yibo Jiang, Cengiz Pehlevan",https://icml.cc/Conferences/2020/Schedule?showEvent=6554,"Recent work showed that overparameterized autoencoders can be trained to implement associative memory via iterative maps, when the trained input-output Jacobian of the network has all of its eigenvalue norms strictly below one. Here, we theoretically analyze this phenomenon for sigmoid networks by leveraging recent developments in deep learning theory, especially the correspondence between training neural networks in the infinite-width limit and performing kernel regression with the Neural Tangent Kernel (NTK). We find that overparameterized sigmoid autoencoders can have attractors in the NTK limit for both training with a single example and multiple examples under certain conditions. In particular, for multiple training examples, we find that the norm of the largest Jacobian eigenvalue drops below one with increasing input norm, leading to associative memory. 
","['Harvard University', 'Harvard University']"
2020,Batch Reinforcement Learning with Hyperparameter  Gradients,"Byung-Jun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, Kee-Eung Kim",https://icml.cc/Conferences/2020/Schedule?showEvent=6388,"We consider the batch reinforcement learning problem where the agent needs to learn only from a fixed batch of data, without further interaction with the environment. In such a scenario, we want to prevent the optimized policy from deviating too much from the data collection policy since the estimation becomes highly unstable otherwise due to the off-policy nature of the problem. However, imposing this requirement too strongly will result in a policy that merely follows the data collection policy. Unlike prior work where this trade-off is controlled by hand-tuned hyperparameters, we propose a novel batch reinforcement learning approach, batch optimization of policy and hyperparameter (BOPAH), that uses a gradient-based optimization of the hyperparameter using held-out data. We show that BOPAH outperforms other batch reinforcement learning algorithms in tabular and continuous control tasks, by finding a good balance to the trade-off between adhering to the data collection policy and pursuing the possible policy improvement.
","['KAIST', 'KAIST', 'PROWLER.io', 'Prowler.io', 'KAIST']"
2020,On the Unreasonable Effectiveness of the Greedy Algorithm: Greedy Adapts to Sharpness,"Sebastian Pokutta, Mohit Singh, Alfredo Torrico",https://icml.cc/Conferences/2020/Schedule?showEvent=5870,"It is well known that the standard greedy algorithm guarantees a worst-case approximation factor of $1-1/e$ when maximizing a monotone submodular function under a cardinality constraint. However, empirical studies show that its performance is substantially better in practice. This raises a natural question of explaining this improved performance of the greedy algorithm. In this work, we define sharpness for submodular functions as a candidate explanation for this phenomenon. We show that the greedy algorithm provably performs better as the sharpness of the submodular function increases. This improvement ties in closely with the faster convergence rates of first order methods for sharp functions in convex optimization.","['ZIB', 'Georgia Institute of Technology', 'CERC Data Science, Polytechnique Montreal']"
2020,Estimating the Number and Effect Sizes of Non-null Hypotheses,"Jennifer Brennan, Ramya Korlakai Vinayak, Kevin Jamieson",https://icml.cc/Conferences/2020/Schedule?showEvent=6165,"We study the problem of estimating the distribution of effect sizes (the mean of the test statistic under the alternate hypothesis) in a multiple testing setting. Knowing this distribution allows us to calculate the power (type II error) of any experimental design. We show that it is possible to estimate this distribution using an inexpensive pilot experiment, which takes significantly fewer samples than would be required by an experiment that identified the discoveries. Our estimator can be used to guarantee the number of discoveries that will be made using a given experimental design in a future experiment. We prove that this simple and computationally efficient estimator enjoys a number of favorable theoretical properties, and demonstrate its effectiveness on data from a gene knockout experiment on influenza inhibition in Drosophila.
","['University of Washington', 'University of Washington', 'University of Washington']"
2020,Nonparametric Score Estimators,"Yuhao Zhou, Jiaxin Shi, Jun Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=6399,"Estimating the score, i.e., the gradient of log density function, from a set of samples generated by an unknown distribution is a fundamental task in inference and learning of probabilistic models that involve flexible yet intractable densities. Kernel estimators based on Stein's methods or score matching have shown promise, however their theoretical properties and relationships have not been fully-understood. We provide a unifying view of these estimators under the framework of regularized nonparametric regression. It allows us to analyse existing estimators and construct new ones with desirable properties by choosing different hypothesis spaces and regularizers. A unified convergence analysis is provided for such estimators. Finally, we propose score estimators based on iterative regularization that enjoy computational benefits from curl-free kernels and fast convergence.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2020,Closing the convergence gap of SGD without replacement,"Shashank Rajput, Anant Gupta, Dimitris Papailiopoulos",https://icml.cc/Conferences/2020/Schedule?showEvent=6774,"Stochastic gradient descent without replacement sampling is widely used in practice for model training. However, the vast majority of SGD analyses assumes data is sampled with replacement, and when the function minimized is strongly convex, an $\mathcal{O}\left(\frac{1}{T}\right)$ rate can be established when SGD is run for $T$ iterations. A recent line of breakthrough works on SGD without replacement (SGDo) established an $\mathcal{O}\left(\frac{n}{T^2}\right)$ convergence rate when the function minimized is strongly convex and is a sum of $n$ smooth functions, and an $\mathcal{O}\left(\frac{1}{T^2}+\frac{n^3}{T^3}\right)$ rate for sums of quadratics. On the other hand, the tightest known lower bound postulates an $\Omega\left(\frac{1}{T^2}+\frac{n^2}{T^3}\right)$ rate, leaving open the possibility of better SGDo convergence rates in the general case. In this paper, we close this gap and show that SGD without replacement achieves a rate of $\mathcal{O}\left(\frac{1}{T^2}+\frac{n^2}{T^3}\right)$ when the sum of the functions is a quadratic, and offer a new lower bound of $\Omega\left(\frac{n}{T^2}\right)$ for strongly convex functions that are sums of smooth functions.","['University of Wisconsin - Madison', 'University of Wisconsin Madison', 'University of Wisconsin-Madison']"
2020,Guided Learning of Nonconvex Models through Successive Functional Gradient Optimization,"Rie Johnson, Tong Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6435,"This paper presents a framework of successive functional gradient optimization for training nonconvex models such as neural networks, where training is driven by mirror descent in a function space.  We provide a theoretical analysis and empirical study of the training method derived from this framework.  It is shown that the method leads to better performance than that of standard training techniques.
","['RJ Research Consulting', 'HKUST']"
2020,Is Local SGD Better than Minibatch SGD?,"Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan McMahan, Ohad Shamir, Nati Srebro",https://icml.cc/Conferences/2020/Schedule?showEvent=6812,"We study local SGD (also known as parallel SGD and federated SGD), a natural and frequently used distributed optimization method. Its theoretical foundations are currently lacking and we highlight how all existing error guarantees in the convex setting are dominated by a simple baseline, minibatch SGD. (1) For quadratic objectives we prove that local SGD strictly dominates minibatch SGD and that accelerated local SGD is minmax optimal for quadratics; (2) For general convex objectives we provide the first guarantee that at least \emph{sometimes} improves over minibatch SGD, but our guarantee does not always improve over, nor even match, minibatch SGD; (3) We show that indeed local SGD does \emph{not} dominate minibatch SGD by presenting a lower bound on the performance of local SGD that is worse than the minibatch SGD guarantee.
","['Toyota Technological Institute at Chicago', 'Toyota Technological Institute at Chicago', 'EPFL', 'University of Chicago', 'TTI Chicago', 'Google', 'Weizmann Institute of Science', 'Toyota Technological Institute at Chicago']"
2020,Near-optimal sample complexity bounds for learning Latent $k-$polytopes and applications to Ad-Mixtures,"Chiranjib Bhattacharyya, Ravindran Kannan",https://icml.cc/Conferences/2020/Schedule?showEvent=6791,"Deriving Optimal bounds on Sample Complexity of Latent Variable models is an
active area of research. Recently such bounds were obtained for Mixture of Gaussians
\cite{HSNCAY18},
no such results are known for Ad-mixtures, a generalization of Mixture distributions.
In this paper we show that $O^*(dk/m)$ samples are sufficient to learn each of
$k-$ topic vectors of LDA, a popular Ad-mixture model, with vocabulary size $d$
and $m\in \Omega(1)$ words per document, to any constant error in $L_1$ norm.
The result is a corollary of the major contribution of this paper: the first sample complexity upper bound for the problem (introduced in \cite{BK20})
of learning the vertices of a Latent $k-$ Polytope in $\RR^d$, given perturbed points from it.
The bound,  $O^*(dk/\beta)$, is optimal and linear in number of parameters.
It applies to many stochastic models including a broad class Ad-mixtures.
 To demonstrate the generality of the approach
we specialize the setting to Mixed Membership Stochastic Block Models(MMSB)
and show for the first time that if an MMSB has $k$ blocks, the sample complexity is $O^*(k^2)$ under usual assumptions.","['Indian Institute of Science', 'Microsoft Research India']"
2020,Uncertainty-Aware Lookahead Factor Models for Quantitative Investing,"Lakshay Chauhan, John Alberg, Zachary Lipton",https://icml.cc/Conferences/2020/Schedule?showEvent=5869,"On a periodic basis, publicly traded companies report fundamentals, financial data including revenue, earnings, debt, among others. Quantitative finance research has identified several factors, functions of the reported data that historically correlate  with stock market performance. In this paper, we first show through simulation that if we could select stocks via factors calculated on future fundamentals (via oracle), that our portfolios would far outperform standard factor models. Motivated by this insight, we train deep nets to forecast future fundamentals from a trailing 5-year history. We propose lookahead factor models which plug these predicted future fundamentals into traditional factors. Finally, we incorporate uncertainty estimates from both neural heteroscedastic regression and a dropout-based heuristic, improving performance by adjusting our portfolios to avert risk. In retrospective analysis, we leverage an industry-grade portfolio simulator (backtester) to show simultaneous improvement in annualized return and Sharpe ratio.  Specifically, the simulated annualized return for the uncertainty-aware model is 17.7% (vs 14.0% for a standard factor model) and the Sharpe ratio is 0.84 (vs 0.52).
","['Euclidean Technologies', 'Euclidean Technologies', 'Carnegie Mellon University']"
2020,Learning Adversarial Markov Decision Processes with Bandit Feedback and Unknown Transition,"Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, Tiancheng Yu",https://icml.cc/Conferences/2020/Schedule?showEvent=5892,"We consider the task of learning in episodic finite-horizon Markov decision processes with an unknown transition function, bandit feedback, and adversarial losses. We propose an efficient algorithm that achieves  O(√L|X|AT ) regret with high probability, where L is the horizon, |X| the number of states, |A| the number of actions, and T the number of episodes. To our knowledge, our algorithm is the first to ensure O(√T) regret in this challenging setting; in fact, it achieves the same regret as (Rosenberg & Mansour, 2019a) who consider the easier setting with full-information. Our key contributions are two-fold: a tighter confidence set for the transition function; and an optimistic loss estimator that is inversely weighted by an ""upper occupancy bound"". 
","['Princeton University', 'University of Southern California', 'University of Southern California', 'MIT', 'MIT']"
2020,Stronger and Faster Wasserstein Adversarial Attacks,"Kaiwen Wu, Allen Wang, Yaoliang Yu",https://icml.cc/Conferences/2020/Schedule?showEvent=5871,"Deep models, while being extremely flexible and accurate, are surprisingly vulnerable to ``small, imperceptible'' perturbations known as adversarial attacks. While the majority of existing attacks focuses on measuring perturbations under the $\ell_p$ metric, Wasserstein distance, which takes geometry in pixel space into account, has long known to be a better metric for measuring image quality and has recently risen as a compelling alternative to the $\ell_p$ metric in adversarial attacks. However, constructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms. We address this gap in two ways: (a) we develop an exact yet efficient projection operator to enable a stronger projected gradient attack; (b) we show for the first time that Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints. Our algorithms not only converge faster but also generate much stronger attacks. For instance, we decrease the accuracy of a residual network on CIFAR-10 to $3.4\%$ within a Wasserstein perturbation ball of radius $0.005$, in contrast to $65.5\%$ using the  previous state-of-the-art attack based on approximate projection.
We show that applying our stronger attacks in adversarial training significantly improves the robustness of adversarially trained models.","['University of Waterloo', 'University of Waterloo', 'University of Waterloo']"
2020,Discount Factor as a Regularizer in Reinforcement Learning ,"Ron Amit, Ron Meir, Kamil Ciosek",https://icml.cc/Conferences/2020/Schedule?showEvent=6021,"Specifying a Reinforcement Learning (RL) task involves choosing a suitable planning horizon, which is typically modeled by a discount factor. It is known that applying RL algorithms with a lower discount factor can act as a regularizer, improving performance in the limited data regime. Yet the exact nature of this regularizer has not been investigated. In this work, we fill in this gap. For several Temporal-Difference (TD) learning methods, we show an explicit equivalence between using a reduced discount factor and adding an explicit regularization term to the algorithm's loss.
Motivated by the equivalence, we empirically study this technique compared to standard L2 regularization by extensive experiments in discrete and continuous domains, using tabular and functional representations.
Our experiments suggest the regularization effectiveness is strongly related to properties of the available data, such as size, distribution, and mixing rate.
","['Technion – Israel Institute of Technology', 'Technion Israeli Institute of Technology', 'Microsoft']"
2020,Online Pricing with Offline Data: Phase Transition and Inverse Square Law,"Jinzhi Bu, David Simchi-Levi, Yunzong Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=5926,"This paper investigates the impact of pre-existing offline data on online learning, in the context of dynamic pricing. We study a single-product dynamic pricing problem over a selling horizon of T periods. The demand in each period is determined by the price of the product according to a linear demand model with unknown parameters. We assume that the seller already has some pre-existing offline data before the start of the selling horizon. The seller wants to utilize both the pre-existing offline data and the sequential online data to minimize the regret of the online learning process. We characterize the joint effect of the size, location and dispersion of the offline data on the optimal regret of the online learning process. Our results reveal surprising transformations of the optimal regret rate with respect to the size of the offline data, which we refer to as phase transitions. In addition, our results demonstrate that the location and dispersion of the offline data also have an intrinsic effect on the optimal regret, and we quantify this effect via the inverse-square law.
","['MIT', 'MIT', 'MIT']"
2020,NADS: Neural Architecture Distribution Search for Uncertainty Awareness,"Randy Ardywibowo, Shahin Boluki, Xinyu Gong, Zhangyang Wang, Xiaoning Qian",https://icml.cc/Conferences/2020/Schedule?showEvent=6692,"Machine learning (ML) systems often encounter Out-of-Distribution (OoD) errors when dealing with testing data coming from a distribution different from training data. It becomes important for ML systems in critical applications to accurately quantify its predictive uncertainty and screen out these anomalous inputs. However, existing OoD detection approaches are prone to errors and even sometimes assign higher likelihoods to OoD samples. Unlike standard learning tasks, there is currently no well established guiding principle for designing OoD detection architectures that can accurately quantify uncertainty. To address these problems, we first seek to identify guiding principles for designing uncertainty-aware architectures, by proposing Neural Architecture Distribution Search (NADS). NADS searches for a distribution of architectures that perform well on a given task, allowing us to identify common building blocks among all uncertainty-aware architectures. With this formulation, we are able to optimize a stochastic OoD detection objective and construct an ensemble of models to perform OoD detection. We perform multiple OoD detection experiments and observe that our NADS performs favorably, with up to 57% improvement in accuracy compared to state-of-the-art methods among 15 different testing configurations.
","['Texas A&M University', 'Texas A&M University', 'Texas A&M University', 'University of Texas at Austin', 'Texas A&M University']"
2020,Boosting for Control of Dynamical Systems,"Naman Agarwal, Nataly Brukhim, Elad Hazan, Zhou Lu",https://icml.cc/Conferences/2020/Schedule?showEvent=6260,"We study the question of how to aggregate controllers for dynamical systems in order to improve their performance. To this end, we propose a framework of boosting for online control. Our main result is an efficient boosting algorithm that combines weak controllers into a provably more accurate one. Empirical evaluation on a host of control settings supports our theoretical findings. 
","['Google Research', 'Princeton University', 'Princeton University', 'Princeton University']"
2020,Informative Dropout for Robust Representation Learning: A Shape-bias Perspective,"Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=5839,"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at https://github.com/bfshi/InfoDrop.
","['Peking University', 'Peking University', 'Microsoft Research', 'Peking University', 'Peking University', 'Microsoft']"
2020,The Tree Ensemble Layer: Differentiability meets Conditional Computation,"Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, Rahul Mazumder",https://icml.cc/Conferences/2020/Schedule?showEvent=6268,"Neural networks and tree ensembles are state-of-the-art learners, each with its unique statistical and computational advantages. We aim to combine these advantages by introducing a new layer for neural networks, composed of an ensemble of differentiable decision trees (a.k.a. soft trees). While differentiable trees demonstrate promising results in the literature, they are typically slow in training and inference as they do not support conditional computation. We mitigate this issue by introducing a new sparse activation function for sample routing, and implement true conditional computation by developing specialized forward and backward propagation algorithms that exploit sparsity. Our efficient algorithms pave the way for jointly training over deep and wide tree ensembles using first-order methods (e.g., SGD). Experiments on 23 classification datasets indicate over 10x speed-ups compared to the differentiable trees used in the literature and over 20x reduction in the number of parameters compared to gradient boosted trees, while maintaining competitive performance. Moreover, experiments on CIFAR, MNIST, and Fashion MNIST indicate that replacing dense layers in CNNs with our tree layer reduces the  test loss by 7-53% and the number of parameters by 8x. We provide an open-source TensorFlow implementation with a Keras API.
","['Massachusetts Institute of Technology', 'Google', 'Google Research', 'Google', 'Massachusetts Institute of Technology']"
2020,Overfitting in adversarially robust deep learning,"Leslie Rice, Eric Wong, Zico Kolter",https://icml.cc/Conferences/2020/Schedule?showEvent=6258,"It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models (L-infinity and L-2). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting.  Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at https://github.com/ locuslab/robust_overfitting.
","['Carnegie Mellon University', 'MIT', 'Carnegie Mellon University / Bosch Center for AI']"
2020,Differentially Private Set Union,"Sivakanth Gopi, Pankaj  Gulhane, Janardhan Kulkarni, Judy Hanwen Shen, Milad Shokouhi, Sergey Yekhanin",https://icml.cc/Conferences/2020/Schedule?showEvent=6541,"We study the basic operation of set union in the global model of differential privacy. In this problem, we are given a universe $U$ of items, possibly of infinite size, and a database $D$ of users. Each user $i$ contributes a subset $W_i \subseteq U$ of items. We want an ($\epsilon$,$\delta$)-differentially private Algorithm which outputs a subset $S \subset \cup_i W_i$ such that the size of $S$ is as large as possible. The problem arises in countless real world applications, and is particularly ubiquitous in natural language processing (NLP) applications. For example, discovering words, sentences, $n$-grams etc., from private text data belonging to users is an instance of the set union problem.
In this paper we design new algorithms for this problem that significantly outperform the best known algorithms.","['Microsoft', 'Microsoft', 'Microsoft Research', 'Microsoft Research', 'Microsoft', 'Microsoft']"
2020,Finite-Time Convergence in Continuous-Time Optimization,"Orlando Romero, mouhacine Benosman",https://icml.cc/Conferences/2020/Schedule?showEvent=6568,"In this paper, we investigate a Lyapunov-like differential inequality that allows us to establish finite-time stability of a continuous-time state-space dynamical system represented via a multivariate ordinary differential equation or differential inclusion. Equipped with this condition, we successfully synthesize first and second-order dynamical systems that achieve finite-time convergence to the minima of a given sufficiently regular cost function. As a byproduct, we show that the p-rescaled gradient flow (p-RGF) proposed by Wibisono et al. (2016) is indeed finite-time convergent, provided the cost function is gradient dominated of order q in (1,p). Thus, we effectively bridge a gap between the p-RGF and the normalized gradient flow (NGF) (p=\infty) proposed by Cortes (2006) in his seminal paper in the context of multi-agent systems. We discuss strategies to discretize our proposed flows and conclude by conducting some numerical experiments to illustrate our results.
","['Rensselaer Polytechnic Institute', 'MERL']"
2020,RIFLE: Backpropagation in Depth for Deep Transfer Learning through Re-Initializing the Fully-connected LayEr,"Xingjian Li, Haoyi Xiong, Haozhe An, Cheng-Zhong Xu, Dejing Dou",https://icml.cc/Conferences/2020/Schedule?showEvent=6393,"Fine-tuning the deep convolution neural network (CNN) using a pre-trained model helps transfer knowledge learned from larger datasets to the target task. While the accuracy could be largely improved even when the training dataset is small, the transfer learning outcome is similar with the pre-trained one with closed CNN weights[17], as the backpropagation here brings less updates to deeper CNN layers. In this work, we propose RIFLE - a simple yet effective strategy that deepens backpropagation in transfer learning settings, through periodically ReInitializing the Fully-connected LayEr with random scratch during the fine-tuning procedure. RIFLE brings significant perturbation to the backpropagation process and leads to deep CNN weights update, while the affects of perturbation can be easily converged throughout the overall learning procedure. The experiments show that the use of RIFLE significantly improves deep transfer learning accuracy on a wide range of datasets, outperforming known tricks for the similar purpose, such as dropout, dropconnect, stochastic depth, and cyclic learning rate, under the same settings with 0.5%-2% higher testing accuracy. Empirical cases and ablation studies further indicate RIFLE brings meaningful updates to deep CNN layers with accuracy improved.
","['Baidu Research', 'Baidu Research', 'Baidu Research', 'University of Macau', 'Baidu']"
2020,Working Memory Graphs,"Ricky Loynd, Roland Fernandez, Asli Celikyilmaz, Adith Swaminathan, Matthew Hausknecht",https://icml.cc/Conferences/2020/Schedule?showEvent=6038,"Transformers have increasingly outperformed gated RNNs in obtaining new state-of-the-art results on supervised tasks involving text sequences. Inspired by this trend, we study the question of how Transformer-based models can improve the performance of sequential decision-making agents. We present the Working Memory Graph (WMG), an agent that employs multi-head self-attention to reason over a dynamic set of vectors representing observed and recurrent state. We evaluate WMG in three environments featuring factored observation spaces: a Pathfinding environment that requires complex reasoning over past observations, BabyAI gridworld levels that involve variable goals, and Sokoban which emphasizes future planning. We find that the combination of WMG's Transformer-based architecture with factored observation spaces leads to significant gains in learning efficiency compared to baseline architectures across all tasks. WMG demonstrates how Transformer-based models can dramatically boost sample efficiency in RL environments for which observations can be factored.
","['Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research']"
2020,Feature Noise Induces Loss Discrepancy Across Groups,"Fereshte Khani, Percy Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=5971,"The performance of standard learning procedures has been observed to differ widely across groups. 
Recent studies usually attribute this loss discrepancy to an information deficiency for one group (e.g., one group has less data). 
In this work, we point to a more subtle source of loss discrepancy---feature noise. 
Our main result is that even when there is no information deficiency specific to one group (e.g., both groups have infinite data), adding the same amount of feature noise to all individuals leads to loss discrepancy.
For linear regression, we thoroughly characterize the effect of feature noise on loss discrepancy in terms of the amount of noise, the difference between moments of the two groups, and whether group information is used or not.
We then show this loss discrepancy does not vanish immediately if a shift in distribution causes the groups to have similar moments. 
On three real-world datasets, we show feature noise increases the loss discrepancy if groups have different distributions, while it does not affect the loss discrepancy on datasets that groups have similar distributions.

","['Stanford University', 'Stanford University']"
2020,Parameterized Rate-Distortion Stochastic Encoder,"Quan Hoang, Trung Le, Dinh Phung",https://icml.cc/Conferences/2020/Schedule?showEvent=6252,"We propose a novel gradient-based tractable approach for the Blahut-Arimoto (BA) algorithm to compute the rate-distortion function where the BA algorithm is fully parameterized. This results in a rich and flexible framework to learn a new class of stochastic encoders, termed PArameterized RAte-DIstortion Stochastic Encoder (PARADISE). The framework can be applied to a wide range of settings from semi-supervised, multi-task to supervised and robust learning. We show that the training objective of PARADISE can be seen as a form of regularization that helps improve generalization. With an emphasis on robust learning we further develop a novel posterior matching objective to encourage smoothness on the loss function and show that PARADISE can significantly improve interpretability as well as robustness to adversarial attacks on the CIFAR-10 and ImageNet datasets. In particular, on the CIFAR-10 dataset, our model reduces standard and adversarial error rates in comparison to the state-of-the-art by 50% and 41%, respectively without the expensive computational cost of adversarial training.
","['Monash University', 'Monash University', 'Monash University, Australia']"
2020,Scalable Identification of Partially Observed Systems with Certainty-Equivalent EM,"Kunal Menda, Jean de Becdelievre, Jayesh K. Gupta, Ilan Kroo, Mykel Kochenderfer, Zachary Manchester",https://icml.cc/Conferences/2020/Schedule?showEvent=6183,"System identification is a key step for model-based control, estimator design, and output prediction. This work considers the offline identification of partially observed nonlinear systems. We empirically show that the certainty-equivalent approximation to expectation-maximization can be a reliable and scalable approach for high-dimensional deterministic systems, which are common in robotics. We formulate certainty-equivalent expectation-maximization as block coordinate-ascent, and provide an efficient implementation. The algorithm is tested on a simulated system of coupled Lorenz attractors, demonstrating its ability to identify high-dimensional systems that can be intractable for particle-based approaches. Our approach is also used to identify the dynamics of an aerobatic helicopter. By augmenting the state with unobserved fluid states, a model is learned that predicts the acceleration of the helicopter better than state-of-the-art approaches. The codebase for this work is available at https://github.com/sisl/CEEM.
","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford']"
2020,Strategyproof Mean Estimation from Multiple-Choice Questions,"Anson Kahng, Gregory Kehne, Ariel Procaccia",https://icml.cc/Conferences/2020/Schedule?showEvent=6049,"Given n values possessed by n agents, we study the problem of estimating the mean by truthfully eliciting agents' answers to multiple-choice questions about their values. We consider two natural candidates for estimation error: mean squared error (MSE) and mean absolute error (MAE). We design a randomized estimator which is asymptotically optimal for both measures in the worst case. In the case where prior distributions over the agents' values are known, we give an optimal, polynomial-time algorithm for MSE, and show that the task of computing an optimal estimate for MAE is #P-hard. Finally, we demonstrate empirically that knowledge of prior distributions gives a significant edge.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Harvard University']"
2020,Efficient Identification in Linear Structural Causal Models with Auxiliary Cutsets,"Daniel Kumor, Carlos Cinelli, Elias Bareinboim",https://icml.cc/Conferences/2020/Schedule?showEvent=6613,"We develop a a new polynomial-time algorithm for identification of structural coefficients in linear causal models that subsumes previous state-of-the-art methods, unifying several disparate approaches to identification in this setting. Building on these results, we develop a procedure for identifying total causal effects in linear systems.
","['Purdue University', 'UCLA', 'Columbia']"
2020,Meta-learning for Mixed Linear Regression,"Weihao Kong, Raghav Somani, Zhao Song, Sham Kakade, Sewoong Oh",https://icml.cc/Conferences/2020/Schedule?showEvent=6753,"In modern supervised learning, there are a large number of tasks, but many of them are associated with only a small amount of labelled data. These include data from medical image processing and robotic interaction. Even though each individual task cannot be meaningfully trained in isolation, one seeks to meta-learn across the tasks from past experiences by exploiting some similarities. We study a fundamental question of interest: When can abundant tasks with small data compensate for lack of tasks with big data? We focus on a canonical scenario where each task is drawn from a mixture of $k$ linear regressions, and identify sufficient conditions for such a graceful exchange to hold; there is little loss in sample complexity even when we only have access to small data tasks. To this end, we introduce a novel spectral approach and show that we can efficiently utilize small data tasks with the help of $\tilde\Omega(k^{3/2})$ medium data tasks each with  $\tilde\Omega(k^{1/2})$ examples.","['University of Washington', 'University of Washington', 'UT-Austin & University of Washington', 'University of Washington', 'University of Washington']"
2020,Angular Visual Hardness,"Beidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava, Animesh Garg, Anima Anandkumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6648,"Recent convolutional neural networks (CNNs) have led to impressive performance but often suffer from poor calibration. They tend to be overconfident, with the model confidence not always reflecting the underlying true ambiguity and hardness. In this paper, we propose angular visual hardness (AVH), a score given by the normalized angular distance between the sample feature embedding and the target classifier to measure sample hardness. We validate this score with an in-depth and extensive scientific study, and observe that CNN models with the highest accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models improve on the classification of harder examples. We observe that the training dynamics of AVH is vastly different compared to the training loss. Specifically, AVH quickly reaches a plateau for all samples even though the training loss keeps improving. This suggests the need for designing better loss functions that can target harder examples more effectively. We also find that AVH has a statistically significant correlation with human visual hardness. Finally, we demonstrate the benefit of AVH to a variety of applications such as self-training for domain adaptation and domain generalization.
","['Rice University', 'Georgia Tech', 'NVIDIA', 'NVIDIA', 'Rice University', 'University of Toronto, Vector Institute, Nvidia', 'Amazon AI & Caltech']"
2020,PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination,"Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sabharwal, Ashish Verma",https://icml.cc/Conferences/2020/Schedule?showEvent=6835,"We develop a novel method, called PoWER-BERT,
for improving the inference time of the popular
BERT model, while maintaining the accuracy. It
works by: a) exploiting redundancy pertaining to
word-vectors (intermediate encoder outputs) and
eliminating the redundant vectors. b) determining which word-vectors to eliminate by developing a strategy for measuring their significance,
based on the self-attention mechanism. c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function.
Experiments on the standard GLUE benchmark
shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with < 1%
loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up
to 6.8x reduction in inference time with < 1%
loss in accuracy when applied over ALBERT, a
highly compressed version of BERT. The code for PoWER-BERT is publicly available at https://github.com/IBM/PoWER-BERT.
","['IBM Research - India', 'IBM Research- India', 'IBM Research - India', '""IBM Research, India""', 'IBM Research - India', 'IBM Research']"
2020,FormulaZero: Distributionally Robust Online Adaptation via Offline Population Synthesis,"Aman Sinha, Matthew O'Kelly, Hongrui Zheng, Rahul Mangharam, John Duchi, Russ Tedrake",https://icml.cc/Conferences/2020/Schedule?showEvent=6277,"Balancing performance and safety is crucial to deploying autonomous vehicles in multi-agent environments. In particular, autonomous racing is a domain that penalizes safe but conservative policies, highlighting the need for robust, adaptive strategies. Current approaches either make simplifying assumptions about other agents or lack robust mechanisms for online adaptation. This work makes algorithmic contributions to both challenges. First, to generate a realistic, diverse set of opponents, we develop a novel method for self-play based on replica-exchange Markov chain Monte Carlo. Second, we propose a distributionally robust bandit optimization procedure that adaptively adjusts risk aversion relative to uncertainty in beliefs about opponents’ behaviors. We rigorously quantify the tradeoffs in performance and robustness when approximating these computations in real-time motion-planning, and we demonstrate our methods experimentally on autonomous vehicles that achieve scaled speeds comparable to Formula One racecars.
","['Stanford University', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'Stanford University', 'MIT']"
2020,Oracle Efficient Private Non-Convex Optimization,"Seth Neel, Aaron Roth, Giuseppe Vietri, Steven Wu",https://icml.cc/Conferences/2020/Schedule?showEvent=5815,"One of the most effective algorithms for differentially private learning and optimization is \emph{objective perturbation}. This technique augments a given optimization problem (e.g. deriving from an ERM problem) with a random linear term, and then exactly solves it. However, to date, analyses of this approach crucially rely on the convexity and smoothness of the objective function. We give two algorithms that extend this approach substantially. The first algorithm requires nothing except boundedness of the loss function, and operates over a discrete domain. Its privacy and accuracy guarantees hold even without assuming convexity. We are able to extend traditional analyses of objective perturbation by introducing a novel normalization step into the algorithm, which provides enough stability to be differentially private even without second-order conditions. The second algorithm operates over a continuous domain and requires only that the loss function be bounded and Lipschitz in its continuous parameter. Its privacy analysis does not even require convexity. Its accuracy analysis does require convexity, but does not require second order conditions like smoothness. We complement our theoretical results with an empirical evaluation of the non-convex case, in which we use an integer program solver as our optimization oracle. We find that for the problem of learning linear classifiers, directly optimizing for 0/1 loss using our approach can out-perform the more standard approach of privately optimizing a convex-surrogate loss function on the Adult dataset.
","['University of Pennsylvania', 'University of Pennsylvania', 'University of Minnesota', 'University of Minnesota']"
2020,Representations for Stable Off-Policy Reinforcement Learning,"Dibya Ghosh, Marc Bellemare",https://icml.cc/Conferences/2020/Schedule?showEvent=6666,"Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical SARSA algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case of a defective transition matrix, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks.
","['Google', 'Google Brain']"
2020,Parametric Gaussian Process Regressors,"Martin Jankowiak, Geoff Pleiss, Jacob Gardner",https://icml.cc/Conferences/2020/Schedule?showEvent=6286,"The combination of inducing point methods with stochastic variational inference has enabled approximate Gaussian Process (GP) inference on large datasets. Unfortunately, the resulting predictive distributions often exhibit substantially underestimated uncertainties. Notably, in the regression case the predictive variance is typically dominated by observation noise, yielding uncertainty estimates that make little use of the input-dependent function uncertainty that makes GP priors attractive. In this work we propose two simple methods for scalable GP regression that address this issue and thus yield substantially improved predictive uncertainties. The first applies variational inference to FITC (Fully Independent Training Conditional; Snelson et. al. 2006). The second bypasses posterior approximations and instead directly targets the posterior predictive distribution. In an extensive empirical comparison with a number of alternative methods for scalable GP regression, we find that the resulting predictive distributions exhibit significantly better calibrated uncertainties and higher log likelihoods--often by as much as half a nat per datapoint.
","['Uber AI Labs', 'Cornell University', 'Uber AI Labs']"
2020,An end-to-end approach for the verification problem: learning the right distance,"Joao Monteiro, Isabela Albuquerque, Jahangir Alam, R Devon Hjelm, Tiago Falk",https://icml.cc/Conferences/2020/Schedule?showEvent=6275,"In this contribution, we augment the metric learning setting by introducing a parametric pseudo-distance, trained jointly with the encoder. Several interpretations are thus drawn for the learned distance-like model's output. We first show it approximates a likelihood ratio which can be used for hypothesis tests, and that it further induces a large divergence across the joint distributions of pairs of examples from the same and from different classes. Evaluation is performed under the verification setting consisting of determining whether sets of examples belong to the same class, even if such classes are novel and were never presented to the model during training. Empirical evaluation shows such method defines an end-to-end approach for the verification problem, able to attain better performance than simple scorers such as those based on cosine similarity and further outperforming widely used downstream classifiers. We further observe training is much simplified under the proposed approach compared to metric learning with actual distances, requiring no complex scheme to harvest pairs of examples.
","['Institut National de la Recherche Scientifique (INRS)', 'Institut National de la Recherche Scientifique', 'Computer Research Institute of Montreal (CRIM), Montreal (Quebec) Canada', 'Microsoft Research', 'INRS-EMT']"
2020,Generative Adversarial Imitation Learning with Neural Network Parameterization: Global Optimality and Convergence Rate,"Yufeng Zhang, Qi Cai, Zhuoran Yang, Zhaoran Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6313,"Generative adversarial imitation learning (GAIL) demonstrates tremendous success in practice, especially when combined with neural networks. Different from reinforcement learning, GAIL learns both policy and reward function from expert (human) demonstration. Despite its empirical success, it remains unclear whether GAIL with neural networks converges to the globally optimal solution. The major difﬁculty comes from the nonconvex-nonconcave minimax optimization structure. To bridge the gap between practice and theory, we analyze a gradient-based algorithm with alternating updates and establish its sublinear convergence to the globally optimal solution. To the best of our knowledge, our analysis establishes the global optimality and convergence rate of GAIL with neural networks for the ﬁrst time.
","['Northwestern University', 'Northwestern University', 'Princeton University', 'Northwestern U']"
2020,LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments,"Ali Teshnizi, Saber Salehkaleybar, Negar Kiyavash",https://icml.cc/Conferences/2020/Schedule?showEvent=5920,"The causal relationships among a set of random variables are commonly represented by a Directed Acyclic Graph (DAG), where there is a directed edge from variable $X$ to variable $Y$ if $X$ is a direct cause of $Y$. From the purely observational data, the true causal graph can be identified up to a Markov Equivalence Class (MEC), which is a set of DAGs with the same conditional independencies between the variables. The size of an MEC is a measure of complexity for recovering the true causal graph by performing interventions. We propose a method for efficient iteration over possible MECs given intervention results. We utilize the proposed method for computing MEC sizes and experiment design in active and passive learning settings. Compared to previous work for computing the size of MEC, our proposed algorithm reduces the time complexity by a factor of $O(n)$ for sparse graphs where $n$ is the number of variables in the system. Additionally, integrating our approach with dynamic programming, we design an optimal algorithm for passive experiment design. Experimental results show that our proposed algorithms for both computing the size of MEC and experiment design outperform the state of the art.","['Sharif University of Technology', 'Sharif University of Technology', 'École Polytechnique Fédérale de Lausanne']"
2020,Disentangling Trainability and Generalization in Deep Neural Networks,"Lechao Xiao, Jeffrey Pennington, Samuel Schoenholz",https://icml.cc/Conferences/2020/Schedule?showEvent=5884,"A longstanding goal in the theory of deep learning is to characterize the conditions under which a given neural network architecture will be trainable, and if so, how well it might generalize to unseen data. In this work, we provide such a characterization in the limit of very wide and very deep networks, for which the analysis simplifies considerably. For wide networks, the trajectory under gradient descent is governed by the Neural Tangent Kernel (NTK), and for deep networks the NTK itself maintains only weak data dependence. By analyzing the spectrum of the NTK, we formulate necessary conditions for trainability and generalization across a range of architectures, including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). We identify large regions of hyperparameter space for which networks can memorize the training set but completely fail to generalize. We find that CNNs without global average pooling behave almost identically to FCNs, but that CNNs with pooling have markedly different and often better generalization performance. These theoretical results are corroborated experimentally on CIFAR10 for a variety of network architectures.
We include a \href{https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/disentanglingtrainabilityand_generalization.ipynb}{colab} notebook that reproduces the essential results of the paper.
","['Google Research', 'Google Brain', 'Google Brain']"
2020,Streaming Coresets for Symmetric Tensor Factorization,"Supratim Shit, Rachit Chhaya, Jayesh Choudhari, Anirban Dasgupta",https://icml.cc/Conferences/2020/Schedule?showEvent=6366,"Factorizing tensors has recently become an important optimization module in a number of machine learning pipelines, especially in latent variable models. We show how to do this efficiently in the streaming setting. Given a set of $n$ vectors, each in $\~R^d$, we present algorithms to select a sublinear number of these vectors as coreset, while guaranteeing that the CP decomposition of the $p$-moment tensor of the coreset approximates the corresponding decomposition of the $p$-moment tensor computed from the full data. We introduce two novel algorithmic techniques: online filtering and kernelization. Using these two, we present four algorithms that achieve different tradeoffs of coreset size, update time and working space, beating or matching various state of the art algorithms. In the case of matrices (2-ordered tensor), our online row sampling algorithm guarantees $(1 \pm \epsilon)$ relative error spectral approximation. We show applications of our algorithms in learning single topic modeling. ","['IIT Gandhinagar', 'IIT Gandhinagar', 'IIT Gandhinagar', 'IIT Gandhinagar']"
2020,CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods,"Wei Zhang, Thomas  Panum, Somesh Jha, Prasad Chalasani, David Page",https://icml.cc/Conferences/2020/Schedule?showEvent=5855,"We study the problem of learning Granger causality between event types from asynchronous, interdependent, multi-type event sequences. Existing work suffers from either limited model flexibility or poor model explainability and thus fails to uncover Granger causality across a wide variety of event sequences with diverse event interdependency. To address these weaknesses, we propose CAUSE (Causality from AttribUtions on Sequence of Events), a novel framework for the studied task. The key idea of CAUSE is to first implicitly capture the underlying event interdependency by fitting a neural point process, and then extract from the process a Granger causality statistic using an axiomatic attribution method. Across multiple datasets riddled with diverse event interdependency, we demonstrate that CAUSE achieves superior performance on correctly inferring the inter-type Granger causality over a range of state-of-the-art methods.
","['Facebook Inc.', 'Aalborg University', 'University of Wisconsin, Madison', 'XaiPient', 'Duke']"
2020,Scalable Deep Generative Modeling for Sparse Graphs,"Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, Dale Schuurmans",https://icml.cc/Conferences/2020/Schedule?showEvent=6334,"Learning graph generative models is a challenging task for deep learning and has wide applicability to a range of domains like chemistry, biology and social science. However current deep neural methods suffer from limited scalability: for a graph with n nodes and m edges, existing deep neural methods require Omega(n^2) complexity by building up the  adjacency matrix. On the other hand, many real world graphs are actually sparse in the sense that m << n^2. Based on this, we develop a novel autoregressive model, named BiGG, that utilizes this sparsity to avoid generating the full adjacency matrix, and importantly reduces the graph generation time complexity to O((n + m) log n). Furthermore, during training this autoregressive model can be parallelized with O(log n) synchronization stages, which makes it much more efficient than other autoregressive models that require Omega(n). Experiments on several benchmarks show that the proposed approach not only scales to orders of magnitude larger graphs than previously possible with deep autoregressive graph generative models, but also yields better graph generation quality.
","['Google Brain', 'Google Brain', 'DeepMind', 'Google Brain', 'Google / University of Alberta']"
2020,Obtaining Adjustable Regularization for Free via Iterate Averaging,"Jingfeng Wu, Vladimir Braverman, Lin Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6220,"Regularization for optimization is a crucial technique to avoid overfitting in machine learning. In order to obtain the best performance, we usually train a model by tuning the regularization parameters. It becomes costly, however, when a single round of training takes significant amount of time. Very recently, Neu and Rosasco show that if we run stochastic gradient descent (SGD) on linear regression problems, then by averaging the SGD iterates properly, we obtain a regularized solution. It left open whether the same phenomenon can be achieved for other optimization problems and algorithms. In this paper, we establish an averaging scheme that provably converts the iterates of SGD on an arbitrary strongly convex and smooth objective function to its regularized counterpart with an adjustable regularization parameter. Our approaches can be used for accelerated and preconditioned optimization methods as well. We further show that the same methods work empirically on more general optimization objectives including neural networks. In sum, we obtain adjustable regularization for free for a large class of optimization problems and resolve an open question raised by Neu and Rosasco.
","['Johns Hopkins University', 'Johns Hopkins University', 'UCLA']"
2020,Beyond UCB: Optimal and Efficient Contextual Bandits with Regression Oracles,"Dylan Foster, Alexander Rakhlin",https://icml.cc/Conferences/2020/Schedule?showEvent=6110,"A fundamental challenge in contextual bandits is to develop flexible, general-purpose algorithms with computational requirements no worse than classical supervised learning tasks such as classification and regression. Algorithms based on regression have shown promising empirical success, but theoretical guarantees have remained elusive except in special cases. We provide the first universal and optimal reduction from contextual bandits to online regression. We show how to transform any oracle for online regression with a given value function class into an algorithm for contextual bandits with the induced policy class, with no overhead in runtime or memory requirements. We characterize the minimax rates for contextual bandits with general, potentially nonparametric function classes, and show that our algorithm is minimax optimal whenever the oracle obtains the optimal rate for regression. Compared to previous results, our algorithm requires no distributional assumptions beyond realizability, and works even when contexts are chosen adversarially.
","['MIT', 'MIT']"
2020,Designing Optimal Dynamic Treatment Regimes: A Causal Reinforcement Learning Approach,Junzhe Zhang,https://icml.cc/Conferences/2020/Schedule?showEvent=6292,"A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine the treatment assignment to patients based on evolving treatments and covariates' history. These regimes are particularly effective for managing chronic disorders and is arguably one of the critical ingredients underlying more personalized decision-making systems. All reinforcement learning algorithms for finding the optimal DTR in online settings will suffer O(\sqrt{|D{X, S}|T}) regret on some environments, where T is the number of experiments, and D{X, S} is the domains of treatments X and covariates S. This implies T = O (|D{X, S}|) trials to generate an optimal DTR. In many applications, domains of X and S could be so enormous that the time required to ensure appropriate learning may be unattainable. We show that, if the causal diagram of the underlying environment is provided, one could achieve regret that is exponentially smaller than D{X, S}. In particular, we develop two online algorithms that satisfy such regret bounds by exploiting the causal structure underlying the DTR; one is based on the principle of optimism in the face of uncertainty (OFU-DTR), and the other uses the posterior sampling learning (PS-DTR). Finally, we introduce efficient methods to accelerate these online learning procedures by leveraging the abundant, yet biased observational (non-experimental) data.
",['Columbia University']
2020,Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation,"Wenxian Shi, Hao Zhou, Ning Miao, Lei Li",https://icml.cc/Conferences/2020/Schedule?showEvent=6321,"Interpretability is important in text generation for guiding the generation with interpretable attributes.
Variational auto-encoder (VAE) with Gaussian distribution as prior has been successfully applied in text generation, but it is hard to interpret the meaning of the latent variable.
To enhance the controllability and interpretability, one can replace the Gaussian prior with a mixture of Gaussian distributions (GM-VAE), whose mixture components could be related to some latent attributes of data.
Unfortunately, straightforward variational training of GM-VAE leads the mode-collapse problem.
In this paper, we find that mode-collapse is a general problem for VAEs with exponential family mixture priors. 
We propose DEM-VAE, which introduces an extra dispersion term to induce a well-structured latent space.
Experimental results show that our approach does obtain a well structured latent space, with which our method outperforms strong baselines in interpretable text generation benchmarks.
","['Bytedance', 'Bytedance', 'ByteDance AI Lab', 'ByteDance AI Lab']"
2020,Variance Reduction in Stochastic Particle-Optimization Sampling,"Jianyi Zhang, Yang Zhao, Changyou Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6204,"Stochastic particle-optimization sampling (SPOS) is a recently-developed scalable Bayesian sampling framework unifying stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) algorithms based on Wasserstein gradient flows. With a rigorous non-asymptotic convergence theory developed, SPOS can avoid the particle-collapsing pitfall of SVGD. However, the variance-reduction effect in SPOS has not been clear. In this paper, we address this gap by presenting several variance-reduction techniques for SPOS. Specifically, we propose three variants of variance-reduced SPOS, called SAGA particle-optimization sampling (SAGA-POS), SVRG particle-optimization sampling (SVRG-POS) and a variant of SVRG-POS which avoids full gradient computations, denoted as SVRG-POS$^+$. Importantly, we provide non-asymptotic convergence guarantees for these algorithms in terms of the 2-Wasserstein metric and analyze their complexities. The results show our algorithms yield better convergence rates than existing variance-reduced variants of stochastic Langevin dynamics, though more space is required to store the particles in training. Our theory aligns well with experimental results on both synthetic and real datasets.","['Duke University', 'University at Buffalo', 'SUNY Buffalo']"
2020,Variable Skipping for Autoregressive Range Density Estimation,"Eric Liang, Zongheng Yang, Ion Stoica, Pieter Abbeel, Yan Duan, Peter Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6199,"Deep autoregressive models compute point likelihood estimates of individual data points. However, many applications (i.e., database cardinality estimation), require estimating range densities, a capability that is under-explored by current neural density estimation literature. In these applications, fast and accurate range density estimates over high-dimensional data directly impact user-perceived performance. In this paper, we explore a technique for accelerating range density estimation over deep autoregressive models. This technique, called variable skipping, exploits the sparse structure of range density queries to avoid sampling unnecessary variables during approximate inference. We show that variable skipping provides 10-100x efficiency improvements when targeting challenging high-quantile error metrics, enables complex applications such as text pattern matching, and can be realized via a simple data augmentation procedure without changing the usual maximum likelihood objective.
","['University of California, Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley & Covariant', 'COVARIANT.AI', 'COVARIANT.AI']"
2020,Nearly Linear Row Sampling Algorithm for Quantile Regression,"Yi Li, Ruosong Wang, Lin Yang, Hanrui Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=5941,"We give a row sampling algorithm for the quantile loss function with sample complexity nearly linear in the dimensionality of the data, improving upon the previous best algorithm whose sampling complexity has at least cubic dependence on the dimensionality. Based upon our row sampling algorithm, we give the fastest known algorithm for quantile regression and a graph sparsification algorithm for balanced directed graphs. Our main technical contribution is to show that Lewis weights sampling, which has been used in row sampling algorithms for $\ell_p$ norms, can also be applied in row sampling algorithms for a variety of loss functions. We complement our theoretical results by experiments to demonstrate the practicality of our approach. ","['Nanyang Technological University', 'Carnegie Mellon University', 'UCLA', 'Duke University']"
2020,Variational Imitation Learning with Diverse-quality Demonstrations,"Voot Tangkaratt, Bo Han, Mohammad Emtiyaz Khan, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=5846,"Learning from demonstrations can be challenging when the quality of demonstrations is diverse, and even more so when the quality is unknown and there is no additional information to estimate the quality. We propose a new method for imitation learning in such scenarios. We show that simple quality-estimation approaches might fail due to compounding error, and fix this issue by jointly estimating both the quality and reward using a variational approach. Our method is easy to implement within reinforcement-learning frameworks and also achieves state-of-the-art performance on continuous-control benchmarks.Our work enables scalable and data-efficient imitation learning under more realistic settings than before.
","['RIKEN AIP', 'HKBU / RIKEN', 'RIKEN', 'RIKEN / The University of Tokyo']"
2020,Task-Oriented Active Perception and Planning in Environments with Partially Known Semantics,"Mahsa Ghasemi, Erdem Bulgur, Ufuk Topcu",https://icml.cc/Conferences/2020/Schedule?showEvent=6551,"We consider an agent that is assigned with a temporal logic task in an environment whose semantic representation is only partially known. We represent the semantics of the environment with a set of state properties, called \textit{atomic propositions} over which, the agent holds a probabilistic belief and updates it as new sensory measurements arrive. The goal is to design a policy for the agent that realizes the task with high probability. We develop a planning strategy that takes the semantic uncertainties into account and by doing so provides probabilistic guarantees on the task success. Furthermore, as new data arrive, the belief over the atomic propositions evolves and, subsequently, the planning strategy adapts accordingly. We evaluate the proposed method on various finite-horizon tasks in planar navigation settings where the empirical results show that the proposed method provides reliable task performance that also improves as the knowledge about the environment enhances.
","['The University of Texas at Austin', 'University of Texas at Austin', 'University of Texas at Austin']"
2020,Fast OSCAR and OWL Regression via Safe Screening Rules,"Runxue Bao, Bin Gu, Heng Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=5874,"Ordered Weighted $L_{1}$ (OWL) regularized regression is a new regression analysis for high-dimensional sparse learning. Proximal gradient methods are used as standard approaches to solve OWL regression. However, it is still a burning issue to solve OWL regression due to considerable computational cost and memory usage when the feature or sample size is large. In this paper, we propose the first safe screening rule for OWL regression by exploring the order of the primal solution with the unknown order structure via an iterative strategy, which overcomes the difficulties of tackling the non-separable regularizer. It effectively avoids the updates of the parameters whose coefficients must be zero during the learning process. More importantly, the proposed screening rule can be easily applied to standard and stochastic proximal gradient methods. Moreover, we prove that the algorithms with our screening rule are guaranteed to have identical results with the original algorithms. Experimental results on a variety of datasets show that our screening rule leads to a significant computational gain without any loss of accuracy, compared to existing competitive algorithms. ","['University of Pittsburgh', 'Nanjing University of Information Science & Technology', 'University of Pittsburgh & JD Finance America Corporation']"
2020,Retrieval Augmented Language Model Pre-Training,"Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Mingwei Chang",https://icml.cc/Conferences/2020/Schedule?showEvent=6294,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.
","['Google', 'Google Research', 'Google', 'Google', 'Google']"
2020,Certified Robustness to Label-Flipping Attacks via Randomized Smoothing,"Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, Zico Kolter",https://icml.cc/Conferences/2020/Schedule?showEvent=6172,"Machine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. In this work, we propose a strategy for building linear classifiers that are certifiably robust against a strong variant of label flipping, where each test example is targeted independently. In other words, for each test point, our classifier includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Our approach leverages randomized smoothing, a technique that has previously been used to guarantee---with high probability---test-time robustness to adversarial manipulation of the input to a classifier. We derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with minimal additional runtime complexity over standard classification and no assumptions on the train or test distributions. We generalize our results to the multi-class case, providing the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.
","['Carnegie Mellon University', '', 'Carnegie Mellon University', 'Carnegie Mellon University / Bosch Center for AI']"
2020,BoXHED: Boosted eXact Hazard Estimator with Dynamic covariates,"Xiaochen Wang, Arash Pakbin, Bobak Mortazavi, Hongyu Zhao, Donald Lee",https://icml.cc/Conferences/2020/Schedule?showEvent=6243,"The proliferation of medical monitoring devices makes it possible to track health vitals at high frequency, enabling the development of dynamic health risk scores that change with the underlying readings. Survival analysis, in particular hazard estimation, is well-suited to analyzing this stream of data to predict disease onset as a function of the time-varying vitals. This paper introduces the software package BoXHED (pronounced `box-head') for nonparametrically estimating hazard functions via gradient boosting. BoXHED 1.0 is a novel tree-based implementation of the generic estimator proposed in Lee et al. (2017), which was designed for handling time-dependent covariates in a fully nonparametric manner. BoXHED is also the first publicly available software implementation for Lee et al. (2017). Applying BoXHED to cardiovascular disease onset data from the Framingham Heart Study reveals novel interaction effects among known risk factors, potentially resolving an open question in clinical literature.
","['Yale University', 'Texas A&M University', 'Texas A&M University', 'Yale University', 'Emory University']"
2020,Learning Algebraic Multigrid Using Graph Neural Networks,"Ilay Luz, Meirav Galun, Haggai Maron, Ronen Basri, Irad Yavneh",https://icml.cc/Conferences/2020/Schedule?showEvent=6369,"Efficient numerical solvers for sparse linear systems are crucial in science and engineering. One of the fastest methods for solving large-scale sparse linear systems is algebraic multigrid (AMG). The main challenge in the construction of AMG algorithms is the selection of the prolongation operator---a problem-dependent sparse matrix which governs the multiscale hierarchy of the solver and is critical to its efficiency. Over many years, numerous methods have been developed for this task, and yet there is no known single right answer except in very special cases. Here we propose a framework for learning AMG prolongation operators for linear systems with sparse symmetric positive (semi-) definite matrices. We train a single graph neural network to learn a mapping from an entire class of such matrices to prolongation operators, using an efficient unsupervised loss function.  Experiments on a broad class of problems demonstrate improved convergence rates compared to classical AMG, demonstrating the potential utility of neural networks for developing sparse system solvers.
","['Weizmann Institute of Science', 'Weizmann Institute of Science', 'NVIDIA Research', 'Weizmann Institute of Science', 'Technion']"
2020,Learning Compound Tasks without Task-specific Knowledge via Imitation and Self-supervised Learning,"Sang-Hyun Lee, Seung-Woo Seo",https://icml.cc/Conferences/2020/Schedule?showEvent=6468,"Most real-world tasks are compound tasks that consist of multiple simpler sub-tasks. The main challenge of learning compound tasks is that we have no explicit supervision to learn the hierarchical structure of compound tasks. To address this challenge, previous imitation learning methods exploit task-specific knowledge, e.g., labeling demonstrations manually or specifying termination conditions for each sub-task. However, the need for task-specific knowledge makes it difficult to scale imitation learning to real-world tasks. In this paper, we propose an imitation learning method that can learn compound tasks without task-specific knowledge. The key idea behind our method is to leverage a self-supervised learning framework to learn the hierarchical structure of compound tasks. Our work also proposes a task-agnostic regularization technique to prevent unstable switching between sub-tasks, which has been a common degenerate case in previous works. We evaluate our method against several baselines on compound tasks. The results show that our method achieves state-of-the-art performance on compound tasks, outperforming prior imitation learning methods.
","['Seoul National University', 'Seoul National University']"
2020,A Markov Decision Process Model for Socio-Economic Systems Impacted by Climate Change,"Salman Sadiq Shuvo, Yasin Yilmaz, Alan Bush, Mark Hafen",https://icml.cc/Conferences/2020/Schedule?showEvent=6436,"Coastal communities are at high risk of natural hazards due to unremitting global warming and sea level rise. Both the catastrophic impacts, e.g., tidal flooding and storm surges, and the long-term impacts, e.g., beach erosion, inundation of low lying areas, and saltwater intrusion into aquifers, cause economic, social, and ecological losses. Creating policies through appropriate modeling of the responses of stakeholders, such as government, businesses, and residents, to climate change and sea level rise scenarios can help to reduce these losses. In this work, we propose a Markov decision process (MDP) formulation for an agent (government) which interacts with the environment (nature and residents) to deal with the impacts of climate change, in particular sea level rise. Through theoretical analysis we show that a reasonable government's policy on infrastructure development ought to be proactive and based on detected sea levels in order to minimize the expected total cost, as opposed to a straightforward government that reacts to observed costs from nature. We also provide a deep reinforcement learning-based scenario planning tool considering different government and resident types in terms of cooperation, and different sea level rise projections by the National Oceanic and Atmospheric Administration (NOAA). 
","['University of South Florida', 'University of South Florida', 'University of South Florida', 'University of South Florida']"
2020,Tightening Exploration in Upper Confidence Reinforcement Learning,"Hippolyte Bourel, Odalric-Ambrym Maillard, Mohammad Sadegh Talebi",https://icml.cc/Conferences/2020/Schedule?showEvent=6471,"The upper confidence reinforcement learning (\UCRL) strategy introduced in \citep{jaksch2010near} is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. 
Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. 
In pursuit of practical efficiency, we present \UCRLnew, following the lines of \UCRL, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities, to compute confidence sets on the reward and (component-wise) transition distributions for each state-action pair. Further, to tighten exploration, it uses an adaptive computation of the support of each transition distributions, which in turn enables us to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism.
We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to \UCRL\ and its variants. On the theoretical side, these key modifications enable us to derive a regret bound for \UCRLnew\ improving on \UCRL, that for the first time makes appear notions of local diameter and effective support, thanks to variance-aware concentration bounds.   
","['ENS Rennes', 'Inria Lille - Nord Europe', 'University of Copenhagen']"
2020,Non-Autoregressive Neural Text-to-Speech,"Kainan Peng, Wei Ping, Zhao Song, Kexin Zhao",https://icml.cc/Conferences/2020/Schedule?showEvent=6714,"In this work, we propose ParaNet, a non-autoregressive seq2seq model that converts text to spectrogram.  It is fully convolutional and brings 46.7 times speed-up over the lightweight Deep Voice 3 at synthesis, while obtaining reasonably good speech quality.  ParaNet also produces stable alignment between text and speech on the challenging test sentences by iteratively improving the attention in a layer-by-layer manner.  Furthermore, we build the parallel text-to-speech system by applying various parallel neural vocoders, which can synthesize speech from text through a single feed-forward pass.  We also explore a novel VAE-based approach to train the inverse autoregressive flow~(IAF) based parallel vocoder from scratch, which avoids the need for distillation from a separately trained WaveNet as previous work.
","['Baidu Research', 'Baidu Research', 'Baidu Research', 'Baidu']"
2020,How Good is the Bayes Posterior in Deep Neural Networks Really?,"Florian Wenzel, Kevin Roth, Bastiaan Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans, Rodolphe Jenatton, Sebastian Nowozin",https://icml.cc/Conferences/2020/Schedule?showEvent=6367,"During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks.  However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice.  In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions when compared to simpler methods including point estimates obtained from SGD.  Furthermore, we demonstrate that predictive performance is improved significantly through the use of a ``cold posterior'' that overcounts evidence.  Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers.  We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments.  Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations?  Instead, we argue that it is timely to focus on understanding the origin of cold posteriors.
","['Google Research', 'ETH Zurich', 'University of Amsterdam', 'University of Warsaw', 'Imperial College London', 'University of California, Irivine', 'Google Brain', 'Google', 'Google Research', 'Microsoft Research']"
2020,Enhanced POET: Open-ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions,"Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, Kenneth Stanley",https://icml.cc/Conferences/2020/Schedule?showEvent=6544,"Creating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges, and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential.  Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved through other means.
","['Uber AI', '', 'Amazon AWS AI Labs', 'Uber AI', 'Uber AI', 'Open AI', 'OpenAI']"
2020,Provable guarantees for decision tree induction: the agnostic setting ,"Guy Blanc, Jane Lange, Li-Yang Tan",https://icml.cc/Conferences/2020/Schedule?showEvent=6806,"We give strengthened provable guarantees on the performance of widely employed and empirically successful {\sl top-down decision tree learning heuristics}.  While prior works have focused on the realizable setting, we consider the more realistic and challenging {\sl agnostic} setting.  We show that for all monotone functions~$f$ and $s\in \mathbb{N}$, these heuristics construct a decision tree of size $s^{\tilde{O}((\log s)/\varepsilon^2)}$ that achieves error $\le \mathsf{opt}_s + \varepsilon$, where $\mathsf{opt}_s$ denotes the error of the optimal size-$s$ decision tree for $f$.  Previously such a guarantee was not known to be achievable by any algorithm, even one that is not based on top-down heuristics.  We complement our algorithmic guarantee with a near-matching $s^{\tilde{\Omega}(\log s)}$ lower bound.","['Stanford University', 'Stanford University', 'Stanford University']"
2020,Two Simple Ways to Learn Individual Fairness Metrics from Data,"Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, Yuekai Sun",https://icml.cc/Conferences/2020/Schedule?showEvent=6761,"Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches. 
","['University of Michigan', 'IBM Research AI', 'University of Michigan', 'University of Michigan']"
2020,The Many Shapley Values for Model Explanation,"Mukund Sundararajan, Amir Najmi",https://icml.cc/Conferences/2020/Schedule?showEvent=5812,"The Shapley value has become the basis for several methods that attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing the uniqueness result from~\cite{Shapley53}, which shows that it is the only method that satisfies certain good properties (\emph{axioms}). 
There are, however, a multiplicity of ways in which the Shapley value is operationalized for model explanation. 
These differ in how they reference the model, the training data, and the explanation context. Hence they differ in output, rendering the uniqueness result inapplicable. Furthermore, the techniques that rely on they training data produce non-intuitive attributions, for instance unused features can still receive attribution.
In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution. We discuss a technique called Baseline Shapley (BShap), provide a proper uniqueness result for it, and contrast it with two other techniques from prior literature, Integrated Gradients~\cite{STY17} and Conditional Expectation Shapley~\cite{Lundberg2017AUA}. 
","['Google Inc.', 'Google']"
2020,Bayesian Graph Neural Networks with Adaptive Connection Sampling,"Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick Duffield, Krishna Narayanan, Xiaoning Qian",https://icml.cc/Conferences/2020/Schedule?showEvent=6755,"We propose a unified framework for adaptive connection sampling in graph neural  networks (GNNs) that generalizes existing stochastic regularization methods for  training GNNs. The proposed framework not only alleviates over-smoothing and  over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning themas model hyperparameters in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training BayesianGNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boost the performance of GNNs in semi-supervised node classification, less prone to over-smoothing and over-fitting with more robust prediction.
","['Texas A&M University', 'Texas A&M University', 'Texas A&M University', 'University of Texas at Austin', 'Texas A&M University', 'Texas A&M University', 'Texas A&M University']"
2020,Multiresolution Tensor Learning for Efficient and Interpretable Spatial Analysis,"Jung Yeon Park, Kenneth Carr, Stephan Zheng, Yisong Yue, Rose Yu",https://icml.cc/Conferences/2020/Schedule?showEvent=6620,"Efficient and interpretable spatial analysis is crucial in many fields such as geology, sports, and climate science. Tensor latent factor models can describe higher-order correlations for spatial data. However, they are computationally expensive to train and are sensitive to initialization, leading to spatially incoherent, uninterpretable results. We develop a novel Multiresolution Tensor Learning (MRTL) algorithm for efficiently learning interpretable spatial patterns. MRTL initializes the latent factors from an approximate full-rank tensor model for improved interpretability and progressively learns from a coarse resolution to the fine resolution for boosted efficiency. We also prove the theoretical convergence and computational complexity of MRTL. When applied to two real-world datasets, MRTL demonstrates 4~5x speedup compared to a fixed resolution approach while yielding accurate and interpretable models.
","['Northeastern University', 'Northeastern University', 'Salesforce', 'Caltech', 'University of California, San Diego']"
2020,Rank Aggregation from Pairwise Comparisons in the Presence of Adversarial Corruptions,"Arpit Agarwal, Shivani Agarwal, Sanjeev Khanna, Prathamesh Patil",https://icml.cc/Conferences/2020/Schedule?showEvent=6577,"Rank aggregation from pairwise preferences has widespread applications in recommendation systems and information retrieval. Given the enormous economic and societal impact of these applications, and the consequent incentives for malicious players to manipulate ranking outcomes in their favor, an important challenge is to make rank aggregation algorithms robust to adversarial manipulations in data. In this paper, we initiate the study of robustness in rank aggregation under the popular Bradley-Terry-Luce (BTL) model for pairwise comparisons. We consider a setting where pairwise comparisons are initially generated according to a BTL model, but a fraction of these comparisons are corrupted by an adversary prior to being reported to us. We consider a strong contamination model, where an adversary having complete knowledge of the initial truthful data and the underlying true BTL parameters, can subsequently corrupt the truthful data by inserting, deleting, or changing data points. The goal is to estimate the true score/weight of each item under the BTL model, even in the presence of these corruptions. We characterize the extent of adversarial corruption under which the true BTL parameters are uniquely identifiable. We also provide a novel pruning algorithm that provably cleans the data of adversarial corruption under reasonable conditions on data generation and corruption. We corroborate our theory with experiments on both synthetic as well as real data showing that previous algorithms are vulnerable to even small amounts of corruption, whereas our algorithm can clean a reasonably high amount of corruption.
","['University of Pennsylvania', 'University of Pennsylvania', 'U of Pennsylvania ', 'University of Pennsylvania']"
2020,Robustness to Spurious Correlations via Human Annotations,"Megha Srivastava, Tatsunori Hashimoto, Percy Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6818,"The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption---useful correlations between features and labels at training time can become useless or even harmful at test time. For example, high obesity is generally predictive for heart disease, but this relation may not hold for smokers who generally have lower rates of obesity and higher rates of heart disease. We present a framework for making models robust to spurious correlations by leveraging humans' common sense knowledge of causality. Specifically, we use human annotation to augment each training example with a potential unmeasured variable (i.e. an underweight patient with heart disease may be a smoker), reducing the problem to a covariate shift problem. We then introduce a new distributionally robust optimization objective over unmeasured variables (UV-DRO) to control the worst-case loss over possible test- time shifts. Empirically, we show improvements of 5--10% on a digit recognition task confounded by rotation, and 1.5--5% on the task of analyzing NYPD Police Stops confounded by location.
","['Stanford University', 'Stanford', 'Stanford University']"
2020,Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences,"Daniel Brown, Russell Coleman, Ravi Srinivasan, Scott Niekum",https://icml.cc/Conferences/2020/Schedule?showEvent=6465,"Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.
","['University of Texas at Austin', 'University of Texas at Austin', 'The University of Texas at Austin', 'University of Texas at Austin']"
2020,Low-loss connection of weight vectors: distribution-based approaches,"Ivan Anokhin, Dmitry Yarotsky",https://icml.cc/Conferences/2020/Schedule?showEvent=6427,"Recent research shows that sublevel sets of the loss surfaces of overparameterized networks are connected, exactly or approximately. We describe and compare experimentally a panel of methods used to connect two low-loss points by a low-loss curve on this surface. Our methods vary in accuracy and complexity. Most of our methods are based on ''macroscopic'' distributional assumptions and are insensitive to the detailed properties of the points to be connected. Some methods require a prior training of a ''global connection model'' which can then be applied to any pair of points. The accuracy of the method generally correlates with its complexity and sensitivity to the endpoint detail.
","['Skolkovo Institute of Science and Technology', 'Skolkovo Institute of Science and Technology']"
2020,Structured Policy Iteration for Linear Quadratic Regulator,"Youngsuk Park, Ryan A. Rossi, Zheng Wen, Gang Wu, Handong Zhao",https://icml.cc/Conferences/2020/Schedule?showEvent=6371,"Linear quadratic regulator (LQR) is one of the most popular frameworks to tackle continuous Markov decision process tasks. With its fundamental theory and tractable optimal policy, LQR has been revisited and analyzed in recent years, in terms of reinforcement learning scenarios such as the model-free or model-based setting. In this paper, we introduce the Structured Policy Iteration (S-PI) for LQR, a method capable of deriving a structured linear policy. Such a structured policy with (block) sparsity or low-rank can have significant advantages over the standard LQR policy: more interpretable, memory-efficient, and well-suited for the distributed setting. In order to derive such a policy, we first cast a regularized LQR problem when the model is known. Then, our Structured Policy Iteration (S-PI) algorithm, which takes a policy evaluation step and a policy improvement step in an iterative manner, can solve this regularized LQR efficiently. We further extend the S-PI algorithm to the model-free setting where a smoothing procedure is adopted to estimate the gradient. In both the known-model and model-free setting, we prove convergence analysis under the proper choice of parameters. Finally, the experiments demonstrate the advantages of S-PI in terms of balancing the LQR performance and level of structure by varying the weight parameter.
","['Stanford University', 'Adobe Research', 'DeepMind', 'Adobe Research', 'Adobe Research']"
2020,Deep k-NN for Noisy Labels,"Dara Bahri, Heinrich Jiang, Maya Gupta",https://icml.cc/Conferences/2020/Schedule?showEvent=6174,"Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple $k$-nearest neighbor-based filtering approach on the logit layer of a preliminary model can remove mislabeled training data and produce more accurate models than many recently proposed methods. We also provide new statistical guarantees into its efficacy.","['Google Research', 'Google Research', 'Google']"
2020,Evolutionary Reinforcement Learning for Sample-Efficient Multiagent Coordination,"Somdeb Majumdar, Shauharda Khadka, Santiago Miret, Stephen Mcaleer, Kagan Tumer",https://icml.cc/Conferences/2020/Schedule?showEvent=5872,"Many cooperative multiagent reinforcement learning environments provide agents with a sparse team-based reward, as well as a dense agent-specific reward that incentivizes learning basic skills. Training policies solely on the team-based reward is often difficult due to its sparsity. Also, relying solely on the agent-specific reward is sub-optimal because it usually does not capture the team coordination objective. A common approach is to use reward shaping to construct a proxy reward by combining the individual rewards. However, this requires manual tuning for each environment. We introduce Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training platform that handles the two objectives separately through two optimization processes. An evolutionary algorithm maximizes the sparse team-based objective through neuroevolution on a population of teams. Concurrently, a gradient-based optimizer trains policies to only maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population as a way of information transfer between the two optimization processes. This enables the evolutionary algorithm to use skills learned via the agent-specific rewards toward optimizing the global objective. Results demonstrate that MERL significantly outperforms state-of-the-art methods, such as MADDPG, on a number of difficult coordination benchmarks.
","['Intel Labs', 'Intel Labs', 'Intel Labs', 'UC Irvine', 'Oregon State University US']"
2020,Adversarial Attacks on Probabilistic Autoregressive Forecasting Models,"Raphaël Dang-Nhu, Gagandeep Singh, Pavol Bielik, Martin Vechev",https://icml.cc/Conferences/2020/Schedule?showEvent=5838,"We develop an effective generation of adversarial attacks on neural models that output a sequence of probability distributions rather than a sequence of single values. This setting includes the recently proposed deep probabilistic autoregressive forecasting models that estimate the probability distribution of a time series given its past and achieve state-of-the-art results in a diverse set of application domains. The key technical challenge we address is how to effectively differentiate through the Monte-Carlo estimation of statistics of the output sequence joint distribution. Additionally, we extend prior work on probabilistic forecasting to the Bayesian setting which allows conditioning on future observations, instead of only on past observations. We demonstrate that our approach can successfully generate attacks with small input perturbations in two challenging tasks where robust decision making is crucial -- stock market trading and prediction of electricity consumption.
","['ETH Zürich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2020,Context Aware Local Differential Privacy,"Jayadev Acharya, Kallista Bonawitz, Peter Kairouz, Daniel  Ramage, Ziteng Sun",https://icml.cc/Conferences/2020/Schedule?showEvent=5775,"Local differential privacy (LDP) is a strong notion of privacy that often leads to a significant drop in utility. The original definition of LDP assumes that all the elements in the data domain are equally sensitive. However, in many real-life applications, some elements are more sensitive than others. We propose a context-aware framework for LDP that allows the privacy level to vary across the data domain, enabling system designers to place privacy constraints where they matter without paying the cost where they do not. For binary data domains, we provide a universally optimal privatization scheme and highlight its connections to Warner’s randomized response and Mangat’s improved response. Motivated by geo-location and web search applications, for k-ary data domains, we consider two special cases of context-aware LDP: block-structured LDP and high-low LDP. We study minimax discrete distribution estimation under both cases and provide communication-efficient, sample-optimal schemes, and information-theoretic lower bounds. We show, using worst-case analyses and experiments on Gowalla’s 3.6 million check-ins to 43,750 locations, that context-aware LDP achieves a far better accuracy under the same number of samples.
","['Cornell University', 'Google', 'Google', 'Google', 'Cornell University']"
2020,Hierarchically Decoupled Imitation For Morphological Transfer,"Donald Hejna, Lerrel Pinto, Pieter Abbeel",https://icml.cc/Conferences/2020/Schedule?showEvent=6826,"Learning long-range behaviors on complex high-dimensional agents is a fundamental problem in robot learning. For such tasks, we argue that transferring learned information from a morphologically simpler agent can massively improve the sample efficiency of a more complex one. To this end, we propose a hierarchical decoupling of policies into two parts: an independently learned low-level policy and a transferable high-level policy. To remedy poor transfer performance due to mismatch in morphologies, we contribute two key ideas. First, we show that incentivizing a complex agent's low-level to imitate a simpler agent's low-level significantly improves zero-shot high-level transfer. Second, we show that KL-regularized training of the high level stabilizes learning and prevents mode-collapse. Finally, on a suite of publicly released navigation and manipulation environments, we demonstrate the applicability of hierarchical transfer on long-range tasks across morphologies.
","['UC Berkeley', 'NYU/Berkeley', 'UC Berkeley']"
2020,Learning and Sampling of Atomic Interventions from Observations,"Arnab Bhattacharyya, Sutanu Gayen, Saravanan Kandasamy, Ashwin Maran, Vinodchandran N. Variyam",https://icml.cc/Conferences/2020/Schedule?showEvent=6800,"We study the problem of efficiently estimating the effect of an intervention on a single variable using observational samples. Our goal is to give algorithms with polynomial time and sample complexity in a non-parametric setting. Tian and Pearl (AAAI '02) have exactly characterized the class of causal graphs for which causal effects of atomic interventions can be identified from observational data. We make their result quantitative.  Suppose 𝒫 is a causal model on a set V of n observable variables with respect to a given causal graph G,  and let do(x) be an identifiable intervention on a variable X.  We show that assuming that G has bounded in-degree and bounded c-components (k) and that the observational distribution satisfies a strong positivity condition:
 (i) [Evaluation] There is an algorithm that outputs with probability 2/3 an evaluator for a distribution P^ that satisfies TV(P(V | do(x)), P^(V)) < eps using m=O~(n/eps^2) samples from P and O(mn) time. The evaluator can return in O(n) time the probability P^(v) for any assignment v to V.
 (ii) [Sampling] There is an algorithm that outputs with probability 2/3 a sampler for a distribution P^ that satisfies TV(P(V | do(x)), P^(V)) < eps using m=O~(n/eps^2) samples from P and O(mn) time. The sampler returns an iid sample from P^ with probability 1 in O(n) time. 
We extend our techniques to estimate P(Y | do(x)) for a subset Y of variables of interest. We also show lower bounds for the sample complexity, demonstrating that our sample complexity has optimal dependence on the parameters n and eps, as well as if k=1 on the strong positivity parameter.
","['National University of Singapore', 'National University of SIngapore', 'Cornell University', 'University of Wisconsin-Madison', 'University of Nebraska, Lincoln']"
2020,Stabilizing Differentiable Architecture Search via Perturbation-based Regularization,"Xiangning Chen, Cho-Jui Hsieh",https://icml.cc/Conferences/2020/Schedule?showEvent=6607,"Differentiable architecture search (DARTS) is a prevailing NAS solution to identify architectures. Based on the continuous relaxation of the architecture space, DARTS learns a differentiable architecture weight and largely reduces the search cost. However, its stability has been challenged for yielding deteriorating architectures as the search proceeds. We find that the precipitous validation loss landscape, which leads to a dramatic performance drop when distilling the final architecture, is an essential factor that causes instability. Based on this observation, we propose a perturbation-based regularization - SmoothDARTS (SDARTS), to smooth the loss landscape and improve the generalizability of DARTS-based methods. In particular, our new formulations stabilize DARTS-based methods by either random smoothing or adversarial attack. The search trajectory on NAS-Bench-1Shot1 demonstrates the effectiveness of our approach and due to the improved stability, we achieve performance gain across various search spaces on 4 datasets. Furthermore, we mathematically show that SDARTS implicitly regularizes the Hessian norm of the validation loss, which accounts for a smoother loss landscape and improved performance.
","['UCLA', 'UCLA']"
2020,Feature Quantization Improves GAN Training,"Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, Changyou Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6569,"The instability in GANs' training has been a long-standing problem despite remarkable research efforts. We identify that instability issues stem from difficulties of performing feature matching with mini-batch statistics, due to a fragile balance between the fixed target distribution and the progressively generated distribution. In this work, we propose feature quantizatoin (FQ) for the discriminator, to embed both true and fake data samples into a shared discrete space. The quantized values of FQ are constructed as an evolving dictionary, which is consistent with feature statistics of the recent distribution history. Hence, FQ implicitly enables robust feature matching in a compact space.  Our method can be easily plugged into existing GAN models, with little computational overhead in training. Extensive experimental results show that the proposed FQ-GAN can improve the FID scores of baseline methods by a large margin on a variety of tasks, including three representative GAN models on 10 benchmarks, achieving new state-of-the-art performance.
","['University at Buffalo', 'Microsoft Research', 'Sony Interactive Entertainment LLC', 'Microsoft Research AI', 'SUNY Buffalo']"
2020,Inverse Active Sensing: Modeling and Understanding Timely Decision-Making,"Daniel Jarrett, Mihaela van der Schaar",https://icml.cc/Conferences/2020/Schedule?showEvent=6463,"Evidence-based decision-making entails collecting (costly) observations about an underlying phenomenon of interest, and subsequently committing to an (informed) decision on the basis of accumulated evidence. In this setting, active sensing is the goal-oriented problem of efficiently selecting which acquisitions to make, and when and what decision to settle on. As its complement, inverse active sensing seeks to uncover an agent's preferences and strategy given their observable decision-making behavior. In this paper, we develop an expressive, unified framework for the general setting of evidence-based decision-making under endogenous, context-dependent time pressure---which requires negotiating (subjective) tradeoffs between accuracy, speediness, and cost of information. Using this language, we demonstrate how it enables modeling intuitive notions of surprise, suspense, and optimality in decision strategies (the forward problem). Finally, we illustrate how this formulation enables understanding decision-making behavior by quantifying preferences implicit in observed decision strategies (the inverse problem).
","['University of Cambridge', 'University of Cambridge and UCLA']"
2020,Being Bayesian about Categorical Probability,"Taejong Joo, Uijung Chung, Min-Gwan Seo",https://icml.cc/Conferences/2020/Schedule?showEvent=6362,"Neural networks utilize the softmax as a building block in classification tasks, which contains an overconfidence problem and lacks an uncertainty representation ability. As a Bayesian alternative to the softmax, we consider a random variable of a categorical probability over class labels. In this framework, the prior distribution explicitly models the presumed noise inherent in the observed label, which provides consistent gains in generalization performance in multiple challenging tasks. The proposed method inherits advantages of Bayesian approaches that achieve better uncertainty estimation and model calibration. Our method can be implemented as a plug-and-play loss function with negligible computational overhead compared to the softmax with the cross-entropy loss function.
","['ESTsoft', 'ESTsoft', 'ESTsoft']"
2020,Fiduciary Bandits,"Gal Bahar, Omer Ben-Porat, Kevin Leyton-Brown, Moshe Tennenholtz",https://icml.cc/Conferences/2020/Schedule?showEvent=5780,"Recommendation systems often face exploration-exploitation tradeoffs: the system can only learn about the desirability of new options by recommending them to some user. Such systems can thus be modeled as multi-armed bandit settings; however, users are self-interested and cannot be made to follow recommendations. We ask whether exploration can nevertheless be performed in a way that scrupulously respects agents' interests---i.e., by a system that acts as a fiduciary. 
More formally, we introduce a  model in which a recommendation system faces an exploration-exploitation tradeoff under the constraint that it can never recommend any action that it knows yields lower reward in expectation than an agent would achieve if it acted alone. Our main contribution is a positive result: an asymptotically optimal, incentive compatible, and ex-ante individually rational recommendation algorithm.
","['Technion – Israel Institute of Technology', 'Technion--Israel Institute of Technology', 'University of British Columbia', 'Technion – Israel Institute of Technology']"
2020,"Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning","Qing Li, Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu, Song-Chun Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=6335,"The goal of neural-symbolic computation is to integrate the connectionist and symbolist paradigms. Prior methods learn the neural-symbolic models using reinforcement learning (RL) approaches, which ignore the error propagation in the symbolic reasoning module and thus converge slowly with sparse rewards. In this paper, we address these issues and close the loop of neural-symbolic learning by (1) introducing the grammar model as a symbolic prior to bridge neural perception and symbolic reasoning, and (2) proposing a novel back-search algorithm which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently. We further interpret the proposed learning framework as maximum likelihood estimation using Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. The experiments are conducted on two weakly-supervised neural-symbolic tasks: (1) handwritten formula recognition on the newly introduced HWF dataset; (2) visual question answering on the CLEVR dataset. The results show that our approach significantly outperforms the RL methods in terms of performance, converging speed, and data efficiency. Our code and data are released at https://liqing-ustc.github.io/NGS.
","['UCLA', 'UCLA', 'University of California, Los Angeles', 'UCLA', 'UCLA', 'UCLA']"
2020,A Chance-Constrained Generative Framework for Sequence Optimization,"Xianggen Liu, Qiang Liu, Sen  Song , Jian Peng",https://icml.cc/Conferences/2020/Schedule?showEvent=6086,"Deep generative modeling has achieved many successes for continuous data generation, such as producing realistic images and controlling their properties (e.g., styles). However, the development of generative modeling techniques for optimizing discrete data, such as sequences or strings, still lags behind largely due to the challenges in modeling complex and long-range constraints, including both syntax and semantics, in discrete structures. In this paper, we formulate the sequence optimization task as a chance-constrained optimization problem. The key idea is to enforce a high probability of generating valid sequences and also optimize the property of interest. We propose a novel minmax algorithm to simultaneously tighten a bound of the valid chance and optimize the expected property. Extensive experimental results in three domains demonstrate the superiority of our approach over the existing sequence optimization methods. 
","['Tsinghua University', 'UT Austin', 'Tsinghua University ', 'UIUC']"
2020,Description Based Text Classification with Reinforcement Learning,"Duo Chai, Wei Wu, Qinghong Han, Fei Wu, Jiwei Li",https://icml.cc/Conferences/2020/Schedule?showEvent=6081,"The task of text classification is usually divided into two stages: text feature extraction and classification. In this standard formalization, categories are merely represented as indexes in the label vocabulary, and the model lacks for explicit instructions on what to classify. Inspired by the current trend of formalizing NLP problems as question answering tasks, we propose a new framework for text classification, in which each category label is associated with a category description. Descriptions are generated by hand-crafted templates or using abstractive/extractive models from reinforcement learning. The concatenation of the description and the text is fed to the classifier to decide whether or not the current label should be assigned to the text. The proposed strategy forces the model to attend to the most salient texts with respect to the label, which can be regarded as a hard version of attention, leading to better performances. We observe significant performance boosts over strong baselines on a wide range of text classification tasks including single-label classification, multi-label classification and multi-aspect sentiment analysis.
","['Shannon.AI', 'Shannon.AI', 'Shannon.AI', 'Zhejiang University, China', 'Shannon.AI']"
2020,Generalization Guarantees for Sparse Kernel Approximation with Entropic Optimal Features,"Liang Ding, Rui Tuo, Shahin Shahrampour",https://icml.cc/Conferences/2020/Schedule?showEvent=6032,"Despite their success, kernel methods suffer from a massive computational cost in practice. In this paper, in lieu of commonly used kernel expansion with respect to $N$ inputs, we develop a novel optimal design maximizing the entropy among kernel features. This procedure results in a kernel expansion with respect to entropic optimal features (EOF), improving the data representation dramatically due to features dissimilarity. Under mild technical assumptions, our generalization bound shows that with only $O(N^{\frac{1}{4}})$ features (disregarding logarithmic factors), we can achieve the optimal statistical accuracy (i.e., $O(1/\sqrt{N})$). The salient feature of our design is its sparsity that significantly reduces the time and space costs. Our numerical experiments on benchmark datasets verify the superiority of EOF over the state-of-the-art in kernel approximation.","['Texas A&M University', 'Texas A&M University', 'Texas A&M University']"
2020,Lookahead-Bounded Q-learning,"Ibrahim El Shar, Daniel Jiang",https://icml.cc/Conferences/2020/Schedule?showEvent=6299,"We introduce the lookahead-bounded Q-learning (LBQL) algorithm, a new, provably convergent variant of Q-learning that seeks to improve the performance of standard Q-learning in stochastic environments through the use of “lookahead” upper and lower bounds. To do this, LBQL employs previously collected experience and each iteration’s state-action values as dual feasible penalties to construct a sequence of sampled information relaxation problems. The solutions to these problems provide estimated upper and lower bounds on the optimal value, which we track via stochastic approximation. These quantities are then used to constrain the iterates to stay within the bounds at every iteration. Numerical experiments on benchmark problems show that LBQL exhibits faster convergence and more robustness to hyperparameters when compared to standard Q-learning and several related techniques. Our approach is particularly appealing in problems that require expensive simulations or real-world interactions.
","['University of Pittsburgh', 'University of Pittsburgh']"
2020,An Investigation of Why Overparameterization Exacerbates Spurious Correlations,"Shiori Sagawa, aditi raghunathan, Pang Wei Koh, Percy Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6809,"We study why overparameterization---increasing model size well beyond the point of zero training error---can hurt test error on minority groups despite improving average test error when there are spurious correlations in the data. Through simulations and experiments on two image datasets, we identify two key properties of the training data that drive this behavior: the proportions of majority versus minority groups, and the signal-to-noise ratio of the spurious correlations. We then analyze a linear setting and theoretically show how the inductive bias of models towards ``memorizing'' fewer examples can cause overparameterization to hurt. Our analysis leads to a counterintuitive approach of subsampling the majority group, which empirically achieves low minority error in the overparameterized regime, even though the standard approach of upweighting the minority fails. Overall, our results suggest a tension between using overparameterized models versus using all the training data for achieving low worst-group error.
","['Stanford University', 'stanford university', 'Stanford University', 'Stanford University']"
2020,Recovery of Sparse Signals from a Mixture of Linear Samples,"Soumyabrata Pal, Arya Mazumdar",https://icml.cc/Conferences/2020/Schedule?showEvent=6787,"Mixture of linear regressions is a popular learning theoretic model that is used widely to represent heterogeneous data. In the simplest form, this model assumes that the labels are generated from either of two different linear models and mixed together. Recent works of Yin et al. and Krishnamurthy et al., 2019, focus on an experimental design setting of model recovery for this problem. It is assumed that the features can be designed and queried with to obtain their label. When queried, an oracle randomly selects one of the two different sparse linear models and generates a label accordingly. How many such oracle queries are needed to recover both of the models simultaneously? This question can also be thought of as a generalization of the well-known compressed sensing problem (Cand`es and Tao, 2005, Donoho, 2006). In this work we address this query complexity problem and provide efficient algorithms that improves on the previously best known results. 
","['Umass Amherst', 'University of Massachusetts Amherst']"
2020,Source Separation with Deep Generative Priors,"Vivek Jayaram, John Thickstun",https://icml.cc/Conferences/2020/Schedule?showEvent=6713,"Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses deep generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation and qualitative discussion of results for CIFAR-10 image separation.
","['University of Washington', 'University of Washington']"
2020,Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization,"Sicheng Zhu, Xiao Zhang, David Evans",https://icml.cc/Conferences/2020/Schedule?showEvent=6604,"Training machine learning models that are robust against adversarial inputs poses seemingly insurmountable challenges. To better understand adversarial robustness, we consider the underlying problem of learning robust representations. We develop a notion of representation vulnerability that captures the maximum change of mutual information between the input and output distributions, under the worst-case input perturbation. Then, we prove a theorem that establishes a lower bound on the minimum adversarial risk that can be achieved for any downstream classifier based on its representation vulnerability. We propose an unsupervised learning method for obtaining intrinsically robust representations by maximizing the worst-case mutual information between the input and output distributions. Experiments on downstream classification tasks %and analyses of saliency maps support the robustness of the representations found using unsupervised learning with our training principle. 
","['University of Virginia', 'University of Virginia', 'University of Virginia']"
2020,On Semi-parametric Inference for BART,Veronika Rockova,https://icml.cc/Conferences/2020/Schedule?showEvent=6511,"There has been a growing realization of the potential of Bayesian machine learning as a platform that can provide both flexible modeling, accurate predictions as well as coherent uncertainty statements. In particular, Bayesian Additive Regression Trees (BART) have emerged as one of today’s most effective general approaches to predictive modeling under minimal assumptions. Statistical theoretical developments for machine learning have been mostly concerned with approximability or rates of estimation when recovering infinite dimensional objects (curves or densities). Despite the impressive array of available theoretical results, the literature has been largely silent about uncertainty quantification. In this work, we continue the theoretical investigation of BART initiated recently by Rockova and van der Pas (2017). We focus on statistical inference questions. In particular, we study the Bernstein-von Mises (BvM) phenomenon (i.e. asymptotic normality) for smooth linear functionals of the regression surface within the framework of non-parametric regression with fixed covariates. Our semi-parametric BvM results show that, beyond rate-optimal estimation, BART can be also used for valid statistical inference. 
",['University of Chicago']
2020,Neural Contextual Bandits with UCB-based Exploration,"Dongruo Zhou, Lihong Li, Quanquan Gu",https://icml.cc/Conferences/2020/Schedule?showEvent=6187,"We study the stochastic contextual bandit problem, where the reward is generated from an unknown function with additive noise.  No assumption is made about the reward function other than boundedness. We propose a new algorithm, NeuralUCB, which leverages the representation power of deep neural networks and uses a neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under standard assumptions, NeuralUCB achieves $\tilde O(\sqrt{T})$ regret, where $T$ is the number of rounds. To the best of our knowledge, it is the first neural network-based contextual bandit algorithm with a near-optimal regret guarantee. We also show the algorithm is empirically competitive against representative baselines in a number of benchmarks.","['UCLA', 'Google Research', 'University of California, Los Angeles']"
2020,Explainable k-Means and k-Medians Clustering,"Michal Moshkovitz, Sanjoy Dasgupta, Cyrus Rashtchian, Nave Frost",https://icml.cc/Conferences/2020/Schedule?showEvent=6284,"Many clustering algorithms lead to cluster assignments that are hard to explain, partially because they depend on all the features of the data in a complicated way. To improve interpretability, we consider using a small decision tree to partition a data set into clusters, so that clusters can be characterized in a straightforward manner. We study this problem from a theoretical viewpoint, measuring cluster quality by the k-means and k-medians objectives. In terms of negative results, we show that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and any clustering based on a tree with k leaves must incur an Omega(log k) approximation factor compared to the optimal clustering. On the positive side, for two means/medians, we show that a single threshold cut can achieve a constant factor approximation, and we give nearly-matching lower bounds; for general k > 2, we design an efficient algorithm that leads to an O(k) approximation to the optimal k-medians and an O(k^2) approximation to the optimal k-means. Prior to our work, no algorithms were known with provable guarantees independent of dimension and input size.
","['University of California San Diego', 'UC San Diego', 'UCSD', 'Tel-Aviv University']"
2020,Bootstrap Latent-Predictive Representations for Multitask Reinforcement Learning,"Zhaohan Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altché, Remi Munos, Mohammad Gheshlaghi Azar",https://icml.cc/Conferences/2020/Schedule?showEvent=6472,"Learning a good representation is an essential component for deep reinforcement learning (RL). Representation learning is especially important in multitask and partially observable settings where building a representation of the unknown environment is crucial to solve the tasks. Here we introduce Predictions of Bootstrapped Latents (PBL), a simple and flexible self-supervised representation learning algorithm for multitask deep RL. PBL builds on multistep predictive representations of future observations, and focuses on capturing structured information about environment dynamics. Specifically, PBL trains its representation by predicting latent embeddings of future observations. These latent embeddings are themselves trained to be predictive of the aforementioned representations. These predictions form a bootstrapping effect, allowing the agent to learn more about the key aspects of the environment dynamics. In addition, by defining prediction tasks completely in latent space, PBL provides the flexibility of using multimodal observations involving pixel images, language instructions, rewards and more. We show in our experiments that PBL delivers across-the-board improved performance over state of the art deep RL agents in the DMLab-30 multitask setting. 
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Deepmind']"
2020,Selective Dyna-style Planning Under Limited Model Capacity,"Zaheer Abbas, Samuel Sokota, Erin Talvitie, Martha White",https://icml.cc/Conferences/2020/Schedule?showEvent=6672,"In model-based reinforcement learning, planning with an imperfect model of the environment has the potential to harm learning progress. But even when a model is imperfect, it may still contain information that is useful for planning. In this paper, we investigate the idea of using an imperfect model selectively. The agent should plan in parts of the state space where the model would be helpful but refrain from using the model where it would be harmful. An effective selective planning mechanism requires estimating predictive uncertainty, which arises out of aleatoric uncertainty, parameter uncertainty, and model inadequacy, among other sources. Prior work has focused on parameter uncertainty for selective planning. In this work, we emphasize the importance of model inadequacy. We show that heteroscedastic regression can signal predictive uncertainty arising from model inadequacy that is complementary to that which is detected by methods designed for parameter uncertainty, indicating that considering both parameter uncertainty and model inadequacy may be a more promising direction for effective selective planning than either in isolation.
","['University of Alberta', 'University of Alberta', 'Harvey Mudd College', 'University of Alberta']"
2020,Logarithmic Regret for Adversarial Online Control,"Dylan Foster, Max Simchowitz",https://icml.cc/Conferences/2020/Schedule?showEvent=6619,"We introduce a new algorithm for online linear-quadratic control in a known system subject to adversarial disturbances. Existing regret bounds for this setting scale as $\sqrt{T}$ unless strong stochastic assumptions are imposed on the disturbance process. We give the first algorithm with logarithmic regret for arbitrary adversarial disturbance sequences, provided the state and control costs are given by known quadratic functions. Our algorithm and analysis use a characterization for the optimal offline control law to reduce the online control problem to (delayed) online learning with approximate advantage functions. Compared to previous techniques, our approach does not need to control movement costs for the iterates, leading to logarithmic regret.","['MIT', 'UC Berkeley']"
2020,On the Theoretical Properties of the Network Jackknife,"Qiaohui Lin, Robert Lunde, Purnamrita Sarkar",https://icml.cc/Conferences/2020/Schedule?showEvent=6756,"We study the properties of a leave-node-out jackknife procedure for network data.  Under the sparse graphon model, we prove an Efron-Stein-type inequality, showing that the network jackknife leads to conservative estimates of the variance (in expectation) for any network functional that is invariant to node permutation.  For a general class of count functionals, we also establish consistency of the network jackknife.  We complement our theoretical analysis with a range of simulated and real-data examples and show that the network jackknife offers competitive performance in cases where other resampling methods are known to be valid. In fact, for several network statistics, we see that the jackknife provides more accurate inferences compared to related methods such as subsampling.
","['University of Texas at Austin', 'University of Texas at Austin', 'UT Austin']"
2020,Structured Prediction with Partial Labelling through the Infimum Loss,"Vivien Cabannnes, Alessandro Rudi, Francis Bach",https://icml.cc/Conferences/2020/Schedule?showEvent=6509,"Annotating datasets is one of the main costs in nowadays supervised learning.
  The goal of weak supervision is to enable models to learn using only forms of
  labelling which are cheaper to collect, as partial labelling. This is a type of
  incomplete annotation where, for each datapoint, supervision is cast as a set
  of labels containing the real one.  The problem of supervised learning with
  partial labelling has been studied for specific instances such as
  classification, multi-label, ranking or segmentation, but a general framework
  is still missing. This paper provides a unified framework based on structured
  prediction and on the concept of {\em infimum loss} to deal with partial
  labelling over a wide family of learning problems and loss functions. The
  framework leads naturally to explicit algorithms that can be easily
  implemented and for which proved statistical consistency and learning rates.
  Experiments confirm the superiority of the proposed approach over commonly
  used baselines. 
","['INRIA', 'École Normale Supérieure  ', 'INRIA - Ecole Normale Supérieure']"
2020,How recurrent networks implement contextual processing in sentiment analysis,"Niru Maheswaranathan, David Sussillo",https://icml.cc/Conferences/2020/Schedule?showEvent=6674,"Neural networks have a remarkable capacity for contextual processing—using recent or nearby inputs to modify processing of current input. For example, in natural language, contextual processing is necessary to correctly interpret negation (e.g. phrases such as ""not bad""). However, our ability to understand how networks process context is limited. Here, we propose general methods for reverse engineering recurrent neural networks (RNNs) to identify and elucidate contextual processing. We apply these methods to understand RNNs trained on sentiment classification. This analysis reveals inputs that induce contextual effects, quantifies the strength and timescale of these effects, and identifies sets of these inputs with similar properties. Additionally, we analyze contextual effects related to differential processing of the beginning and end of documents. Using the insights learned from the RNNs we improve baseline Bag-of-Words models with simple extensions that incorporate contextual modification, recovering greater than 90% of the RNN's performance increase over the baseline. This work yields a new understanding of how RNNs process contextual information, and provides tools that should provide similar insight more broadly.
","['Google Brain', 'Google Brain, Google Inc.']"
2020,Responsive Safety in Reinforcement Learning by PID Lagrangian Methods,"Adam Stooke, Joshua Achiam, Pieter Abbeel",https://icml.cc/Conferences/2020/Schedule?showEvent=6173,"Lagrangian methods are widely used algorithms for constrained optimization problems, but their learning dynamics exhibit oscillations and overshoot which, when applied to safe reinforcement learning, leads to constraint-violating behavior during agent training.  We address this shortcoming by proposing a novel Lagrange multiplier update method that utilizes derivatives of the constraint function.  We take a controls perspective, wherein the traditional Lagrange multiplier update behaves as \emph{integral} control; our terms introduce \emph{proportional} and \emph{derivative} control, achieving favorable learning dynamics through damping and predictive measures.  We apply our PID Lagrangian methods in deep RL, setting a new state of the art in Safety Gym, a safe RL benchmark.  Lastly, we introduce a new method to ease controller tuning by providing invariance to the relative numerical scales of reward and cost.  Our extensive experiments demonstrate improved performance and hyperparameter robustness, while our algorithms remain nearly as simple to derive and implement as the traditional Lagrangian approach.
","['UC Berkeley', 'OpenAI', 'UC Berkeley & Covariant']"
2020,SoftSort: A Continuous Relaxation for the argsort Operator,"Sebastian Prillo, Julian M Eisenschlos",https://icml.cc/Conferences/2020/Schedule?showEvent=5841,"While sorting is an important procedure in computer science, the argsort operator - which takes as input a vector and returns its sorting permutation - has a discrete image and thus zero gradients almost everywhere. This prohibits end-to-end, gradient-based learning of models that rely on the argsort operator. A natural way to overcome this problem is to replace the argsort operator with a continuous relaxation. Recent work has shown a number of ways to do this, but the relaxations proposed so far are computationally complex. In this work we propose a simple continuous relaxation for the argsort operator which has the following qualities: it can be implemented in three lines of code, achieves state-of-the-art performance, is easy to reason about mathematically - substantially simplifying proofs - and is faster than competing approaches. We open source the code to reproduce all of the experiments and results.
","['UC Berkeley', 'Google']"
2020,Sub-Goal Trees -- a Framework for Goal-Based Reinforcement Learning,"Tom Jurgenson, Or Avner, Edward Groshev, Aviv Tamar",https://icml.cc/Conferences/2020/Schedule?showEvent=6389,"Many AI problems, in robotics and other domains, are goal-directed, essentially seeking a trajectory leading to some goal state. Reinforcement learning (RL), building on Bellman's optimality equation, naturally optimizes for a single goal, yet can be made goal-directed by augmenting the state with the goal. Instead, we propose a new RL framework, derived from a dynamic programming equation for the all pairs shortest path (APSP) problem, which naturally solves goal-directed queries. We show that this approach has computational benefits for both standard and approximate dynamic programming.
Interestingly, our formulation prescribes a novel protocol for computing a trajectory: instead of predicting the next state given its predecessor, as in standard RL, a goal-conditioned trajectory is constructed by first predicting an intermediate state between start and goal, partitioning the trajectory into two. Then, recursively, predicting intermediate points on each sub-segment, until a complete trajectory is obtained. We call this trajectory structure a sub-goal tree. Building on it, we additionally extend the policy gradient methodology to recursively predict sub-goals, resulting in novel goal-based algorithms. Finally, we apply our method to neural motion planning, where we demonstrate significant improvements compared to standard RL on navigating a 7-DoF robot arm between obstacles.
","['Technion', 'Technion', 'Osaro, Inc.', 'Technion']"
2020,Differentiable Likelihoods for Fast Inversion of 'Likelihood-Free' Dynamical Systems,"Hans Kersting, Nicholas Krämer, Martin Schiegg, Christian Daniel, Michael Schober, Philipp Hennig",https://icml.cc/Conferences/2020/Schedule?showEvent=6011,"Likelihood-free (a.k.a. simulation-based) inference problems are inverse problems with expensive, or intractable, forward models. ODE inverse problems are commonly treated as likelihood-free, as their forward map has to be numerically approximated by an ODE solver. This, however, is not a fundamental constraint but just a lack of functionality in classic ODE solvers, which do not return a likelihood but a point estimate. To address this shortcoming, we employ Gaussian ODE filtering (a probabilistic numerical method for ODEs) to construct a local Gaussian approximation to the likelihood. This approximation yields tractable estimators for the gradient and Hessian of the (log-)likelihood. Insertion of these estimators into existing gradient-based optimization and sampling methods engenders new solvers for ODE inverse problems. We demonstrate that these methods outperform standard likelihood-free approaches on three benchmark-systems.
","['University of Tuebingen', 'University of Tübingen', 'Bosch Center for Artificial Intelligence', 'Bosch Center for Artificial Intelligence', 'Bosch Center for Artificial Intelligence', 'University of Tuebingen']"
2020,Influence Diagram Bandits: Variational Thompson Sampling for Structured Bandit Problems,"Tong Yu, Branislav Kveton, Zheng Wen, Ruiyi Zhang, Ole J. Mengshoel",https://icml.cc/Conferences/2020/Schedule?showEvent=6097,"We propose a novel framework for structured bandits, which we call an influence diagram bandit. Our framework captures complex statistical dependencies between actions, latent variables, and observations; and thus unifies and extends many existing models, such as combinatorial semi-bandits, cascading bandits, and low-rank bandits. We develop novel online learning algorithms that learn to act efficiently in our models. The key idea is to track a structured posterior distribution of model parameters, either exactly or approximately. To act, we sample model parameters from their posterior and then use the structure of the influence diagram to find the most optimistic action under the sampled parameters. We empirically evaluate our algorithms in three structured bandit problems, and show that they perform as well as or better than problem-specific state-of-the-art baselines.
","['Carnegie Mellon University', 'Google Research', 'DeepMind', 'Duke University', 'Carnegie Mellon University']"
2020,On Learning Sets of Symmetric Elements,"Haggai Maron, Or Litany, Gal Chechik, Ethan Fetaya",https://icml.cc/Conferences/2020/Schedule?showEvent=6022,"Learning from unordered sets is a fundamental learning setup, which is attracting increasing attention. Research in this area has focused on the case where elements of the set are represented by feature vectors, and far less emphasis has been given to the common case where set elements themselves adhere to certain symmetries. That case is relevant to numerous applications, from deblurring image bursts to multi-view 3D shape recognition and reconstruction. In this paper, we present a principled approach to learning sets of general symmetric elements. We first characterize the space of linear layers that are equivariant both to element reordering and to the inherent symmetries of elements, like translation in the case of images. We further show that networks that are composed of these layers, called Deep Sets for Symmetric elements layers (DSS), are universal approximators of both invariant and equivariant functions. DSS layers are also straightforward to implement. Finally, we show that they improve over existing set-learning architectures in a series of experiments with images, graphs, and point-clouds.
","['NVIDIA Research', 'Stanford University', 'NVIDIA / Bar-Ilan University', 'Bar-Ilan University']"
2020,Revisiting Spatial Invariance with Low-Rank Local Connectivity,"Gamaleldin Elsayed, Prajit Ramachandran, Jon Shlens, Simon Kornblith",https://icml.cc/Conferences/2020/Schedule?showEvent=5859,"Convolutional neural networks are among the most successful architectures in deep learning with this success at least partially attributable to the efficacy of spatial invariance as an inductive bias. Locally connected layers, which differ from convolutional layers only in their lack of spatial invariance, usually perform poorly in practice. However, these observations still leave open the possibility that some degree of relaxation of spatial invariance may yield a better inductive bias than either convolution or local connectivity. To test this hypothesis, we design a method to relax the spatial invariance of a network layer in a controlled manner; we create a \textit{low-rank} locally connected layer, where the filter bank applied at each position is constructed as a linear combination of basis set of filter banks with spatially varying combining weights. By varying the number of basis filter banks, we can control the degree of relaxation of spatial invariance. In experiments with small convolutional networks, we find that relaxing spatial invariance improves classification accuracy over both convolution and locally connected layers across MNIST, CIFAR-10, and CelebA datasets, thus suggesting that spatial invariance may be an overly restrictive prior.
","['Google Research, Brain Team', 'Google', 'Google Brain', 'Google Brain']"
2020,When are Non-Parametric Methods Robust?,"Robi Bhattacharjee, Kamalika Chaudhuri",https://icml.cc/Conferences/2020/Schedule?showEvent=6228,"A growing body of research has shown that many classifiers are susceptible to adversarial examples -- small strategic modifications to test inputs that lead to misclassification. In this work, we study general non-parametric methods, with a view towards understanding when they are robust to these modifications. We establish general conditions under which non-parametric methods are r-consistent -- in the sense that they converge to optimally robust and accurate classifiers in the large sample limit. 
Concretely, our results show that when data is well-separated, nearest neighbors and kernel classifiers are r-consistent, while histograms are not. For general data distributions, we prove that preprocessing by Adversarial Pruning (Yang et. al., 2019)-- that makes data well-separated -- followed by nearest neighbors or kernel classifiers also leads to r-consistency. 
","['UCSD', 'University of California at San Diego']"
2020,Optimally Solving Two-Agent Decentralized POMDPs Under One-Sided Information Sharing ,"Yuxuan Xie, Jilles Dibangoye, Olivier Buffet",https://icml.cc/Conferences/2020/Schedule?showEvent=6368,"Optimally solving decentralized partially observable Markov decision processes under either full or no information sharing received significant attention in recent years. However, little is known about how partial information sharing affects existing theory and algorithms. This paper addresses this question for a team of two agents, with one-sided information sharing---\ie both agents have imperfect information about the state of the world, but only one has access to what the other sees and does. From the perspective of a central planner, we show that the original problem can be reformulated into an equivalent information-state Markov decision process and solved as such. Besides, we prove that the optimal value function exhibits a specific form of uniform continuity. We also present a heuristic search algorithm utilizing this property and providing the first results for this family of problems.
","['INSA de Lyon', 'INSA Lyon, INRIA', 'INRIA - LORIA']"
2020,"Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers","Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, Joseph Gonzalez",https://icml.cc/Conferences/2020/Schedule?showEvent=6828,"Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.
","['UC Berkeley', 'U.C. Berkeley', 'University of California, Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2020,A simpler approach to accelerated optimization: iterative averaging meets optimism,"Pooria Joulani, Anant Raj, András György, Csaba Szepesvari",https://icml.cc/Conferences/2020/Schedule?showEvent=6822,"Recently there have been several attempts to extend Nesterov's accelerated algorithm to smooth stochastic and variance-reduced optimization. In this paper, we show that there is a simpler approach to acceleration: applying optimistic online learning algorithms and querying the gradient oracle at the online average of the intermediate optimization iterates. In particular, we tighten a recent result of Cutkosky (2019) to demonstrate theoretically that online iterate averaging results in a reduced optimization gap, independently of the algorithm involved. We show that carefully combining this technique with existing generic optimistic online learning algorithms yields the optimal accelerated rates for optimizing strongly-convex and non-strongly-convex, possibly composite objectives, with deterministic as well as stochastic first-order oracles. We further extend this idea to variance-reduced optimization. Finally, we also provide ``universal'' algorithms that achieve the optimal rate for smooth and non-smooth composite objectives simultaneously without further tuning, generalizing the results of Kavis et al. (2019) and solving a number of their open problems.
","['DeepMind', 'Max-Planck Institute for Intelligent Systems', 'DeepMind', 'DeepMind/University of Alberta']"
2020,Generative Pretraining From Pixels,"Mark Chen, Alec Radford, Rewon Child, Jeffrey K Wu, Heewoo Jun, David Luan, Ilya Sutskever",https://icml.cc/Conferences/2020/Schedule?showEvent=6739,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.
","['OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI']"
2020,Error Estimation for Sketched SVD via the Bootstrap,"Miles Lopes, N. Benjamin Erichson, Michael Mahoney",https://icml.cc/Conferences/2020/Schedule?showEvent=6270,"In order to compute fast approximations to the singular value decompositions (SVD) of very large matrices, randomized sketching algorithms have become a leading approach. However, a key practical difficulty of sketching an SVD is that the user does not know how far the sketched singular vectors/values are from the exact ones. Indeed, the user may be forced to rely on analytical worst-case error bounds, which may not account for the unique structure of a given problem. As a result, the lack of tools for error estimation often leads to much more computation than is really necessary. To overcome these challenges, this paper develops a fully data-driven bootstrap method that numerically estimates the actual error of sketched singular vectors/values. Furthermore, the method is computationally inexpensive, because it operates only on sketched objects, and hence it requires no extra passes over the full matrix being factored. 
","['University of California, Davis', 'University of California, Berkeley', 'UC Berkeley']"
2020,Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data,"Marc Finzi, Samuel Stanton, Pavel Izmailov, Andrew Wilson",https://icml.cc/Conferences/2020/Schedule?showEvent=6320,"The translation equivariance of convolutional layers enables CNNs to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data.
We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.
","['New York University', 'New York University', 'New York University', 'New York University']"
2020,Optimal Robust Learning of Discrete Distributions from Batches,"Ayush Jain, Alon Orlitsky",https://icml.cc/Conferences/2020/Schedule?showEvent=6242,"Many applications, including natural language processing, sensor networks, collaborative filtering, and federated learning, call for estimating discrete distributions from data collected in batches, some  of which may be untrustworthy, erroneous, faulty, or even adversarial. Previous estimators for this setting ran in exponential time, and for some regimes required a suboptimal number of batches. We provide the first polynomial-time estimator that is optimal in the number of batches and achieves essentially the best possible estimation accuracy.
","['UC San Diego', 'UCSD']"
2020,Harmonic Decompositions of Convolutional Networks,"Meyer Scetbon, Zaid Harchaoui",https://icml.cc/Conferences/2020/Schedule?showEvent=6178,"We present a description of the function space and the  smoothness class associated with a convolutional network using the machinery of reproducing kernel Hilbert spaces. 
We show that the mapping associated with a convolutional network expands into a sum involving elementary functions akin to spherical harmonics. The functional decomposition can be related to functional ANOVA decompositions in nonparametric statistics. Building off this functional characterization, we obtain statistical bounds which highlight an interesting trade-off between the approximation error and the estimation error.
","['CREST, ENSAE', 'University of Washington']"
2020,Topological Autoencoders,"Michael Moor, Max Horn, Bastian Rieck, Karsten Borgwardt",https://icml.cc/Conferences/2020/Schedule?showEvent=5851,"We propose a novel approach for preserving topological structures of the
input space in latent representations of autoencoders. Using persistent
homology, a technique from topological data analysis, we calculate
topological signatures of both the input and latent space to derive
a topological loss term.  Under weak theoretical assumptions, we
construct this loss in a differentiable manner, such that the encoding
learns to retain multi-scale connectivity information. We show that our
approach is theoretically well-founded and that it exhibits favourable
latent representations on a synthetic manifold as well as on real-world
image data sets, while preserving low reconstruction errors.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2020,Robust One-Bit Recovery via ReLU Generative Networks: Near-Optimal Statistical Rate and Global Landscape Analysis,"Shuang Qiu, Xiaohan Wei, Zhuoran Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6772,"We study the robust one-bit compressed sensing problem whose goal is to design an algorithm that faithfully recovers any sparse target vector $\theta_0\in\mathbb{R}^d$   \textit{uniformly} via $m$ quantized noisy measurements. Specifically, we consider a new framework for this problem where the sparsity is implicitly enforced via mapping a low dimensional representation $x_0 \in \mathbb{R}^k$  through a known $n$-layer ReLU generative network $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$ such that $\theta_0 = G(x_0)$.  Such a framework poses low-dimensional priors on $\theta_0$ without a known sparsity basis. We propose to recover the target $G(x_0)$ solving an unconstrained empirical risk minimization (ERM). Under a weak \textit{sub-exponential measurement assumption}, we establish a joint statistical and computational analysis. In particular, we prove that the ERM estimator in this new framework achieves a statistical rate of $m=\widetilde{\mathcal{O}}(kn \log d /\varepsilon^2)$ recovering any $G(x_0)$ uniformly up to an error $\varepsilon$. When the network is shallow (i.e., $n$ is small), we show this rate matches the information-theoretic lower bound up to logarithm factors on $\varepsilon^{-1}$. From the lens of computation, we prove that under proper conditions on the network weights, our proposed empirical risk, despite non-convexity, has no stationary point outside of small neighborhoods around the true representation $x_0$ and its negative multiple;  furthermore, we show that the global minimizer of the empirical risk stays within the neighborhood around $x_0$ rather than its negative multiple under further assumptions on weights.","['University of Michigan', 'University of Southern California', 'Princeton University']"
2020,Automatic Shortcut Removal for Self-Supervised Representation Learning,"Matthias Minderer, Olivier Bachem, Neil Houlsby, Michael Tschannen",https://icml.cc/Conferences/2020/Schedule?showEvent=6498,"In self-supervised visual representation learning, a feature extractor is trained on a ""pretext task"" for which labels can be generated cheaply, without human annotation. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such ""shortcut"" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for mitigating the effect shortcut features. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a ""lens"" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.
","['Google Research', 'Google Brain', 'Google', 'Google Brain']"
2020,Online Control of the False Coverage Rate and False Sign Rate,"Asaf Weinstein, Aaditya Ramdas",https://icml.cc/Conferences/2020/Schedule?showEvent=6560,"The reproducibility debate has caused a renewed interest in changing how one reports uncertainty, from $p$-value for testing a null hypothesis to a confidence interval (CI) for the corresponding parameter. When CIs for multiple selected parameters are being reported, the analog of the false discovery rate (FDR) is the false coverage rate (FCR), which is the expected ratio of number of reported CIs failing to cover their respective parameters to the total number of reported CIs. Here, we consider the general problem of FCR control in the online setting, where one encounters an infinite sequence of fixed unknown parameters ordered by time. We propose a novel solution to the problem which only requires the scientist to be able to construct marginal CIs. As special cases, our framework yields algorithms for online FDR control and online sign-classification procedures that control the false sign rate (FSR). All of our methodology applies equally well to prediction intervals, having particular implications for selective conformal inference.","['The Hebrew University of Jerusalem', 'Carnegie Mellon University']"
2020,Deep Isometric Learning for Visual Recognition,"Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, Jitendra Malik",https://icml.cc/Conferences/2020/Schedule?showEvent=6255,"Initialization, normalization, and skip connections are believed to be three indispensable techniques for training very deep convolutional neural networks and obtaining state-of-the-art performance. This paper shows that deep vanilla ConvNets without normalization nor skip connections can also be trained to achieve surprisingly good performance on standard image recognition benchmarks. This is achieved by enforcing the convolution kernels to be near isometric during initialization and training, as well as by using a variant of ReLU that is shifted towards being isometric. Further experiments show that if combined with skip connections, such near isometric networks can achieve performances on par with (for ImageNet) and better than (for COCO) the standard ResNet, even without normalization at all. Our code is available at https://github.com/HaozhiQi/ISONet.
","['UC Berkeley', 'University of California, Berkeley', 'UCSD', 'UC Berkeley', 'University of California at Berkeley']"
2020,Federated Learning with Only Positive Labels,"Felix Xinnan Yu, Ankit Singh Rawat, Aditya Menon, Sanjiv Kumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6592,"We consider learning a multi-class classification model in the federated setting, where each user has access to the positive data associated with only a single class. As a result, during each federated learning round, the users need to locally update the classifier without having access to the features and the model parameters for the negative classes. Thus, naively employing conventional decentralized learning such as distributed SGD or Federated Averaging may lead to trivial or extremely poor classifiers. In particular, for embedding based classifiers, all the class embeddings might collapse to a single point. To address this problem, we propose a generic framework for training with only positive labels, namely Federated Averaging with Spreadout (FedAwS), where the server imposes a geometric regularizer after each round to encourage classes to be spreadout in the embedding space. We show, both theoretically and empirically, that FedAwS can almost match the performance of conventional learning where users have access to negative labels. We further extend the proposed method to settings with large output spaces.
","['Google', 'Google', 'Google Research', 'Google Research, NY']"
2020,Incremental Sampling Without Replacement for Sequence Models,"Kensen Shi, David Bieber, Charles Sutton",https://icml.cc/Conferences/2020/Schedule?showEvent=6198,"Sampling is a fundamental technique, and sampling without replacement is often desirable when duplicate samples are not beneficial. Within machine learning, sampling is useful for generating diverse outputs from a trained model. We present an elegant procedure for sampling without replacement from a broad class of randomized programs, including generative neural models that construct outputs sequentially. Our procedure is efficient even for exponentially-large output spaces. Unlike prior work, our approach is incremental, i.e., samples can be drawn one at a time, allowing for increased flexibility. We also present a new estimator for computing expectations from samples drawn without replacement. We show that incremental sampling without replacement is applicable to many domains, e.g., program synthesis and combinatorial optimization.
","['Google', 'Google Research', 'Google']"
2020,Learning To Stop While Learning To Predict,"Xinshi Chen, Hanjun Dai, Yu Li, Xin Gao, Le Song",https://icml.cc/Conferences/2020/Schedule?showEvent=6279,"There is a recent surge of interest in designing deep architectures based on the update steps in traditional algorithms, or learning neural networks to improve and replace traditional algorithms. While traditional algorithms have certain stopping criteria for outputting results at different iterations, many algorithm-inspired deep models are restricted to a fixed-depth'' for all inputs. Similar to algorithms, the optimal depth of a deep architecture may be different for different input instances, either to avoidover-thinking'', or because we want to compute less for operations converged already. In this paper, we tackle this varying depth problem using a steerable architecture, where a feed-forward deep model and a variational stopping policy are learned together to sequentially determine the optimal number of layers for each input instance. Training such architecture is very challenging. We provide a variational Bayes perspective and design a novel and effective training procedure which decomposes the task into an oracle model learning stage and an imitation stage. Experimentally, we show that the learned deep model along with the stopping policy improves the performances on a diverse set of tasks, including learning sparse recovery, few-shot meta learning, and computer vision tasks.
","['Georgia Institution of Technology', 'Google Brain', 'King Abdullah University of Science and Technology', 'Kaust', 'Georgia Institute of Technology']"
2020,Fully Parallel Hyperparameter Search: Reshaped Space-Filling,"Marie-Liesse Cauwet, Camille Couprie, Julien Dehos, Pauline Luc, Jeremy Rapin, Morgane Riviere, Fabien Teytaud, Olivier Teytaud, Nicolas Usunier",https://icml.cc/Conferences/2020/Schedule?showEvent=6020,"Space-filling designs such as Low Discrepancy Sequence (LDS), Latin Hypercube Sampling (LHS) and Jittered Sampling (JS) were proposed for fully parallel hyperparameter search, and were shown to be more effective than random and grid search. We prove that LHS and JS outperform random search only by a constant factor. Consequently, we introduce a new sampling approach based on the reshaping of the search distribution, and we show both theoretically and numerically that it leads to significant gains over random search. Two methods are proposed for the reshaping: Recentering (when the distribution of the optimum is known), and Cauchy transformation (when the distribution of the optimum is unknown). The proposed methods are first validated on artificial experiments and simple real-world tests on clustering and Salmon mappings. Then we demonstrate that they drive performance improvement in a wide range of expensive artificial intelligence tasks, namely attend/infer/repeat, video next frame segmentation forecasting and progressive generative adversarial networks.
","['Université Paris-Est, LIGM (UMR 8049), CNRS, ESIEE Paris', 'FAIR', ""LISIC, Université du Littoral Côte d'Opale"", 'Deepmind', 'Facebook AI Research', 'Facebook Artificial Intelligence Research', ""LISIC, Université du Littoral Côte d'Opale"", 'Facebook', 'Facebook AI Research']"
2020,Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models,"Yiding Feng, Ekaterina Khmelnitskaya, Denis Nekipelov",https://icml.cc/Conferences/2020/Schedule?showEvent=6217,"Discrete choice models with unobserved heterogeneity are commonly used Econometric models for dynamic Economic behavior which have been adopted in practice to predict behavior of individuals and firms from schooling and job choices to strategic decisions in market competition. These models feature optimizing agents who choose among a finite set of options in a sequence of periods and receive choice-specific payoffs that depend on both variables that are observed by the agent and recorded in the data and variables that are only observed by the agent but not recorded in the data. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. We show that in an important class of discrete choice models the value function is globally concave in the policy. That means that simple algorithms that do not require fixed point computation, such as the policy gradient algorithm, globally converge to the optimal policy. This finding can both be used to relax behavioral assumption regarding the optimizing agents and to facilitate Econometric analysis of dynamic behavior. In particular, we demonstrate significant computational advantages in using a simple implementation policy gradient algorithm over existing “nested fixed point” algorithms used in Econometrics. 
","['Northwestern University', 'University of Virginia', '']"
2020,A Sample Complexity Separation between Non-Convex and Convex Meta-Learning,"Nikunj Umesh Saunshi, Yi Zhang, Mikhail Khodak, Sanjeev Arora",https://icml.cc/Conferences/2020/Schedule?showEvent=6663,"One popular trend in meta-learning is to learn from many training tasks a common initialization for a gradient-based method that can be used to solve a new task with few samples. The theory of meta-learning is still in its early stages, with several recent learning-theoretic analyses of methods such as Reptile [Nichol et al., 2018] being for {\em convex models}. This work shows that convex-case analysis might be insufficient to understand the success of meta-learning, and that even for non-convex models it is important to look inside the optimization black-box, specifically at properties of the optimization trajectory. We construct a simple meta-learning instance that captures the problem of one-dimensional subspace learning. For the convex formulation of linear regression on this instance, we show that the new task sample complexity of any {\em initialization-based meta-learning} algorithm is $\Omega(d)$, where $d$ is the input dimension. In contrast, for the non-convex formulation of a two layer linear network on the same instance, we show that both Reptile and multi-task representation learning can have new task sample complexity of $O(1)$, demonstrating a separation from convex meta-learning. Crucially, analyses of the training dynamics of these methods reveal that they can meta-learn the correct subspace onto which the data should be projected.","['Princeton University', 'Princeton University', 'CMU', 'Princeton University and Institute for Advanced Study']"
2020,Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data,"Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth Stanley, Jeffrey Clune",https://icml.cc/Conferences/2020/Schedule?showEvent=6508,"This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate \emph{through the entire learning process} via meta-gradients to update the GTN parameters to improve performance on the target task. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS). GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.
","['Uber AI Labs', 'Amazon AWS AI Labs', '', 'OpenAI', 'Open AI']"
2020,Training Deep Energy-Based Models with f-Divergence Minimization,"Lantao Yu, Yang Song, Jiaming Song, Stefano Ermon",https://icml.cc/Conferences/2020/Schedule?showEvent=5781,"Deep energy-based models (EBMs) are very flexible in distribution parametrization but computationally challenging because of the intractable partition function. They are typically trained via maximum likelihood, using contrastive divergence to approximate the gradient of the KL divergence between data and model distribution. While KL divergence has many desirable properties, other f-divergences have shown advantages in training implicit density generative models such as generative adversarial networks. In this paper, we propose a general variational framework termed f-EBM to train EBMs using any desired f-divergence. We introduce a corresponding optimization algorithm and prove its local convergence property with non-linear dynamical systems theory. Experimental results demonstrate the superiority of f-EBM over contrastive divergence, as well as the benefits of training EBMs using f-divergences other than KL.
","['Stanford University', 'Stanford University', 'Stanford', 'Stanford University']"
2020,SCAFFOLD: Stochastic Controlled Averaging for Federated Learning,"Sai Praneeth Reddy Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Jakkam Reddi, Sebastian Stich, Ananda Theertha Suresh",https://icml.cc/Conferences/2020/Schedule?showEvent=5880,"Federated learning is a key scenario in modern large-scale machine learning where the data remains distributed over a large number of clients and the task is to learn a centralized model without transmitting the client data. The standard optimization algorithm used in this setting is Federated Averaging (FedAvg) due to its low communication cost. We obtain a tight characterization of the convergence of FedAvg and prove that heterogeneity (non-iid-ness) in the client's data results in a `drift' in the local updates resulting in poor performance.
As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the `client drift'. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client's data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.
","['EPFL', 'Google', 'Google Research and Courant Institute of Mathematical Sciences', 'Google', 'EPFL', 'Google Research']"
2020,Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning,"Amin Rakhsha, Goran Radanovic, Rati Devidze, Jerry Zhu, Adish Singla",https://icml.cc/Conferences/2020/Schedule?showEvent=5957,"We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes average reward in undiscounted infinite-horizon problem settings. The attacker can manipulate the rewards or the transition dynamics in the learning environment at training-time and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an \emph{optimal stealthy attack} for different measures of attack cost. We provide sufficient technical conditions under which the attack is feasible and provide lower/upper bounds on the attack cost. We instantiate our attacks in two settings: (i) an \emph{offline} setting where the agent is doing planning in the poisoned environment, and (ii) an \emph{online} setting where the agent is learning a policy using a regret-minimization framework with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.
","['Max Planck Institute for Software Systems (MPI-SWS)', 'Max Planck Institute for Software Systems', 'Max Planck Institute for Software Systems', 'University of Wisconsin-Madison', 'Max Planck Institute (MPI-SWS)']"
2020,XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation,"Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson",https://icml.cc/Conferences/2020/Schedule?showEvent=6459,"Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We will release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.
","['Carnegie Mellon University', 'DeepMind', 'Google Research', 'Carnegie Mellon University', 'Google', 'Google']"
2020,Domain Adaptive Imitation Learning,"Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, Stefano Ermon",https://icml.cc/Conferences/2020/Schedule?showEvent=6042,"We study the question of how to imitate tasks across domains with discrepancies such as embodiment, viewpoint, and dynamics mismatch. Many prior works require paired, aligned demonstrations and an additional RL step that requires environment interactions. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. 
In this work, we formalize the Domain Adaptive Imitation Learning (DAIL) problem - a unified framework for imitation learning in the presence of viewpoint, embodiment, and/or dynamics mismatch. Informally, DAIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to DAIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from \emph{unpaired, unaligned} demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when DAIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability.
We experimentally evaluate GAMA against baselines in embodiment, viewpoint, and dynamics mismatch scenarios where aligned demonstrations don't exist and show the effectiveness of our approach
","['Stanford University', 'Tsinghua University', 'Stanford', 'Stanford University', 'Stanford University']"
2020,Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,"Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, Yi Ma",https://icml.cc/Conferences/2020/Schedule?showEvent=6259,"The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation of this by measuring the bias and variance of neural networks: while the bias is {\em monotonically decreasing} as in the classical theory, the variance is {\em unimodal} or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent in the recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.
","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'UC Berkeley', 'UC Berkeley']"
2020,"Smaller, more accurate regression forests using tree alternating optimization","Arman Zharmagambetov, Miguel Carreira-Perpinan",https://icml.cc/Conferences/2020/Schedule?showEvent=6675,"Regression forests, based on ensemble approaches such as bagging or boosting, have long been recognized as the leading off-the-shelf method for regression. However, forests rely on a greedy top-down procedure such as CART to learn each tree. We extend a recent algorithm for learning classification trees, Tree Alternating Optimization (TAO), to the regression case, and use it with bagging to construct regression forests of oblique trees, having hyperplane splits at the decision nodes. In a wide range of datasets, we show that the resulting forests exceed the accuracy of state-of-the-art algorithms such as random forests, AdaBoost or gradient boosting, often considerably, while yielding forests that have usually fewer and shallower trees and hence fewer parameters and faster inference overall. This result has an immense practical impact and advocates for the power of optimization in ensemble learning.
","['University of California, Merced', 'University of California, Merced']"
2020,Bandits with Adversarial Scaling,"Thodoris Lykouris, Vahab Mirrokni, Renato Leme",https://icml.cc/Conferences/2020/Schedule?showEvent=6296,"We study ""adversarial scaling"", a multi-armed bandit model where rewards have a stochastic and an adversarial component. Our model captures display advertising where the ""click-through-rate"" can be decomposed to a (fixed across time) arm-quality component and a  non-stochastic user-relevance component (fixed across arms). Despite the relative stochasticity of our model, we demonstrate two settings where most bandit algorithms suffer. On the positive side, we show that two algorithms, one from the action elimination and one from the mirror descent family are adaptive enough to be robust to adversarial scaling. Our results shed light on the robustness of adaptive parameter selection in stochastic bandits, which may be of independent interest.
","['Microsoft Research NYC', 'Google Research', 'Google Research']"
2020,Explainable and Discourse Topic-aware Neural Language Understanding,"Yatin Chaudhary, Hinrich Schuetze, Pankaj Gupta",https://icml.cc/Conferences/2020/Schedule?showEvent=6656,"Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document. This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion. Moreover, we retain sentence-topic association along with document-topic association by modeling topical discourse for every sentence in the document. We present a novel neural composite language modeling (NCLM) framework that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models. Experiments over a range of tasks such as language modeling, word sense disambiguation, document classification, retrieval and text generation demonstrate ability of the proposed model in improving language understanding.
","['Siemens AG', 'University of Munich (LMU)', 'Siemens AG']"
2020,Near-linear time Gaussian process optimization with adaptive batching and resparsification,"Daniele Calandriello, Luigi Carratino, Alessandro Lazaric, Michal Valko, Lorenzo Rosasco",https://icml.cc/Conferences/2020/Schedule?showEvent=6521,"Gaussian processes (GP) are one of the most successful frameworks to model uncertainty. However, GP optimization (e.g., GP-UCB) suffers from major scalability issues. Experimental time grows linearly with the number of evaluations, unless candidates are selected in batches (e.g., using GP-BUCB) and evaluated in parallel. Furthermore, computational cost is often prohibitive since algorithms such as GP-BUCB require a time at least quadratic in the number of dimensions and iterations to select each batch.
In this paper, we introduce BBKB (Batch Budgeted Kernel Bandits), the first no-regret GP optimization algorithm that provably runs in near-linear time and selects candidates in batches. This is obtained with a new guarantee for the tracking of the posterior variances that allows BBKB to choose increasingly larger batches, improving over GP-BUCB. Moreover, we show that the same bound can be used to adaptively delay costly updates to the sparse GP approximation used by BBKB, achieving a near-constant per-step amortized cost. These findings are then confirmed in several experiments, where BBKB is much faster than state-of-the-art methods.
","['IIT/DeepMind', 'University of Genoa', 'Facebook AI Research', 'DeepMind', 'unige, mit, iit']"
2020,Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation,"Steven Kleinegesse, Michael Gutmann",https://icml.cc/Conferences/2020/Schedule?showEvent=6444,"Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.
","['University of Edinburgh', 'University of Edinburgh']"
2020,Consistent Structured Prediction with Max-Min Margin Markov Networks,"Alex Nowak, Francis Bach, Alessandro Rudi",https://icml.cc/Conferences/2020/Schedule?showEvent=6103,"Max-margin methods for binary classification such as the support vector machine (SVM) have been extended to the structured prediction setting under the name of max-margin Markov networks ($M^3N$), or more generally structural SVMs. Unfortunately, these methods are statistically inconsistent when the relationship between inputs and labels is far from deterministic.
We overcome such limitations by defining the learning problem in terms of a ``max-min'' margin formulation, naming the resulting method max-min margin Markov networks ($M^4N$). We prove consistency and finite sample generalization bounds for $M^4N$ and provide an explicit algorithm to compute the estimator. The algorithm achieves a generalization error of $O(1/\sqrt{n})$ for a total cost of $O(n)$ projection-oracle calls (which have at most the same cost as the max-oracle from $M^3N$). Experiments on multi-class classification, ordinal regression, sequence prediction and matching demonstrate the effectiveness of the proposed method.","['INRIA - Ecole Normale Supérieure', 'INRIA - Ecole Normale Supérieure', 'INRIA, École Normale Supérieure']"
2020,Quantum Expectation-Maximization for Gaussian mixture models,"Alessandro Luongo, Iordanis Kerenidis, Anupam Prakash",https://icml.cc/Conferences/2020/Schedule?showEvent=6102,"We define a quantum version of Expectation-Maximization (QEM), a fundamental tool in unsupervised machine learning, often used to solve Maximum Likelihood (ML) and Maximum A Posteriori (MAP) estimation problems. We use QEM to fit a Gaussian Mixture Model, and show how to generalize it to fit mixture models with base distributions in the exponential family. Given quantum access to a dataset, our algorithm has convergence and precision guarantees similar to the classical algorithm, while the runtime is polylogarithmic in the number of elements in the training set and polynomial in other parameters, such as the dimension of the feature space and the number of components in the mixture. We discuss the performance of the algorithm on a dataset that is expected to be classified successfully by classical EM and provide guarantees for its runtime.
","['Université Paris', 'CNRS and QC Ware', 'Université Paris Diderot']"
2020,A Swiss Army Knife for Minimax Optimal Transport,"Sofien Dhouib, Ievgen Redko, Tanguy Kerdoncuff, Rémi Emonet, Marc Sebban",https://icml.cc/Conferences/2020/Schedule?showEvent=6474,"The Optimal transport (OT) problem and its associated Wasserstein distance have recently become a topic of great interest in the machine learning community. However, the underlying optimization problem is known to have two major restrictions: (i) it largely depends on the choice of the cost function and (ii) its sample complexity scales exponentially with the dimension. In this paper, we propose a general formulation of a minimax OT problem that can tackle these restrictions by jointly optimizing the cost matrix and the transport plan, allowing us to define a robust distance between distributions. We propose to use a cutting-set method to solve this general problem and show its links and advantages compared to other existing minimax OT approaches. Additionally, we use this method to define a notion of stability allowing us to select the most robust cost matrix. Finally, we provide an experimental study highlighting the efficiency of our approach.
","['CREATIS Laboratory INSA Lyon', 'Laboratoire Hubert Curien', 'Laboratoire Hubert Curien', 'Laboratoire Hubert Curien', 'Jean Monnet University']"
2020,Adversarial Robustness for Code,"Pavol Bielik, Martin Vechev",https://icml.cc/Conferences/2020/Schedule?showEvent=6485,"Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code including -- finding and fixing bugs, code completion, decompilation, malware detection, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) developing a set of novel techniques that enable training robust and accurate models of code.
","['ETH Zurich', 'ETH Zurich']"
2020,Optimal Non-parametric Learning in Repeated Contextual Auctions with  Strategic Buyer,Alexey Drutsa,https://icml.cc/Conferences/2020/Schedule?showEvent=5850,"We study learning algorithms that optimize revenue in repeated contextual posted-price auctions where a seller interacts with a single strategic buyer that seeks to maximize his cumulative discounted surplus.
The buyer's valuation of a good is a fixed private function of a $d$-dimensional context (feature) vector that describes the good being sold.
In contrast to existing studies on repeated contextual auctions with strategic buyer, in our work, the seller is not assumed to know the parametric model that underlies this valuation function.
We introduce a novel non-parametric learning algorithm that is horizon-independent and has tight strategic regret upper bound of $\Theta(T^{d/(d+1)})$.
We also non-trivially generalize several value-localization techniques of non-contextual repeated auctions  to make them effective in  the considered contextual non-parametric learning of the buyer valuation function. ",['Yandex']
2020,Information-Theoretic Local Minima Characterization and Regularization,"Zhiwei Jia, Hao Su",https://icml.cc/Conferences/2020/Schedule?showEvent=6611,"Recent advances in deep learning theory have evoked the study of generalizability across different local minima of deep neural networks (DNNs). While current work focused on either discovering properties of good local minima or developing regularization techniques to induce good local minima, no approach exists that can tackle both problems. We achieve these two goals successfully in a unified manner. Specifically, based on the observed Fisher information we propose a metric both strongly indicative of generalizability of local minima and effectively applied as a practical regularizer. We provide theoretical analysis including a generalization bound and empirically demonstrate the success of our approach in both capturing and improving the generalizability of DNNs. Experiments are performed on CIFAR-10, CIFAR-100 and ImageNet for various network architectures.
","['University of California, San Diego', 'UCSD']"
2020,Causal Structure Discovery from Distributions Arising from Mixtures of DAGs,"Basil Saeed, Snigdha Panigrahi, Caroline Uhler",https://icml.cc/Conferences/2020/Schedule?showEvent=6655,"We consider distributions arising from a mixture of causal models, where each model is represented by a directed acyclic graph (DAG). We provide a graphical representation of such mixture distributions and prove that this representation encodes the conditional independence relations of the mixture distribution. We  then  consider  the  problem  of  structure  learning  based  on  samples  from  such distributions. Since the mixing variable is latent, we consider causal structure discovery algorithms such as FCI that can deal with latent variables. We show that such algorithms recover a “union” of the component DAGs and can identify variables whose conditional distribution across the component DAGs vary. We demonstrate our results on synthetic and real data showing that the inferred graph identifies nodes that vary between the different mixture components. As an immediate application, we demonstrate how retrieval of this causal information can be used to cluster samples according to each mixture component.
","['MIT', 'University of Michigan', 'Massachusetts Institute of Technology']"
2020,LowFER: Low-rank Bilinear Pooling for Link Prediction,"Saadullah Amin, Stalin Varanasi, Katherine Ann Dunfield, Günter Neumann",https://icml.cc/Conferences/2020/Schedule?showEvent=6251,"Knowledge graphs are incomplete by nature, with only a limited number of observed facts from world knowledge being represented as structured relations between entities. To partly address this issue, an important task in statistical relational learning is that of link prediction or knowledge graph completion. Both linear and non-linear models have been proposed to solve the problem of knowledge graph completion, with the former being parameter efficient and interpretable. Bilinear models, while expressive, are prone to overfitting and lead to quadratic growth of parameters in number of relations. Simpler models have become more standard, with certain constraints on bilinear maps as relation parameters. In this work, we propose a factorized bilinear pooling model, commonly used in multi-modal learning, for better fusion of entities and relations, leading to an efficient and constraint-free model. We prove that our model is fully expressive, providing bounds on embedding dimensionality and factorization rank. Our model naturally generalizes TuckER (Balazevic et al., 2019), which has been shown to generalize other models, as efficient low-rank approximation without substantially compromising performance. Due to low-rank approximation, the model complexity can be controlled by the factorization rank, avoiding the possible cubic growth of TuckER. Empirically, we evaluate on real-world datasets, reaching on par or state-of-the-art performance.
","['German Research Center for Aritificial Intelligence (DFKI)', 'German Research Center for Artificial Intelligence (DFKI)', 'German Research Center for Artificial Intelligence (DFKI)', 'German Research Center for Artificial Intelligence (DFKI)']"
2020,Learning Deep Kernels for Non-Parametric Two-Sample Tests,"Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, D.J. Sutherland",https://icml.cc/Conferences/2020/Schedule?showEvent=5999,"We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data. The code of our deep-kernel-based two-sample tests is available at github.com/fengliu90/DK-for-TST.
","['UTS/UCL', 'Gatsby Unit，UCL', 'University of Technology Sydney', 'University of Technology Sydney', 'Gatsby Computational Neuroscience Unit', 'TTI-Chicago']"
2020,"Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills","Victor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giro-i-Nieto, Jordi  Torres",https://icml.cc/Conferences/2020/Schedule?showEvent=6014,"Acquiring abilities in the absence of a task-oriented reward function is at the frontier of reinforcement learning research. This problem has been studied through the lens of empowerment, which draws a connection between option discovery and information theory. Information-theoretic skill discovery methods have garnered much interest from the community, but little research has been conducted in understanding their limitations.  Through theoretical analysis and empirical evidence, we show that existing algorithms suffer from a common limitation -- they discover options that provide a poor coverage of the state space. In light of this, we propose Explore, Discover and Learn (EDL), an alternative approach to information-theoretic skill discovery. Crucially, EDL optimizes the same information-theoretic objective derived from the empowerment literature, but addresses the optimization problem using different machinery. We perform an extensive evaluation of skill discovery methods on controlled environments and show that EDL offers significant advantages, such as overcoming the coverage problem, reducing the dependence of learned skills on the initial state, and allowing the user to define a prior over which behaviors should be learned.
","['Barcelona Supercomputing Center', 'Salesforce Research', 'Salesforce', 'Salesforce', 'Universitat Politecnica de Catalunya', 'Barcelona Supercomputing Center']"
2020,Maximum Likelihood with Bias-Corrected Calibration is Hard-To-Beat at Label Shift Adaptation,"Amr Mohamed Alexandari, Anshul Kundaje, Avanti Shrikumar",https://icml.cc/Conferences/2020/Schedule?showEvent=5886,"Label shift refers to the phenomenon where the prior class probability p(y) changes between the training and test distributions, while the conditional probability p(x|y) stays fixed. Label shift arises in settings like medical diagnosis, where a classifier trained to predict disease given symptoms must be adapted to scenarios where the baseline prevalence of the disease is different. Given estimates of p(y|x) from a predictive model, Saerens et al. proposed an efficient maximum likelihood algorithm to correct for label shift that does not require model retraining, but a limiting assumption of this algorithm is that p(y|x) is calibrated, which is not true of modern neural networks. Recently, Black Box Shift Learning (BBSL) and Regularized Learning under Label Shifts (RLLS) have emerged as state-of-the-art techniques to cope with label shift when a classifier does not output calibrated probabilities, but both methods require model retraining with importance weights and neither has been benchmarked against maximum likelihood. Here we (1) show that combining maximum likelihood with a type of calibration we call bias-corrected calibration outperforms both BBSL and RLLS across diverse datasets and distribution shifts, (2) prove that the maximum likelihood objective is concave, and (3) introduce a principled strategy for estimating source-domain priors that improves robustness to poor calibration. This work demonstrates that the maximum likelihood with appropriate calibration is a formidable and efficient baseline for label shift adaptation; notebooks reproducing experiments available at https://github.com/kundajelab/labelshiftexperiments
","['Stanford University', 'Stanford University', 'Stanford University']"
2020,Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates,"Yang Liu, Hongyi Guo",https://icml.cc/Conferences/2020/Schedule?showEvent=6576,"Learning with noisy labels is a common challenge in supervised learning. Existing approaches often require practitioners to specify noise rates, i.e., a set of parameters controlling the severity of label noises in the problem, and the specifications are either assumed to be given or estimated using additional steps. In this work, we introduce a new family of loss functions that we name as peer loss functions, which enables learning from noisy labels and does not require a priori specification of the noise rates. Peer loss functions work within the standard empirical risk minimization (ERM) framework. We show that, under mild conditions, performing ERM with peer loss functions on the noisy data leads to the optimal or a near-optimal classifier as if performing ERM over the clean training data, which we do not have access to. We pair our results with an extensive set of experiments. Peer loss provides a way to simplify model development when facing potentially noisy training labels, and can be promoted as a robust candidate loss function in such situations.
","['UC Santa Cruz', 'Shanghai Jiao Tong University']"
2020,Kernel Methods for Cooperative Multi-Agent Contextual Bandits,"Abhimanyu Dubey, Alex `Sandy' Pentland",https://icml.cc/Conferences/2020/Schedule?showEvent=5805,"Cooperative multi-agent decision making involves a group of agents cooperatively solving learning problems while communicating over a network with delays. In this paper, we consider the kernelised contextual bandit problem, where the reward obtained by an agent is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS), and a group of agents must cooperate to collectively solve their unique decision problems. For this problem, we propose Coop-KernelUCB, an algorithm that provides near-optimal bounds on the per-agent regret, and is both computationally and communicatively efficient. For special cases of the cooperative problem, we also provide variants of Coop-KernelUCB that provides optimal per-agent regret. In addition, our algorithm generalizes several existing results in the multi-agent bandit setting. Finally, on a series of both synthetic and real-world multi-agent network benchmarks, we demonstrate that our algorithm significantly outperforms existing benchmarks.
","['Massachusetts Institute of Technology', 'MIT']"
2020,Which Tasks Should Be Learned Together in Multi-task Learning?,"Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, Silvio Savarese",https://icml.cc/Conferences/2020/Schedule?showEvent=5875,"Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using multi-task learning. This can save computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives can compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We study task cooperation and competition in several different learning settings and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.
","['Stanford University', 'Swiss Federal Institute of Technology (EPFL)', 'Google', 'Stanford University', 'University of California at Berkeley', 'Stanford University']"
2020,Understanding and Mitigating the Tradeoff between Robustness and Accuracy,"Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, Percy Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6801,"Adversarial training augments the training set with perturbations to improve the robust error (over worst-case perturbations), but it often leads to an increase in the standard error (on unperturbed test inputs). Previous explanations for this tradeoff rely on the assumption that no predictor in the hypothesis class has low standard and robust error. In this work, we precisely characterize the effect of augmentation on the standard error in linear regression when the optimal linear predictor has zero standard and robust error. In particular, we show that the standard error could increase even when the augmented perturbations have noiseless observations from the optimal linear predictor. We then prove that the recently proposed robust self-training (RST) estimator improves robust error without sacrificing standard error for noiseless linear regression. Empirically, for neural networks, we find that RST with different adversarial training methods improves both standard and robust error for random and adversarial rotations and adversarial l_infty perturbations in CIFAR-10.
","['Stanford', 'Stanford University', 'ETH', 'Stanford University', 'Stanford University']"
2020,Online metric algorithms with untrusted predictions,"Antonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, Bertrand Simon",https://icml.cc/Conferences/2020/Schedule?showEvent=6830,"Machine-learned predictors, although achieving very good results for inputs resembling training data, cannot possibly provide perfect predictions in all situations. Still, decision-making systems that are based on such predictors need not only to benefit from good predictions but also to achieve a decent performance when the predictions are inadequate. In this paper, we propose a prediction setup for arbitrary metrical task systems (MTS) (e.g., caching, k-server and convex body chasing) and online matching on the line. We utilize results from the theory of online algorithms to show how to make the setup robust. Specifically for caching, we present an algorithm whose performance, as a function of the prediction error, is exponentially better than what is achievable for general MTS. Finally, we present an empirical evaluation of our methods on real world datasets, which suggests practicality.
","['MPII', 'CWI', 'École polytechnique fédérale de Lausanne', 'Jagiellonian University', 'University of Bremen']"
2020,Stochastically Dominant Distributional Reinforcement Learning,"John Martin, Michal Lyskawinski, Xiaohu Li, Brendan Englot",https://icml.cc/Conferences/2020/Schedule?showEvent=6410,"We describe a new approach for managing aleatoric uncertainty in the Reinforcement Learning (RL) paradigm. Instead of selecting actions according to a single statistic, we propose a distributional method based on the second-order stochastic dominance (SSD) relation. This compares the inherent dispersion of random returns induced by actions, producing a comprehensive evaluation of the environment’s uncertainty. The necessary conditions for SSD require estimators to predict accurate second moments. To accommodate this, we map the distributional RL problem to a Wasserstein gradient flow, treating the distributional Bellman residual as a potential energy functional. We propose a particle-based algorithm for which we prove optimality and convergence. Our experiments characterize the algorithm’s performance and demonstrate how uncertainty and performance are better balanced using an SSD policy than with other risk measures.
","['Stevens Institute of Technology', 'Stevens Institute of Technology', 'Stevens Institute of Technology', 'Stevens Institute of Technology']"
2020,When Explanations Lie: Why Many Modified BP Attributions Fail,"Leon Sixt, Maximilian Granz, Tim Landgraf",https://icml.cc/Conferences/2020/Schedule?showEvent=5929,"Attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically.
","['Freie Universität Berlin', 'Freie Universität Berlin', 'Freie Universität Berlin']"
2020,Optimization and Analysis of the pAp@k Metric for Recommender Systems,"Gaurush Hiranandani, Warut Vijitbenjaronk, Sanmi Koyejo, Prateek Jain",https://icml.cc/Conferences/2020/Schedule?showEvent=5928,"Modern recommendation and notification systems must be robust to data imbalance, limitations on the number of recommendations/notifications, and heterogeneous engagement profiles across users. The pAp@k metric, which combines the partial-AUC and the precision@k metrics, was recently proposed to evaluate such recommendation systems and has been used in real-world deployments. Conceptually, pAp@k measures the probability of correctly ranking a top-ranked positive instance over top-ranked negative instances. Due to the combinatorial aspect surfaced by top-ranked points, little is known about the characteristics and optimization methods of pAp@k. In this paper, we analyze the learning-theoretic properties of pAp@k, particularly its benefits in evaluating modern recommender systems, and propose novel surrogates that are consistent under certain data regularity conditions. We then provide gradient descent based algorithms to optimize the surrogates directly. Our analysis and experimental evaluation suggest that pAp@k indeed exhibits a certain dual behavior with respect to partial-AUC and precision@k. Moreover, the proposed methods outperform all the baselines in various applications. Taken together, our results motivate the use of pAp@k for large-scale recommender systems with heterogeneous user-engagement.
","['UIUC', 'University of Illinois, Urbana-Champaign', 'Illinois / Google', 'Microsoft Research']"
2020,Partial Trace Regression and Low-Rank Kraus Decomposition,"Hachem Kadri, Stephane Ayache, Riikka Huusari, alain rakotomamonjy, Ralaivola Liva",https://icml.cc/Conferences/2020/Schedule?showEvent=6614,"The trace regression model, a direct extension of the well-studied linear regression model, allows one to map matrices to real-valued outputs. We here introduce an even more general model, namely the  partial-trace regression model, a family of linear mappings from matrix-valued inputs to matrix-valued outputs; this model subsumes the trace regression model and thus the linear regression model. Borrowing tools from quantum information theory, where partial trace operators have been extensively studied, we propose a framework for learning partial trace regression models from data by taking advantage of the so-called low-rank Kraus representation of completely positive maps. We show the relevance of our framework with synthetic and real-world experiments  conducted for  both i) matrix-to-matrix regression and ii) positive semidefinite matrix completion, two tasks which can be formulated as partial trace regression problems.
","['Aix-Marseille University', 'AMU LIS', 'Aalto University', 'Universite de Rouen Normandie / Criteo AI Lab', 'Criteo AI Lab']"
2020,Meta-Learning with Shared Amortized Variational Inference,"Ekaterina Iakovleva, Jakob Verbeek, Karteek Alahari",https://icml.cc/Conferences/2020/Schedule?showEvent=6439,"We propose a novel amortized variational inference scheme for an empirical Bayes meta-learning model, where model parameters are treated as latent variables. We learn the prior distribution over model parameters conditioned on limited training data using a variational autoencoder approach. Our framework proposes sharing the same amortized inference network between the conditional prior and variational posterior distributions over the model parameters. While the posterior leverages both the labeled support and query data, the conditional prior is based only on the labeled support data. We show that in earlier work, relying on Monte-Carlo approximation, the conditional prior collapses to a Dirac delta function. In contrast, our variational approach prevents this collapse and preserves uncertainty over the model parameters. We evaluate our approach on the miniImageNet and FC100 datasets, and present results demonstrating its advantages over previous work.
","['Inria', 'Facebook', 'Inria']"
2020,Weakly-Supervised Disentanglement Without Compromises,"Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Schölkopf, Olivier Bachem, Michael Tschannen",https://icml.cc/Conferences/2020/Schedule?showEvent=6488,"Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation.
First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed.
Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.
","['ETH Zurich - Max Planck Institute', 'Google Brain', 'ETH Zurich', 'MPI for Intelligent Systems Tübingen, Germany', 'Google Brain', 'Google Brain']"
2020,Stochastic Frank-Wolfe for Constrained Finite-Sum Minimization,"Geoffrey Negiar, Gideon Dresdner, Alicia Yi-Ting Tsai, Laurent El Ghaoui, Francesco Locatello, Robert Freund, Fabian Pedregosa",https://icml.cc/Conferences/2020/Schedule?showEvent=5792,"We propose a novel Stochastic Frank-Wolfe (a. k. a. conditional gradient) algorithm for constrained smooth finite-sum minimization with a generalized linear prediction/structure. This class of problems includes empirical risk minimization with sparse, low-rank, or other structured constraints. The proposed method is simple to implement, does not require step-size tuning, and has a constant per-iteration cost that is independent of the dataset size. Furthermore, as a byproduct of the method we obtain a stochastic estimator of the Frank-Wolfe gap that can be used as a stopping criterion. Depending on the setting, the proposed method matches or improves on the best computational guarantees for Stochastic Frank-Wolfe algorithms. Benchmarks on several datasets highlight different regimes in which the proposed method exhibits a faster empirical convergence than related methods. Finally, we provide an implementation of all considered methods in an open-source package.
","['UC Berkeley', 'ETH Zürich', 'University of California, Berkeley', 'UC Berkeley', 'ETH Zurich - Max Planck Institute', 'MIT', 'Google']"
2020,SimGANs: Simulator-Based Generative Adversarial Networks for ECG Synthesis to Improve Deep ECG Classification,"Tomer Golany, Kira Radinsky, Daniel Freedman",https://icml.cc/Conferences/2020/Schedule?showEvent=6235,"Generating training examples for supervised tasks is a long sought after goal in AI. We study the problem of heart signal electrocardiogram (ECG) synthesis for improved heartbeat classification. ECG synthesis is challenging: the generation of training examples for such biological-physiological systems is not straightforward, due to their dynamic nature in which the various parts of the system interact in complex ways.
However, an understanding of these dynamics has been developed for years in the form of mathematical process simulators. We study how to incorporate this knowledge into the generative process by leveraging a biological simulator for the task of ECG classification.
Specifically, we use a system of ordinary differential equations representing heart dynamics, and incorporate this ODE system into the optimization process of a generative adversarial network to create biologically plausible ECG training examples. 
We perform empirical evaluation and show that heart simulation knowledge during the generation process improves ECG classification.
","['Technion - Israel Institute of Technology', 'Technion- Israel institute of technology', 'Google Israel']"
2020,Normalizing Flows on Tori and Spheres,"Danilo J. Rezende, George Papamakarios, Sebastien Racaniere, Michael Albergo, Gurtej Kanwar, Phiala Shanahan, Kyle Cranmer",https://icml.cc/Conferences/2020/Schedule?showEvent=6493,"Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.
","['DeepMind', 'DeepMind', 'DeepMind', 'New York University', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'New York University, CERN']"
2020,TaskNorm: Rethinking Batch Normalization for Meta-Learning,"John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, Richard E Turner",https://icml.cc/Conferences/2020/Schedule?showEvent=6200,"Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting.  We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based- and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.
","['University of Cambridge', 'University of Cambridge', 'University of Cambridge', 'Microsoft Research', 'University of Cambridge']"
2020,Understanding the Curse of Horizon in Off-Policy Evaluation via Conditional Importance Sampling,"Yao Liu, Pierre-Luc Bacon, Emma Brunskill",https://icml.cc/Conferences/2020/Schedule?showEvent=5901,"Off-policy policy estimators that use importance sampling (IS) can suffer from high variance in long-horizon domains, and there has been particular excitement over new IS methods that leverage the structure of Markov decision processes. We analyze the variance of the most popular approaches through the viewpoint of conditional Monte Carlo. Surprisingly, we find that in finite horizon MDPs there is no strict variance reduction of per-decision importance sampling or stationary importance sampling, comparing with vanilla importance sampling. We then provide sufficient conditions under which the per-decision or stationary estimators will provably reduce the variance over importance sampling with finite horizons. For the asymptotic (in terms of horizon $T$) case, we develop upper and lower bounds on the variance of those estimators which yields sufficient conditions under which there exists an exponential v.s. polynomial gap between the variance of importance sampling and that of the per-decision or stationary estimators. These results help advance our understanding of if and when new types of IS estimators will improve the accuracy of off-policy estimation.","['Stanford University', 'Stanford University', 'Stanford University']"
2020,Learning the piece-wise constant graph structure of a varying Ising model,"Batiste Le Bars, Pierre Humbert, Argyris Kalogeratos, Nicolas Vayatis",https://icml.cc/Conferences/2020/Schedule?showEvent=6175,"This work focuses on the estimation of multiple change-points in a time-varying Ising model that evolves piece-wise constantly. The aim is to identify both the moments at which significant changes occur in the Ising model, as well as the underlying graph structures.
For this purpose, we propose to estimate the neighborhood of each node by maximizing a penalized version of its conditional log-likelihood. The objective of the penalization is twofold: it imposes sparsity in the learned graphs and, thanks to a fused-type penalty, it also enforces them to evolve piece-wise constantly. Using few assumptions, we provide two change-points consistency theorems. Those are the first in the context of unknown number of change-points detection in time-varying Ising model. Finally, experimental results on several synthetic datasets and a real-world dataset demonstrate the performance of our method.
","['Centre Borelli - ENS Paris-Saclay', 'Ecole Normale Supérieure Paris-Saclay, Université Paris Saclay', 'Center Borelli - ENS Paris-Saclay', 'ENS Cachan']"
2020,Learning disconnected manifolds: a no GAN's land,"Ugo Tanielian, Thibaut Issenhuth, Elvis Dohmatob, Jeremie Mary",https://icml.cc/Conferences/2020/Schedule?showEvent=6395,"Typical architectures of Generative Adversarial Networks make use of a unimodal latent/input distribution transformed by a continuous generator. Consequently, the modeled distribution always has connected support which is cumbersome when learning a disconnected set of manifolds. We formalize this problem by establishing a ""no free lunch"" theorem for the disconnected manifold learning stating an upper-bound on the precision of the targeted distribution. This is done by building on the necessary existence of a low-quality region where the generator continuously samples data between two disconnected modes.  Finally, we derive a rejection sampling method based on the norm of generator’s Jacobian and show its efficiency on several generators including BigGAN.
","['Criteo', 'Criteo AI Lab', 'Criteo AI Lab', 'Criteo AI Lab']"
2020,Learning to Simulate Complex Physics with Graph Networks,"Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex (Zhitao) Ying, Jure Leskovec, Peter Battaglia",https://icml.cc/Conferences/2020/Schedule?showEvent=6849,"Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term ""Graph Network-based Simulators"" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.
","['DeepMind', 'DeepMind', 'DeepMind', 'Stanford University', 'Stanford University', 'DeepMind']"
2020,Ready Policy One: World Building Through Active Learning,"Philip Ball, Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, Stephen Roberts",https://icml.cc/Conferences/2020/Schedule?showEvent=5951,"Model-Based Reinforcement Learning (MBRL) offers a promising direction for sample efficient learning, often achieving state of the art results for continuous control tasks. However many existing MBRL methods rely on combining greedy policies with exploration heuristics, and even those which utilize principled exploration bonuses construct dual objectives in an ad hoc fashion. In this paper we introduce Ready Policy One (RP1), a framework that views MBRL as an active learning problem, where we aim to improve the world model in the fewest samples possible. RP1 achieves this by utilizing a hybrid objective function, which crucially adapts during optimization, allowing the algorithm to trade off reward v.s. exploration at different stages of learning. In addition, we introduce a principled mechanism to terminate sample collection once we have a rich enough trajectory batch to improve the model. We rigorously evaluate our method on a variety of continuous control tasks, and demonstrate statistically significant gains over existing approaches.
","['University of Oxford', 'University of Oxford', 'UC Berkeley', 'Google', 'University of Oxford']"
2020,Restarted Bayesian Online Change-point Detector achieves Optimal Detection Delay,"REDA ALAMI, Odalric-Ambrym Maillard, Raphaël Féraud",https://icml.cc/Conferences/2020/Schedule?showEvent=6442,"we consider the problem of sequential change-point detection where 
both the change-points and the distributions before and after the change are assumed to be unknown. For this problem of primary importance in statistical and sequential learning theory,  we derive a variant of the Bayesian Online Change Point Detector proposed by \cite{fearnhead2007line}
which is easier to analyze than the original version while keeping its powerful message-passing algorithm. 
We provide a non-asymptotic analysis of the false-alarm rate and the detection delay that matches the existing lower-bound. We further provide the first explicit high-probability control of the detection delay for such approach. Experiments on synthetic and real-world data show that this proposal outperforms the state-of-art change-point detection strategy, namely the Improved Generalized Likelihood Ratio (Improved GLR) while compares favorably with the original Bayesian Online Change Point Detection strategy.

","['TOTAL', 'Inria Lille - Nord Europe', 'Orange Labs']"
2020,On Contrastive Learning for Likelihood-free Inference ,"Conor Durkan, Iain Murray, George Papamakarios",https://icml.cc/Conferences/2020/Schedule?showEvent=6219,"Likelihood-free methods perform parameter inference in stochastic simulator models where evaluating the likelihood is intractable but sampling synthetic data is possible. One class of methods for this likelihood-free problem uses a classifier to distinguish between pairs of parameter-observation samples generated using the simulator and pairs sampled from some reference distribution, which implicitly learns a density ratio proportional to the likelihood. Another popular class of methods fits a conditional distribution to the parameter posterior directly, and a particular recent variant allows for the use of flexible neural density estimators for this task. In this work, we show that both of these approaches can be unified under a general contrastive learning scheme, and clarify how they should be run and compared.
","['University of Edinburgh', 'University of Edinburgh', 'DeepMind']"
2020,A distributional view on multi-objective policy optimization,"Abbas Abdolmaleki, Sandy Huang, Leonard Hasenclever, Michael Neunert, Francis Song, Martina Zambelli, Murilo Martins, Nicolas Heess, Raia Hadsell, Martin Riedmiller",https://icml.cc/Conferences/2020/Schedule?showEvent=6838,"Many real-world problems require trading off multiple competing objectives. However, these objectives are often in different units and/or scales, which can make it challenging for practitioners to express numerical preferences over objectives in their native units. In this paper we propose a novel algorithm for multi-objective reinforcement learning that enables setting desired preferences for objectives in a scale-invariant way. We propose to learn an action distribution for each objective, and we use supervised learning to fit a parametric policy to a combination of these distributions. We demonstrate the effectiveness of our approach on challenging high-dimensional real and simulated robotics tasks, and show that setting different preferences in our framework allows us to trace out the space of nondominated solutions.
","['DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2020,Bayesian Sparsification of Deep C-valued Networks,"Ivan Nazarov, Evgeny Burnaev",https://icml.cc/Conferences/2020/Schedule?showEvent=6836,"With continual miniaturization ever more applications of deep learning can be found in embedded systems, where it is common to encounter data with natural representation in the complex domain. To this end we extend Sparse Variational Dropout to complex-valued neural networks and verify the proposed Bayesian technique by conducting a large numerical study of the performance-compression trade-off of C-valued networks on two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription on MusicNet. We replicate the state-of-the-art result by Trabelsi et al. (2018) on MusicNet with a complex-valued network compressed by 50-100x at a small performance penalty.
","['Skolkovo Institute of Science and Technology', 'Skoltech']"
2020,Learning Portable Representations for High-Level Planning,"Steven James, Benjamin Rosman, George Konidaris",https://icml.cc/Conferences/2020/Schedule?showEvent=6120,"We present a framework for autonomously learning a portable representation that describes a collection of low-level continuous environments. We show that these abstract representations can be learned in a task-independent egocentric space specific to the agent that, when grounded with problem-specific information, are provably sufficient for planning. We demonstrate transfer in two different domains, where an agent learns a portable, task-independent symbolic vocabulary, as well as operators expressed in that vocabulary, and then learns to instantiate those operators on a per-task basis. This reduces the number of samples required to learn a representation of a new task.
","['University of the Witwatersrand', 'University of the Witwatersrand, South Africa', 'Brown']"
2020,IPBoost – Non-Convex Boosting via Integer Programming,"Marc Pfetsch, Sebastian Pokutta",https://icml.cc/Conferences/2020/Schedule?showEvent=6598,"Recently non-convex optimization approaches for solving machine learning problems have gained significant attention. In this paper we explore non-convex boosting in classification by means of integer programming and demonstrate real-world practicability of the approach while circumvent- ing shortcomings of convex boosting approaches. We report results that are comparable to or better than the current state-of-the-art.
","['TU Darmstadt', 'ZIB']"
2020,Word-Level Speech Recognition With a Letter to Word Encoder,"Ronan Collobert, Awni Hannun, Gabriel Synnaeve",https://icml.cc/Conferences/2020/Schedule?showEvent=6625,"We propose a direct-to-word sequence model which uses a word network to learn word embeddings from letters. The word network can be integrated seamlessly with arbitrary sequence models including Connectionist Temporal Classification and encoder-decoder models with attention. We show our direct-to-word model can achieve word error rate gains over sub-word level models for speech recognition. We also show that our direct-to-word approach retains the ability to predict words not seen at training time without any retraining. Finally, we demonstrate that a word-level model can use a larger stride than a sub-word level model while maintaining accuracy. This makes the model more efficient both for training and inference.
","['Facebook AI Research', 'Facebook AI Research', 'Facebook AI Research']"
2020,Multi-step Greedy Reinforcement Learning Algorithms,"Manan Tomar, Yonathan Efroni, Mohammad Ghavamzadeh",https://icml.cc/Conferences/2020/Schedule?showEvent=6698,"Multi-step greedy policies have been extensively used in model-based reinforcement learning (RL), both when a model of the environment is available (e.g.,~in the game of Go) and when it is learned. In this paper, we explore their benefits in model-free RL, when employed using multi-step dynamic programming algorithms: $\kappa$-Policy Iteration ($\kappa$-PI) and $\kappa$-Value Iteration ($\kappa$-VI). These methods iteratively compute the next policy ($\kappa$-PI) and value function ($\kappa$-VI) by solving a surrogate decision problem with a shaped reward and a smaller discount factor. We derive model-free RL algorithms based on $\kappa$-PI and $\kappa$-VI in which the surrogate problem can be solved by any discrete or continuous action RL method, such as DQN and TRPO. We identify the importance of a hyper-parameter that controls the extent to which the surrogate problem is solved and suggest a way to set this parameter. When evaluated on a range of Atari and MuJoCo benchmark tasks, our results indicate that for the right range of $\kappa$, our algorithms outperform DQN and TRPO. This shows that our multi-step greedy algorithms are general enough to be applied over any existing RL algorithm and can significantly improve its performance.","['Facebook AI Research', 'Technion', 'Google Research']"
2020,Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics,"Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, Dmitry Vetrov",https://icml.cc/Conferences/2020/Schedule?showEvent=6846,"The overestimation bias is one of the major impediments to accurate off-policy learning. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method---Truncated Quantile Critics, TQC,---blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. Distributional representation and truncation allow for arbitrary granular overestimation control, while ensembling provides additional score improvements. TQC outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment.
","['Samsung', 'Samsung Artificial Intelligence Center ', 'Higher School of Economics', 'Higher School of Economics, Samsung AI Center Moscow']"
2020,Entropy Minimization In Emergent Languages,"Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, Marco Baroni",https://icml.cc/Conferences/2020/Schedule?showEvent=6017,"There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel.  We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.
","['Facebook AI', 'Facebook/ENS/INRIA', 'Facebook AI', 'Facebook Artificial Intelligence Research']"
2020,Constructive Universal High-Dimensional Distribution Generation through Deep ReLU Networks,"Dmytro Perekrestenko, Stephan Müller, Helmut Bölcskei",https://icml.cc/Conferences/2020/Schedule?showEvent=6841,"We present an explicit deep neural network construction that transforms uniformly distributed one-dimensional noise into an arbitrarily close approximation of any two-dimensional Lipschitz-continuous target distribution. The key ingredient of our design is a generalization of the  ""space-filling"" property of sawtooth functions discovered in (Bailey & Telgarsky, 2018). We elicit the importance of depth - in our neural network construction - in driving the Wasserstein distance between the target distribution and the approximation realized by the network to zero. An extension to output distributions of arbitrary dimension is outlined. Finally, we show that the proposed construction does not incur a cost - in terms of error measured in Wasserstein-distance - relative to generating $d$-dimensional target distributions from $d$ independent random variables.","['ETH Zürich', 'ETH Zurich', 'ETH Zurich']"
2020,Learning with Good Feature Representations in Bandits and in RL with a Generative Model,"Tor Lattimore, Csaba Szepesvari, Gellért Weisz",https://icml.cc/Conferences/2020/Schedule?showEvent=6647,"The construction in the recent paper by Du et al. [2019] implies that searching for a near-optimal action in a bandit sometimes requires examining essentially all the actions, even if the learner is given linear features in R^d that approximate the rewards with a small uniform error. We use the Kiefer-Wolfowitz theorem to prove a positive result that by checking only a few actions, a learner can always find an action that is suboptimal with an error of at most O(ε√d) where ε is the approximation error of the features. Thus, features are useful when the approximation error is small relative to the dimensionality of the features. The idea is applied to stochastic bandits and reinforcement learning with a generative model where the learner has access to d-dimensional linear features that approximate the action-value functions for all policies to an accuracy of ε. For linear bandits, we prove a bound on the regret of order d√(n log(k)) + εn√d log(n) with k the number of actions and n the horizon. For RL we show that approximate policy iteration can learn a policy that is optimal up to an additive error of order ε√d/(1 − γ)^2 and using about d/(ε^2(1 − γ)^4) samples from the generative model. These bounds are independent of the finer details of the features. We also investigate how the structure of the feature set impacts the tradeoff between sample complexity and estimation error.
","['DeepMind', 'DeepMind/University of Alberta', 'DeepMind']"
2020,Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities,"Jonas Köhler, Leon Klein, Frank Noe",https://icml.cc/Conferences/2020/Schedule?showEvent=6834,"Normalizing flows are exact-likelihood generative neural networks which approximately transform samples from a simple prior distribution to samples of the probability distribution of interest. Recent work showed that such generative models can be utilized in statistical mechanics to sample equilibrium states of many-body systems in physics and chemistry. To scale and generalize these results, it is essential that the natural symmetries in the probability density -- in physics defined by the invariances of the target potential -- are built into the flow. 
We provide a theoretical sufficient criterion showing that the distribution generated by equivariant normalizing flows is invariant with respect to these symmetries by design. Furthermore, we propose building blocks for flows which preserve symmetries which are usually found in physical/chemical many-body particle systems. Using benchmark systems motivated from molecular physics, we demonstrate that those symmetry preserving flows can provide better generalization capabilities and sampling efficiency.
","['Freie Universität Berlin', 'Freie Universität Berlin', 'FU Berlin']"
2020,Adaptive Sampling for Estimating Probability Distributions,"Shubhanshu Shekhar, Tara Javidi, Mohammad Ghavamzadeh",https://icml.cc/Conferences/2020/Schedule?showEvent=6627,"We consider the problem of allocating a fixed budget of samples to a finite set of discrete distributions to learn them uniformly well (minimizing the maximum error) in terms of four common distance measures: $\ell_2^2$, $\ell_1$, $f$-divergence, and separation distance. To present a unified treatment of these distances, we first propose a general \emph{optimistic tracking algorithm} and analyze its sample allocation performance w.r.t.~an oracle. We then instantiate this algorithm for the four distance measures and derive bounds on their regret. We also show that the allocation performance of the proposed algorithm cannot, in general, be improved, by deriving lower-bounds on the expected deviation from the oracle allocation for any adaptive scheme. We verify our theoretical findings through some experiments. Finally, we show that the techniques developed in the paper can be easily extended to learn some classes of continuous distributions as well as to the related setting of minimizing the average error (in terms of the four distances) in learning a set of distributions. ","['University of California, San Diego', 'University of California San Diego', 'Google Research']"
2020,Graph Random Neural Features for Distance-Preserving Graph Representations,"Daniele Zambon, Cesare Alippi, Lorenzo Livi",https://icml.cc/Conferences/2020/Schedule?showEvent=6027,"We present Graph Random Neural Features (GRNF), a novel embedding method from graph-structured data to real vectors based on a family of graph neural networks.  The embedding naturally deals with graph isomorphism and preserves the metric structure of the graph domain, in probability.  In addition to being an explicit embedding method, it also allows us to efficiently and effectively approximate graph metric distances (as well as complete kernel functions); a criterion to select the embedding dimension trading off the approximation accuracy with the computational cost is also provided.  GRNF can be used within traditional processing methods or as a training-free input layer of a graph neural network.  The theoretical guarantees that accompany GRNF ensure that the considered graph distance is metric, hence allowing to distinguish any pair of non-isomorphic graphs. 
","['Università della Svizzera Italiana', 'Università della Svizzera Italiana', 'University of Manitoba']"
2020,Latent Space Factorisation and Manipulation via Matrix Subspace Projection,"Xiao Li, Chenghua Lin, Ruizhe Li, Chaozheng Wang, Frank Guerin",https://icml.cc/Conferences/2020/Schedule?showEvent=6063,"We tackle the problem disentangling the latent space of an autoencoder in order to separate labelled attribute information from other characteristic information. This then allows us to change selected attributes while preserving other information. Our method, matrix subspace projection, is much simpler than previous approaches to latent space factorisation, for  example not requiring multiple discriminators or a careful weighting among their loss functions.
Furthermore our new model can be applied to autoencoders as a plugin, and works across diverse domains such as images or text. We demonstrate the utility of our method for attribute manipulation in autoencoders trained across varied domains, using both human evaluation and automated methods. The quality of generation of our new model (e.g. reconstruction, conditional generation) is highly competitive to a number of strong baselines. 
","['University of Aberdeen', 'University of Sheffield', 'The University of Sheffield', 'University of Aberdeen', 'University of Surrey']"
2020,Implicit differentiation of Lasso-type models for hyperparameter optimization,"Quentin Bertrand, Quentin Klopfenstein, Mathieu Blondel, Samuel Vaiter, Alexandre Gramfort, Joseph Salmon",https://icml.cc/Conferences/2020/Schedule?showEvent=6062,"Setting regularization parameters for Lasso-type estimators is notoriously difficult, though crucial for obtaining the best accuracy. The most popular hyperparameter optimization approach is grid-search on a held-out dataset. However, grid-search requires to choose a predefined grid of parameters and scales exponentially in the number of parameters. Another class of approaches casts hyperparameter optimization as a bi-level optimization problem, typically solved by gradient descent. The key challenge for these approaches is the estimation of the gradient w.r.t. the hyperparameters. Computing that gradient via forward or backward automatic differentiation usually suffers from high memory consumption, while implicit differentiation typically involves solving a linear system which can be prohibitive and numerically unstable. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case of Lasso-type problems. This work introduces an efficient implicit differentiation algorithm, without matrix inversion, tailored for Lasso-type problems. Our proposal scales to high-dimensional data by leveraging the sparsity of the solutions. Empirically, we demonstrate that the proposed method outperforms a large number of standard methods for hyperparameter optimization.
","['INRIA', 'Université de Bourgogne Franche-Comté', 'Google', 'CNRS', 'Inria', 'Université de Montpellier']"
2020,Growing Action Spaces,"Gregory Farquhar, Laura  Gustafson, Zeming Lin, Shimon Whiteson, Nicolas Usunier, Gabriel Synnaeve",https://icml.cc/Conferences/2020/Schedule?showEvent=6168,"In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress.
In this work, we use a curriculum of progressively growing action spaces to accelerate learning.
We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space.
Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data,  value estimates, and state representations from restricted action spaces to the full task.
We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.
","['Deepmind', 'Facebook AI Research', 'New York University', 'Oxford University', 'Facebook AI Research', 'Facebook AI Research']"
2020,Constant Curvature Graph Convolutional Networks,"Gregor Bachmann, Gary Becigneul, Octavian Ganea",https://icml.cc/Conferences/2020/Schedule?showEvent=6615,"Interest has been rising lately towards methods representing data in non-Euclidean spaces, e.g. hyperbolic or spherical that provide specific inductive biases useful for certain real-world data properties, e.g. scale-free, hierarchical or cyclical. However, the popular graph neural networks are currently limited in modeling data only via Euclidean geometry and associated vector space operations. Here, we bridge this gap by proposing mathematically grounded generalizations of graph convolutional networks (GCN) to (products of) constant curvature spaces. We do this by i) introducing a unified formalism permitting
a differentiable interpolation between all geometries of constant curvature irrespective of their sign, ii) leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Our class of models smoothly recover their Euclidean counterparts when the curvature goes to zero from either side. Empirically, we outperform Euclidean GCNs in the tasks of node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior, according to their discrete curvature.
","['ETH Zurich', 'MIT', 'MIT']"
2020,Distinguishing Cause from Effect Using Quantiles: Bivariate Quantile Causal Discovery,"Natasa Tagasovska, Valérie Chavez-Demoulin, Thibault Vatter",https://icml.cc/Conferences/2020/Schedule?showEvent=6405,"Causal inference using observational data is challenging, especially in the bivariate case.
Through the minimum description length principle, we link the postulate of independence between the generating mechanisms of the cause and of the effect given the cause to quantile regression.
Based on this theory, we develop Bivariate Quantile Causal Discovery (bQCD), a new method to distinguish cause from effect
assuming no confounding, selection bias or feedback.
Because it uses multiple quantile levels instead of the conditional mean only, bQCD is adaptive not only to additive, but also to multiplicative or even location-scale generating mechanisms.
To illustrate the effectiveness of our approach, we perform an extensive empirical comparison on both synthetic and real datasets.
This study shows that bQCD is robust across different implementations of the method (i.e., the quantile regression), computationally efficient, and compares favorably to state-of-the-art methods.
","['Swiss Data Science Center EPFL/ETH', 'University of Lausanne', 'Columbia University']"
2020,Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning,"Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav Sukhatme, Vladlen Koltun",https://icml.cc/Conferences/2020/Schedule?showEvent=6333,"Increasing the scale of reinforcement learning experiments has allowed researchers to achieve unprecedented results in both training sophisticated agents for video games, and in sim-to-real transfer for robotics. Typically such experiments rely on large distributed systems and require expensive hardware setups, limiting wider access to this exciting area of research. In this work we aim to solve this problem by optimizing the efficiency and resource utilization of reinforcement learning algorithms instead of relying on distributed computation. We present the ""Sample Factory"", a high-throughput training system optimized for a single-machine setting. Our architecture combines a highly efficient, asynchronous, GPU-based sampler with off-policy correction techniques, allowing us to achieve throughput higher than $10^5$ environment frames/second on non-trivial control problems in 3D without sacrificing sample efficiency. We extend Sample Factory to support self-play and population-based training and apply these techniques to train highly capable agents for a multiplayer first-person shooter game. Github: https://github.com/alex-petrenko/sample-factory","['University of Southern California', 'University of Southern California', 'University of Southern California', 'University of Southern California', 'Intel Labs']"
2020,Supervised learning: no loss no cry,"Richard Nock, Aditya Menon",https://icml.cc/Conferences/2020/Schedule?showEvent=6079,"Supervised learning requires the specification of a loss function to minimise.
While the theory of admissible losses from both a computational and statistical perspective is well-developed,
these offer a panoply of different choices.
In practice, this choice is typically made in an \emph{ad hoc} manner.
In hopes of making this procedure more principled,
the problem of \emph{learning the loss function} for a downstream task (e.g., classification) has garnered recent interest.
However, works in this area have been generally empirical in nature.
In this paper, 
we revisit the {\sc SLIsotron} algorithm of Kakade et al. (2011) through a novel lens, 
derive a generalisation based on Bregman divergences,
and show how it provides a principled procedure for learning the loss.
In detail, 
we cast
{\sc SLIsotron}
as learning a loss from a family of composite square losses.
By interpreting this through the lens of \emph{proper losses},
we derive a generalisation of {\sc SLIsotron} based on Bregman divergences.
The resulting {\sc BregmanTron} algorithm
jointly learns the loss along with the classifier. 
It comes equipped with a simple guarantee of convergence for the loss it learns, and its set of possible outputs comes with a guarantee of agnostic approximability of Bayes rule.
Experiments indicate that the {\sc BregmanTron} significantly outperforms the {\sc SLIsotron}, and that the loss it learns can be minimized by other algorithms for different tasks, thereby opening the interesting problem of \textit{loss transfer} between domains.
","['Data61, The Australian National University and the University of Sydney', 'Google Research']"
2020,Uncertainty Estimation Using a Single Deep Deterministic Neural Network,"Joost van Amersfoort, Lewis Smith, Yee-Whye Teh, Yarin Gal",https://icml.cc/Conferences/2020/Schedule?showEvent=6512,"We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.
","['University of Oxford', 'University of Oxford', 'Oxford and DeepMind', 'University of Oxford']"
2020,StochasticRank: Global Optimization of Scale-Free Discrete Functions,"Aleksei Ustimenko, Liudmila Prokhorenkova",https://icml.cc/Conferences/2020/Schedule?showEvent=6037,"In this paper, we introduce a powerful and efficient framework for direct optimization of ranking metrics. The problem is ill-posed due to the discrete structure of the loss, and to deal with that, we introduce two important techniques: a stochastic smoothing and a novel gradient estimate based on partial integration. We also address the problem of smoothing bias and present a universal solution for a proper debiasing. To guarantee the global convergence of our method, we adopt a recently proposed Stochastic Gradient Langevin Boosting algorithm. Our algorithm is implemented as a part of the CatBoost gradient boosting library and outperforms the existing approaches on several learning-to-rank datasets. In addition to ranking metrics, our framework applies to any scale-free discrete loss function. 
","['Yandex', 'Yandex']"
2020,Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions,"Ahmed Alaa, Mihaela van der Schaar",https://icml.cc/Conferences/2020/Schedule?showEvent=5883,"Recurrent neural networks (RNNs) are instrumental in modelling sequential and time-series data. Yet, when using RNNs to inform decision-making, predictions by themselves are not sufficient — we also need estimates of predictive uncertainty. Existing approaches for uncertainty quantification in RNNs are based predominantly on Bayesian methods; these are computationally prohibitive, and require major alterations to the RNN architecture and training. Capitalizing on ideas from classical jackknife resampling, we develop a frequentist alternative that: (a) does not interfere with model training or compromise its accuracy, (b) applies to any RNN architecture, and (c) provides theoretical coverage guarantees on the estimated uncertainty intervals. Our method derives predictive uncertainty from the variability of the (jackknife) sampling distribution of the RNN outputs, which is estimated by repeatedly deleting “blocks” of (temporally-correlated) training data, and collecting the predictions of the RNN re-trained on the remaining data. To avoid exhaustive re-training, we utilize influence functions to estimate the effect of removing training data blocks on the learned RNN parameters. Using data from a critical care setting, we demonstrate the utility of uncertainty quantification in sequential decision-making.
","['UCLA', 'University of Cambridge and UCLA']"
2020,Predictive Sampling with Forecasting Autoregressive Models,"Auke Wiggers, Emiel Hoogeboom",https://icml.cc/Conferences/2020/Schedule?showEvent=6489,"Autoregressive models (ARMs) currently hold state-of-the-art performance in likelihood-based modeling of image and audio data. Generally, neural network based ARMs are designed to allow fast inference, but sampling from these models is impractically slow. In this paper, we introduce the predictive sampling algorithm: a procedure that exploits the fast inference property of ARMs in order to speed up sampling, while keeping the model intact. We propose two variations of predictive sampling, namely sampling with ARM fixed-point iteration and learned forecasting modules. Their effectiveness is demonstrated in two settings: i) explicit likelihood modeling on binary MNIST, SVHN and CIFAR10, and ii) discrete latent modeling in an autoencoder trained on SVHN, CIFAR10 and Imagenet32. Empirically, we show considerable improvements over baselines in number of ARM inference calls and sampling speed.
","['Qualcomm AI Research', 'University of Amsterdam']"
2020,Fractional Underdamped Langevin Dynamics: Retargeting SGD with Momentum under Heavy-Tailed Gradient Noise,"Umut Simsekli, Lingjiong Zhu, Yee-Whye Teh, Mert Gurbuzbalaban",https://icml.cc/Conferences/2020/Schedule?showEvent=5774,"Stochastic gradient descent with momentum (SGDm) is one of the most popular optimization algorithms in deep learning. While there is a rich theory of SGDm for convex problems, the theory is considerably less developed in the context of deep learning where the  problem is non-convex and the gradient noise might exhibit a heavy-tailed behavior, as empirically observed in recent studies. In this study, we consider a \emph{continuous-time} variant of SGDm, known as the underdamped Langevin dynamics (ULD), and investigate its asymptotic properties under heavy-tailed perturbations. Supported by recent studies from statistical physics, we argue both theoretically and empirically that the heavy-tails of such perturbations can result in a bias even when the step-size is small, in the sense that \emph{the optima of stationary distribution} of the dynamics might not match \emph{the optima of the cost function to be optimized}. As a remedy, we develop a novel framework, which we coin as \emph{fractional} ULD (FULD), and prove that FULD targets the so-called Gibbs distribution, whose optima exactly match the optima of the original cost. We observe that the Euler discretization of FULD has noteworthy algorithmic similarities with \emph{natural gradient} methods and \emph{gradient clipping}, bringing a new perspective on understanding their role in deep learning. We support our theory with experiments conducted on a synthetic model and neural networks.
","['Institut Polytechnique de Paris / University of Oxford', 'FSU', 'Oxford and DeepMind', 'Rutgers University']"
2020,Inexact Tensor Methods with Dynamic Accuracies,"Nikita Doikov, Yurii Nesterov",https://icml.cc/Conferences/2020/Schedule?showEvent=6420,"In this paper, we study inexact high-order Tensor Methods for solving convex optimization problems with composite objective. At every step of such methods, we use approximate solution of the auxiliary problem, defined by the bound for the residual in function value. We propose two dynamic strategies for choosing the inner accuracy: the first one is decreasing as $1/k^{p + 1}$, where $p \geq 1$ is the order of the method and $k$ is the iteration counter, and the second approach is using for the inner accuracy the last progress in the target objective. We show that inexact Tensor Methods with these strategies achieve the same global convergence rate as in the error-free case. For the second approach we also establish local superlinear rates (for $p \geq 2$), and propose the accelerated scheme. Lastly, we present computational results on a variety of machine learning problems for several methods and different accuracy policies.","['Université catholique de Louvain', 'Universite catholique de Louvain']"
2020,Duality in RKHSs with Infinite Dimensional Outputs: Application to Robust Losses,"Pierre Laforgue, Alex Lambert, Luc Brogat-Motte, Florence d'Alche-Buc",https://icml.cc/Conferences/2020/Schedule?showEvent=5964,"Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing Kernel Hilbert Spaces provide an elegant way to extend scalar kernel methods when the output space is a Hilbert space. Although primarily used in finite dimension for problems like multi-task regression, the ability of this framework to deal with infinite dimensional output spaces unlocks many more applications, such as functional regression, structured output prediction, and structured data representation. However, these sophisticated schemes crucially rely on the kernel trick in the output space, so that most of previous works have focused on the square norm loss function, completely neglecting robustness issues that may arise in such surrogate problems. To overcome this limitation, this paper develops a duality approach that allows to solve OVK machines for a wide range of loss functions. The infinite dimensional Lagrange multipliers are handled through a Double Representer Theorem, and algorithms for \epsilon-insensitive losses and the Huber loss are thoroughly detailed. Robustness benefits are emphasized by a theoretical stability analysis, as well as empirical improvements on structured data applications.
","['Télécom Paris', 'Télécom ParisTech', 'Télécom Paris', 'Télécom ParisTech, Université Paris-Saclay,Paris, France']"
2020,Sequential Transfer in Reinforcement Learning with a Generative Model,"Andrea Tirinzoni, Riccardo Poiani, Marcello Restelli",https://icml.cc/Conferences/2020/Schedule?showEvent=6567,"We are interested in how to design reinforcement learning agents that provably reduce the sample complexity for learning new tasks by transferring knowledge from previously-solved ones. The availability of solutions to related problems poses a fundamental trade-off: whether to seek policies that are expected to immediately achieve high (yet sub-optimal) performance in the new task or whether to seek information to quickly identify an optimal solution, potentially at the cost of poor initial behaviour. In this work, we focus on the second objective when the agent has access to a generative model of state-action pairs. First, given a set of solved tasks containing an approximation of the target one, we design an algorithm that quickly identifies an accurate solution by seeking the state-action pairs that are most informative for this purpose. We derive PAC bounds on its sample complexity which clearly demonstrate the benefits of using this kind of prior knowledge. Then, we show how to learn these approximate tasks sequentially by reducing our transfer setting to a hidden Markov model and employing spectral methods to recover its parameters. Finally, we empirically verify our theoretical findings in simple simulated domains.
","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano']"
2020,Generalization to New Actions in Reinforcement Learning,"Ayush Jain, Andrew Szot, Joseph Lim",https://icml.cc/Conferences/2020/Schedule?showEvent=5768,"A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances, such as making decisions from new action choices. However, standard reinforcement learning assumes a fixed set of actions and requires expensive retraining when given a new action set. To make learning agents more adaptable, we introduce the problem of zero-shot generalization to new actions. We propose a two-stage framework where the agent first infers action representations from action information acquired separately from the task. A policy flexible to varying action sets is then trained with generalization objectives. We benchmark generalization on sequential tasks, such as selecting from an unseen tool-set to solve physical reasoning puzzles and stacking towers with novel 3D shapes. Videos and code are available at https://sites.google.com/view/action-generalization.
","['University of Southern California', 'University of Southern California', 'Univ. of Southern California']"
2020,Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks,"David Stutz, Matthias Hein, Bernt Schiele",https://icml.cc/Conferences/2020/Schedule?showEvent=6559,"Adversarial training yields robust models against a specific threat model, e.g., $L_\infty$ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other $L_p$ norms, or larger perturbations. Our confidence-calibrated adversarial training (CCAT) tackles this problem by biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on $L_\infty$ adversarial examples, increases robustness against larger $L_\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing confidence. For each threat model, we use $7$ attacks with up to $50$ restarts and $5000$ iterations and report worst-case robust test error, extended to our confidence-thresholded setting, across all attacks.","['Max Planck Institute for Informatics', 'University of Tübingen', 'MPI Informatics']"
2020,An Explicitly Relational Neural Network Architecture,"Murray Shanahan, Kyriacos Nikiforou, Antonia Creswell, Christos Kaplanis, David GT Barrett, Marta Garnelo",https://icml.cc/Conferences/2020/Schedule?showEvent=6177,"With a view to bridging the gap between deep learning and symbolic AI, we present a novel end-to-end neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. In order to evaluate and analyse the architecture, we introduce a family of simple visual relational reasoning tasks of varying complexity. We show that the proposed architecture, when pre-trained on a curriculum of such tasks, learns to generate reusable representations that better facilitate subsequent learning on previously unseen tasks when compared to a number of baseline architectures. The workings of a successfully trained model are visualised to shed some light on how the architecture functions.
","['DeepMind / Imperial College London', 'DeepMind', 'Deep Mind', 'DeepMind Technologies Ltd', 'DeepMind', 'DeepMind']"
2020,Optimal Randomized First-Order Methods for Least-Squares Problems,"Jonathan Lacotte, Mert Pilanci",https://icml.cc/Conferences/2020/Schedule?showEvent=6353,"We provide an exact analysis of a class of randomized algorithms for solving overdetermined least-squares problems. We consider first-order methods, where the gradients are pre-conditioned by an approximation of the Hessian, based on a subspace embedding of the data matrix. This class of algorithms encompasses several randomized methods among the fastest solvers for least-squares problems. We focus on two classical embeddings, namely, Gaussian projections and subsampled randomized Hadamard transforms (SRHT). Our key technical innovation is the derivation of the limiting spectral density of SRHT embeddings. Leveraging this novel result, we derive the family of normalized orthogonal polynomials of the SRHT density and we find the optimal pre-conditioned first-order method along with its rate of convergence. Our analysis of Gaussian embeddings proceeds similarly, and leverages classical random matrix theory results. In particular, we show that for a given sketch size, SRHT embeddings exhibits a faster rate of convergence than Gaussian embeddings. Then, we propose a new algorithm by optimizing the computational complexity over the choice of the sketching dimension. To our knowledge, our resulting algorithm yields the best known complexity for solving least-squares problems with no condition number dependence.
","['Stanford University', 'Stanford']"
2020,Near-Tight Margin-Based Generalization Bounds for Support Vector Machines,"Allan Grønlund, Lior Kamma, Kasper Green Larsen",https://icml.cc/Conferences/2020/Schedule?showEvent=6269,"Support Vector Machines (SVMs) are among the most fundamental tools for binary classification. 
In its simplest formulation, an SVM produces a hyperplane separating two classes of data using the largest possible margin to the data. 
The focus on maximizing the margin has been well motivated through numerous generalization bounds. 
In this paper, we revisit and improve the classic generalization bounds in terms of margins. 
Furthermore, we complement our new generalization bound by a nearly matching lower bound, thus almost settling the generalization performance of SVMs in terms of margins.
","['Aarhus University', 'Aarhus University', 'Aarhus University, MADALGO']"
2020,GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation,Marc Brockschmidt,https://icml.cc/Conferences/2020/Schedule?showEvent=5897,"This paper presents a new Graph Neural Network (GNN) type using feature-wise linear modulation (FiLM).
Many standard GNN variants propagate information along the edges of a graph by computing messages based only on the representation of the source of each edge.
In GNN-FiLM, the representation of the target node of an edge is used to compute a transformation that can be applied to all incoming messages, allowing feature-wise modulation of the passed information.
Different GNN architectures are compared in extensive experiments on three tasks from the literature, using re-implementations of many baseline methods.
Hyperparameters for all methods were found using extensive search, yielding somewhat surprising results: differences between state of the art models are much smaller than reported in the literature and well-known simple baselines that are often not compared to perform better than recently proposed GNN variants.
Nonetheless, GNN-FiLM outperforms these methods on a regression task on molecular graphs and performs competitively on other tasks.
",['Microsoft Research']
2020,Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization?,"Yaniv Blumenfeld, Dar Gilboa, Daniel Soudry",https://icml.cc/Conferences/2020/Schedule?showEvent=6408,"Deep neural networks are typically initialized with random weights, with variances chosen to facilitate signal propagation and stable gradients. It is also believed that diversity of features is an important property of these initializations. We construct a deep convolutional network with identical features by initializing almost all the weights to $0$. The architecture also enables perfect signal propagation and stable gradients, and achieves high accuracy on standard benchmarks. This indicates that random, diverse initializations are \textit{not} necessary for training neural networks. An essential element in training this network is a mechanism of symmetry breaking; we study this phenomenon and find that standard GPU operations, which are non-deterministic, can serve as a sufficient source of symmetry breaking to enable training.","['Technion', 'Columbia University', 'Technion']"
2020,Likelihood-free MCMC with Amortized Approximate Ratio Estimators,"Joeri Hermans, Volodimir Begy, Gilles Louppe",https://icml.cc/Conferences/2020/Schedule?showEvent=5834,"Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to rely on approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model. We achieve this by learning a flexible amortized estimator which approximates the likelihood-to-evidence ratio. We demonstrate that the learned ratio estimator can be embedded in \textsc{mcmc} samplers to approximate likelihood-ratios between consecutive states in the Markov chain, allowing us to draw samples from the intractable posterior. Techniques are presented to improve the numerical stability and to measure the quality of an approximation. The accuracy of our approach is demonstrated on a variety of benchmarks against well-established techniques. Scientific applications in physics show its applicability.
","['University of Liège', 'University of Vienna', 'University of Liège']"
2020,Temporal Logic Point Processes,"Shuang Li, Lu Wang, Ruizhi Zhang, xiaofu Chang, Xuqin Liu, Yao Xie, Yuan Qi, Le Song",https://icml.cc/Conferences/2020/Schedule?showEvent=6570,"We propose a modeling framework for event data and aim to answer questions such as {\it when} and {\it why} the next event would happen. Our proposed model excels in small data regime with the ability to incorporate domain knowledge in terms of logic rules. We model the dynamics of the event starts and ends via intensity function with the structures informed by a set of first-order temporal logic rules. Using the softened representation of temporal relations, and a weighted combination of logic rules, our probabilistic model can deal with uncertainty in events. Furthermore, many well-known point processes (e.g., Hawkes process, self-correcting point process) can be interpreted as special cases of our model given simple temporal logic rules. Our model, therefore, riches the family of point processes. We derive a maximum likelihood estimation procedure for our model and show that it can lead to accurate predictions when data are sparse and domain knowledge is critical. 
","['Harvard University', 'East China Normal University', 'University of Nebraska-Lincoln', 'Ant Financial Services Group', 'Ant Financial Services Group', 'Georgia Institute of Technology', 'Ant Financial Services Group', 'Georgia Institute of Technology']"
2020,Bayesian Differential Privacy for Machine Learning,"Aleksei Triastcyn, Boi Faltings",https://icml.cc/Conferences/2020/Schedule?showEvent=6547,"Traditional differential privacy is independent of the data distribution. However, this is not well-matched with the modern machine learning context, where models are trained on specific data. As a result, achieving meaningful privacy guarantees in ML often excessively reduces accuracy. We propose Bayesian differential privacy (BDP), which takes into account the data distribution to provide more practical privacy guarantees. We also derive a general privacy accounting method under BDP, building upon the well-known moments accountant. Our experiments demonstrate that in-distribution samples in classic machine learning datasets, such as MNIST and CIFAR-10, enjoy significantly stronger privacy guarantees than postulated by DP, while models maintain high classification accuracy.
","['EPFL', 'EPFL']"
2020,Meta-learning with Stochastic Linear Bandits,"Leonardo Cella, Alessandro Lazaric, Massimiliano Pontil",https://icml.cc/Conferences/2020/Schedule?showEvent=6532,"We investigate meta-learning procedures in the setting of stochastic linear bandits tasks. The goal is to select a learning algorithm which works well on average over a class of bandits tasks, that are sampled from a task-distribution. Inspired by recent work on learning-to-learn linear regression, we consider a class of bandit algorithms that implement a regularized version of the well-known OFUL algorithm, where the regularization is a square euclidean distance to a bias vector. We first study the benefit of the biased OFUL algorithm in terms of regret minimization. We then propose two strategies to estimate the bias within the learning-to-learn setting. We show both theoretically and experimentally, that when the number of tasks grows and the variance of the task-distribution is small, our strategies have a significant advantage over learning the tasks in isolation.
","['University of Milan', 'Facebook AI Research', 'Istituto Italiano di Tecnologia and University College London']"
2020,Inertial Block Proximal Methods for Non-Convex Non-Smooth Optimization,"Hien Le, Nicolas Gillis, Panagiotis Patrinos",https://icml.cc/Conferences/2020/Schedule?showEvent=5876,"We propose inertial versions of block coordinate descent methods for solving non-convex non-smooth composite optimization problems. Our methods possess three main advantages compared to current state-of-the-art accelerated first-order methods: (1) they allow using two different extrapolation points to evaluate the gradients and to add the inertial force (we will empirically show that it is more efficient than using a single extrapolation point), (2) they allow to randomly select the block of variables to update, and (3) they do not require a restarting step. We prove the subsequential convergence of the generated sequence under mild assumptions, prove the global convergence under some additional assumptions, and provide convergence rates. We deploy the proposed methods to solve non-negative matrix factorization (NMF) and show that they compete favorably with the state-of-the-art NMF algorithms. Additional experiments on non-negative approximate canonical polyadic decomposition, also known as nonnegative tensor factorization, are also provided.  
","['University of Mons, Belgium.', 'Université de Mons', 'KU Leuven']"
2020,Linear Mode Connectivity and the Lottery Ticket Hypothesis,"Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, Michael Carbin",https://icml.cc/Conferences/2020/Schedule?showEvent=6699,"We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).
","['MIT CSAIL', 'Element AI', 'University of Toronto; Vector Institute', 'MIT']"
2020,Online Convex Optimization in the Random Order Model,"Dan Garber, Gal Korcia, Kfir Levy",https://icml.cc/Conferences/2020/Schedule?showEvent=6561,"Online Convex Optimization (OCO) is a powerful framework for sequential prediction, portraying the natural uncertainty inherent in data-streams as though the data were generated by an almost omniscient adversary. However, this view, which is often too pessimistic for real-world data, comes with a price. The complexity of solving many important online tasks in this adversarial framework becomes much worse than that of their offline and even stochastic counterparts.
In this work we consider a natural random-order version of the OCO model, in which the adversary can choose the set of loss functions, but does not get to choose the order in which they are supplied to the learner; Instead, they are observed in uniformly random order. Focusing on two important families of online tasks, one which includes online linear regression, and the other being online $k$-PCA, we show that under standard well-conditioned-data assumptions, standard online gradient descent (OGD) methods become much more efficient in the random-order model. In particular, for the first group of tasks OGD guarantees poly-logarithmic regret (this result holds even without assuming convexity of individual loss functions). In the case of online $k$-PCA, OGD guarantees sublinear regret using only a rank-$k$ SVD on each iteration and memory linear in the size of the solution.","['Technion', 'Technion - Israel Institute of Technology', 'Technion']"
2020,The Role of Regularization in Classification of High-dimensional Noisy Gaussian Mixture,"Francesca Mignacco, Florent Krzakala, Yue Lu, Pierfrancesco Urbani, Lenka Zdeborova",https://icml.cc/Conferences/2020/Schedule?showEvent=6492,"We consider a high-dimensional mixture of two Gaussians in the noisy regime where even an oracle knowing the centers of the clusters misclassifies a small but finite fraction of the points. We provide a rigorous analysis of the generalization error of regularized convex classifiers, including ridge, hinge and logistic regression, in the high-dimensional limit where the number $n$ of samples and their dimension $d$ go to infinity while their ratio is fixed to $\alpha=n/d$. We discuss surprising effects of the regularization that in some cases allows to reach the Bayes-optimal performances. We also illustrate the interpolation peak at low regularization, and analyze the role of the respective sizes of the two clusters. ","['IPhT, CEA Saclay', 'ENS', 'Harvard University, USA', 'Institut de Physique Théorique', 'CNRS']"
2020,Non-Stationary Delayed Bandits with Intermediate Observations,"Claire Vernade, András György, Timothy Mann",https://icml.cc/Conferences/2020/Schedule?showEvent=6127,"Online recommender systems often face long delays in receiving feedback, especially when optimizing for some long-term metrics. While mitigating the effects of delays in learning is well-understood in stationary environments, the problem becomes much more challenging when the environment changes. In fact, if the timescale of the change is comparable to the delay, it is impossible to learn about the environment, since the available observations are already obsolete. However, the arising issues can be addressed if intermediate signals are available without delay, such that given those signals, the long-term behavior of the system is stationary. To model this situation, we introduce the problem of stochastic, non-stationary, delayed bandits with intermediate observations. We develop a computationally efficient algorithm based on UCRL, and prove sublinear regret guarantees for its performance. Experimental results demonstrate that our method is able to learn in non-stationary delayed environments where existing methods fail. 
","['DeepMind', 'DeepMind', 'DeepMind']"
2020,Voice Separation with an Unknown Number of Multiple Speakers,"Eliya Nachmani, Yossi Adi, Lior Wolf",https://icml.cc/Conferences/2020/Schedule?showEvent=6008,"We present a new method for separating a mixed audio sequence, in which multiple voices speak simultaneously. The new method employs gated neural networks that are trained to separate the voices at multiple processing steps, while maintaining the speaker in each output channel fixed. A different model is trained for every number of possible speakers, and the model with the largest number of speakers is employed to select the actual number of speakers in a given sample. Our method greatly outperforms the current state of the art, which, as we show, is not competitive for more than two speakers.
","['Tel-Aviv University & Facebook AI Research', 'Bar-Ilan University', 'Facebook AI Research and Tel Aviv University']"
2020,Finding trainable sparse networks through Neural Tangent Transfer ,"Tianlin Liu, Friedemann Zenke",https://icml.cc/Conferences/2020/Schedule?showEvent=6844,"Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria.  In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.
","['University of Basel', 'Friedrich Miescher Institute']"
2020,Optimal Estimator for Unlabeled Linear Regression,"Hang Zhang, Ping Li",https://icml.cc/Conferences/2020/Schedule?showEvent=6786,"Unlabeled linear regression, or ``linear regression with an 
unknown permutation'',  has attracted increasing 
attentions due to its applications in (e.g.,) linkage record and
de-anonymization. 
However, the computation of unlabeled linear regression proves to be cumbersome and
existing algorithms typically require considerable time, especially in the 
high dimensional regime. In this paper, we propose a one-step estimator which 
is optimal from both the computational and the statistical  aspects. 
From the computational perspective, our estimator 
exhibits the same order of computational complexity 
as that of the oracle case (which means the regression coefficients
are known in advance and only the permutation 
needs recovery). 
From the statistical 
perspective, when comparing with the necessary conditions for permutation recovery, our requirement on the {\it signal-to-noise ratio}
($\mathsf{SNR}$) agrees up to merely $\Omega\left(\log \log n\right)$ difference
when the stable rank of the regression coefficients $\ensuremath{\mathbf{B}}^{\natural}$ is much less than $\log n/\log \log n$.  %i.e., 
Numerical experiments are also provided to 
corroborate the theoretical claims. ","['Georgia Tech', 'Baidu']"
2020,Debiased Sinkhorn barycenters,"Hicham Janati, Marco Cuturi, Alexandre Gramfort",https://icml.cc/Conferences/2020/Schedule?showEvent=6012,"Entropy regularization in optimal transport (OT) has been the driver of many recent interests for Wasserstein metrics and barycenters in machine learning. It allows to keep the appealing geometrical properties of the unregularized Wasserstein distance while having a significantly lower complexity thanks to Sinkhorn's algorithm. However, entropy brings some inherent smoothing bias, resulting for example in blurred barycenters. This side effect has prompted an increasing temptation in the community to settle for a slower algorithm such as log-domain stabilized Sinkhorn which breaks the parallel structure that can be leveraged on GPUs, or even go back to unregularized OT. Here we show how this bias is tightly linked to the reference measure that defines the entropy regularizer and propose debiased Sinkhorn barycenters that preserve the best of worlds: fast Sinkhorn-like iterations without entropy smoothing. Theoretically, we prove that this debiasing is perfect for Gaussian distributions with equal variance. Empirically, we illustrate the reduced blurring and the computational advantage.
","['Inria', 'Google', 'Inria']"
2020,Extreme Multi-label Classification from Aggregated Labels,"Yanyao Shen, Hsiang-Fu Yu, Sujay Sanghavi, Inderjit Dhillon",https://icml.cc/Conferences/2020/Schedule?showEvent=5982,"Extreme multi-label classification (XMC) is the problem of finding the relevant labels for an input, from a very large universe of possible labels. We consider XMC in the setting where labels are available only for groups of samples - but not for individual ones. Current XMC approaches are not built for such multi-instance multi-label (MIML) training data, and MIML approaches do not scale to XMC sizes. We develop a new and scalable algorithm to impute individual-sample labels from the group labels; this can be paired with any existing XMC method to solve the aggregated label problem. We characterize the statistical properties of our algorithm under mild assumptions, and provide a new end-to-end framework for MIML as an extension. Experiments on both aggregated label XMC and MIML tasks show the advantages over existing approaches.
","['UT Austin', 'Amazon', 'UT Austin', 'UT Austin & Amazon']"
2020,On the Iteration Complexity of Hypergradient Computation,"Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, Saverio Salzo",https://icml.cc/Conferences/2020/Schedule?showEvent=6515,"We study a general class of bilevel problems,  consisting in the minimization of an upper-level objective which depends on the solution to a parametric fixed-point equation. Important instances arising in machine learning include hyperparameter optimization, meta-learning, and certain graph and recurrent neural networks. Typically the gradient of the upper-level objective (hypergradient) is hard or even impossible to compute exactly, which has raised the interest in approximation methods. We investigate some popular approaches to compute the hypergradient, based on reverse mode iterative differentiation and approximate implicit differentiation. Under the hypothesis that the fixed point equation is defined by a contraction mapping, we present a unified analysis which allows for the first time to quantitatively compare these methods, providing explicit bounds for their iteration complexity. This analysis suggests a hierarchy in terms of computational efficiency among the above methods, with approximate implicit differentiation based on conjugate gradient performing best. We present an extensive experimental comparison among the methods which confirm the theoretical findings. 
","['Istituto Italiano di Tecnologia - University College London', 'Istituto Italiano di Tecnologia - University College London', 'Istituto Italiano di Tecnologia and University College London', 'Istituto Italiano di Tecnologia']"
2020,OPtions as REsponses: Grounding behavioural hierarchies in multi-agent reinforcement learning,"Alexander Vezhnevets, Yuhuai Wu, Maria Eckstein, Rémi Leblond, Joel Z Leibo",https://icml.cc/Conferences/2020/Schedule?showEvent=5970,"This paper investigates generalisation in multi-agent games, where the generality of the agent can be evaluated by playing against opponents it hasn't seen during training. We propose two new games with concealed information and complex, non-transitive reward structure (think rock-paper-scissors). It turns out that most current deep reinforcement learning methods fail to efficiently explore the strategy space, thus learning policies that generalise poorly to unseen opponents. We then propose a novel hierarchical agent architecture, where the hierarchy is grounded in the game-theoretic structure of the game -- the top level chooses strategic responses to opponents, while the low level implements them into policy over primitive actions. This grounding facilitates credit assignment across the levels of hierarchy. Our experiments show that the proposed hierarchical agent is capable of generalisation to unseen opponents, while conventional baselines fail to generalise whatsoever.
","['DeepMind', 'University of Toronto', 'UC Berkeley', 'DeepMind', 'DeepMind']"
2020,Knowing The What But Not The Where in Bayesian Optimization,"Vu Nguyen, Michael A Osborne",https://icml.cc/Conferences/2020/Schedule?showEvent=6137,"Bayesian optimization has demonstrated impressive success in finding the optimum input x∗ and output f∗ = f(x∗) = max f(x) of a black-box function f. In some applications, however, the optimum output is known in advance and the goal is to find the corresponding optimum input. Existing work in Bayesian optimization (BO) has not effectively exploited the knowledge of f∗ for optimization. In this paper, we consider a new setting in BO in which the knowledge of the optimum output is available. Our goal is to exploit the knowledge about f∗ to search for the input x∗ efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information about the optimum output. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization, which exploit the knowledge about the optimum output to identify the optimum input more efficient. We show that our approaches work intuitively and quantitatively better performance against standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.
","['University of Oxford', 'U Oxford']"
2020,Decentralised Learning with Random Features and Distributed Gradient Descent,"Dominic Richards, Patrick Rebeschini, Lorenzo Rosasco",https://icml.cc/Conferences/2020/Schedule?showEvent=6212,"We investigate the generalisation performance of Distributed Gradient Descent with implicit regularisation and random features in the homogenous setting where a network of agents are given data sampled independently from the same unknown distribution. Along with reducing the memory footprint, random features are particularly convenient in this setting as they provide a common parameterisation across agents that allows to overcome previous difficulties in implementing decentralised kernel regression. Under standard source and capacity assumptions, we establish high probability bounds on the predictive performance for each agent as a function of the step size, number of iterations, inverse spectral gap of the communication matrix and number of random features. By tuning these parameters, we obtain statistical rates that are minimax optimal with respect to the total number of samples in the network. The algorithm provides a linear improvement over single-machine gradient descent in memory cost and, when agents hold enough data with respect to the network size and inverse spectral gap, a linear speed up in computational run-time for any network topology. We present simulations that show how the number of random features, iterations and samples impact predictive performance.
","['University of Oxford', 'University of Oxford', 'unige, mit, iit']"
2020,Fast Differentiable Sorting and Ranking,"Mathieu Blondel, Olivier Teboul, Quentin Berthet, Josip Djolonga",https://icml.cc/Conferences/2020/Schedule?showEvent=6445,"The sorting operation is one of the most commonly used building blocks in computer programming. In machine learning, it is often used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks where it is non-differentiable. More problematic is the related ranking operator, often used for order statistics and ranking metrics.  It is a piecewise constant function, meaning that its derivatives are null or undefined.  While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the $O(n \log n)$ time complexity one would expect from sorting and ranking operations. In this paper, we propose the first differentiable sorting and ranking operators with $O(n \log n)$ time and $O(n)$ space complexity.  Our proposal in addition enjoys exact computation and differentiation.  We achieve this feat by constructing differentiable operators as projections onto the permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization.  Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: differentiable Spearman's rank correlation coefficient and least trimmed squares.","['Google', 'Google Brain', 'Google Brain', 'Google Research, Zurich']"
2020,Training Neural Networks for and by Interpolation,"Leonard Berrada, Andrew Zisserman, M. Pawan Kumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6207,"In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning, which we term Adaptive Learning-rates for Interpolation with Gradients (ALI-G). ALI-G retains the two main advantages of Stochastic Gradient Descent (SGD), which are (i) a low computational cost per iteration and (ii) good generalization performance in practice. At each iteration, ALI-G exploits the interpolation property to compute an adaptive learning-rate in closed form. In addition, ALI-G clips the learning-rate to a maximal value, which we prove to be helpful for non-convex problems. Crucially, in contrast to the learning-rate of SGD, the maximal learning-rate of ALI-G does not require a decay schedule. This makes ALI-G considerably easier to tune than SGD. We prove the convergence of ALI-G in various stochastic settings. Notably, we tackle the realistic case where the interpolation property is satisfied up to some tolerance. We also provide experiments on a variety of deep learning architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.
","['University of Oxford', 'University of Oxford, DeepMind', 'University of Oxford']"
2020,Continuously Indexed Domain Adaptation,"Hao Wang, Hao He, Dina Katabi",https://icml.cc/Conferences/2020/Schedule?showEvent=5986,"Existing domain adaptation focuses on transferring knowledge between domains with categorical indices (e.g., between datasets A and B). However, many tasks involve continuously indexed domains. For example, in medical applications, one often needs to transfer disease analysis and prediction across patients of different ages, where age acts as a continuous domain index. Such tasks are challenging for prior domain adaptation methods since they ignore the underlying relation among domains. In this paper, we propose the first method for continuously indexed domain adaptation. Our approach combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution. Our theoretical analysis demonstrates the value of leveraging the domain index to generate invariant features across a continuous range of domains. Our empirical results show that our approach outperforms the state-of-the-art domain adaption methods on both synthetic and real-world medical datasets.
","['MIT', 'Massachusetts Institute of Technology', 'MIT']"
2020,Learning Similarity Metrics for Numerical Simulations,"Georg Kohl, Kiwon Um, Nils Thuerey",https://icml.cc/Conferences/2020/Schedule?showEvent=6024,"We propose a neural network-based approach that computes a stable and generalizing metric (LSiM) to compare data from a variety of numerical simulation sources. We focus on scalar time-dependent 2D data that commonly arises from motion and transport-based partial differential equations (PDEs). Our method employs a Siamese network architecture that is motivated by the mathematical properties of a metric. We leverage a controllable data generation setup with PDE solvers to create increasingly different outputs from a reference simulation in a controlled environment. A central component of our learned metric is a specialized loss function that introduces knowledge about the correlation between single data samples into the training process. To demonstrate that the proposed approach outperforms existing metrics for vector spaces and other learned, image-based metrics, we evaluate the different methods on a large range of test data. Additionally, we analyze generalization benefits of an adjustable training data difficulty and demonstrate the robustness of LSiM via an evaluation on three real-world data sets.
","['Technical University of Munich', 'Technical University of Munich', 'Technical University of Munich']"
2020,Logarithmic Regret for Learning Linear Quadratic Regulators Efficiently,"Asaf Cassel, Alon Cohen, Tomer Koren",https://icml.cc/Conferences/2020/Schedule?showEvent=6054,"We consider the problem of learning in Linear Quadratic Control systems whose transition parameters are initially unknown. Recent results in this setting have demonstrated efficient learning algorithms with regret growing with the square root of the number of decision steps.  We present new efficient algorithms that achieve, perhaps surprisingly,regret that scales only (poly-)logarithmically with the number of steps, in two scenarios: when only the state transition matrix A is unknown, and when only the state-action transition matrix B is unknown and the optimal policy satisfies a certain non-degeneracy condition.  On the other hand, we give a lower bound which shows that when the latter condition is violated, square root regret is unavoidable.
","['Tel Aviv University', 'Technion, Google', 'Tel Aviv University']"
2020,Improving Molecular Design by Stochastic Iterative Target Augmentation,"Kevin Yang, Wengong Jin, Kyle Swanson, Regina Barzilay, Tommi Jaakkola",https://icml.cc/Conferences/2020/Schedule?showEvent=6770,"Generative models in molecular design tend to be richly parameterized, data-hungry neural models, as they must create complex structured objects as outputs. Estimating such models from data may be challenging due to the lack of sufficient training data. In this paper, we propose a surprisingly effective self-training approach for iteratively creating additional molecular targets. We first pre-train the generative model together with a simple property predictor. The property predictor is then used as a likelihood model for filtering candidate structures from the generative model. Additional targets are iteratively produced and used in the course of stochastic EM iterations to maximize the log-likelihood that the candidate structures are accepted. A simple rejection (re-weighting) sampler suffices to draw posterior samples since the generative model is already reasonable after pre-training. We demonstrate significant gains over strong baselines for both unconditional and conditional molecular design. In particular, our approach outperforms the previous state-of-the-art in conditional molecular design by over 10% in absolute gain. Finally, we show that our approach is useful in other domains as well, such as program synthesis. 
","['UC Berkeley', 'MIT', 'University of Cambridge', 'MIT CSAIL', 'MIT']"
2020,Thompson Sampling via Local Uncertainty,"Zhendong Wang, Mingyuan Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=6757,"Thompson sampling is an efficient algorithm for sequential decision making, which exploits the posterior uncertainty to address the exploration-exploitation dilemma. There has been significant recent interest in integrating Bayesian neural networks into Thompson sampling. Most of these methods rely on global variable uncertainty for exploration. In this paper, we propose a new probabilistic modeling framework for Thompson sampling, where local latent variable uncertainty is used to sample the mean reward. Variational inference is used to approximate the posterior of the local variable, and semi-implicit structure is further introduced to enhance its expressiveness. Our experimental results on eight contextual bandit benchmark datasets show that Thompson sampling guided by local uncertainty achieves state-of-the-art performance while having low computational complexity.
","['University of Texas, Austin', 'University of Texas at Austin']"
2020,Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures,"Mohamed El Amine Seddik, Cosme Louart, Mohamed Tamaazousti, Romain COUILLET",https://icml.cc/Conferences/2020/Schedule?showEvent=5888,"This paper shows that deep learning (DL) representations of data produced by generative adversarial nets (GANs) are random vectors which fall within the class of so-called \textit{concentrated} random vectors. Further exploiting the fact that Gram matrices, of the type $G = X^\intercal X$ with $X=[x_1,\ldots,x_n]\in \mathbb{R}^{p\times n}$ and $x_i$ independent concentrated random vectors from a mixture model, behave asymptotically (as $n,p\to \infty$) as if the $x_i$ were drawn from a Gaussian mixture, suggests that DL representations of GAN-data can be fully described by their first two statistical moments for a wide range of standard classifiers. Our theoretical findings are validated by generating images with the BigGAN model and across different popular deep representation networks.","['CEA', 'CEA', 'CEA Saclay', 'CentraleSupélec']"
2020,Infinite attention: NNGP and NTK for deep attention networks,"Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, Roman Novak",https://icml.cc/Conferences/2020/Schedule?showEvent=6526,"There is a growing amount of literature on the relationship between wide neural networks (NNs) and Gaussian processes (GPs), identifying an equivalence between the two for a variety of NN architectures. This equivalence enables, for instance, accurate approximation of the behaviour of wide Bayesian NNs without MCMC or variational approximations, or characterisation of the distribution of randomly initialised wide NNs optimised by gradient descent without ever running an optimiser. We provide a rigorous extension of these results to NNs involving attention layers, showing that unlike single-head attention, which induces non-Gaussian behaviour, multi-head attention architectures behave as GPs as the number of heads tends to infinity. We further discuss the effects of positional encodings and layer normalisation, and propose modifications of the attention mechanism which lead to improved results for both finite and infinitely wide NNs. We evaluate attention kernels empirically, leading to a moderate improvement upon the previous state-of-the-art on CIFAR-10 for GPs without trainable kernels and advanced data preprocessing. Finally, we introduce new features to the Neural Tangents library (Novak et al.,2020) allowing applications of NNGP/NTK models, with and without attention, to variable-length sequences, with an example on the IMDb reviews dataset.
","['University of Cambridge', 'Google Brain', 'Google Brain', 'Google Brain']"
2020,Near Input Sparsity Time Kernel Embeddings via Adaptive Sampling,"David Woodruff, Amir Zandieh",https://icml.cc/Conferences/2020/Schedule?showEvent=6790,"To accelerate kernel methods, we propose a near input sparsity time method for sampling the high-dimensional space implicitly defined by a kernel transformation. Our main contribution is an importance sampling method for subsampling the feature space of a degree $q$ tensoring of data points in almost input sparsity time, improving the recent oblivious sketching of (Ahle et al., 2020) by a factor of $q^{5/2}/\epsilon^2$. This leads to a subspace embedding for the polynomial kernel as well as the Gaussian kernel with a target dimension that is only linearly dependent on the statistical dimension of the kernel and in time which is only linearly dependent on the sparsity of the input dataset. We show how our subspace embedding bounds imply new statistical guarantees for kernel ridge regression. Furthermore, we empirically show that in large-scale regression tasks, our algorithm outperforms state-of-the-art kernel approximation methods.","['CMU', 'EPFL']"
2020,Compressive sensing with un-trained neural networks: Gradient descent finds a smooth approximation,"Reinhard Heckel, Mahdi Soltanolkotabi",https://icml.cc/Conferences/2020/Schedule?showEvent=6659,"Un-trained convolutional neural networks have emerged as highly successful tools for image recovery and restoration. They are capable of solving standard inverse problems such as denoising and compressive sensing with excellent results by simply fitting a neural network model to measurements from a single image or signal without the need for any additional training data. For some applications, this critically requires additional regularization in the form of early stopping the optimization. For signal recovery from a few measurements, however, un-trained convolutional networks have an intriguing self-regularizing property: Even though the network can perfectly fit any image, the network recovers a natural image from few measurements when trained with gradient descent until convergence. In this paper, we provide numerical evidence for this property and study it theoretically. We show that---without any further regularization---an un-trained convolutional neural network can approximately reconstruct signals and images that are sufficiently structured, from a near minimal number of random measurements.
","['Rice University', 'University of Southern California']"
2020,Simple and sharp analysis of k-means||,Vaclav Rozhon,https://icml.cc/Conferences/2020/Schedule?showEvent=6495,"We present a simple analysis of k-means|| (Bahmani et al., PVLDB 2012) - a distributed variant of the k-means++ algorithm (Arthur and Vassilvitskii, SODA 2007). Moreover, the bound on the number of rounds is improved from  $O(\log n)$ to $O(\log n / \log\log n)$, which we show to be tight.",['ETH']
2020,"Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure",John Sipple,https://icml.cc/Conferences/2020/Schedule?showEvent=6171,"In this paper we propose a scalable, unsupervised approach for detecting anomalies in the Internet of Things (IoT). Complex devices are connected daily and eagerly generate vast streams of multidimensional telemetry. These devices often operate in distinct modes based on external conditions (day/night, occupied/vacant, etc.), and to prevent complete or partial system outage, we would like to recognize as early as possible when these devices begin to operate outside the normal modes. We propose an unsupervised anomaly detection method that creates a negative sample from the positive, observed sample, and trains a classifier to distinguish between positive and negative samples. Using the Concentration Phenomenon, we explain why such a classifier ought to establish suitable decision boundaries between normal and anomalous regions, and show how Integrated Gradients can attribute the anomaly to specific dimensions within the anomalous state vector. We have demonstrated that negative sampling with random forest or neural network classifiers yield significantly higher AUC scores compared to state-of-the-art approaches against benchmark anomaly detection datasets, and a multidimensional, multimodal dataset from real climate control devices. Finally, we describe how negative sampling with neural network classifiers have been successfully deployed at large scale to predict failures in real time in over 15,000 climate-control and power meter devices in 145 office buildings within the California Bay Area.
",['Google']
2020,Why Are Learned Indexes So Effective?,"Paolo Ferragina, Fabrizio Lillo, Giorgio Vinciguerra",https://icml.cc/Conferences/2020/Schedule?showEvent=5873,"A recent trend in algorithm design consists of augmenting classic data structures with machine learning models, which are better suited to reveal and exploit patterns and trends in the input data so to achieve outstanding practical improvements in space occupancy and time efficiency. 
This is especially known in the context of indexing data structures where, despite few attempts in evaluating their asymptotic efficiency, theoretical results are yet missing in showing that learned indexes are provably better than classic indexes, such as B+ trees and their variants.
In this paper, we present the first mathematically-grounded answer to this open problem. We obtain this result by discovering and exploiting a link between the original problem and a mean exit time problem over a proper stochastic process which, we show, is related to the space and time occupancy of those learned indexes. Our general result is then specialised to five well-known distributions: Uniform, Lognormal, Pareto, Exponential, and Gamma; and it is corroborated in precision and robustness by a large set of experiments.
","['Università di Pisa', 'Università di Bologna', 'Università di Pisa']"
2020,Learning Reasoning Strategies in End-to-End Differentiable Proving,"Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, Tim Rocktäschel",https://icml.cc/Conferences/2020/Schedule?showEvent=6364,"Attempts to render deep learning models interpretable, data-efficient, and robust have seen some success through hybridisation with rule-based systems, for example, in Neural Theorem Provers (NTPs). These neuro-symbolic models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions. However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them unfit for large-scale applications. We present Conditional Theorem Provers (CTPs), an extension to NTPs that learns an optimal rule selection strategy via gradient-based optimisation. We show that CTPs are scalable and yield state-of-the-art results on the CLUTRR dataset, which tests systematic generalisation of neural models by learning to reason over smaller graphs and evaluating on larger ones. Finally, CTPs show better link prediction results on standard benchmarks in comparison with other neural-symbolic models, while being explainable.
","['University College London', 'UCL', 'University College London', 'Facebook AI Research / UCL', 'Facebook AI Research & University College London']"
2020,Why bigger is not always better: on finite and infinite neural networks,Laurence Aitchison,https://icml.cc/Conferences/2020/Schedule?showEvent=6384,"Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning.
",['University of Bristol']
2020,When deep denoising meets iterative phase retrieval,"Yaotian Wang, Xiaohang Sun, Jason Fleischer",https://icml.cc/Conferences/2020/Schedule?showEvent=6709,"Recovering a signal from its Fourier intensity underlies many important applications, including lensless imaging and imaging through scattering media. Conventional algorithms for retrieving the phase suffer when noise is present but display global convergence when given clean data. Neural networks have been used to improve algorithm robustness, but efforts to date are sensitive to initial conditions and give inconsistent performance. Here, we combine iterative methods from phase retrieval with image statistics from deep denoisers, via regularization-by-denoising. The resulting methods inherit the advantages of each approach and outperform other noise-robust phase retrieval algorithms. Our work paves the way for hybrid imaging methods that integrate machine-learned constraints in conventional algorithms.
","['Princeton University', 'Princeton University', 'Princeton University']"
2020,Invariant Causal Prediction for Block MDPs,"Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, Doina Precup",https://icml.cc/Conferences/2020/Schedule?showEvent=6475,"Generalization across environments is critical to the successful application of reinforcement learning (RL) algorithms to real-world challenges. In this work we propose a method for learning state abstractions which generalize to novel observation distributions in the multi-environment RL setting. We prove that for certain classes of environments, this approach outputs, with high probability, a state abstraction corresponding to the causal feature set with respect to the return. We give empirical evidence that analogous methods for the nonlinear setting can also attain improved generalization over single- and multi-task baselines. Lastly, we provide bounds on model generalization error in the multi-environment setting, in the process showing a connection between causal variable identification and the state abstraction framework for MDPs.
","['McGill University', 'University of Oxford', 'Facebook AI Research', 'University of Oxford', 'Oxford University', 'McGill University / Facebook', 'University of Oxford', 'McGill University / DeepMind']"
2020,Unsupervised Speech Decomposition via Triple Information Bottleneck,"Kaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson, David Cox",https://icml.cc/Conferences/2020/Schedule?showEvent=6456,"Speech information can be roughly decomposed into four components: language content, timbre, pitch, and rhythm. Obtaining disentangled representations of these components is useful in many speech analysis and generation applications. Recently, state-of-the-art voice conversion systems have led to speech representations that can disentangle speaker-dependent and independent information. However, these systems can only disentangle timbre, while information about pitch, rhythm and content is still mixed together. Further disentangling the remaining speech components is an under-determined problem in the absence of explicit annotations for each component, which are difficult and expensive to obtain. In this paper, we propose SpeechSplit, which can blindly decompose speech into its four components by introducing three carefully designed information bottlenecks. SpeechSplit is among the first algorithms that can separately perform style transfer on timbre, pitch and rhythm without text labels. Our code is publicly available at https://github.com/auspicious3000/SpeechSplit.
","['University of Illinois at Urbana-Champaign', 'MIT-IBM Watson AI Lab', 'MIT-IBM Watson AI Lab', 'University of Illinois', 'MIT-IBM Watson AI Lab']"
2020,Double Reinforcement Learning for Efficient and Robust Off-Policy Evaluation,"Nathan Kallus, Masatoshi Uehara",https://icml.cc/Conferences/2020/Schedule?showEvent=5944,"Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of $q$-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent.
We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.","['Cornell University', 'Harvard University']"
2020,Estimating Generalization under Distribution Shifts via Domain-Invariant Representations,"Ching-Yao Chuang, Antonio Torralba, Stefanie Jegelka",https://icml.cc/Conferences/2020/Schedule?showEvent=6795,"When machine learning models are deployed on a test distribution different from the training distribution, they can perform poorly, but overestimate their performance. In this work, we aim to better estimate a model's performance under distribution shift, without supervision. To do so, we use a set of domain-invariant predictors as a proxy for the unknown, true target labels. Since the error of the resulting risk estimate depends on the target risk of the proxy model, we study generalization of domain-invariant representations and show that the complexity of the latent representation has a significant influence on the target risk. Empirically, our approach (1) enables self-tuning of domain adaptation models, and (2) accurately estimates the target error of given models under distribution shift. Other applications include model selection, deciding early stopping and error detection.
","['MIT', 'MIT', 'Massachusetts Institute of Technology']"
2020,Thompson Sampling Algorithms for Mean-Variance Bandits,"Qiuyu Zhu, Vincent Tan",https://icml.cc/Conferences/2020/Schedule?showEvent=6010,"The multi-armed bandit (MAB) problem is a classical learning task that exemplifies the exploration-exploitation tradeoff.  However, standard formulations do not take into account risk. In online decision making systems, risk is a primary concern. In this regard, the mean-variance risk measure is one of the most common objective functions. Existing algorithms for mean-variance optimization in the context of MAB problems have unrealistic assumptions on the reward distributions. We develop Thompson Sampling-style algorithms for mean-variance MAB and provide comprehensive regret analyses for Gaussian and Bernoulli bandits with fewer assumptions. Our algorithms achieve the best known regret bounds for mean-variance MABs and also attain the information-theoretic bounds in some parameter regimes. Empirical simulations show that our algorithms significantly outperform existing LCB-based algorithms for all risk tolerances.
","['National University of Singapore', 'National University of Singapore']"
2020,Interferometric Graph Transform: a Deep Unsupervised Graph Representation,Edouard Oyallon,https://icml.cc/Conferences/2020/Schedule?showEvent=6148,"We propose the Interferometric Graph Transform (IGT), which is a new class of deep unsupervised graph convolutional neural network for building graph representations. Our first contribution is to propose a generic, complex-valued spectral graph architecture obtained from a generalization of the Euclidean Fourier transform. We show that our learned representation consists of both discriminative and invariant features, thanks to a novel greedy concave objective. From our experiments, we conclude that our learning procedure exploits the topology of the spectral domain, which is normally a flaw of spectral methods, and in particular our method can recover an analytic operator for vision tasks. We test our algorithm on various and challenging tasks such as image classification (MNIST, CIFAR-10), community detection (Authorship, Facebook graph) and action recognition from 3D skeletons videos (SBU, NTU), exhibiting a new state-of-the-art in spectral graph unsupervised settings.
",['CNRS/LIP6']
2020,Invertible generative models for inverse problems: mitigating representation error and dataset bias,"Muhammad Asim, Grady Daniels, Oscar Leong, Ali Ahmed, Paul Hand",https://icml.cc/Conferences/2020/Schedule?showEvent=6190,"Trained generative models have shown remarkable performance as priors for inverse problems in imaging -- for example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.  Given a trained generative model, we study the empirical risk formulation of the desired inverse problem under a regularization that promotes high likelihood images, either directly by penalization or algorithmically by initialization. For compressive sensing, invertible priors can yield higher accuracy than sparsity priors across almost all undersampling ratios, and due to their lack of representation error, invertible priors can yield better reconstructions than GAN priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  We additionally compare performance for compressive sensing to unlearned methods, such as the deep decoder, and we establish theoretical bounds on expected recovery error in the case of a linear invertible model.
","['Information Technology University, Lahore', 'Northeastern University', 'Rice University', 'Information Technology University', 'Northeastern University']"
2020,Learning to Learn Kernels with Variational Random Features,"Xiantong Zhen, Haoliang Sun, Yingjun Du, Jun Xu, Yilong Yin, Ling Shao, Cees Snoek",https://icml.cc/Conferences/2020/Schedule?showEvent=6847,"We introduce kernels with random Fourier features in the meta-learning framework for few-shot learning.
We propose meta variational random features (MetaVRF) to learn adaptive kernels for the base-learner, which is developed in a latent variable model by treating the random feature basis as the latent variable. We formulate the optimization of MetaVRF as a variational inference problem by deriving an evidence lower bound under the meta-learning framework. To incorporate shared knowledge from related tasks, we propose a context inference of the posterior, which is established by an LSTM architecture. The LSTM-based inference network can effectively integrate the context information of previous tasks with task-specific information, generating informative and adaptive features. The learned MetaVRF can produce kernels of high representational power with a relatively low spectral sampling rate and also enables fast adaptation to new tasks. Experimental results on a variety of few-shot regression and classification tasks demonstrate that MetaVRF delivers much better, or at least competitive, performance compared to existing meta-learning alternatives.
","['University of Amsterdam', 'Shandong University', 'University of Amsterdam', 'Nankai University', 'Shandong University', 'Inception Institute of Artificial Intelligence', 'University of Amsterdam']"
2020,Logistic Regression for Massive Data with Rare Events,HaiYing Wang,https://icml.cc/Conferences/2020/Schedule?showEvent=6536,"This paper studies binary logistic regression for rare events data, or imbalanced data, where the number of events (observations in one class, often called cases) is significantly smaller than the number of nonevents (observations in the other class, often called controls). We first derive the asymptotic distribution of the maximum likelihood estimator (MLE) of the unknown parameter, which shows that the asymptotic variance convergences to zero in a rate of the inverse of the number of the events instead of the inverse of the full data sample size, indicating that the available information in rare events data is at the scale of the number of events instead of the full data sample size. Furthermore, we prove that under-sampling a small proportion of the nonevents, the resulting under-sampled estimator may have identical asymptotic distribution to the full data MLE. This demonstrates the advantage of under-sampling nonevents for rare events data, because this procedure may significantly reduce the computation and/or data collection costs. Another common practice in analyzing rare events data is to over-sample (replicate) the events, which has a higher computational cost. We show that this procedure may even result in efficiency loss in terms of parameter estimation.
",['University of Connecticut']
2020,Accelerating the diffusion-based ensemble sampling by non-reversible dynamics,"Futoshi Futami, Issei Sato, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6280,"Posterior distribution approximation is a central task in Bayesian inference. Stochastic gradient Langevin dynamics (SGLD) and its extensions have been practically used and theoretically studied. While SGLD updates a single particle at a time, ensemble methods that update multiple particles simultaneously have been recently gathering attention. Compared with the naive parallel-chain SGLD that updates multiple particles independently, ensemble methods update particles with their interactions.  Thus, these methods are expected to be more particle-efficient than the naive parallel-chain SGLD because particles can be aware of other particles' behavior through their interactions. Although ensemble methods numerically demonstrated their superior performance, no theoretical guarantee exists to assure such particle-efficiency and it is unclear whether those ensemble methods are really superior to the naive parallel-chain SGLD in the non-asymptotic settings. To cope with this problem, we propose a novel ensemble method that uses a non-reversible Markov chain for the interaction, and we present a non-asymptotic theoretical analysis for our method. Our analysis shows that, for the first time, the interaction causes a faster convergence rate than the naive parallel-chain SGLD in the non-asymptotic setting if the discretization error is appropriately controlled. Numerical experiments show that we can control the discretization error by tuning the interaction appropriately.
","['NTT', 'University of Tokyo / RIKEN', 'RIKEN / The University of Tokyo']"
2020,On the Relation between Quality-Diversity Evaluation and Distribution-Fitting Goal in Text Generation,"Jianing Li, Yanyan Lan, Jiafeng Guo, Xueqi Cheng",https://icml.cc/Conferences/2020/Schedule?showEvent=6291,"The goal of text generation models is to fit the
underlying real probability distribution of text.
For performance evaluation, quality and diversity
metrics are usually applied. However, it is still
not clear to what extend can the quality-diversity
evaluation reflect the distribution-fitting goal. In
this paper, we try to reveal such relation in a
theoretical approach. We prove that under certain
conditions, a linear combination of quality and
diversity constitutes a divergence metric between
the generated distribution and the real distribution.
We also show that the commonly used BLEU/Self-BLEU metric pair fails to match any divergence
metric, thus propose CR/NRR as a substitute for
quality/diversity metric pair.
","['Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Institute of Computing Technology, CAS, China']"
2020,Counterfactual Cross-Validation: Stable Model Selection Procedure for Causal Inference Models,"Yuta Saito, Shota Yasui",https://icml.cc/Conferences/2020/Schedule?showEvent=5863,"We study the model selection problem in \textit{conditional average treatment effect} (CATE) prediction. Unlike previous works on this topic, we focus on preserving the rank order of the performance of candidate CATE predictors to enable accurate and stable model selection. To this end, we analyze the model performance ranking problem and formulate guidelines to obtain a better evaluation metric. We then propose a novel metric that can identify the ranking of the performance of CATE predictors with high confidence. Empirical evaluations demonstrate that our metric outperforms existing metrics in both model selection and hyperparameter tuning tasks.
","['Tokyo Institute of Technology.', 'Cyberagent']"
2020,Learning De-biased Representations with Biased Representations,"Hyojin Bahng, SANGHYUK CHUN, Sangdoo Yun, Jaegul Choo, Seong Joon Oh",https://icml.cc/Conferences/2020/Schedule?showEvent=5783,"Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led to interesting advancement, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles), resulting in biased models that fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. We demonstrate the efficacy of our method across a variety of synthetic and real-world biases; our experiments show that the method discourages models from taking bias shortcuts, resulting in improved generalisation. Source code is available at https://github.com/clovaai/rebias.
","['Korea University', 'Naver corp.', ' Clova AI Research, NAVER Corp.', 'KAIST', 'Clova AI Research, NAVER Corp.']"
2020,Intrinsic Reward Driven Imitation Learning via Generative Model,"Xingrui Yu, Yueming LYU, Ivor Tsang",https://icml.cc/Conferences/2020/Schedule?showEvent=6386,"Imitation learning in a high-dimensional environment is challenging. Most inverse reinforcement learning (IRL) methods fail to outperform the demonstrator in such a high-dimensional environment, e.g., Atari domain. To address this challenge, we propose a novel reward learning module to generate intrinsic reward signals via a generative model. Our generative method can perform better forward state transition and backward action encoding, which improves the module's dynamics modeling ability in the environment. Thus, our module provides the imitation agent both the intrinsic intention of the demonstrator and a better exploration ability, which is critical for the agent to outperform the demonstrator. Empirical results show that our method outperforms state-of-the-art IRL methods on multiple Atari games, even with one-life demonstration. Remarkably, our method achieves performance that is up to 5 times the performance of the demonstration.
","['University of Technology Sydney', 'University of Technology Sydney', 'University of Technology Sydney']"
2020,Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?,"Kei Ota, Tomoaki Oiki, Devesh Jha, Toshisada Mariyama, Daniel Nikovski",https://icml.cc/Conferences/2020/Schedule?showEvent=6387,"Deep reinforcement learning (RL) algorithms have recently achieved remarkable successes in various sequential decision making tasks, leveraging advances in methods for training large deep networks.
However, these methods usually require large amounts of training data, which is often a big problem for real-world applications. One natural question to ask is whether learning good representations for states and using larger networks helps in learning better policies. In this paper, we try to study if increasing input dimensionality helps improve performance and sample efficiency of model-free deep RL algorithms. To do so, we propose an online feature extractor network (OFENet) that uses neural nets to produce \textit{good} representations to be used as inputs to an off-policy RL algorithm. Even though the high dimensionality of input is usually thought to make learning of RL agents more difficult, we show that the RL agents in fact learn more efficiently with the high-dimensional representation than with the lower-dimensional state observations. We believe that stronger feature propagation together with larger networks allows RL agents to learn more complex functions of states and thus improves the sample efficiency. Through numerical experiments, we show that the proposed method achieves much higher sample efficiency and better performance.
Codes for the proposed method are available at http://www.merl.com/research/license/OFENet
","['Mitsubishi Electric Corporation', 'Mitsubishi Electric', 'Mitsubishi Electric Research Labs', 'Mitsubishi Electric', 'Mitsubishi Electric Research Labs']"
2020,Efficiently Learning Adversarially Robust Halfspaces with Noise,"Omar Montasser, Surbhi Goel, Ilias Diakonikolas, Nati Srebro",https://icml.cc/Conferences/2020/Schedule?showEvent=6754,"We study the problem of learning adversarially robust halfspaces in the distribution-independent setting. In the realizable setting, we provide necessary and sufficient conditions on the adversarial perturbation sets under which halfspaces are efficiently robustly learnable. In the presence of random label noise, we give a simple computationally efficient algorithm for this problem with respect to any $\ell_p$-perturbation.","['TTI-Chicago', 'University of Texas at Austin', 'University of Wisconsin-Madison', 'Toyota Technological Institute at Chicago']"
2020,Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its Parallelization,"Shion Takeno, Hitoshi Fukuoka, Yuhki Tsukada, Toshiyuki Koyama, Motoki Shiga, Ichiro Takeuchi, Masayuki Karasuyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6780,"In a standard setting of Bayesian optimization (BO), the objective function evaluation is assumed to be highly expensive. Multi-fidelity Bayesian optimization (MFBO) accelerates BO by incorporating lower fidelity observations available with a lower sampling cost. We propose a novel information-theoretic approach to MFBO, called multi-fidelity max-value entropy search (MF-MES), that enables us to obtain a more reliable evaluation of the information gain compared with existing information-based methods for MFBO. Further, we also propose a parallelization of MF-MES mainly for the asynchronous setting because queries typically occur asynchronously in MFBO due to a variety of sampling costs. We show that most of computations in our acquisition functions can be derived analytically, except for at most only two dimensional numerical integration that can be performed efficiently by simple approximations. We demonstrate effectiveness of our approach by using benchmark datasets and a real-world application to materials science data.
","['Nagoya Institute of Technology', 'Hitachi Metals, Ltd.', 'Nagoya University', 'Nagoya University', 'Gifu University', 'Nagoya Institute of Technology / RIKEN', 'Nagoya Institute of Technology']"
2020,Striving for Simplicity and Performance in Off-Policy DRL: Output Normalization and Non-Uniform Sampling,"Che Wang, Yanqiu Wu, Quan Vuong, Keith Ross",https://icml.cc/Conferences/2020/Schedule?showEvent=5994,"We aim to develop off-policy DRL algorithms that not only exceed state-of-the-art performance but are also simple and minimalistic. For standard continuous control benchmarks, Soft Actor-Critic (SAC), which employs entropy maximization, currently provides state-of-the-art performance. We first demonstrate that the entropy term in SAC addresses action saturation due to the bounded nature of the action spaces, with this insight, we propose a streamlined algorithm with a simple normalization scheme or with inverted gradients. We show that both approaches can match SAC's sample efficiency performance without the need of entropy maximization, we then propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. Extensive experimental results demonstrate that our proposed sampling scheme leads to state of the art sample efficiency on challenging continuous control tasks. We combine all of our findings into one simple algorithm, which we call Streamlined Off Policy with Emphasizing Recent Experience, for which we provide robust public-domain code. 
","['New York University', 'New York University', 'University of California San Diego', 'New York University Shanghai']"
2020,Learning with Feature and Distribution Evolvable Streams,"Zhen-Yu Zhang, Peng Zhao, Yuan Jiang, Zhi-Hua Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=6162,"In many real-world applications, data are collected in the form of a stream, whose feature space can evolve over time. For instance, in the environmental monitoring task, features can be dynamically vanished or augmented due to the existence of expired old sensors and deployed new sensors. Furthermore, besides the evolvable feature space, the data distribution is usually changing in the streaming scenario. When both feature space and data distribution are evolvable, it is quite challenging to design algorithms with guarantees, particularly theoretical understandings of generalization ability. To address this difficulty, we propose a novel discrepancy measure for data with evolving feature space and data distribution, named the \emph{evolving discrepancy}. Based on that, we present the generalization error analysis, and the theory motivates the design of a learning algorithm which is further implemented by deep neural networks. Empirical studies on synthetic data verify the rationale of our proposed discrepancy measure, and extensive experiments on real-world tasks validate the effectiveness of our algorithm.
","['Nanjing University', 'Nanjing University', 'Nanjing University', 'Nanjing University']"
2020,Rate-distortion optimization guided autoencoder for isometric embedding in Euclidean latent space,"Keizo Kato, Jing Zhou, Tomotake Sasaki, Akira Nakagawa",https://icml.cc/Conferences/2020/Schedule?showEvent=6351,"To analyze high-dimensional and complex data in the real world, deep generative models such as variational autoencoder (VAE) embed data in a reduced dimensional latent space and learn the probabilistic model in the latent space. 
However, they struggle to reproduce the probability distribution function (PDF) in the input space from that of the latent space accurately. 
If the embedding were isometric, this problem can be solved since PDFs in both spaces become proportional. 
To achieve isometric property, we propose Rate-Distortion Optimization guided autoencoder inspired by orthonormal transform coding. 
We show our method has the following properties: (i) the columns of the Jacobian matrix between two spaces is constantly-scaled orthonormal system and enable to embed data in latent space isometrically; (ii) the PDF of the latent space is proportional to that of the data observation space. 
Furthermore, our method outperforms state-of-the-art methods in unsupervised anomaly detection with four public datasets. 
","['Fujitsu Laboratories Ltd.', 'Alibaba', 'Fujitsu Laboratories Ltd.', 'Fujitsu Laboratories Ltd.']"
2020,Hybrid Stochastic-Deterministic Minibatch Proximal Gradient: Less-Than-Single-Pass Optimization with Nearly Optimal Generalization,"Pan Zhou, Xiao-Tong Yuan",https://icml.cc/Conferences/2020/Schedule?showEvent=5788,"Stochastic variance-reduced gradient (SVRG) algorithms have been shown to work favorably in solving large-scale learning problems. Despite the remarkable success, the stochastic gradient complexity of SVRG-type algorithms usually scales linearly with data size and thus could still be expensive for huge data. To address this deficiency, we propose a hybrid stochastic-deterministic minibatch proximal gradient (HSDMPG) algorithm for strongly-convex problems that enjoys provably improved data-size-independent complexity guarantees. More precisely, for quadratic loss $F(\wm)$ of $n$ components, we prove that HSDMPG can attain an $\epsilon$-optimization-error  $E[F(\theta)-F(\theta^*)] \leq \epsilon$ within $\mathcal{O}\Big(\!\frac{\kappa^{1.5}}{\epsilon^{0.25}}\!  \log^{\!1.5}\!\!\big(\frac{1}{\epsilon}\big) \wedge   \Big(\!\kappa \sqrt{n}  \log^2\!\!\big(\frac{1}{\epsilon}\big) \!+\! \frac{\kappa^3}{n^{1.5}\epsilon} \!\Big)\!\Big)$ stochastic gradient evaluations, where $\kappa$ is condition number. For generic strongly convex loss functions, we prove a nearly identical complexity bound though at the cost of slightly increased logarithmic factors. For large-scale learning problems, our complexity bounds are superior to those of the prior state-of-the-art SVRG algorithms with or without dependence on data size. Particularly, in the case of $\epsilon\!=\!\mathcal{O}\big(1/\sqrt{n}\big)$ which is at the order of intrinsic excess error bound of a learning model and thus sufficient for generalization,  the stochastic gradient complexity bounds of HSDMPG~for quadratic and generic loss functions are respectively $\mathcal{O} (n^{0.875}\log^{1.5}(n))$ and $\mathcal{O} (n^{0.875}\log^{2.25}(n))$, which to our best knowledge, for the first time achieve optimal generalization in less than a single pass over data. Extensive numerical results demonstrate the computational advantages of our algorithm over the prior ones.","['Salesforce', 'Nanjing University of Information Science & Technology']"
2020,Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems,"Kaixuan Wei, Angelica I Aviles-Rivero, Jingwei Liang, Ying Fu, Carola-Bibiane Schönlieb, Hua Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=6447,"Plug-and-play (PnP) is a non-convex framework that combines ADMM or other proximal algorithms  with advanced denoiser priors.  Recently, PnP has achieved great empirical  success, especially with the integration of deep learning-based denoisers. However, a key problem of PnP based approaches is that they require manual parameter tweaking. It is necessary to obtain high-quality results across the high discrepancy in terms of imaging conditions and varying scene content. In this work, we present a tuning-free PnP proximal algorithm, which can automatically  determine the internal parameters including  the penalty parameter, the denoising strength  and the terminal time. A key part of our approach is to develop a policy network for automatic search of parameters, which can be effectively learned via mixed model-free and model-based deep reinforcement learning. We demonstrate, through numerical and visual experiments, that the learned policy can customize different parameters for different states, and often more efficient and effective than existing handcrafted criteria. Moreover, we discuss the practical considerations of the plugged denoisers, which together with our learned policy yield  state-of-the-art results. This is prevalent on both linear and nonlinear exemplary inverse imaging problems, and in particular, we show promising results on Compressed Sensing MRI and phase retrieval. 
","['Beijing Institute of Technology', 'University of Cambridge', 'University of Cambridge', 'Beijing Institute of Technology', 'University of Cambridge', 'Beijing Institute of Technology']"
2020,Attacks Which Do Not Kill Training Make Adversarial Learning Stronger,"Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, Mohan Kankanhalli",https://icml.cc/Conferences/2020/Schedule?showEvent=5835,"Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question—do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel formulation of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial data (i.e., friendly adversarial data) minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively—adversarial robustness can indeed be achieved without compromising the natural generalization.
","['National University of Singapore', 'Shandong University', 'HKBU / RIKEN', 'RIKEN', 'ShanDong University', 'RIKEN / The University of Tokyo', 'National University of Singapore,']"
2020,Unbiased Risk Estimators Can Mislead: A Case Study of Learning with Complementary Labels,"Yu-Ting Chou, Gang Niu, Hsuan-Tien Lin, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6302,"In weakly supervised learning, unbiased risk estimator(URE) is a powerful tool for training classifiers when training and test data are drawn from different distributions. Nevertheless, UREs lead to overfitting in many problem settings when the models are complex like deep networks. In this paper, we investigate reasons for such overfitting by studying a weakly supervised problem called learning with complementary labels. We argue the quality of gradient estimation matters more in risk minimization. Theoretically, we show that a URE gives an unbiased gradient estimator(UGE). Practically, however, UGEs may suffer from huge variance, which causes empirical gradients to be usually far away from true gradients during minimization. To this end, we propose a novel surrogate complementary loss(SCL) framework that trades zero bias with reduced variance and makes empirical gradients more aligned with true gradients in the direction. Thanks to this characteristic, SCL successfully mitigates the overfitting issue and improves URE-based methods.
","['National Taiwan University', 'RIKEN', 'National Taiwan University', 'RIKEN / The University of Tokyo']"
2020,DessiLBI: Exploring Structural Sparsity of Deep Networks via Differential Inclusion Paths,"Yanwei Fu, Chen Liu, Donghao Li, Xinwei Sun, Jinshan ZENG, Yuan Yao",https://icml.cc/Conferences/2020/Schedule?showEvent=6087,"Over-parameterization is ubiquitous nowadays in training neural networks to benefit both optimization in seeking global optima and generalization in reducing prediction error. However, compressive networks are desired in many real world ap- plications and direct training of small networks may be trapped in local optima. In this paper, in- stead of pruning or distilling over-parameterized models to compressive ones, we propose a new approach based on differential inclusions of in- verse scale spaces. Specifically, it generates a family of models from simple to complex ones that couples a pair of parameters to simultaneously train over-parameterized deep models and structural sparsity on weights of fully connected and convolutional layers. Such a differential inclusion scheme has a simple discretization, pro- posed as Deep structurally splitting Linearized Bregman Iteration (DessiLBI), whose global convergence analysis in deep learning is established that from any initializations, algorithmic iterations converge to a critical point of empirical risks. Experimental evidence shows that DessiLBI achieve comparable and even better performance than the competitive optimizers in exploring the structural sparsity of several widely used backbones on the benchmark datasets. Remarkably, with early stopping, DessiLBI unveils “winning tickets” in early epochs: the effective sparse structure with comparable test accuracy to fully trained over- parameterized models.
","['Fudan university', 'Fudan University', 'HKUST', 'MSRA', 'Hongkong University of Science and Technology', 'HongKong University of Science and Technology and Peking University']"
2020,Neural Architecture Search in A Proxy Validation Loss Landscape,"Yanxi Li, Minjing Dong, Yunhe Wang, Chang Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=5824,"This paper searches for the optimal neural architecture  by minimizing a proxy of validation loss. Existing neural architecture search (NAS) methods used to discover the optimal neural architecture that best fits the validation examples given the up-to-date network weights. However, back propagation with a number of validation examples could be time consuming, especially when it needs to be repeated many times in NAS. Though these  intermediate validation results are invaluable, they would be wasted if we cannot use them to predict the future from the past. In this paper, we propose to approximate the validation loss landscape by learning a mapping from neural architectures to their corresponding validate losses. The optimal neural architecture thus can be easily identified as the minimum of this proxy validation loss landscape. A novel sampling strategy is further developed for an efficient approximation of the loss landscape. Theoretical analysis indicates that our sampling strategy can reach a lower error rate and a lower label complexity compared with a uniform sampling. Experimental results on benchmarks demonstrate that the architecture searched by the proposed algorithm can achieve a satisfactory accuracy with less time cost.
","['University of Sydney', 'The University of Sydney', ""Noah's Ark Lab, Huawei Technologies."", 'University of Sydney']"
2020,Finite-Time Last-Iterate Convergence for Multi-Agent Learning in Games,"Darren Lin, Zhengyuan Zhou, Panayotis Mertikopoulos, Michael Jordan",https://icml.cc/Conferences/2020/Schedule?showEvent=6358,"In this paper, we consider multi-agent learning via online gradient descent in a class of games called $\lambda$-cocoercive games, a fairly broad class of games that admits many Nash equilibria and that properly includes unconstrained strongly monotone games. We characterize the finite-time last-iterate convergence rate for joint OGD learning on $\lambda$-cocoercive games; further, building on this result, we develop a fully adaptive OGD learning algorithm that does not require any knowledge of problem parameter (e.g. cocoercive constant $\lambda$) and show, via a novel double-stopping time technique, that this adaptive algorithm achieves same finite-time last-iterate convergence rate as non-adaptive counterpart. Subsequently, we extend OGD learning to the noisy gradient feedback case and establish last-iterate convergence results--first qualitative almost sure convergence, then quantitative finite-time convergence rates-- all under non-decreasing step-sizes. To our knowledge, we provide the first set of results that fill in several gaps of the existing multi-agent online learning literature, where three aspects--finite-time convergence rates, non-decreasing step-sizes, and fully adaptive algorithms have been unexplored before.","['UC Berkeley', 'Stanford University', 'CNRS and Criteo AI Lab', 'UC Berkeley']"
2020,Channel Equilibrium Networks for Learning Deep Representation,"Wenqi Shao, Shitao Tang, Xingang Pan, Ping Tan, Xiaogang Wang, Ping Luo",https://icml.cc/Conferences/2020/Schedule?showEvent=5849,"Convolutional Neural Networks (CNNs) are typically constructed by stacking multiple building blocks, each of which contains a normalization layer such as batch normalization (BN) and a rectified linear function such as ReLU. However, this work shows that the combination of normalization and rectified linear function leads to inhibited channels, which have small magnitude and contribute little to the learned feature representation, impeding the generalization ability of CNNs. Unlike prior arts that simply removed the inhibited channels, we propose to ``wake them up'' during training by designing a novel neural building block, termed Channel Equilibrium (CE) block, which enables channels at the same layer to contribute equally to the learned representation. We show that CE is able to prevent inhibited channels both empirically and theoretically. CE has several appealing benefits. (1) It can be integrated into many advanced CNN architectures such as ResNet and MobileNet, outperforming their original networks. (2) CE has an interesting connection with the Nash Equilibrium, a well-known solution of a non-cooperative game. (3) Extensive experiments show that CE achieves state-of-the-art performance on various challenging benchmarks such as ImageNet and COCO.
","['The Chinese University of HongKong', 'Simon Fraser University', 'The Chinese University of Hong Kong', 'Simon Fraser University', 'Chinese University of Hong Kong, Hong Kong', 'The University of Hong Kong']"
2020,Abstraction Mechanisms Predict Generalization in Deep Neural Networks,"Alex Gain, Hava Siegelmann",https://icml.cc/Conferences/2020/Schedule?showEvent=6750,"A longstanding problem for Deep Neural Networks (DNNs) is understanding their puzzling ability to generalize well. We approach this problem through the unconventional angle of \textit{cognitive abstraction mechanisms}, drawing inspiration from recent neuroscience work, allowing us to define the Cognitive Neural Activation metric (CNA) for DNNs, which is the correlation between information complexity (entropy) of given input and the concentration of higher activation values in deeper layers of the network. The CNA is highly predictive of generalization ability, outperforming norm-and-sharpness-based generalization metrics on an extensive evaluation of close to 200 network instances comprising a breadth of dataset-architecture combinations, especially in cases where additive noise is present and/or training labels are corrupted. These strong empirical results show the usefulness of the CNA as a generalization metric and encourage further research on the connection between information complexity and representations in the deeper layers of networks in order to better understand the generalization capabilities of DNNs.
","['Johns Hopkins University', 'UMass Amherst; DARPA']"
2020,Learning Opinions in Social Networks,"Vincent Conitzer, Debmalya Panigrahi, Hanrui Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6035,"We study the problem of learning opinions in social networks.  The learner observes the states of some sample nodes from a social network, and tries to infer the states of other nodes, based on the structure of the network.  We show that sample-efficient learning is impossible when the network exhibits strong noise, and give a polynomial-time algorithm for the problem with nearly optimal sample complexity when the network is sufficiently stable.
","['Duke', 'Duke University', 'Duke University']"
2020,Optimal approximation for unconstrained non-submodular minimization,"Marwa El Halabi, Stefanie Jegelka",https://icml.cc/Conferences/2020/Schedule?showEvent=5939,"Submodular function minimization is well studied, and existing algorithms solve it exactly or up to arbitrary accuracy. However, in many applications, such as structured sparse learning or batch Bayesian optimization, the objective function is not exactly submodular, but close. In this case, no theoretical guarantees exist. Indeed, submodular minimization algorithms rely on intricate connections between submodularity and convexity. We show how these relations can be extended to obtain approximation guarantees for minimizing non-submodular functions, characterized by how close the function is to submodular. We also extend this result to noisy function evaluations. Our approximation results are the first for minimizing non-submodular functions, and are optimal, as established by our matching lower bound.
","['MIT', 'Massachusetts Institute of Technology']"
2020,Sequential Cooperative Bayesian Inference,"Junqi Wang, Pei Wang, Patrick Shafto",https://icml.cc/Conferences/2020/Schedule?showEvent=6050,"Cooperation is often implicitly assumed when learning from other agents. Cooperation implies that the agent selecting the data, and the agent learning from the data, have the same goal, that the learner infer the intended hypothesis. Recent models in human and machine learning have demonstrated the possibility of cooperation. We seek foundational theoretical results for cooperative inference by Bayesian agents through sequential data. We develop novel approaches analyzing consistency, rate of convergence and stability of Sequential Cooperative Bayesian Inference (SCBI). Our analysis of the effectiveness, sample efficiency and robustness show that cooperation is not only possible but theoretically well-founded. We discuss implications for human-human and human-machine cooperation.
","['Rutgers University-Newark', 'Rutgers University-Newark', 'Rutgers University-Newark']"
2020,The Cost-free Nature of Optimally Tuning Tikhonov Regularizers and Other Ordered Smoothers,"Pierre C Bellec, Dana Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=5916,"We consider the problem of selecting the best estimator among a family of Tikhonov regularized estimators, or, alternatively, to select a linear combination of these regularizers that is as good as the best regularizer in the family. Our theory reveals that if the Tikhonov regularizers share the same penalty matrix with different tuning parameters, a convex procedure based on $Q$-aggregation achieves the mean square error of the best estimator, up to a small error term no larger than $C\sigma^2$, where $\sigma^2$ is the noise level and $C>0$ is an absolute constant. Remarkably, the error term does not depend on the penalty matrix or the number of estimators as long as they share the same penalty matrix, i.e., it applies to any grid of tuning parameters, no matter how large the cardinality of the grid is. This reveals the surprising ""cost-free"" nature of optimally tuning Tikhonov regularizers, in striking contrast with the existing literature on aggregation of estimators where one typically has to pay a cost of $\sigma^2\log(M)$ where $M$ is the number of estimators in the family. The result holds, more generally, for any family of ordered linear smoothers; this encompasses Ridge regression as well as Principal Component Regression. The result is extended to the problem of tuning Tikhonov regularizers with different penalty matrices.","['Rutgers', 'Duke University']"
2020,Identifying Statistical Bias in Dataset Replication,"Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Jacob Steinhardt, Aleksander Madry",https://icml.cc/Conferences/2020/Schedule?showEvent=6807,"Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models' ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6% of the original 11.7% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available: https://git.io/data-rep-analysis.
","['MIT', 'Massachusetts Institute of Technology', 'MIT', 'MIT', 'University of California, Berkeley', 'MIT']"
2020,Implicit Generative Modeling for Efficient Exploration,"Neale Ratzlaff, Qinxun Bai, Fuxin Li, Wei Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=5932,"Efficient exploration remains a challenging problem in reinforcement learning, especially for those tasks where rewards from environments are sparse. 
In this work, we introduce an exploration approach based on a novel implicit generative modeling algorithm to estimate a Bayesian uncertainty of the agent's belief of the environment dynamics. Each random draw from our generative model is a neural network that instantiates the dynamic function, hence multiple draws would approximate the posterior, and the variance in the predictions based on this posterior is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on the amortized Stein Variational Gradient Descent. In experiments, we demonstrate the effectiveness of this exploration algorithm in both pure exploration tasks and a downstream task, comparing with state-of-the-art intrinsic reward-based exploration approaches, including two recent approaches based on an ensemble of dynamic models. In challenging exploration tasks, our implicit generative model consistently outperforms competing approaches regarding data efficiency in exploration.
","['Oregon State University', 'Horizon Robotics', 'Oregon State University', 'Horizon Robotics']"
2020,Population-Based Black-Box Optimization for Biological Sequence Design,"Christof Angermueller, David Belanger, Andreea Gane, Zelda Mariet, David Dohan, Kevin Murphy, Lucy Colwell, D. Sculley",https://icml.cc/Conferences/2020/Schedule?showEvent=6792,"The use of black-box optimization for the design of new biological sequences is an emerging research area with potentially revolutionary impact. The cost and latency of wet-lab experiments requires methods that find good sequences in few experimental rounds of large batches of sequences --- a setting that off-the-shelf black-box optimization methods are ill-equipped to handle. We find that the performance of existing methods varies drastically across optimization tasks, posing a significant obstacle to real-world applications. To improve robustness, we propose Population-Based Black-Box Optimization (P3BO), which generates batches of sequences by sampling from an ensemble of methods. The number of sequences sampled from any method is proportional to the quality of sequences it previously proposed, allowing P3BO to combine the strengths of individual methods while hedging against their innate brittleness. Adapting the hyper-parameters of each of the methods online using evolutionary optimization further improves performance. Through extensive experiments on in-silico optimization tasks, we show that P3BO outperforms any single method in its population, proposing  higher quality sequences as well as more diverse batches. As such, P3BO and Adaptive-P3BO are a crucial step towards deploying ML to real-world sequence design.
","['Google', 'Google', 'Google', 'Google Inc.', 'Google', 'Google Brain', 'Google', 'Google']"
2020,Causal Modeling for Fairness In Dynamical Systems,"Elliot Creager, David Madras, Toniann Pitassi, Richard Zemel",https://icml.cc/Conferences/2020/Schedule?showEvent=6548,"In many applications areas---lending, education, and online recommenders, for example---fairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where sound causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and estimation by adjustment (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels.
","['University of Toronto', 'University of Toronto', 'University of Toronto', 'Vector Institute']"
2020,Complexity of Finding Stationary Points of Nonconvex Nonsmooth Functions,"Jingzhao Zhang, Hongzhou Lin, Stefanie Jegelka, Suvrit Sra, Ali Jadbabaie",https://icml.cc/Conferences/2020/Schedule?showEvent=6070,"We provide the first non-asymptotic analysis for finding stationary points of nonsmooth, nonconvex functions. In particular, we study the class of Hadamard semi-differentiable functions, perhaps the largest class of nonsmooth functions for which the chain rule of calculus holds. This class contains important examples such as ReLU neural networks and others with non-differentiable activation functions. First, we show that finding an epsilon-stationary point with first-order methods is
impossible in finite time. Therefore, we introduce the notion of (delta, epsilon)-stationarity, a generalization that allows for a point to be within distance delta of an epsilon-stationary point and reduces to epsilon-stationarity for smooth functions. We propose a series of randomized first-order methods and analyze their complexity
of finding a (delta, epsilon)-stationary point. Furthermore, we provide a lower bound and show that our stochastic algorithm has min-max optimal dependence on delta. Empirically, our methods perform well for training ReLU neural networks.
","['MIT', 'MIT', 'Massachusetts Institute of Technology', 'MIT', 'Massachusetts Institute of Technology']"
2020,On Validation and Planning of An Optimal Decision Rule with Application in Healthcare Studies,"Hengrui Cai, Wenbin Lu, Rui Song",https://icml.cc/Conferences/2020/Schedule?showEvent=5909,"In the current era of personalized recommendation, one major interest is to develop an optimal individualized decision rule that assigns individuals with the best treatment option according to their covariates. Estimation of optimal decision rules (ODR) has been extensively investigated recently, however, at present, no testing procedure is proposed to verify whether these ODRs are significantly better than the naive decision rule that always assigning individuals to a fixed treatment option. In this paper, we propose a testing procedure for detecting the existence of an ODR that is better than the naive decision rule under the randomized trials. We construct the proposed test based on the difference of estimated value functions using the augmented inverse probability weighted method. The asymptotic distributions of the proposed test statistic under the null and local alternative hypotheses are established. Based on the established asymptotic distributions, we further develop a sample size calculation formula for testing the existence of an ODR in designing A/B tests. Extensive simulations and a real data application to a schizophrenia clinical trial data are conducted to demonstrate the empirical validity of the proposed methods.
","['North Carolina State University', 'North Carolina State University', 'North Carolina State University']"
2020,Optimal Bounds between f-Divergences and Integral Probability Metrics,"Rohit Agrawal, Thibaut Horel",https://icml.cc/Conferences/2020/Schedule?showEvent=6817,"The families of f-divergences (e.g. the Kullback-Leibler divergence) and Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies) are commonly used in optimization and estimation. In this work, we systematically study the relationship between these two families from the perspective of convex duality. Starting from a tight variational representation of the f-divergence, we derive a generalization of the moment generating function, which we show exactly characterizes the best lower bound of the f-divergence as a function of a given IPM. Using this characterization, we obtain new bounds on IPMs defined by classes of unbounded functions, while also recovering in a unified manner well-known results for bounded and subgaussian functions (e.g. Pinsker's inequality and Hoeffding's lemma).
","['Harvard University', 'MIT']"
2020,An Accelerated DFO Algorithm for Finite-sum Convex Functions,"Yuwen Chen, Antonio Orvieto, Aurelien Lucchi",https://icml.cc/Conferences/2020/Schedule?showEvent=5852,"Derivative-free optimization (DFO) has recently gained a lot of momentum in machine learning, spawning interest in the community to design faster methods for problems where gradients are not accessible. While some attention has been given to the concept of acceleration in the DFO literature, existing stochastic algorithms for objective functions with a finite-sum structure have not been shown theoretically to achieve an accelerated rate of convergence. Algorithms that use
acceleration in such a setting are prone to instabilities, making it difficult to reach convergence. In this work, we exploit the finite-sum structure of the objective in order to design a variance-reduced DFO algorithm that provably yields acceleration. We prove rates of convergence for both smooth convex and strongly-convex finite-sum objective functions. Finally, we validate our theoretical results empirically on several tasks and datasets.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2020,Learning Quadratic Games on Networks,"Yan Leng, Xiaowen Dong, Junfeng Wu, Alex `Sandy' Pentland",https://icml.cc/Conferences/2020/Schedule?showEvent=6193,"Individuals, or organizations, cooperate with or compete against one another in a wide range of practical situations. Such strategic interactions are often modeled as games played on networks, where an individual's payoff depends not only on her action but also on that of her neighbors. The current literature has largely focused on analyzing the characteristics of network games in the scenario where the structure of the network, which is represented by a graph, is known beforehand. It is often the case, however, that the actions of the players are readily observable while the underlying interaction network remains hidden. In this paper, we propose two novel frameworks for learning, from the observations on individual actions, network games with linear-quadratic payoffs, and in particular, the structure of the interaction network. Our frameworks are based on the Nash equilibrium of such games and involve solving a joint optimization problem for the graph structure and the individual marginal benefits. Both synthetic and real-world experiments demonstrate the effectiveness of the proposed frameworks, which have theoretical as well as practical implications for understanding strategic interactions in a network environment.
","['The University of Texas at Austin', 'University of Oxford', 'Zhejiang University', 'MIT']"
2020,Learning What to Defer for Maximum Independent Sets,"Sungsoo Ahn, Younggyo Seo, Jinwoo Shin",https://icml.cc/Conferences/2020/Schedule?showEvent=6339,"Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget. 
","['KAIST', 'KAIST', 'KAIST']"
2020,SIGUA: Forgetting May Make Learning with Noisy Labels More Robust,"Bo Han, Gang Niu, Xingrui Yu, QUANMING YAO, Miao Xu, Ivor Tsang, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=5866,"Given data with noisy labels, over-parameterized deep networks can gradually memorize the data, and fit everything in the end. Although equipped with corrections for noisy labels, many learning methods in this area still suffer overfitting due to undesired memorization. In this paper, to relieve this issue, we propose stochastic integrated gradient underweighted ascent (SIGUA): in a mini-batch, we adopt gradient descent on good data as usual, and learning-rate-reduced gradient ascent} on bad data; the proposal is a versatile approach where data goodness or badness is w.r.t. desired or undesired memorization given a base learning method. Technically, SIGUA pulls optimization back for generalization when their goals conflict with each other; philosophically, SIGUA shows forgetting undesired memorization can reinforce desired memorization. Experiments demonstrate that SIGUA successfully robustifies two typical base learning methods, so that their performance is often significantly improved.
","['HKBU / RIKEN', 'RIKEN', 'University of Technology Sydney', '4Paradigm', 'University of Queensland/ RIKEN AIP', 'University of Technology Sydney', 'RIKEN / The University of Tokyo']"
2020,Eliminating the Invariance on the Loss Landscape of Linear Autoencoders,"Reza Oftadeh, Jiayi Shen, Zhangyang Wang, Dylan Shell",https://icml.cc/Conferences/2020/Schedule?showEvent=6297,"This paper proposes a new loss function for linear autoencoders (LAEs) and analytically identifies the structure of the associated loss surface. Optimizing the conventional Mean Square Error (MSE) loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but, owing to an invariance that cancels out in the global map, it will  fail to identify the exact eigenvectors. We show here that our proposed loss function eliminates this issue, so the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. We characterize the full structure of the new loss landscape by establishing an analytical expression for the set of all critical points, showing that it is a subset of critical points of MSE, and that all local minima are still global. Specifically, the invariant global minima under MSE are shown to become saddle points under the new loss. Additionally, the computational complexity of the loss and its gradients are the same as MSE and, thus, the new loss is not only of theoretical importance but is of practical value, e.g., for low-rank approximation.
","['Texas A&M University', 'Texas A&M', 'University of Texas at Austin', 'Texas A&M University']"
2020,One Size Fits All: Can We Train One Denoiser for All Noise Levels?,"Abhiram Gnanasambandam, Stanley Chan",https://icml.cc/Conferences/2020/Schedule?showEvent=5896,"When training an estimator such as a neural network for tasks like image denoising, it is often preferred to train one estimator and apply it to all noise levels. The de facto training protocol to achieve this goal is to train the estimator with noisy samples whose noise levels are uniformly distributed across the range of interest. However, why should we allocate the samples uniformly? Can we have more training samples that are less noisy, and fewer samples that are more noisy? What is the optimal distribution? How do we obtain such a distribution? The goal of this paper is to address this training sample distribution problem from a minimax risk optimization perspective. We derive a dual ascent algorithm to determine the optimal sampling distribution of which the convergence is guaranteed as long as the set of admissible estimators is closed and convex. For estimators with non-convex admissible sets such as deep neural networks, our dual formulation converges to a solution of the convex relaxation. We discuss how the algorithm can be implemented in practice. We evaluate the algorithm on linear estimators and deep networks.
","['Purdue University', 'Purdue University, USA']"
2020,Hierarchical Generation of Molecular Graphs using Structural Motifs,"Wengong Jin, Regina Barzilay, Tommi Jaakkola",https://icml.cc/Conferences/2020/Schedule?showEvent=6213,"Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.
","['MIT', 'MIT CSAIL', 'MIT']"
2020,Second-Order Provable Defenses against Adversarial Attacks,"Sahil Singla, Soheil Feizi",https://icml.cc/Conferences/2020/Schedule?showEvent=6256,"A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {\it any} perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for neural networks is difficult in general since it requires solving a non-convex optimization. In this paper, we provide computationally-efficient robustness certificates for neural networks with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded (globally or locally), we can compute a robustness certificate in the $l_2$ norm efficiently using convex optimization. Second, we derive a computationally-efficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness. Putting these results together leads to our proposed {\bf C}urvature-based {\bf R}obustness {\bf C}ertificate (CRC) and {\bf C}urvature-based {\bf R}obust {\bf T}raining (CRT). Our numerical results show that CRT leads to significantly higher certified robust accuracy compared to interval-bound propagation based training.","['University of Maryland', 'University of Maryland']"
2020,Efficient Non-conjugate Gaussian Process Factor Models for Spike Count Data using Polynomial Approximations,"Stephen Keeley, David Zoltowski, Yiyi Yu, Spencer Smith, Jonathan Pillow",https://icml.cc/Conferences/2020/Schedule?showEvent=6631,"Gaussian Process Factor Analysis (GPFA) has been broadly applied to the problem of identifying smooth, low-dimensional temporal structure underlying large-scale neural recordings. However, spike trains are non-Gaussian, which motivates combining GPFA with discrete observation models for binned spike count data. The drawback to this approach is that GPFA priors are not conjugate to count model likelihoods, which makes inference challenging. Here we address this obstacle by introducing a fast, approximate inference method for non-conjugate GPFA models. Our approach uses orthogonal second-order polynomials to approximate the nonlinear terms in the non-conjugate log-likelihood, resulting in a method we refer to as polynomial approximate log-likelihood (PAL) estimators. This approximation allows for accurate closed-form evaluation of marginal likelihoods and fast numerical optimization for parameters and hyperparameters. We derive PAL estimators for GPFA models with binomial, Poisson, and negative binomial observations and find the PAL estimation is highly accurate, and achieves faster convergence times compared to existing state-of-the-art inference methods. We also find that PAL hyperparameters can provide sensible initialization for black box variational inference (BBVI), which improves BBVI accuracy. We demonstrate that PAL estimators achieve fast and accurate extraction of latent structure from multi-neuron spike train data.
","['Princeton University', 'Princeton University', 'UNC', 'UC Santa Barbara', 'Princeton University']"
2020,Private Query Release Assisted by Public Data,"Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, Steven Wu",https://icml.cc/Conferences/2020/Schedule?showEvent=6329,"We study the problem of differentially private query release assisted by access to public data. In this problem, the goal is to answer a large class $\mathcal{H}$ of statistical queries with error no more than $\alpha$ using a combination of public and private samples. The algorithm is required to satisfy differential privacy only with respect to the private samples. We study the limits of this task in terms of the private and public sample complexities. Our upper and lower bounds on the private sample complexity have matching dependence on the dual VC-dimension of $\mathcal{H}$. For a large category of query classes, our bounds on the public sample complexity have matching dependence on $\alpha$.","['The Ohio State University', 'Northeastern University', 'IAS, Princeton', 'University of Toronto', 'Northeastern University', 'University of Minnesota']"
2020,InstaHide: Instance-hiding Schemes for Private Distributed Learning,"Yangsibo Huang, Zhao Song, Kai Li, Sanjeev Arora",https://icml.cc/Conferences/2020/Schedule?showEvent=6210,"How can multiple distributed entities train a shared deep net on their private data while protecting data privacy? This paper introduces InstaHide, a simple encryption of training images. Encrypted images can be used in standard deep learning pipelines (PyTorch, Federated Learning etc.) with no additional setup or infrastructure. The encryption has a minor effect on test accuracy (unlike differential privacy).
Encryption consists of mixing the image with a set of other images (in the sense of Mixup data augmentation technique (Zhang et al., 2018)) followed by applying a random pixel-wise mask on the mixed image. Other contributions of this paper are: (a) Use of large public dataset of images (e.g. ImageNet) for mixing during encryption; this improves security. (b) Experiments demonstrating effectiveness in protecting privacy against known attacks while preserving model accuracy. (c) Theoretical analysis showing that successfully attacking privacy requires attackers to solve a difficult computational problem. (d) Demonstration that Mixup alone is insecure as (contrary to recent proposals), by showing some efficient attacks. (e) Release of a challenge dataset to allow design of new attacks.
","['Princeton University', 'IAS/Princeton', 'Princeton University', 'Princeton University and Institute for Advanced Study']"
2020,Stabilizing Transformers for Reinforcement Learning,"Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew Botvinick, Nicolas Heess, Raia Hadsell",https://icml.cc/Conferences/2020/Schedule?showEvent=6176,"Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP). Harnessing the transformer’s ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. 
","['Carnegie Mellon University', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Deepmind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2020,On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm,"Khiem Pham, Khang Le, Nhat Ho, Tung Pham, Hung Bui",https://icml.cc/Conferences/2020/Schedule?showEvent=6163,"We provide a computational complexity analysis for the Sinkhorn algorithm that solves the entropic regularized Unbalanced Optimal Transport (UOT) problem between two measures of possibly different masses with at most $n$ components. We show that the complexity of the Sinkhorn algorithm for finding an $\varepsilon$-approximate solution to the UOT problem is of order $\widetilde{\mathcal{O}}(n^2/ \varepsilon)$. To the best of our knowledge, this complexity is better than the best known complexity upper bound of the Sinkhorn algorithm for solving the Optimal Transport (OT) problem, which is of order $\widetilde{\mathcal{O}}(n^2/\varepsilon^2)$. Our proof technique is based on the geometric convergence rate of the Sinkhorn updates to the optimal dual solution of the entropic regularized UOT problem and scaling properties of the primal solution. It is also different from the proof technique used to establish the complexity of the Sinkhorn algorithm for approximating the OT problem since the UOT solution does not need to meet the marginal constraints of the measures.","['VinAI Research, Vietnam', 'VinAI Research, Vietnam', 'University of California, Berkeley', 'VinAI Research;  Vietnam National University, Hanoi', 'VinAI Research']"
2020,Causal Effect Identifiability under Partial-Observability,"Sanghack Lee, Elias Bareinboim",https://icml.cc/Conferences/2020/Schedule?showEvent=6440,"Causal effect identifiability is concerned with establishing the effect of intervening on a set of variables on another set of variables from observational or interventional distributions under causal assumptions that  are usually encoded in the form of a causal graph. Most of the results of this literature implicitly assume that every variable modeled in the graph is measured in the available distributions. In practice, however, the data collections of the different studies considered do not measure the same variables, consistently. In this paper, we study the causal effect identifiability problem when the available distributions encompass different sets of variables, which we refer to as identification under partial-observability. We study a number of properties of the factors that comprise a causal effect under various levels of abstraction, and then characterize the relationship between them with respect to their status relative to the identification of a targeted intervention. We establish a sufficient graphical criterion for determining whether the effects are identifiable from partially-observed distributions. Finally, building on these graphical properties, we develop an algorithm that returns a formula for a causal effect in terms of the available distributions.
","['Columbia University', 'Columbia']"
2020,An Optimistic Perspective on Offline Deep Reinforcement Learning,"Rishabh Agarwal, Dale Schuurmans, Mohammad Norouzi",https://icml.cc/Conferences/2020/Schedule?showEvent=6643,"Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.
","['Google Research, Brain Team', 'Google / University of Alberta', 'Google Research, Brain Team']"
2020,VFlow: More Expressive Generative Flows with Variational Data Augmentation,"Jianfei Chen, Cheng Lu, Biqi Chenli, Jun Zhu, Tian Tian",https://icml.cc/Conferences/2020/Schedule?showEvent=6019,"Generative flows are promising tractable models for density modeling that define probabilistic distributions with invertible transformations. However, tractability imposes architectural constraints on generative flows. In this work, we study a previously overlooked constraint that all the intermediate representations must have the same dimensionality with the data due to invertibility, limiting the width of the network. We propose VFlow to tackle this constraint on dimensionality. VFlow augments the data with extra dimensions and defines a maximum evidence lower bound (ELBO) objective for estimating the distribution of augmented data jointly with the variational data augmentation distribution. Under mild assumptions, we show that the maximum ELBO solution of VFlow is always better than the original maximum likelihood solution. For image density modeling on the CIFAR-10 dataset, VFlow achieves a new state-of-the-art 2.98 bits per dimension.
","['University of California, Berkeley', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'RealAI']"
2020,Operation-Aware Soft Channel Pruning using Differentiable Masks,"Minsoo Kang, Bohyung Han",https://icml.cc/Conferences/2020/Schedule?showEvent=5997,"We propose a simple but effective data-driven channel pruning algorithm, which compresses deep neural networks in a differentiable way by exploiting the characteristics of operations.
The proposed approach makes a joint consideration of batch normalization (BN) and rectified linear unit (ReLU) for channel pruning; it estimates how likely the two successive operations deactivate each feature map and prunes the channels with high probabilities.
To this end, we learn differentiable masks for individual channels and make soft decisions throughout the optimization procedure, which facilitates to explore larger search space and train more stable networks.
The proposed framework enables us to identify compressed models via a joint learning of model parameters and channel pruning without an extra procedure of fine-tuning.
We perform extensive experiments and achieve outstanding performance in terms of the accuracy of output networks given the same amount of resources when compared with the state-of-the-art methods.
","['Seoul National University', 'Seoul National University']"
2020,Learning Structured Latent Factors from Dependent Data:A Generative Model Framework from Information-Theoretic Perspective,"Ruixiang ZHANG, Masanori Koyama, Katsuhiko Ishiguro",https://icml.cc/Conferences/2020/Schedule?showEvent=5931,"Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning.
In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space.
We represent the inductive bias in the form of mask variables to model the dependency structure in the graphical model and extend the theory of multivariate information bottleneck~\cite{mib} to enforce it.
Our model provides a principled approach to learn a set of semantically meaningful latent factors that reflect various types of desired structures like capturing correlation or encoding invariance, while also offering the flexibility to automatically estimate the dependency structure from data.
We show that our framework unifies many existing generative models and can be applied to a variety of tasks, including multi-modal data modeling, algorithmic fairness, and out-of-distribution generalization.
","['Mila/UdeM', 'Preferred Networks Inc. ', 'Preferred Networks, Inc.']"
2020,Message Passing Least Squares Framework and its Application to Rotation Synchronization,"Yunpeng Shi, Gilad Lerman",https://icml.cc/Conferences/2020/Schedule?showEvent=6785,"We propose an efficient algorithm for solving group synchronization under high levels of corruption and noise, while we focus on rotation synchronization. We first describe our recent theoretically guaranteed message passing algorithm that estimates the corruption levels of the measured group ratios. We then propose a novel reweighted least squares method to estimate the group elements, where the weights are initialized and iteratively updated using the estimated corruption levels. We demonstrate the superior performance of our algorithm over state-of-the-art methods for rotation synchronization using both synthetic and real data.
","['University of Minnesota', 'University of Minnesota']"
2020,Don't Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript,"Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, Bin Cui",https://icml.cc/Conferences/2020/Schedule?showEvent=6771,"Recent years have witnessed intensive research interests on training deep neural networks (DNNs) more efficiently by quantization-based compression methods, which facilitate DNNs training in two ways: (1) activations are quantized to shrink the memory consumption, and (2) gradients are quantized to decrease the communication cost. However, existing methods mostly use a uniform mechanism that quantizes the values evenly. Such a scheme may cause a large quantization variance and slow down the convergence in practice.
In this work, we introduce TinyScript, which applies a non-uniform quantization algorithm to both activations and gradients. TinyScript models the original values by a family of Weibull distributions and searches for ''quantization knobs'' that minimize quantization variance. We also discuss the convergence of the non-uniform quantization algorithm on DNNs with varying depths, shedding light on the number of bits required for convergence. Experiments show that TinyScript always obtains lower quantization variance, and achieves comparable model qualities against full precision training using 1-2 bits less than the uniform-based counterpart.
","['Peking University', 'Peking University', 'Peking University', 'ETH Zurich', 'BUPT', 'ETH Zurich', 'Peking University']"
2020,Online Bayesian Moment Matching based SAT Solver Heuristics,"Haonan Duan, Saeed Nejati, George Trimponias, Pascal Poupart, Vijay Ganesh",https://icml.cc/Conferences/2020/Schedule?showEvent=6731,"In this paper, we present a Bayesian Moment Matching (BMM) based method aimed at solving the initialization problem in Boolean SAT solvers. The initialization problem can be stated as follows: given a SAT formula $\phi$, compute an initial order over the variables of $\phi$ and values/polarity for these variables such that the runtime of SAT solvers on input $\phi$ is minimized. At the start of a solver run, our BMM-based methods compute a posterior probability distribution for an assignment to the variables of the input formula after analyzing its clauses, which will then be used by the solver to initialize its search. We perform extensive experiments to evaluate the efficacy of our BMM-based heuristic against 4 other initialization methods (random, survey propagation, Jeroslow-Wang, and default) in state-of-the-art solvers, MapleCOMSPS and MapleLCMDistChronotBT over the SAT competition 2018 application benchmark, as well as the best-known solvers in the cryptographic category, namely, CryptoMiniSAT, Glucose, and MapleSAT. On the cryptographic benchmark, BMM-based solvers out-perform all other initialization methods. Further, the BMM-based MapleCOMSPS significantly out-perform the same solver using all other initialization methods by 12 additional instances solved and better average runtime, over the SAT 2018 competition benchmark.","['University of Waterloo', 'University of Waterloo', ""Noah's Ark Lab"", 'University of Waterloo and Borealis AI', 'University of Waterloo, Electrical and Computer Engineering']"
2020,Best Arm Identification for Cascading Bandits in the Fixed Confidence Setting,"Zixin Zhong, Wang Chi Cheung, Vincent Tan",https://icml.cc/Conferences/2020/Schedule?showEvent=5819,"We design and analyze CascadeBAI, an algorithm for finding the best set of K items, also called an arm, within the framework of cascading bandits. An upper bound on the time complexity of CascadeBAI is derived by overcoming a crucial analytical challenge, namely, that of probabilistically estimating the amount of available feedback at each step. To do so, we define a new class of random variables (r.v.'s) which we term as left-sided sub-Gaussian r.v.'s; these are r.v.'s whose cumulant generating functions (CGFs) can be bounded by a quadratic only for non-positive arguments of the CGFs. This enables the application of a sufficiently tight Bernstein-type concentration inequality. We show, through the derivation of a lower bound on the time complexity, that the performance of CascadeBAI is optimal in some practical regimes. Finally, extensive numerical simulations corroborate the efficacy of CascadeBAI as well as the tightness of our upper bound on its time complexity.
","['National University of Singapore', 'National University of Singapore', 'National University of Singapore']"
2020,The Differentiable Cross-Entropy Method,"Brandon Amos, Denis Yarats",https://icml.cc/Conferences/2020/Schedule?showEvent=6662,"We study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline where this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show how to embed optimal action sequences into a lower-dimensional space. This enables us to use policy optimization to fine-tune modeling components by differentiating through the CEM-based controller.
","['Facebook AI Research', 'New York University']"
2020,Accelerating Large-Scale Inference with Anisotropic Vector Quantization,"Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, Sanjiv Kumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6689,"Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach, whose implementation is open-source, achieves state-of-the-art results on the public benchmarks available at ann-benchmarks.com.
","['Google Research', 'Google', 'Google Research', 'Google', 'Google', 'Google AI', 'Google Research, NY']"
2020,Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction,"Filipe de Avila Belbute-Peres, Thomas Economon, Zico Kolter",https://icml.cc/Conferences/2020/Schedule?showEvent=6802,"Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. By combining an actual CFD simulator (run on a much coarser resolution representation of the problem) with the graph network, we show that we can both generalize well to new situations and benefit from the substantial speedup of neural network CFD predictions, while also substantially outperforming the coarse CFD simulation alone.
","['Carnegie Mellon University', 'SU2 Foundation', 'Carnegie Mellon University / Bosch Center for AI']"
2020,Variational Label Enhancement,"Ning Xu, Jun Shu, Yun-Peng Liu, Xin Geng",https://icml.cc/Conferences/2020/Schedule?showEvent=6295,"Label distribution  covers a certain number of labels, representing the degree to which each label describes the instance. The learning process on the  instances labeled by label distributions is called label distribution learning (LDL). Unfortunately, many training sets only contain  simple logical labels rather than label distributions due to the difficulty of obtaining the label distributions directly.  To solve this problem,  we consider the label distributions as the latent vectors and  infer the label distributions from the logical labels in the training datasets by using variational inference. After that, we induce a predictive model to train the label distribution data by employing the multi-output regression technique. The recovery experiment  on thirteen real-world LDL  datasets  and the predictive experiment on ten multi-label learning datasets validate the advantage of our approach  over the state-of-the-art  approaches. 
","['Southeast University', ""Xi'an Jiaotong University"", 'Southeast University', 'Southeast University']"
2020,Convolutional dictionary learning based auto-encoders for natural exponential-family distributions,"Bahareh Tolooshams, Andrew Song, Simona Temereanca, Demba Ba",https://icml.cc/Conferences/2020/Schedule?showEvent=6690,"We introduce a class of auto-encoder neural networks tailored to data from the natural exponential family (e.g., count data). The architectures are inspired by the problem of learning the filters in a convolutional generative model with sparsity constraints, often referred to as convolutional dictionary learning (CDL). Our work is the first to combine ideas from convolutional generative models and deep learning for data that are naturally modeled with a non-Gaussian distribution (e.g., binomial and Poisson). This perspective provides us with a scalable and flexible framework that can be re-purposed for a wide range of tasks and assumptions on the generative model. Specifically, the iterative optimization procedure for solving CDL, an unsupervised task, is mapped to an unfolded and constrained neural network, with iterative adjustments to the inputs to account for the generative distribution. We also show that the framework can easily be extended for discriminative training, appropriate for a supervised task. We demonstrate 1) that fitting the generative model to learn, in an unsupervised fashion, the latent stimulus that underlies neural spiking data leads to better goodness-of-fit compared to other baselines, 2) competitive performance compared to state-of-the-art algorithms for supervised Poisson image denoising, with significantly fewer parameters, and 3) gradient dynamics of shallow binomial auto-encoder.
","['Harvard University', 'MIT', 'Brown University', 'Harvard']"
2020,Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control,"Jie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, Wojciech Matusik",https://icml.cc/Conferences/2020/Schedule?showEvent=5933,"Many real-world control problems involve conflicting objectives where we desire a dense and high-quality set of control policies that are optimal for different objective preferences (called Pareto-optimal). While extensive research in multi-objective reinforcement learning (MORL) has been conducted to tackle such problems, multi-objective optimization for complex continuous robot control is still under-explored. In this work, we propose an efficient evolutionary learning algorithm to find the Pareto set approximation for continuous robot control problems, by extending a state-of-the-art RL algorithm and presenting a novel prediction model to guide the learning process. In addition to efficiently discovering the individual policies on the Pareto front, we construct a continuous set of Pareto-optimal solutions by Pareto analysis and interpolation. Furthermore, we design seven multi-objective RL environments with continuous action space, which is the first benchmark platform to evaluate MORL algorithms on various robot control problems. We test the previous methods on the proposed benchmark problems, and the experiments show that our approach is able to find a much denser and higher-quality set of Pareto policies than the existing algorithms.
","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'MIT', 'MIT CSAIL', 'Texas A&M University', 'MIT']"
2020,Preference Modeling with Context-Dependent Salient Features,"Amanda Bower, Laura Balzano",https://icml.cc/Conferences/2020/Schedule?showEvent=6525,"We consider the problem of estimating a ranking on a set of items from noisy pairwise comparisons given item features. We address the fact that pairwise comparison data often reflects irrational choice, e.g. intransitivity. Our key observation is that two items compared in isolation from other items may be compared based on only a salient subset of features. Formalizing this framework, we propose the salient feature preference model and prove a finite sample complexity result for learning the parameters of our model and the underlying ranking with maximum likelihood estimation. We also provide empirical results that support our theoretical bounds and illustrate how our model explains systematic intransitivity. Finally we demonstrate strong performance of maximum likelihood estimation of our model on both synthetic data and two real data sets: the UT Zappos50K data set and comparison data about the compactness of legislative districts in the US.
","['University of Michigan', 'University of Michigan']"
2020,Progressive Graph Learning for Open-Set Domain Adaptation,"Yadan Luo, Zijian Wang, Zi Huang, Mahsa Baktashmotlagh",https://icml.cc/Conferences/2020/Schedule?showEvent=5782,"Domain shift is a fundamental problem in visual recognition which typically arises when the source and target data follow different distributions. The existing domain adaptation approaches which tackle this problem work in the ""closed-set"" setting with the assumption that the source and the target data share exactly the same classes of objects. In this paper, we tackle a more realistic problem of the ""open-set"" domain shift where the target data contains additional classes that were not present in the source data. More specifically, we introduce an end-to-end Progressive Graph Learning (PGL) framework where a graph neural network with episodic training is integrated to suppress underlying conditional shift and adversarial learning is adopted to close the gap between the source and target distributions. Compared to the existing open-set adaptation approaches, our approach guarantees to achieve a tighter upper bound of the target error. Extensive experiments on three standard open-set benchmarks evidence that our approach significantly outperforms the state-of-the-arts in open-set domain adaptation.
","['The University of Queensland', 'University of Queensland', 'University of Queensland', 'University of Queensland']"
2020,Fair k-Centers via Maximum Matching,"Matthew Jones, Huy Nguyen, Thy Nguyen",https://icml.cc/Conferences/2020/Schedule?showEvent=6460,"The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each ""demographic group"" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best of each algorithm by presenting a linear-time algorithm with a guaranteed 3-approximation factor and provides empirical evidence of both the algorithm's runtime and effectiveness.
","['Khoury College of Computer Sciences', 'Northeastern University', 'Northeastern University']"
2020,Adaptive Estimator Selection for Off-Policy Evaluation,"Yi Su, Pavithra Srinath, Akshay Krishnamurthy",https://icml.cc/Conferences/2020/Schedule?showEvent=6121,"We develop a generic data-driven method for estimator selection in off-policy policy evaluation settings. We establish a strong performance guarantee for the method, showing that it is competitive with the oracle estimator, up to a constant factor. Via in-depth case studies in contextual bandits and reinforcement learning, we demonstrate the generality and applicability of the method. We also perform comprehensive experiments, demonstrating the empirical efficacy of our approach and comparing with related approaches. In both case studies, our method compares favorably with existing methods.
","['Cornell University', 'Microsoft Research', 'Microsoft Research']"
2020,TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics,"Alexander Tong, Jessie Huang, Guy Wolf, David van Dijk, Smita Krishnaswamy",https://icml.cc/Conferences/2020/Schedule?showEvent=6491,"It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present {\em TrajectoryNet}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.
","['Yale University', 'Yale University', 'Université de Montréal; Mila', 'Yale University', 'Yale University']"
2020,Can Stochastic Zeroth-Order Frank-Wolfe Method Converge Faster for Non-Convex Problems?,"Hongchang Gao, Heng Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=6437,"Frank-Wolfe algorithm is an efficient method for optimizing  non-convex constrained problems. However, most of existing methods focus on the first-order case. In real-world applications, the gradient is not always available. To address the problem  of lacking gradient in many applications, we propose two new stochastic zeroth-order Frank-Wolfe algorithms and theoretically proved that they have a faster convergence rate than existing methods for non-convex problems. Specifically, the function queries oracle of the proposed faster zeroth-order Frank-Wolfe (FZFW) method is $O(\frac{n^{1/2}d}{\epsilon^2})$  which can match the iteration complexity of the first-order counterpart approximately. As for the proposed faster zeroth-order conditional gradient sliding (FZCGS) method, its function queries oracle is  improved to $O(\frac{n^{1/2}d}{\epsilon})$, indicating that its iteration complexity is even better than that of its first-order counterpart NCGS-VR. In other words, the iteration complelxity of the  accelerated first-order Frank-Wolfe method NCGS-VR is suboptimal. 
Then, we  proposed a new algorithm to improve its IFO (incremental first-order oracle) to $O(\frac{n^{1/2}}{\epsilon})$. At last, the empirical studies on benchmark datasets validate our theoretical results.","['University of Pittsburgh', 'University of Pittsburgh & JD Finance America Corporation']"
2020,The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization,"Ben Adlam, Jeffrey Pennington",https://icml.cc/Conferences/2020/Schedule?showEvent=6710,"Modern deep learning models employ considerably more parameters than required to fit the training data. Whereas conventional statistical wisdom suggests such models should drastically overfit, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a \emph{double descent} curve, in which increasing a model's capacity causes its test error to first decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has nonmonotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.
","['Google Brain', 'Google Brain']"
2020,Dynamics of Deep Neural Networks and Neural Tangent Hierarchy,"Jiaoyang Huang, Horng-Tzer Yau",https://icml.cc/Conferences/2020/Schedule?showEvent=5976,"The evolution of a deep neural network trained by the gradient descent in the overparametrization regime can be described by its neural tangent kernel (NTK) \cite{jacot2018neural, du2018gradient1,du2018gradient2,arora2019fine}. It was observed \cite{arora2019exact} that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks. We study the dynamic of neural networks of finite width and derive an infinite hierarchy of differential equations, the neural tangent hierarchy (NTH). We prove that the NTH hierarchy truncated at the level $p\geq 2$ approximates the dynamic of the NTK up to arbitrary precision under certain conditions on the neural network width and the data set dimension. The assumptions needed for these approximations become weaker as $p$ increases. Finally, NTH can be viewed as higher order extensions of NTK. In particular, the NTH truncated at $p=2$ recovers the NTK dynamics.","['IAS', 'Harvard']"
2020,Data preprocessing to mitigate bias: A maximum entropy based approach,"L. Elisa Celis, Vijay Keswani, Nisheeth K. Vishnoi",https://icml.cc/Conferences/2020/Schedule?showEvent=6215,"Data containing human or social attributes may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. This paper presents an algorithmic framework that can be used as a data preprocessing method towards mitigating such bias. Unlike prior work, it can efficiently learn distributions over large domains, controllably adjust the representation rates of protected groups and achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach leverages the principle of maximum entropy – amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main contribution is an instantiation of this framework for our set of constraints and priors, which encode our bias mitigation goals, and that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.
","['Yale University', 'Yale University', 'Yale University']"
2020,Learnable Group Transform For Time-Series,"Romain Cosentino, Behnaam Aazhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6701,"We propose a novel approach to filter bank learning for time-series by considering spectral decompositions of signals defined as a Group Transform. This framework allows us to generalize classical time-frequency transformations such as the Wavelet Transform, and  to efficiently learn the representation of signals. While the creation of the wavelet transform filter-bank relies on affine transformations of a mother filter, our approach allows for non-linear transformations. The transformations induced by such maps enable us to span a larger class of signal representations, from wavelet to chirplet-like filters. We propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific task and signal. The Learnable Group Transform can be cast into a Deep Neural Network. The experiments on diverse time-series datasets demonstrate the expressivity of this framework, which competes with state-of-the-art performances.
","['Rice University', 'Rice University']"
2020,Improved Communication Cost in Distributed PageRank Computation – A Theoretical Study,Siqiang Luo,https://icml.cc/Conferences/2020/Schedule?showEvent=6185,"PageRank is a widely used approach for measuring the importance of a node in a graph. Due to the rapid growth of the graph size in the real world, the importance of computing PageRanks in a distributed environment has been increasingly recognized. However, only a few previous works can provide a provable complexity and accuracy for distributed PageRank computation. Given a constant $d\ge 1$ and a graph of $n$ nodes, the state-of-the-art approach, Radar-Push, uses $O(\log\log{n}+\log{d})$ communication rounds to approximate the PageRanks within a relative error $O(\frac{1}{\log^d{n}})$ under a generalized congested clique distributed model. However, Radar-Push entails as large as $O(\log^{2d+3}{n})$ bits of bandwidth (e.g., the communication cost between a pair of nodes per round) in the worst case. In this paper, we provide a new algorithm that uses asymptotically the same communication round complexity while using only $O(d\log^3{n})$ bits of bandwidth. ",['Harvard']
2020,"Schatten Norms in Matrix Streams: Hello Sparsity, Goodbye Dimension","Vladimir Braverman, Robert Krauthgamer, Aditya Krishnan, Roi Sinoff",https://icml.cc/Conferences/2020/Schedule?showEvent=6145,"Spectral functions of large matrices contains important structural information about the underlying data, and is thus becoming increasingly important. Many times, large matrices representing real-world data are sparse or doubly sparse (i.e., sparse in both rows and columns), and are accessed as a stream of updates, typically organized in row-order. In this setting, where space (memory) is the limiting resource, all known algorithms require space that is polynomial in the dimension of the matrix, even for sparse matrices. We address this challenge by providing the first algorithms whose space requirement is independent of the matrix dimension, assuming the matrix is doubly-sparse and presented in row-order. Our algorithms approximate the Schatten p-norms, which we use in turn to approximate other spectral functions, such as logarithm of the determinant, trace of matrix inverse, and Estrada index. We validate these theoretical performance bounds by numerical experiments on real-world matrices representing social networks. We further prove that multiple passes are unavoidable in this setting, and show extensions of our primary technique, including a trade-off between space requirements and number of passes.
","['Johns Hopkins University', 'Weizmann Institute of Science', 'Johns Hopkins University', 'Weizmann Institute of Science, Israel']"
2020,Efficient Intervention Design for Causal Discovery with Latents,"Raghavendra Addanki, Shiva Kasiviswanathan, Andrew McGregor, Cameron Musco",https://icml.cc/Conferences/2020/Schedule?showEvent=5894,"We consider recovering a causal graph in presence of latent variables, where we seek to minimize the cost of interventions used in the recovery process. We consider two intervention cost models: (1) a linear cost model where the cost of an intervention on a subset of variables has a linear form, and (2) an identity cost model where the cost of an intervention is the same, regardless of what variables it is on, i.e., the goal is just to minimize the number of interventions. Under the linear cost model, we give an algorithm to identify the ancestral relations of the underlying causal graph, achieving within a $2$-factor of the optimal intervention cost. This approximation factor can be improved to $1+\eps$ for any $\eps > 0$ under some mild restrictions. Under the identity cost model, we bound the number of interventions needed to recover the entire causal graph, including the latent variables, using a parameterization of the causal graph  through a special type of colliders. In particular, we introduce the notion of $p$-colliders, that are colliders between pair of nodes arising from a specific type of conditioning in the causal graph, and provide an upper bound on the number of interventions as a function of the maximum number of $p$-colliders between any two nodes in the causal graph.","['University of Massachusetts Amherst', 'Amazon', 'University of Massachusetts Amherst', 'UMass']"
2020,Robust and Stable Black Box Explanations,"Hima Lakkaraju, Nino Arsov, Osbert Bastani",https://icml.cc/Conferences/2020/Schedule?showEvent=6726,"As machine learning black boxes are increasingly being deployed in real-world applications, there has been a growing interest in developing post hoc explanations that summarize the behaviors of these black boxes. However, existing algorithms for generating such explanations have been shown to lack stability and robustness to distribution shifts. We propose a novel framework for generating robust and stable explanations of black box models based on adversarial training. Our framework optimizes a minimax objective that aims to construct the highest fidelity explanation with respect to the worst-case over a set of adversarial perturbations. We instantiate this algorithm for explanations in the form of linear models and decision sets by devising the required optimization procedures. To the best of our knowledge, this work makes the first attempt at generating post hoc explanations that are robust to a general class of adversarial perturbations that are of practical interest. Experimental evaluation with real-world and synthetic datasets demonstrates that our approach substantially improves robustness of explanations without sacrificing their fidelity on the original data distribution.
","['Harvard', 'Macedonian Academy of Arts and Sciences', 'University of Pennsylvania']"
2020,Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods,"Dan Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon  Fatahalian, Christopher Re",https://icml.cc/Conferences/2020/Schedule?showEvent=6736,"Weak supervision is a popular method for building machine learning models without relying on ground truth annotations. Instead, it generates probabilistic training labels by estimating the accuracies of multiple noisy labeling sources (e.g., heuristics, crowd workers). Existing approaches use latent variable estimation to model the noisy sources, but these methods can be computationally expensive, scaling superlinearly in the data. In this work, we show that, for a class of latent variable models highly applicable to weak supervision, we can find a closed-form solution to model parameters, obviating the need for iterative solutions like stochastic gradient descent (SGD). We use this insight to build FlyingSquid, a weak supervision framework that runs orders of magnitude faster than previous weak supervision approaches and requires fewer assumptions. In particular, we prove bounds on generalization error without assuming that the latent variable model can exactly parameterize the underlying data distribution. Empirically, we validate FlyingSquid on benchmark weak supervision datasets and find that it achieves the same or higher quality compared to previous approaches without the need to tune an SGD procedure, recovers model parameters 170 times faster on average, and enables new video analysis and online learning applications.
","['Stanford University', 'Stanford University', 'Stanford', 'Stanford University', 'Stanford', 'Stanford']"
2020,Label-Noise Robust Domain Adaptation,"Xiyu Yu, Tongliang Liu, Mingming Gong, Kun Zhang, Kayhan Batmanghelich, Dacheng Tao",https://icml.cc/Conferences/2020/Schedule?showEvent=6080,"Domain adaptation aims to correct the classifiers when faced with distribution shift between source (training) and target (test) domains. State-of-the-art domain adaptation methods make use of deep networks to extract domain-invariant representations. However, existing methods assume that all the instances in the source domain are correctly labeled; while in reality, it is unsurprising that we may obtain a source domain with noisy labels. In this paper, we are the first to comprehensively investigate how label noise could adversely affect existing domain adaptation methods in various scenarios. Further, we theoretically prove that there exists a method that can essentially reduce the side-effect of noisy source labels in domain adaptation. Specifically, focusing on the generalized target shift scenario, where both label distribution $P_Y$ and the class-conditional distribution $P_{X|Y}$ can change, we discover that the denoising Conditional Invariant Component (DCIC) framework can provably ensures (1) extracting invariant representations given examples with noisy labels in the source domain and unlabeled examples in the target domain and (2) estimating the label distribution in the target domain with no bias. Experimental results on both synthetic and real-world data verify the effectiveness of the proposed method.","['Baidu Inc.', 'The University of Sydney', 'University of Melbourne', 'Carnegie Mellon University', 'University of Pittsburgh', 'The University of Sydney']"
2020,Moniqua: Modulo Quantized Communication in Decentralized SGD,"Yucheng Lu, Christopher De Sa",https://icml.cc/Conferences/2020/Schedule?showEvent=5885,"Running Stochastic Gradient Descent (SGD) in a decentralized fashion has shown promising results. In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication. We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication. Moniqua improves upon prior works in that it (1) requires zero additional memory, (2) works with 1-bit quantization, and (3) is applicable to a variety of decentralized algorithms. We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms.  We also show that Moniqua is robust to very low bit-budgets, allowing  $1$-bit-per-parameter communication without compromising validation accuracy when training ResNet20 and ResNet110 on CIFAR10.","['Cornell University', 'Cornell']"
2020,Consistent Estimators for Learning to Defer to an Expert,"Hussein Mozannar, David Sontag",https://icml.cc/Conferences/2020/Schedule?showEvent=6448,"Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given only samples of the expert's decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a novel reduction to cost sensitive learning where we give a  consistent surrogate loss for cost sensitive learning that generalizes the cross entropy loss. We show the effectiveness of our approach on a variety of experimental tasks. 
","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']"
2020,Median Matrix Completion: from Embarrassment to Optimality,"Weidong Liu, Xiaojun Mao, Ka Wai Wong",https://icml.cc/Conferences/2020/Schedule?showEvent=6153,"In this paper, we consider matrix completion with absolute deviation loss and obtain an estimator of the median matrix. Despite several appealing properties of median, the non-smooth absolute deviation loss leads to computational challenge for large-scale data sets which are increasingly common among matrix completion problems. A simple solution to large-scale problems is parallel computing. However, embarrassingly parallel fashion often leads to inefficient estimators. Based on the idea of pseudo data, we propose a novel refinement step, which turns such inefficient estimators into a rate (near-)optimal matrix completion procedure. The refined estimator is an approximation of a regularized least median estimator, and therefore not an ordinary regularized empirical risk estimator. This leads to a non-standard analysis of asymptotic behaviors. Empirical results are also provided to confirm the effectiveness of the proposed method.
","['Shanghai Jiao Tong University', 'Fudan University', 'Texas A&M University']"
2020,Robust Bayesian Classification Using An Optimistic Score Ratio,"Viet Anh Nguyen, Nian Si, Jose Blanchet",https://icml.cc/Conferences/2020/Schedule?showEvent=6138,"We build a Bayesian contextual classification model using an optimistic score ratio for robust binary classification when there is limited information on the class-conditional, or contextual, distribution. The optimistic score searches for the distribution that is most plausible to explain the observed outcomes in the testing sample among all distributions belonging to the contextual ambiguity set which is prescribed using a limited structural constraint on the mean vector and the covariance matrix of the underlying contextual distribution. We show that the Bayesian classifier using the optimistic score ratio is conceptually attractive, delivers solid statistical guarantees and is computationally tractable. We showcase the power of the proposed optimistic score ratio classifier on both synthetic and empirical data.
","['Stanford University', 'Stanford University', 'Stanford University']"
2020,Privately Learning Markov Random Fields,"Huanyu Zhang, Gautam Kamath, Janardhan Kulkarni, Steven Wu",https://icml.cc/Conferences/2020/Schedule?showEvent=5776,"We consider the problem of learning Markov Random Fields (including
  the prototypical example, the Ising model) under the constraint of
  differential privacy.  Our learning goals include both
  \emph{structure learning}, where we try to estimate the underlying
  graph structure of the model, as well as the harder goal of
  \emph{parameter learning}, in which we additionally estimate the
  parameter on each edge.  We provide algorithms and lower bounds for
  both problems under a variety of privacy constraints --
  namely pure, concentrated, and approximate differential privacy.
  While non-privately, both learning goals enjoy roughly the same
  complexity, we show that this is not the case under differential
  privacy.  In particular, only structure learning under approximate
  differential privacy maintains the non-private logarithmic
  dependence on the dimensionality of the data, while a change in
  either the learning goal or the privacy notion would necessitate a
  polynomial dependence. As a result, we show that the privacy
    constraint imposes a strong separation between these two learning
    problems in the high-dimensional data regime.
","['Cornell University', 'University of Waterloo', 'Microsoft Research, Redmond', 'University of Minnesota']"
2020,On Learning Language-Invariant Representations for Universal Machine Translation,"Han Zhao, Junjie Hu, Andrej Risteski",https://icml.cc/Conferences/2020/Schedule?showEvent=6658,"The goal of universal machine translation is to learn to translate between any pair of languages, given pairs of translated documents for \emph{some} of these languages. Despite impressive empirical results and an increasing interest in massively multilingual models, theoretical analysis on translation errors made by such universal machine translation models is only nascent. In this paper, we take one step towards better understanding of universal machine translation by first proving an impossibility theorem in the general case. In particular, we derive a lower bound on the translation error in the many-to-one translation setting, which shows that any algorithm aiming to learn shared sentence representations among multiple language pairs has to make a large translation error on at least one of the translation tasks, if no assumption on the structure of the languages is made. On the positive side, we show that if the documents follow a natural encoder-decoder generative process, then we can expect a natural notion of ``generalization'': a linear number of pairs, rather than quadratic, suffices. Our theory also explains what kinds of connection graphs between pairs of languages are better suited: ones with longer paths result in worse sample complexity in terms of the total number of documents per language pair needed. We believe our theoretical insights and implications contribute to the future algorithmic design of universal machine translation.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'CMU']"
2020,Sub-linear Memory Sketches for Near Neighbor Search on Streaming Data,"Benjamin Coleman, Richard Baraniuk, Anshumali Shrivastava",https://icml.cc/Conferences/2020/Schedule?showEvent=6814,"We present the first sublinear memory sketch that can be queried to find the nearest neighbors in a dataset. Our online sketching algorithm compresses an N element dataset to a sketch of size $O(N^b \log^3 N)$ in $O(N^{(b+1)} \log^3 N)$ time, where $b < 1$. This sketch can correctly report the nearest neighbors of any query that satisfies a stability condition parameterized by $b$. We achieve sublinear memory performance on stable queries by combining recent advances in locality sensitive hash (LSH)-based estimators, online kernel density estimation, and compressed sensing. Our theoretical results shed new light on the memory-accuracy tradeoff for nearest neighbor search, and our sketch, which consists entirely of short integer arrays, has a variety of attractive features in practice. We evaluate the memory-recall tradeoff of our method on a friend recommendation task in the Google plus social media network. We obtain orders of magnitude better compression than the random projection based alternative while retaining the ability to report the nearest neighbors of practical queries.","['Rice University', 'OpenStax / Rice University', 'Rice University']"
2020,Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics,"Debjani Saha, Candice Schumann, Duncan McElfresh, John P Dickerson, Michelle Mazurek, Michael Tschantz",https://icml.cc/Conferences/2020/Schedule?showEvent=5977,"Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions--demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.
","['University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'International Computer Science Institute']"
2020,What can I do here? A Theory of Affordances in Reinforcement Learning,"Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, Doina Precup",https://icml.cc/Conferences/2020/Schedule?showEvent=6274,"Reinforcement learning algorithms usually assume that all actions are always available to an agent. However, both people and animals understand the general link between the features of their environment and the actions that are feasible. Gibson (1977) coined the term ""affordances"" to describe the fact that certain states enable an agent to do certain actions, in the context of embodied agents. In this paper, we develop a theory of affordances for agents who learn and plan in Markov Decision Processes. Affordances play a dual role in this case. On one hand, they allow faster planning, by reducing the number of actions available in any given situation. On the other hand, they facilitate more efficient and precise learning of transition models from data, especially when such models require function approximation. We establish these properties through theoretical results as well as illustrative examples. We also propose an approach to learn affordances and use it to estimate transition models that are simpler and generalize better.
","['McGill University,  Mila Montreal', 'DeepMind', 'DeepMind', 'Brown University', 'DeepMind']"
2020,Adversarial Risk via Optimal Transport and Optimal Couplings,"Muni Sreenivas Pydi, Varun Jog",https://icml.cc/Conferences/2020/Schedule?showEvent=6263,"The accuracy of modern machine learning algorithms deteriorates severely on adversarially manipulated test data. Optimal adversarial risk quantifies the best error rate of any classifier in the presence of adversaries, and optimal adversarial classifiers are sought that minimize adversarial risk. In this paper, we investigate the optimal adversarial risk and optimal adversarial classifiers from an optimal transport perspective. We present a new and simple approach to show that the optimal adversarial risk for binary classification with 0 − 1 loss function is completely characterized by an optimal transport cost between the probability distributions of the two classes, for a suitably defined cost function. We propose a novel coupling strategy that achieves the optimal transport cost for several univariate distributions like Gaussian, uniform and triangular. Using the optimal couplings, we obtain the optimal adversarial classifiers in these settings and show how they differ from optimal classifiers in the absence of adversaries. Based on our analysis, we evaluate algorithm-independent fundamental limits on adversarial risk for CIFAR-10, MNIST, Fashion-MNIST and SVHN datasets, and Gaussian mixtures based on them.
","['University of Wisconsin-Madison', 'University of Wisconsin - Madison']"
2020,Reinforcement Learning for Integer Programming: Learning to Cut,"Yunhao Tang, Shipra Agrawal, Yuri Faenza",https://icml.cc/Conferences/2020/Schedule?showEvent=5903,"Integer programming is a general optimization framework with a wide variety of applications, e.g., in scheduling, production planning, and graph optimization. As Integer Programs (IPs) model many provably hard to solve problems, modern IP solvers rely on heuristics. These heuristics are often human-designed, and tuned over time using experience and data.  The goal of this work is to show that the performance of those solvers can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the Cutting Plane Method.  This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that our trained RL agent significantly outperforms human-designed heuristics, and effectively generalizes to larger instances and across IP problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-and-Cut algorithm, which is the backbone of state-of-the-art commercial IP solvers.
","['Columbia University', 'Columbia University', 'Columbia University']"
2020,Adaptive Droplet Routing in Digital Microfluidic Biochips Using Deep Reinforcement Learning,"Tung-Che Liang, Zhanwei Zhong, Yaas Bigdeli, Tsung-Yi Ho, Krishnendu Chakrabarty, Richard Fair",https://icml.cc/Conferences/2020/Schedule?showEvent=6330,"We present and investigate a novel application domain for deep reinforcement learning (RL): droplet routing on digital microfluidic biochips (DMFBs). A DMFB, composed of a two-dimensional electrode array, manipulates discrete fluid droplets to automatically execute biochemical protocols such as high-throughput DNA sequencing and point-of-care clinical diagnosis. However, a major concern associated with the use of DMFBs is that electrodes in a biochip can degrade over time. Droplet-transportation operations associated with the degraded electrodes can fail, thereby compromising the integrity of the bioassay outcome. While it is not feasible to detect the degradation of an electrode by simply examining its appearance, we show that casting droplet transportation as an RL problem enables the training of deep network policies to capture the underlying health conditions of electrodes and to provide reliable fluidic operations. We propose a new RL-based droplet-routing flow that can be used for various sizes of DMFBs, and demonstrate reliable execution of an epigenetic bioassay with the RL droplet router on a fabricated DMFB. To facilitate further research, we also present a simulation environment based on the OpenAI Gym Interface for RL-guided droplet-routing problems on DMFBs.
","['Duke University', 'Duke University', 'Duke Univsersity', 'National Tsing Hua University', 'Duke University', 'Duke University']"
2020,On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings,"Mahmoud Assran, Michael Rabbat",https://icml.cc/Conferences/2020/Schedule?showEvent=6664,"We study Nesterov's accelerated gradient method with constant step-size and momentum parameters in the stochastic approximation setting (unbiased gradients with bounded variance) and the finite-sum setting (where randomness is due to sampling mini-batches). To build better insight into the behavior of Nesterov's method in stochastic settings, we focus throughout on objectives that are smooth, strongly-convex, and twice continuously differentiable. In the stochastic approximation setting, Nesterov's method converges to a neighborhood of the optimal point at the same accelerated rate as in the deterministic setting. Perhaps surprisingly, in the finite-sum setting, we prove that Nesterov's method may diverge with the usual choice of step-size and momentum, unless additional conditions on the problem related to conditioning and data coherence are satisfied. Our results shed light as to why Nesterov's method may fail to converge or achieve acceleration in the finite-sum setting. 
","['McGill University; Mila; Facebook AI Research', 'Facebook']"
2020,Pretrained Generalized Autoregressive Model with Adaptive Probabilistic Label Clusters for Extreme Multi-label Text Classification,"Hui Ye, Zhiyu Chen, Da-Han Wang, Brian D Davison",https://icml.cc/Conferences/2020/Schedule?showEvent=5882,"Extreme multi-label text classification (XMTC) is a task for tagging a given text with the most relevant labels from an extremely large label set. We propose a novel deep learning method called APLC-XLNet. Our approach fine-tunes the recently released generalized autoregressive pretrained model (XLNet) to learn a dense representation for the input text. We propose Adaptive Probabilistic Label Clusters (APLC) to approximate the cross entropy loss by exploiting the unbalanced label distribution to form clusters that explicitly reduce the computational time. Our experiments, carried out on five benchmark datasets, show that our approach significantly outperforms existing state-of-the-art methods. Our source code is available publicly at https://github.com/huiyegit/APLC_XLNet.
","['Lehigh University', 'Lehigh University', 'Xiamen University of Technology', 'Lehigh University']"
2020,Coresets for Clustering in Graphs of Bounded Treewidth,"Daniel Baker, Vladimir Braverman, Lingxiao Huang, Shaofeng H.-C. Jiang, Robert Krauthgamer, Xuan Wu",https://icml.cc/Conferences/2020/Schedule?showEvent=6114,"We initiate the study of coresets for clustering in graph metrics, i.e., the shortest-path metric of edge-weighted graphs. Such clustering problems are essential to data analysis and used for example in road networks and data visualization. A coreset is a compact summary of the data that approximately preserves the clustering objective for every possible center set, and it offers significant efficiency improvements in terms of running time, storage, and communication, including in streaming and distributed settings. Our main result is a near-linear time construction of a coreset for k-Median in a general graph $G$, with size $O_{\epsilon, k}(\mathrm{tw}(G))$ where $\mathrm{tw}(G)$ is the treewidth of $G$, and we complement the construction with a nearly-tight size lower bound. The construction is based on the framework of Feldman and Langberg [STOC 2011], and our main technical contribution, as required by this framework, is a uniform bound of $O(\mathrm{tw}(G))$ on the shattering dimension under any point weights. We validate our coreset on real-world road networks, and our scalable algorithm constructs tiny coresets with high accuracy, which translates to a massive speedup of existing approximation algorithms such as local search for graph k-Median.","['Johns Hopkins University', 'Johns Hopkins University', 'Yale University', 'Weizmann Institute of Science', 'Weizmann Institute of Science', 'Johns Hopkins University']"
2020,The Non-IID Data Quagmire of Decentralized Machine Learning,"Kevin Hsieh, Amar Phanishayee, Onur Mutlu, Phillip Gibbons",https://icml.cc/Conferences/2020/Schedule?showEvent=6306,"Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across locations/devices. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization layers; and (iii) the degree of skewness is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the skew-induced accuracy loss of batch normalization.
","['Microsoft Research', 'Microsoft Research', 'ETH Zurich', 'CMU']"
2020,Adversarial Filters of Dataset Biases,"Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish Sabharwal, Yejin Choi",https://icml.cc/Conferences/2020/Schedule?showEvent=6628,"Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLITE, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLITE, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLITE is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92% to 62% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.
","['Allen Institute for AI', 'Allen Institute for AI', 'AI2', 'University of Washington', 'AI2', 'Allen Institute for AI', 'University of Washington']"
2020,Sample Amplification: Increasing Dataset Size even when Learning is Impossible,"Brian Axelrod, Shivam Garg, Vatsal Sharan, Gregory Valiant",https://icml.cc/Conferences/2020/Schedule?showEvent=6345,"Given data drawn from an unknown distribution, D, to what extent is it possible to amplify'' this dataset and faithfully output an even larger set of samples that appear to have been drawn from D? We formalize this question as follows: an (n,m) amplification procedure takes as input n independent draws from an unknown distribution D, and outputs a set of m > nsamples'' which must be indistinguishable from m samples drawn iid from D. We consider this sample amplification problem in two fundamental settings: the case where D is an arbitrary discrete distribution supported on k elements, and the case where D is a d-dimensional Gaussian with unknown mean, and fixed covariance matrix. Perhaps surprisingly, we show a valid amplification procedure exists for both of these settings, even in the regime where the size of the input dataset, n, is significantly less than what would be necessary to learn distribution D to non-trivial accuracy. We also show that our procedures are optimal up to constant factors.  Beyond these results, we describe potential applications of such data amplification, and formalize a number of curious directions for future research along this vein. 
","['Stanford', 'Stanford University', 'Stanford University', 'Stanford University']"
2020,Adversarial Mutual Information for Text Generation,"Boyuan Pan, Yazheng Yang, Kaizhao Liang, Bhavya Kailkhura, Zhongming Jin, Xian-Sheng Hua, Deng Cai, Bo Li",https://icml.cc/Conferences/2020/Schedule?showEvent=6796,"Recent advances in maximizing mutual information (MI) between the source and target have demonstrated its effectiveness in text generation. However, previous works paid little attention to modeling the backward network of MI (i.e., dependency from the target to the source), which is crucial to the tightness of the variational information maximization lower bound. In this paper, we propose Adversarial Mutual Information (AMI): a text generation framework which is formed as a novel saddle point (min-max) optimization aiming to identify joint interactions between the source and target. Within this framework, the forward and backward networks are able to iteratively promote or demote each other's generated instances by comparing the real and synthetic data distributions. We also develop a latent noise sampling strategy that leverages random variations at the high-level semantic space to enhance the long term dependency in the generation process. Extensive experiments based on different text generation tasks demonstrate that the proposed AMI framework can significantly outperform several strong baselines, and we also show that AMI has potential to lead to a tighter lower bound of maximum mutual information for the variational information maximization problem.
","['Zhejiang University', 'Zhejiang University', 'University of Illinois, Urbana Champaign', 'Lawrence Livermore National Laboratory', 'Alibaba Group', 'Alibaba Group', 'ZJU', 'UIUC']"
2020,Sparse Shrunk Additive Models,"guodong liu, Hong Chen, Heng Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=6090,"Most existing feature selection methods in literature are linear models, so that the nonlinear relations between features and response variables are not considered. Meanwhile, in these feature selection models, the interactions between features are often ignored or just discussed under prior structure information. To address these challenging issues, we consider the problem of sparse additive models for high-dimensional nonparametric regression with the allowance of the flexible interactions between features. A new method, called as sparse shrunk additive models (SSAM), is proposed to explore the structure information among features. This method bridges sparse kernel regression and sparse feature selection. Theoretical results on the convergence rate and sparsity characteristics of SSAM are established by the novel analysis techniques with integral operator and concentration estimate. In particular, our algorithm and theoretical analysis only require the component functions to be continuous and bounded, which are not necessary to be in reproducing kernel Hilbert spaces. Experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed approach.
","['university of Pittsburgh', 'Huazhong Agricultural University', 'University of Pittsburgh & JD Finance America Corporation']"
2020,Black-box Certification and Learning under Adversarial Perturbations,"Hassan Ashtiani, Vinayak Pathak, Ruth Urner",https://icml.cc/Conferences/2020/Schedule?showEvent=6708,"We formally study the problem of classification under adversarial perturbations, both from the learner's perspective, and from the viewpoint of a third-party who aims at certifying the robustness of a given black-box classifier. 
We  analyze a PAC-type framework of semi-supervised learning and identify possibility and impossibility results for proper learning of VC-classes in this setting.
We further introduce and study a new setting of black-box certification under limited query budget. We analyze this for various classes of predictors and types of perturbation. 
We also consider the viewpoint of a black-box adversary that aims at finding adversarial examples, showing that the existence of an adversary with polynomial query complexity implies the existence of a robust learner with small sample complexity.
","['McMaster University', 'Scotiabank', 'York University']"
2020,Communication-Efficient Distributed PCA by Riemannian Optimization,"Long-Kai Huang, Jialin Pan",https://icml.cc/Conferences/2020/Schedule?showEvent=6359,"In this paper, we study the leading eigenvector problem in a statistically distributed setting and propose a communication-efficient algorithm based on Riemannian optimization, which trades local computation for global communication. Theoretical analysis shows that the proposed algorithm linearly converges to the centralized empirical risk minimization solution regarding the number of communication rounds. When the number of data points in local machines is sufficiently large, the proposed algorithm achieves a significant reduction of communication cost over existing distributed PCA algorithms. Superior performance in terms of communication cost of the proposed algorithm is verified on real-world and synthetic datasets.
","['NTU, Singapore', 'NTU, Singapore']"
2020,Negative Sampling in Semi-Supervised learning,"John Chen, Vatsal Shah, Anastasios Kyrillidis",https://icml.cc/Conferences/2020/Schedule?showEvent=6132,"We introduce Negative Sampling in Semi-Supervised Learning (NS^3L), a simple, fast, easy to tune algorithm for semi-supervised learning (SSL). NS^3L is motivated by the success of negative sampling/contrastive estimation. We demonstrate that adding the NS^3L loss to state-of-the-art SSL algorithms, such as the Virtual Adversarial Training (VAT), significantly improves upon vanilla VAT and its variant, VAT with Entropy Minimization. By adding the NS^3L loss to MixMatch, the current state-of-the-art approach on semi-supervised tasks, we observe significant improvements over vanilla MixMatch. We conduct extensive experiments on the CIFAR10, CIFAR100, SVHN and STL10 benchmark datasets. Finally, we perform an ablation study for NS3L regarding its hyperparameter tuning.
","['Rice University', 'University of Texas at Austin', 'Rice University']"
2020,Structure Adaptive Algorithms for Stochastic Bandits,"Rémy Degenne, Han Shao, Wouter Koolen",https://icml.cc/Conferences/2020/Schedule?showEvent=6523,"We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are \emph{flexible} (in that they easily adapt to different structures), \emph{powerful} (in that they perform well empirically and/or provably match instance-dependent lower bounds) and \emph{efficient} in that the per-round computational burden is small.
We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the sub-optimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. 
Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.
","['Inria Paris', 'Toyota Technological Institute at Chicago', 'Centrum Wiskunde & Informatica, Amsterdam']"
2020,Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent,"Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, Adam Klivans",https://icml.cc/Conferences/2020/Schedule?showEvent=6700,"We give the first superpolynomial lower bounds for learning one-layer neural networks with respect to the Gaussian distribution for a broad class of algorithms.  In the regression setting, we prove that gradient descent run on any classifier with respect to square loss will fail to achieve small test error in polynomial time.  Prior work held only for gradient descent run with small batch sizes and sufficiently smooth classifiers. For classification, we give a stronger result, namely that any statistical query (SQ) algorithm will fail to achieve small test error in polynomial time.  Our lower bounds hold for commonly used activations such as ReLU and sigmoid.  The core of our result relies on a novel construction of a simple family of neural networks that are exactly orthogonal with respect to all spherically symmetric distributions.
","['University of Texas at Austin', 'University of Texas at Austin', 'Shanghai Jiao Tong University', 'University of Texas at Austin', 'University of Texas at Austin']"
2020,Stochastic Gradient and Langevin Processes,"Xiang Cheng, Dong Yin, Peter Bartlett, Michael Jordan",https://icml.cc/Conferences/2020/Schedule?showEvent=5923,"We prove quantitative convergence rates at which discrete Langevin-like processes converge to the invariant distribution of a related stochastic differential equation. We study the setup where the additive noise can be non-Gaussian and state-dependent and the potential function can be non-convex. We show that the key properties of these processes depend on the potential function and the second moment of the additive noise. We apply our theoretical findings to studying the convergence of Stochastic Gradient Descent (SGD) for non-convex problems and corroborate them with experiments using SGD to train deep neural networks on the CIFAR-10 dataset.
","['UC Berkeley', 'UC Berkeley', 'Berkeley', 'UC Berkeley']"
2020,Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent,"Yunwen Lei, Yiming Ying",https://icml.cc/Conferences/2020/Schedule?showEvent=5918,"Recently there are a considerable amount of work devoted to the study of the algorithmic stability and  generalization for stochastic gradient descent (SGD). However, the existing stability analysis requires to impose restrictive assumptions on the boundedness of gradients, smoothness and convexity of loss functions. In this paper, we provide a fine-grained analysis of stability and generalization for SGD by substantially relaxing these assumptions. Firstly, we establish stability and generalization for SGD by removing the existing bounded gradient assumptions. The key idea is the introduction of a new stability measure called on-average model stability, for which we develop novel bounds controlled by the risks of SGD iterates. This yields  generalization bounds depending on the behavior of the best model, and leads to the first-ever-known fast bounds in the low-noise setting using stability approach. Secondly, the smoothness assumption is relaxed by considering loss functions with Holder continuous (sub)gradients for which we show that optimal bounds are still achieved by balancing computation and stability. To our best knowledge, this gives the first-ever-known stability and generalization bounds for SGD with non-smooth loss functions (e.g., hinge loss). Finally, we study learning problems with (strongly) convex objectives but non-convex loss functions.
","['University of Kaiserslautern', 'SUNY Albany']"
2020,Alleviating Privacy Attacks via Causal Learning,"Shruti Tople, Amit Sharma, Aditya Nori",https://icml.cc/Conferences/2020/Schedule?showEvent=6346,"Machine learning models, especially deep neural networks have been shown to be  susceptible to privacy attacks such as membership inference where an adversary can detect whether a data point was used for training a black-box model. Such privacy risks are exacerbated when a model's predictions are used on an unseen data distribution. To alleviate privacy attacks, we demonstrate  the benefit of predictive models that are based on the causal relationship between input features and the outcome. We first show that models learnt using causal structure generalize better to unseen data, especially on data from different distributions than the train distribution. Based on this generalization property, we establish a theoretical link between causality and privacy: compared to associational models, causal models provide stronger differential privacy guarantees and are more robust to membership inference attacks. Experiments on simulated Bayesian networks and the colored-MNIST dataset show that associational models exhibit up to 80% attack accuracy under different test distributions and sample sizes whereas causal models exhibit attack accuracy close to a random guess.
","['Microsoft Research', 'Microsoft Research', 'Microsoft Research Cambridge']"
2020,Safe screening rules for L0-regression from Perspective Relaxations,"Alper Atamturk, Andres Gomez",https://icml.cc/Conferences/2020/Schedule?showEvent=6308,"We give safe screening rules to eliminate variables from regression with $\ell_0$
regularization or cardinality constraint. These rules are based on guarantees that a feature may or may not be selected in an optimal solution. The screening rules can be computed from a convex relaxation solution in linear time, without solving the L0-optimization problem. Thus, they can be used in a preprocessing step to safely remove variables from consideration apriori. Numerical experiments on real and synthetic data indicate that a significant number of the variables can be removed quickly, hence reducing the computational burden for optimization substantially. Therefore, the proposed fast and effective screening rules extend the scope of algorithms for L0-regression to larger data sets.","['UC Berkeley', 'USC']"
2020,Learning and Evaluating Contextual Embedding of Source Code,"Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi",https://icml.cc/Conferences/2020/Schedule?showEvent=6645,"Recent research has achieved impressive results on understanding and improving source code
by building up on machine-learning techniques developed for natural languages.
A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as BERT, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies.
However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate.
Specifically, first, we curate a massive, deduplicated corpus of 6M Python files from GitHub, which we use to pre-train CuBERT, an open-sourced code-understanding BERT model;
and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before.
We fine-tune CuBERT on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, BiLSTM and Transformer models, as well as published state-of-the-art models, showing that CuBERT outperforms them all, even with shorter training, and with fewer labeled examples.
Future work on source-code embedding can benefit from reusing our benchmark, and comparing against CuBERT models as a strong baseline.
","['Indian Institute of Science and Google Brain', 'Google Research', 'Google', 'Google']"
2020,Online mirror descent and dual averaging: keeping pace in the dynamic case,"Huang Fang, Nick Harvey, Victor Sanches Portella, Michael Friedlander",https://icml.cc/Conferences/2020/Schedule?showEvent=6068,"Online mirror descent (OMD) and dual averaging (DA)---two fundamental algorithms for online convex optimization---are known to have very similar (and sometimes identical) performance guarantees when used with a \emph{fixed} learning rate. Under \emph{dynamic} learning rates, however, OMD is provably inferior to DA and suffers a  linear regret, even in common settings such as prediction with expert advice. We modify the OMD algorithm through a simple technique that we call \emph{stabilization}. We give essentially the same abstract regret bound for OMD with stabilization and for DA by modifying the classical OMD convergence analysis in a careful and modular way that allows for straightforward and flexible proofs. Simple corollaries of these bounds show that OMD with stabilization and DA enjoy the same performance guarantees in many applications---even under dynamic learning rates. We also shed light on the similarities between OMD and DA and show simple conditions under which stabilized-OMD and DA generate the same iterates.
","['University of British Columbia', 'University of British Columbia', 'University of British Columbia', 'University of British Columbia']"
2020,Decoupled Greedy Learning of CNNs,"Eugene Belilovsky, Michael Eickenberg, Edouard Oyallon",https://icml.cc/Conferences/2020/Schedule?showEvent=6264,"A commonly cited inefficiency of neural network training by back-propagation is
the update locking problem: each layer must wait for the signal to propagate through
the network before updating. In recent years multiple authors have considered
alternatives that can alleviate this issue. In this context, we consider a simpler, but
more effective, substitute that uses minimal feedback, which we call Decoupled
Greedy Learning (DGL). It is based on a greedy relaxation of the joint training
objective, recently shown to be effective in the context of Convolutional Neural
Networks (CNNs) on large-scale image classification. We consider an optimization
of this objective that permits us to decouple the layer training, allowing for layers
or modules in networks to be trained with a potentially linear parallelization in
layers. We show theoretically and empirically that this approach converges. Then,
we empirically find that it can lead to better generalization than sequential greedy
optimization and sometimes end-to-end back-propagation. We show an extension
of this approach to asynchronous settings, where modules can operate with large
communication delays, is possible with the use of a replay buffer. We demonstrate
the effectiveness of DGL on the CIFAR-10 dataset against alternatives and on the
large-scale ImageNet dataset.
","['Mila', 'Flatiron Institute CCM', 'CNRS/LIP6']"
2020,Scaling up Hybrid Probabilistic Inference with Logical and Arithmetic Constraints via Message Passing,"Zhe Zeng, Paolo Morettin, Fanqi Yan, Antonio Vergari, Guy Van den Broeck",https://icml.cc/Conferences/2020/Schedule?showEvent=6688,"Weighted model integration (WMI) is an appealing framework for probabilistic inference: it allows for expressing the complex dependencies in real-world problems, where variables are both continuous and discrete, via the language of Satisfiability Modulo Theories (SMT), as well as to compute probabilistic queries with complex logical and arithmetic constraints. Yet, existing WMI solvers are not ready to scale to these problems. They either ignore the intrinsic dependency structure of the problem entirely, or they are limited to overly restrictive structures. To narrow this gap, we derive a factorized WMI computation enabling us to devise a scalable WMI solver based on message passing, called MP-WMI. Namely, MP-WMI is the first WMI solver that can (i) perform exact inference on the full class of tree-structured WMI problems, and (ii) perform inter-query amortization, e.g., to compute all marginal densities simultaneously. Experimental results show that our solver dramatically outperforms the existingWMI solvers on a large set of benchmarks.
","['University of California, Los Angeles', 'University of Trento', 'UCAS', 'University of California, Los Angeles', 'University of California, Los Angeles']"
2020,Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings,"Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, Dinesh Jayaraman",https://icml.cc/Conferences/2020/Schedule?showEvent=6633,"Reinforcement learning (RL) in real-world safety-critical target settings like urban driving is hazardous, imperiling the RL agent, other agents, and the environment. To overcome this difficulty, we propose a ""safety-critical adaptation"" task setting: an agent first trains in non-safety-critical ""source"" environments such as in a simulator, before it adapts to the target environment where failures carry heavy costs. We propose a solution approach, CARL, that builds on the intuition that prior experience in diverse environments equips an agent to estimate risk, which in turn enables relative safety through risk-averse, cautious adaptation. CARL first employs model-based RL to train a probabilistic model to capture uncertainty about transition dynamics and catastrophic states across varied source environments. Then, when exploring a new safety-critical environment with unknown dynamics, the CARL agent plans to avoid actions that could lead to catastrophic states. In experiments on car driving, cartpole balancing, and half-cheetah locomotion, CARL successfully acquires cautious exploration behaviors, yielding higher rewards with fewer failures than strong RL adaptation baselines.
","['UC Berkeley', 'UC Berkeley', 'Stanford', 'UC Berkeley', 'University of Pennsylvania']"
2020,Undirected Graphical Models as Approximate Posteriors,"Arash Vahdat, Evgeny Andriyash, William Macready",https://icml.cc/Conferences/2020/Schedule?showEvent=5975,"The representation of the approximate posterior is a critical aspect of effective variational autoencoders (VAEs). Poor choices for the approximate posterior have a detrimental impact on the generative performance of VAEs due to the mismatch with the true posterior. We extend the class of posterior models that may be learned by using undirected graphical models. We develop an efficient method to train undirected approximate posteriors by showing that the gradient of the training objective with respect to the parameters of the undirected posterior can be computed by backpropagation through Markov chain Monte Carlo updates. We apply these gradient estimators for training discrete VAEs with Boltzmann machines as approximate posteriors and demonstrate that undirected models outperform previous results obtained using directed graphical models. Our implementation is publicly available.
","['NVIDIA Research', 'D-Wave Systems Inc.', 'Sanctuary AI']"
2020,The Usual Suspects? Reassessing Blame for VAE Posterior Collapse,"Bin Dai, Ziyu Wang, David Wipf",https://icml.cc/Conferences/2020/Schedule?showEvent=6433,"In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions.  Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice.  However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks.  In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances.  Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.
","['Samsung Research China - Beijing', 'Tsinghua University', 'Microsoft Research']"
2020,Class-Weighted Classification: Trade-offs and Robust Approaches,"Ziyu Xu, Chen Dan, Justin Khim, Pradeep Ravikumar",https://icml.cc/Conferences/2020/Schedule?showEvent=5823,"We consider imbalanced classification, the problem in which a label may have low marginal probability relative to other labels, by weighting losses according to the correct class. 
First, we examine the convergence rates of the expected excess weighted risk of plug-in classifiers where the weighting for the plug-in classifier and the risk may be different.
This leads to irreducible errors that do not converge to the weighted Bayes risk, which motivates our consideration of robust risks.
We define a robust risk that minimizes risk over a set of weightings, show excess risk bounds for this problem, and demonstrate that particular choices of the weighting set leads to a special instance of conditional value at risk (CVaR) from stochastic programming, which we call label conditional value at risk (LCVaR).
Additionally, we generalize this weighting to derive a new robust risk problem that we call label heterogeneous conditional value at risk (LHCVaR).
Finally, we empirically demonstrate the efficacy of LCVaR and LHCVaR on improving class conditional risks.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2020,Spectral Graph Matching and Regularized Quadratic Relaxations: Algorithm and Theory,"Zhou Fan, Cheng Mao, Yihong Wu, Jiaming Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=6051,"Graph matching, also known as network alignment, aims at recovering the latent vertex correspondence between two unlabeled, edge-correlated weighted graphs. To tackle this task, we propose a spectral method, GRAph Matching by Pairwise eigen-Alignments (GRAMPA), which first constructs a similarity matrix as a weighted sum of outer products between all pairs of eigenvectors of the two graphs, and then outputs a matching by a simple rounding procedure. For a universality class of correlated Wigner models, GRAMPA achieves exact recovery of the latent matching between two graphs with edge correlation $1 - 1/\mathrm{polylog}(n)$ and average degree at least $\mathrm{polylog}(n)$. This matches the state-of-the-art guarantees for polynomial-time algorithms established for correlated Erd\H{o}s-R\'{e}nyi graphs, and significantly improves over existing spectral methods. The superiority of GRAMPA is also demonstrated on a variety of synthetic and real datasets, in terms of both statistical accuracy and computational efficiency.","['Yale Univ', 'Georgia Institute of Technology', 'Yale University', 'Duke University']"
2020,Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Search,"Yong Guo, Yaofo Chen, Yin Zheng, Peilin Zhao, Jian Chen, Junzhou Huang, Mingkui Tan",https://icml.cc/Conferences/2020/Schedule?showEvent=5803,"Neural architecture search (NAS) has become an important approach to automatically find effective architectures. To cover all possible good architectures, we need to search in an extremely large search space with billions of candidate architectures. More critically, given a large search space, we may face a very challenging issue of space explosion. However, due to the limitation of computational resources, we can only sample a very small proportion of the architectures, which provides insufficient information for the training. As a result, existing methods may often produce suboptimal architectures. To alleviate this issue, we propose a curriculum search method that starts from a small search space and gradually incorporates the learned knowledge to guide the search in a large space. With the proposed search strategy, our Curriculum Neural Architecture Search (CNAS) method significantly improves the search efficiency and finds better architectures than existing NAS methods. Extensive experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
","['South China University of Technology', 'South China University of Technology', 'Tencent', 'Artificial Intelligence Department, Ant \u200bFinancial', '""South China University of Technology, China""', 'University of Texas at Arlington / Tencent AI Lab', 'South China University of Technology']"
2020,Approximating Stacked and Bidirectional Recurrent Architectures with the Delayed Recurrent Neural Network,"Javier Turek, Shailee Jain, Vy Vo, Mihai Capotă, Alexander Huth, Theodore Willke",https://icml.cc/Conferences/2020/Schedule?showEvent=6693,"Recent work has shown that topological enhancements to recurrent neural networks (RNNs) can increase their expressiveness and representational capacity. Two popular enhancements are stacked RNNs, which increases the capacity for learning non-linear functions, and bidirectional processing, which exploits acausal information in a sequence. In this work, we explore the delayed-RNN, which is a single-layer RNN that has a delay between the input and output. We prove that a weight-constrained version of the delayed-RNN is equivalent to a stacked-RNN. We also show that the delay gives rise to partial acausality, much like bidirectional networks. Synthetic experiments confirm that the delayed-RNN can mimic bidirectional networks, solving some acausal tasks similarly, and outperforming them in others. Moreover, we show similar performance to bidirectional networks in a real-world natural language processing task. These results suggest that delayed-RNNs can approximate topologies including stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs -- but with equivalent or faster runtimes for the delayed-RNNs.
","['Intel Labs', 'The University of Texas at Austin', 'Intel Labs', 'Intel Labs', 'The University of Texas at Austin', 'Intel Corporation']"
2020,Self-Modulating Nonparametric Event-Tensor Factorization,"Zheng Wang, Xinqi Chu, Shandian Zhe",https://icml.cc/Conferences/2020/Schedule?showEvent=5981,"Tensor factorization is a fundamental framework to analyze high-order interactions in data. Despite the success of the existing methods, the valuable temporal information are severely underused. The timestamps of the interactions are either ignored or discretized into crude steps. The recent work although formulates event-tensors to keep the timestamps in factorization and can capture mutual excitation effects among the interaction events, it overlooks another important type of temporal influence, inhibition. In addition, it uses a local window to exclude all the long-term dependencies. To overcome these limitations, we propose a self-modulating nonparametric Bayesian factorization model. We use the latent factors to construct mutually governed, general random point processes, which can capture various short-term/long-term, excitation/inhibition effects, so as to encode the complex temporal dependencies into factor representations.  In addition, our model couples with a latent Gaussian process to estimate and fuse nonlinear yet static relationships between the entities. For efficient inference, we derive a fully decomposed model evidence lower bound to dispense with the huge kernel matrix and costly summations inside the rate and log rate functions. We then develop an efficient stochastic optimization algorithm. We show the advantage of our method in four real-world applications. 
","['University of Utah', 'Xjera Labs, Pte.Ltd', 'University of Utah']"
2020,Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations,"Florian Tramer, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, Joern-Henrik Jacobsen",https://icml.cc/Conferences/2020/Schedule?showEvent=6653,"Adversarial examples are malicious inputs crafted to induce misclassification. Commonly studied \emph{sensitivity-based} adversarial examples introduce semantically-small changes to an input that result in a different model prediction. 
This paper studies a complementary failure mode, \emph{invariance-based} adversarial examples, that introduce minimal semantic changes that modify an input's true label yet preserve the model's prediction.
We demonstrate fundamental tradeoffs between these two types of adversarial examples.
We show that defenses against sensitivity-based attacks 
actively harm a model's accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types.
In particular, we break state-of-the-art adversarially-trained and \emph{certifiably-robust} models by generating small perturbations that the models are (provably) robust to, yet that change an input's class according to human labelers.
Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of \emph{overly-robust} predictive features in standard datasets. 
","['Stanford University', 'University of Bremen', 'Google', 'University of Toronto and Vector Institute', 'Apple Inc.']"
2020,Predictive Multiplicity in Classification,"Charles Marx, Flavio Calmon, Berk Ustun",https://icml.cc/Conferences/2020/Schedule?showEvent=6624,"Prediction problems often admit competing models that perform almost equally well.  This effect challenges key assumptions in machine learning when competing models assign conflicting predictions. In this paper, we define predictive multiplicity as the ability of a prediction problem to admit competing models with conflicting predictions. We introduce measures to evaluate the severity of predictive multiplicity, and develop integer programming tools to compute these measures exactly for linear classification problems. We apply our tools to measure predictive multiplicity in recidivism prediction problems. Our results show that real-world datasets may admit competing models that assign wildly conflicting predictions, and motivate the need to report predictive multiplicity in model development.
","['Haverford College', 'Harvard University', 'Harvard University']"
2020,Planning to Explore via Self-Supervised World Models,"Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak",https://icml.cc/Conferences/2020/Schedule?showEvent=6556,"Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code: https://ramanans1.github.io/plan2explore/
","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'UC Berkeley & Covariant', 'Google Brain & University of Toronto', 'CMU, FAIR']"
2020,Identifying the Reward Function by Anchor Actions,"Sinong Geng, Houssam Nassif, Charlie Manzanares, Max Reppen, Ronnie Sircar",https://icml.cc/Conferences/2020/Schedule?showEvent=6529,"We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. We name our method PQR, as it sequentially estimates the Policy, the $Q$-function, and the Reward function. PQR does not assume that the reward solely depends on the state, instead it allows for a dependency on the choice of action. Moreover, PQR allows for stochastic state transitions. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the PQR reward estimator uniquely recovers the true reward. With unknown transitions, we bound the estimation error of PQR. Finally, the performance of PQR is demonstrated by synthetic and real-world datasets.","['Princeton University', 'amazon', 'Amazon', 'Princeton', 'Princeton']"
2020,A general recurrent state space framework for modeling neural dynamics during decision-making,"David Zoltowski, Jonathan Pillow, Scott Linderman",https://icml.cc/Conferences/2020/Schedule?showEvent=6043,"An open question in systems and computational neuroscience is how neural circuits accumulate evidence towards a decision. Fitting models of decision-making theory to neural activity helps answer this question, but current approaches limit the number of these models that we can fit to neural data. Here we propose a general framework for modeling neural activity during decision-making. The framework includes the canonical drift-diffusion model and enables extensions such as multi-dimensional accumulators, variable and collapsing boundaries, and discrete jumps. Our framework is based on constraining the parameters of recurrent state space models, for which we introduce a scalable variational Laplace EM inference algorithm. We applied the modeling approach to spiking responses recorded from monkey parietal cortex during two decision-making tasks. We found that a two-dimensional accumulator better captured the responses of a set of parietal neurons than a single accumulator model, and we identified a variable lower boundary in the responses of a parietal neuron during a random dot motion task. We expect this framework will be useful for modeling neural dynamics in a variety of decision-making settings.
","['Princeton University', 'Princeton University', 'Stanford']"
2020,Randomly Projected Additive Gaussian Processes for Regression,"Ian Delbridge, David S Bindel, Andrew Wilson",https://icml.cc/Conferences/2020/Schedule?showEvent=6466,"Gaussian processes (GPs) provide flexible distributions over functions, with inductive biases controlled by a kernel. However, in many applications 
Gaussian processes can struggle with even moderate input dimensionality. Learning a low dimensional projection can help alleviate this curse of dimensionality, but introduces many trainable hyperparameters, which can be cumbersome, especially in the small data regime. We use additive sums of kernels for GP regression, where each kernel operates on a different random projection of its inputs. Surprisingly, we find that as the number of random projections increases, the predictive performance of this approach quickly converges to the performance of a kernel operating on the original full dimensional inputs, over a wide range of data sets, even if we are projecting into a single dimension. As a consequence, many problems can remarkably be reduced to one dimensional input spaces, without learning a transformation. We prove this convergence and its rate, and additionally propose a deterministic approach that converges more quickly than purely random projections. Moreover, we demonstrate our approach can achieve faster inference and improved predictive accuracy for high-dimensional inputs compared to kernels in the original input space. 
","['Cornell University', 'Cornell University', 'New York University']"
2020,Polynomial Tensor Sketch for Element-wise Function of Low-Rank Matrix,"Insu Han, Haim Avron, Jinwoo Shin",https://icml.cc/Conferences/2020/Schedule?showEvent=6419,"This paper studies how to sketch element-wise functions of low-rank matrices. Formally, given low-rank matrix A = [Aij] and scalar non-linear function f, we aim for finding an approximated low-rank representation of the (possibly high-rank) matrix [f(Aij)]. To this end, we propose an efficient sketching-based algorithm whose complexity is significantly lower than the number of entries of A, i.e., it runs without accessing all entries of [f(Aij)] explicitly. The main idea underlying our method is to combine a polynomial approximation of f with the existing tensor sketch scheme for approximating monomials of entries of A. To balance the errors of the two approximation components in an optimal manner, we propose a novel regression formula to find polynomial coefficients given A and f. In particular, we utilize a coreset-based regression with a rigorous approximation guarantee. Finally, we demonstrate the applicability and superiority of the proposed scheme under various machine learning tasks.
","['KAIST', 'Tel Aviv University', 'KAIST']"
2020,Exploration Through Reward Biasing: Reward-Biased Maximum Likelihood Estimation for Stochastic Multi-Armed Bandits,"Xi Liu, Ping-Chun Hsieh, Yu Heng Hung, Anirban  Bhattacharya, P. Kumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6116,"Inspired by the Reward-Biased Maximum Likelihood Estimate method of adaptive control, we propose RBMLE -- a novel family of learning algorithms for stochastic multi-armed bandits (SMABs). For a broad range of SMABs including both the parametric Exponential Family as well as the non-parametric sub-Gaussian/Exponential family, we show that RBMLE yields an index policy.
To choose the bias-growth rate $\alpha(t)$ in RBMLE, we reveal the nontrivial interplay between $\alpha(t)$ and the regret bound that generally applies in both the Exponential Family as well as the sub-Gaussian/Exponential family bandits. To quantify the finite-time performance, we prove that RBMLE attains order-optimality by adaptively estimating the unknown constants in the expression of $\alpha(t)$ for Gaussian and sub-Gaussian bandits. Extensive experiments demonstrate that the proposed RBMLE achieves empirical regret performance competitive with the state-of-the-art methods, while being more computationally efficient and scalable in comparison to the best-performing ones among them.","['Texas A&M University', 'National Chiao Tung University', 'National Chiao Tung University', 'Texas A&M University', 'Texas A&M University']"
2020,InfoGAN-CR and ModelCentrality: Self-supervised Model Training and Selection for Disentangling GANs,"Zinan Lin, Kiran Thekumparampil, Giulia Fanti, Sewoong Oh",https://icml.cc/Conferences/2020/Schedule?showEvent=6490,"Disentangled generative models map a latent code vector to a target space, while enforcing that a subset of the learned latent codes are interpretable and associated with distinct properties of the target distribution. Recent advances have been dominated by Variational AutoEncoder (VAE)-based methods, while training disentangled generative adversarial networks (GANs) remains challenging. In this work, we show that the dominant challenges facing disentangled GANs can be mitigated through the use of self-supervision. We make two main contributions: first, we design a novel approach for training disentangled GANs with self-supervision. We propose contrastive regularizer, which is inspired by a natural notion of disentanglement: latent traversal. This achieves higher disentanglement scores than state-of-the-art VAE- and GAN-based approaches. Second, we propose an unsupervised model selection scheme called ModelCentrality, which uses generated synthetic samples to compute the medoid (multi-dimensional generalization of median) of a collection of models. Perhaps surprisingly, this unsupervised ModelCentrality is able to select a model that outperforms those trained with existing supervised hyper-parameter selection techniques. Combining contrastive regularization with ModelCentrality, we obtain state-of-the-art disentanglement scores by a substantial margin, without requiring supervised hyper-parameter selection.
","['Carnegie Mellon University', 'University of Illinois at Urbana-Champaign', 'CMU', 'University of Washington']"
2020,Boosting Deep Neural Network Efficiency with Dual-Module Inference,"Liu Liu, Lei Deng, Zhaodong Chen, yuke wang, Shuangchen Li, Jingwei Zhang, Yihua Yang, Zhenyu Gu, Yufei Ding, Yuan Xie",https://icml.cc/Conferences/2020/Schedule?showEvent=6670,"Using Deep Neural Networks (DNNs) in machine learning tasks is promising in delivering high-quality results but challenging to meet stringent latency requirements and energy constraints because of the memory-bound and the compute-bound execution pattern of DNNs. We propose a big-little dual-module inference to dynamically skip unnecessary memory access and computation to speedup DNN inference. Leveraging the error-resilient feature of nonlinear activation functions used in DNNs, we propose to use a lightweight little module that approximates the original DNN layer, which is referred to as the big module, to compute activations of the insensitive region that are more error-resilient. The expensive memory access and computation of the big module can be reduced as the results are only used in the sensitive region. For memory-bound models, our method can reduce the overall memory access by 40% on average and achieve 1.54x to 1.75x speedup on a commodity CPU-based server platform with a negligible impact on model quality. In addition, our method can reduce the operations of the compute-bound ResNet model by 3.02x, with only a 0.5% accuracy drop.
","['University of California, Santa Barbara', 'University of California, Santa Barbara', 'University of California, Santa Barbara', 'ucsb', 'Alibaba Inc.', 'Alibaba Inc.', 'Alibaba Inc.', 'Alibaba Inc.', 'University of California, Santa Barbara', 'University of California, Santa Barbara']"
2020,Strength from Weakness: Fast Learning Using Weak Supervision,"Joshua Robinson, Stefanie Jegelka, Suvrit Sra",https://icml.cc/Conferences/2020/Schedule?showEvent=6691,"We study generalization properties of weakly supervised learning, that is, learning where only a few ""strong"" labels (the actual target for prediction) are present but many more ""weak"" labels are available. In particular, we show that pretraining using weak labels and finetuning using strong can accelerate the learning rate for the strong task to the fast rate of O(1/n), where n is the number of strongly labeled data points. This acceleration can happen even if, by itself, the strongly labeled data admits only the slower  O(1/\sqrt{n}) rate. The acceleration depends continuously on the number of weak labels available, and on the relation between the two tasks. Our theoretical results are reflected empirically across a range of tasks and illustrate how weak labels speed up learning on the strong task.
","['MIT', 'MIT', 'MIT']"
2020,Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism,"Wang Chi Cheung, David Simchi-Levi, Ruihao Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=5829,"We consider un-discounted reinforcement learning (RL) in  Markov decision processes (MDPs) under drifting non-stationarity, \ie, both the reward and state transition distributions are allowed to evolve over time, as long as their respective total variations, quantified by suitable metrics, do not exceed certain \emph{variation budgets}. We first develop the Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence Widening (\texttt{SWUCRL2-CW}) algorithm, and establish its dynamic regret bound when the variation budgets are known. In addition, we propose the Bandit-over-Reinforcement Learning (\texttt{BORL}) algorithm to adaptively tune the \sw~to achieve the same dynamic regret bound, but  in a \emph{parameter-free} manner, \ie, without knowing the variation budgets. Notably, learning drifting MDPs via conventional optimistic exploration presents a unique challenge absent in existing (non-stationary) bandit learning settings. We overcome the challenge by a novel confidence widening technique that incorporates additional optimism.
","['National University of Singapore', 'MIT', 'MIT']"
2020, Predicting deliberative outcomes,"Vikas K Garg, Tommi Jaakkola",https://icml.cc/Conferences/2020/Schedule?showEvent=6707,"We extend structured prediction to deliberative outcomes. Specifically, we learn parameterized games that can map any inputs to equilibria as the outcomes. Standard structured prediction models rely heavily on global scoring functions and are therefore unable to model individual player preferences or how they respond to others asymmetrically. Our games take as input, e.g., UN resolution to be voted on, and map such contexts to initial strategies, player utilities, and interactions. Players are then thought to repeatedly update their strategies in response to weighted aggregates of other players' choices towards maximizing their individual utilities. The output from the game is a sample from the resulting (near) equilibrium mixed strategy profile. We characterize conditions under which players' strategies converge to an equilibrium in such games and when the game parameters can be provably recovered from observations. Empirically, we demonstrate on two real voting datasets that our games can recover interpretable strategic interactions, and predict strategies for players in new settings.
","['Massachusetts Institute of Technology', 'MIT']"
2020,Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks,"Ahmed T. Elthakeb, Prannoy Pilligundla, FatemehSadat Mireshghallah, Alexander Cloninger, Hadi Esmaeilzadeh",https://icml.cc/Conferences/2020/Schedule?showEvent=6676,"The deep layers of modern neural networks extract a rather rich set of features as an input propagates through the network, this paper sets out to harvest these rich intermediate representations for quantization with minimal accuracy loss while significantly reducing the memory footprint and compute intensity of the DNN. This paper utilizes knowledge distillation through teacher-student paradigm (Hinton
et al., 2015) in a novel setting that exploits the feature extraction capability of DNNs for higher accuracy quantization. As such, our algorithm logically divides a pretrained full-precision DNN to multiple sections, each of which exposes intermediate features to train a team of students independently
in the quantized domain and simply stitching them afterwards. This divide and conquer strategy, makes the training of each student section possible in isolation, speeding up training by enabling parallelization. Experiments on various DNNs (AlexNet, LeNet, MobileNet, ResNet-18, ResNet-20, SVHN and VGG-11) show that, this approach—called DCQ (Divide and Conquer Quantization)—on average, improves the performance of a state-of-the-art quantized training technique, DoReFa-Net (Zhou et al., 2016) by 21.6% and 9.3% for binary and ternary quantization, respectively. Additionally, we show that incorporating DCQ to existing quantized training methods leads to improved accuracies as compared to previously reported by multiple state-of-the-art quantized training methods.
","['University of California, San Diego', 'University of California, San Diego', 'University of California San Diego', 'University of California San Diego', 'University of California, San Diego']"
2020,Robust Outlier Arm Identification,"Yinglun Zhu, Sumeet Katariya, Robert Nowak",https://icml.cc/Conferences/2020/Schedule?showEvent=5992,"We study the problem of Robust Outlier Arm Identification (ROAI), where the goal is to identify arms whose expected rewards deviate substantially from the majority, by adaptively sampling from their reward distributions. We compute the outlier threshold using the median and median absolute deviation of the expected rewards. This is a robust choice for the threshold compared to using the mean and standard deviation, since it can correctly identify outlier arms even in the presence of extreme outlier values. Our setting is different from existing pure exploration problems where the threshold is pre-specified as a given value or rank. This is useful in applications where the goal is to identify the set of promising items but the cardinality of this set is unknown, such as finding promising drugs for a new disease or identifying items favored by a population. We propose two computationally efficient $\delta$-PAC algorithms for ROAI, which includes the first UCB-style algorithm for outlier detection, and derive upper bounds on their sample complexity. We also prove a matching, up to logarithmic factors, worst case lower bound for the problem, indicating that our upper bounds are generally unimprovable. Experimental results show that our algorithms are both robust and at least $5$x sample efficient compared to state-of-the-art.","['University of Wisconsin-Madison', 'UW-Madison and Amazon', 'University of Wisconsion-Madison']"
2020,Proper Network Interpretability Helps Adversarial Robustness in Classification,"Akhilan Boopathy, Sijia Liu, Gaoyuan Zhang, Cynthia Liu, Pin-Yu Chen, Shiyu Chang, Luca Daniel",https://icml.cc/Conferences/2020/Schedule?showEvent=6031,"Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, 
as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classification and robust interpretation, outperforming state-of-the-art adversarial training methods against attacks of large perturbation in particular.
","['MIT', 'MIT-IBM Watson AI Lab', 'IBM Research', 'Massachusetts Institute of Technology', 'IBM Research AI', 'MIT-IBM Watson AI Lab', 'Massachusetts Institute of Technology']"
2020,Sets Clustering,"Ibrahim Jubran, Murad Tukan, Alaa Maalouf, Dan Feldman",https://icml.cc/Conferences/2020/Schedule?showEvent=6396,"The input to the \emph{sets-$k$-means} problem is an integer $k\geq 1$ and a set $\mathcal{P}=\{P_1,\cdots,P_n\}$ of fixed sized sets in $\mathbb{R}^d$. The goal is to compute a set $C$ of $k$ centers (points) in $\mathbb{R}^d$ that minimizes the sum $\sum_{P\in \mathcal{P}} \min_{p\in P, c\in C}\left\| p-c \right\|^2$ of squared distances to these sets.
An  \emph{$\varepsilon$-core-set} for this problem is a weighted subset of $\mathcal{P}$ that approximates this sum up to $1\pm\varepsilon$ factor, for \emph{every} set $C$ of $k$ centers in $\mathbb{R}^d$.
We prove that such a core-set of $O(\log^2{n})$ sets always exists, and can be computed in $O(n\log{n})$ time, for every input $\mathcal{P}$ and every fixed $d,k\geq 1$ and $\varepsilon \in (0,1)$. The result easily generalized for any metric space, distances to the power of $z>0$, and M-estimators that handle outliers. Applying an inefficient but optimal algorithm on this coreset allows us to obtain the first PTAS ($1+\varepsilon$ approximation) for the sets-$k$-means problem that takes time near linear in $n$.
This is the first result even for sets-mean on the plane ($k=1$, $d=2$).
Open source code and experimental results for document classification and facility locations are also provided.","['The University of Haifa', 'University of Haifa', 'The University of Haifa', 'The University of Haifa']"
2020,Budgeted Online Influence Maximization,"Pierre Perrault, Jennifer Healey, Zheng Wen, Michal Valko",https://icml.cc/Conferences/2020/Schedule?showEvent=6378,"We introduce a new budgeted framework for online influence maximization,   considering the total cost of an advertising campaign instead of the common  cardinality constraint on a chosen influencer set. Our approach models better the real-world setting where the cost of influencers varies and advertizers want to find the best value for their overall social advertising budget. We propose an algorithm assuming an independent cascade diffusion model and edge-level semi-bandit feedback, and provide both theoretical and experimental results.  Our analysis is also valid for the cardinality-constraint setting and improves the state of the art regret bound in this case.
","['ENS Paris-Saclay, Inria', 'Adobe', 'DeepMind', 'DeepMind']"
2020,Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning,"Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, Jinwoo Shin",https://icml.cc/Conferences/2020/Schedule?showEvent=6363,"Model-based reinforcement learning (RL) enjoys several benefits, such as data-efficiency and planning, by learning a model of the environment's dynamics. However, learning a global model that can generalize across different dynamics remains a challenge. To tackle this problem, we decompose the task of learning a global dynamics model into two stages: (a) learning a context latent vector that captures the local dynamics, then (b) predicting the next state conditioned on it. In order to encode dynamics-specific information into the context latent vector, we introduce a novel loss function that encourages the context latent vector to be useful for predicting both forward and backward dynamics. The proposed method achieves superior generalization ability across various simulated robotics and control tasks, compared to existing RL schemes.
","['UC Berkeley', 'KAIST', 'KAIST', 'Google / U. Michigan', 'KAIST']"
2020,(Locally) Differentially Private Combinatorial Semi-Bandits,"Xiaoyu Chen, Kai Zheng, Zixin Zhou, Yunchang Yang, Wei Chen, Liwei Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6315,"In this paper, we study Combinatorial Semi-Bandits (CSB) that is an extension of classic Multi-Armed Bandits (MAB) under Differential Privacy (DP) and stronger Local Differential Privacy (LDP) setting. Since the server receives more information from users in CSB, it usually causes additional dependence on the dimension of data, which is a notorious side-effect for privacy preserving learning. However for CSB under two common smoothness assumptions, we show it is possible to remove this side-effect. In detail, for $B_{\infty}$-bounded smooth CSB under either $\varepsilon$-LDP or $\varepsilon$-DP, we prove the optimal regret bound is $\Theta(\frac{mB^2_{\infty}\ln T } {\Delta\varepsilon^2})$ or $\tilde{\Theta}(\frac{mB^2_{\infty}\ln T} { \Delta\varepsilon})$ respectively, where $T$ is time period,  $\Delta$ is the gap of rewards and $m$ is the number of base arms, by proposing novel algorithms and matching lower bounds. For $B_1$-bounded smooth CSB under $\varepsilon$-DP, we also prove the optimal regret bound is $\tilde{\Theta}(\frac{mKB^2_1\ln T} {\Delta\varepsilon})$ with both upper bound and lower bound, where $K$ is the maximum number of feedback in each round. All above results nearly match corresponding non-private optimal rates, which imply there is no additional price for (locally) differentially private CSB in above common settings. 
","['Peking University', 'Peking University', 'Peking University', 'Center for Data Science, Peking University', 'Microsoft', 'Peking University']"
2020,Frustratingly Simple Few-Shot Object Detection,"Xin Wang, Thomas Huang, Joseph E Gonzalez, Trevor Darrell, Fisher Yu",https://icml.cc/Conferences/2020/Schedule?showEvent=6261,"Detecting rare objects from a few examples is an emerging problem. Prior works show meta-learning is a promising approach. But, fine-tuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by roughly 2~20 points on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: PASCAL VOC, COCO and LVIS. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks. The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection.
","['UC Berkeley', 'University of Michigan', 'UC Berkeley', 'University of California at Berkeley', 'UC Berkeley']"
2020,Efficient Policy Learning from Surrogate-Loss Classification Reductions,"Andrew Bennett, Nathan Kallus",https://icml.cc/Conferences/2020/Schedule?showEvent=6218,"Recent work on policy learning from observational data has highlighted the importance of efficient policy evaluation and has proposed reductions to weighted (cost-sensitive) classification. But, efficient policy evaluation need not yield efficient estimation of policy parameters. We consider the estimation problem given by a weighted surrogate-loss classification with any score function, either direct, inverse-propensity-weighted, or doubly robust. We show that, under a correct specification assumption, the weighted classification formulation need not be efficient for policy parameters. We draw a contrast to actual (possibly weighted) binary classification, where correct specification implies a parametric model, while for policy learning it only implies a semi-parametric model. In light of this, we instead propose an estimation approach based on generalized method of moments, which is efficient for the policy parameters. We propose a particular method based on recent developments on solving moment problems using neural networks and demonstrate the efficiency and regret benefits of this method empirically.
","['Cornell University', 'Cornell University']"
2020,Imputer: Sequence Modelling via Imputation and Dynamic Programming,"William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, Navdeep Jaitly",https://icml.cc/Conferences/2020/Schedule?showEvent=5985,"This paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generation model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.
","['Google Brain', 'Google', 'Google', 'Google Research, Brain Team', 'D. E. Shaw']"
2020,Learning to Score Behaviors for Guided Policy Optimization,"Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Krzysztof Choromanski, Anna Choromanska, Michael Jordan",https://icml.cc/Conferences/2020/Schedule?showEvent=6184,"We introduce a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. We show that by utilizing the dual formulation of the WD, we can learn score functions over policy behaviors that can in turn be used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, the dual formulation allows us to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. We incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-Guided Evolution Strategies, which we demonstrate can outperform existing methods in a variety of challenging environments. We also provide an open source demo.
","['UC Berkeley', 'University of Oxford', 'Columbia University', 'Google', 'NYU Tandon School of Engineering', 'UC Berkeley']"
2020,Understanding Self-Training for Gradual Domain Adaptation,"Ananya Kumar, Tengyu Ma, Percy Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6815,"Machine learning systems must adapt to data distributions that evolve over time, in applications ranging from sensor networks and self-driving car perception modules to brain-machine interfaces. Traditional domain adaptation is only guaranteed to work when the distribution shift is small; empirical methods combine several heuristics for larger shifts but can be dataset specific. To adapt to larger shifts we consider gradual domain adaptation, where the goal is to adapt an initial classifier trained on a source domain given only unlabeled data that shifts gradually in distribution towards a target domain. We prove the first non-vacuous upper bound on the error of self-training with gradual shifts, under settings where directly adapting to the target domain can result in unbounded error. The theoretical analysis leads to algorithmic insights, highlighting that regularization and label sharpening are essential even when we have infinite data. Leveraging the gradual shift structure leads to higher accuracies on a rotating MNIST dataset, a forest Cover Type dataset, and a realistic Portraits dataset.
","['Stanford University', 'Stanford', 'Stanford University']"
2020,Private Outsourced Bayesian Optimization,"Dmitrii Kharkovskii, Zhongxiang Dai, Bryan Kian Hsiang Low",https://icml.cc/Conferences/2020/Schedule?showEvent=6783,"This paper presents the private-outsourced-Gaussian process-upper confidence bound (PO-GP-UCB) algorithm, which is the first algorithm for privacy-preserving Bayesian optimization (BO) in the outsourced setting with a provable performance guarantee. We consider the outsourced setting where the entity holding the dataset and the entity performing BO are represented by different parties, and the dataset cannot be released non-privately. For example, a hospital holds a dataset of sensitive medical records and outsources the BO task on this dataset to an industrial AI company. The key idea of our approach is to make the BO performance of our algorithm similar to that of non-private GP-UCB run using the original dataset, which is achieved by using a random projection-based transformation that preserves both privacy and the pairwise distances between inputs. Our main theoretical contribution is to show that a regret bound similar to that of the standard GP-UCB algorithm can be established for our PO-GP-UCB algorithm. We empirically evaluate the performance of our PO-GP-UCB algorithm with synthetic and real-world datasets.
","['National University of Singapore', 'National University of Singapore', 'National University of Singapore']"
2020,Optimizing Black-box Metrics with Adaptive Surrogates,"Qijia Jiang, Olaoluwa Adigun, Harikrishna Narasimhan, Mahdi Milani Fard, Maya Gupta",https://icml.cc/Conferences/2020/Schedule?showEvent=6041,"We address the problem of training models with black-box and hard-to-optimize metrics by expressing the metric as a monotonic function of a small number of easy-to-optimize surrogates. We pose the training problem as an optimization over a relaxed surrogate space, which we solve by estimating local gradients for the metric and performing inexact convex projections. We analyze gradient estimates based on finite differences and local linear interpolations, and show convergence of our approach under smoothness assumptions with respect to the surrogates. Experimental results on classification and ranking problems verify the proposal performs on par with methods that know the mathematical formulation, and adds notable value when the form of the metric is unknown.
","['Stanford University', 'University of Southern California Los Angeles', 'Google Research', 'Google', 'Google']"
2020,On conditional versus marginal bias in multi-armed bandits,"Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo",https://icml.cc/Conferences/2020/Schedule?showEvent=6720,"The bias of the sample means of the arms in multi-armed bandits is an important issue in adaptive data analysis that has recently received considerable attention in the literature. Existing results relate in precise ways the sign and magnitude of the bias to various sources of data adaptivity, but do not apply to the conditional inference setting in which the sample means are computed only if some specific conditions are satisfied. In this paper, we characterize the sign of the conditional bias of monotone functions of the rewards, including the sample mean. Our results hold for arbitrary conditioning events and leverage natural monotonicity properties of the data collection policy. We further demonstrate, through several examples from sequential testing and best arm identification, that the sign of the conditional and marginal bias of the sample mean of an arm can be different, depending on the conditioning event. Our analysis offers new and interesting perspectives on the subtleties of assessing the bias in data adaptive settings.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2020,Circuit-Based Intrinsic Methods to Detect Overfitting,"Satrajit Chatterjee, Alan Mishchenko",https://icml.cc/Conferences/2020/Schedule?showEvent=6717,"The focus of this paper is on intrinsic methods to detect overfitting. By intrinsic methods, we mean methods that rely only on the model and the training data, as opposed to traditional methods (we call them extrinsic methods) that rely on performance on a test set or on bounds from model complexity. We propose a family of intrinsic methods called Counterfactual Simulation (CFS) which analyze the flow of training examples through the model by identifying and perturbing rare patterns. By applying CFS to logic circuits we get a method that has no hyper-parameters and works uniformly across different types of models such as neural networks, random forests and lookup tables. Experimentally, CFS can separate models with different levels of overfit using only their logic circuit representations without any access to the high level structure. By comparing lookup tables, neural networks, and random forests using CFS, we get insight into why neural networks generalize. In particular, we find that stochastic gradient descent in neural nets does not lead to ""brute force"" memorization, but finds common patterns (whether we train with actual or randomized labels), and neural networks are not unlike forests in this regard. Finally, we identify a limitation with our proposal that makes it unsuitable in an adversarial setting, but points the way to future work on robust intrinsic methods.
","['Google', 'UC Berkeley']"
2020,Choice Set Optimization Under Discrete Choice Models of Group Decisions,"Kiran Tomlinson, Austin Benson",https://icml.cc/Conferences/2020/Schedule?showEvent=6069,"The way that people make choices or exhibit preferences can be strongly affected by the set of available alternatives, often called the choice set. Furthermore, there are usually heterogeneous preferences, either at an individual level within small groups or within sub-populations of large groups. Given the availability of choice data, there are now many models that capture this behavior in order to make effective predictions---however, there is little work in understanding how directly changing the choice set can be used to influence the preferences of a collection of decision-makers. Here, we use discrete choice modeling to develop an optimization framework of such interventions for several problems of group influence, namely maximizing agreement or disagreement and promoting a particular choice. We show that these problems are NP-hard in general, but imposing restrictions reveals a fundamental boundary:  promoting a choice can be easier than encouraging consensus or sowing discord. We design approximation algorithms for the hard problems and show that they work well on real-world choice data.
","['Cornell University', 'Cornell University']"
2020,A Sequential Self Teaching Approach for Improving Generalization in Sound Event Recognition,"Anurag Kumar, Vamsi Krishna Ithapu",https://icml.cc/Conferences/2020/Schedule?showEvent=6711,"An important problem in machine auditory perception is to recognize and detect sound events. In this paper, we propose a sequential self-teaching approach to learn sounds. Our main proposition is that it is harder to learn sounds in adverse situations such as from weakly labeled and/or noisy labeled data,  and in these situations a single stage of learning is not sufficient. Our proposal is a sequential stage-wise learning process that improves generalization capabilities of a given modeling system. We justify this method via technical results and on Audioset, the largest sound events dataset, our sequential learning approach can lead to up to 9% improvement in performance. A comprehensive evaluation also shows that the method leads to improved transferability of knowledge from previously trained models, thereby leading to improved generalization capabilities on transfer learning tasks. 
","['Facebook Reality Labs', 'Facebook Reality Labs']"
2020,Representing Unordered Data Using Complex-Weighted Multiset Automata,"Justin DeBenedetto, David Chiang",https://icml.cc/Conferences/2020/Schedule?showEvent=6660,"Unordered, variable-sized inputs arise in many settings across multiple fields. The ability for set- and multiset-oriented neural networks to handle this type of input has been the focus of much work in recent years. We propose to represent multisets using complex-weighted \emph{multiset automata} and show how the multiset representations of certain existing neural architectures can be viewed as special cases of ours. Namely, (1) we provide a new theoretical and intuitive justification for the Transformer model's representation of positions using sinusoidal functions, and (2) we extend the DeepSets model to use complex numbers, enabling it to outperform the existing model on an extension of one of their tasks. 
","['University of Notre Dame', 'University of Notre Dame']"
2020,Inductive Relation Prediction by Subgraph Reasoning,"Komal Teru, Etienne Denis, Will Hamilton",https://icml.cc/Conferences/2020/Schedule?showEvent=6618,"The dominant paradigm for relation prediction in knowledge graphs involves learning and operating on latent representations (i.e., embeddings) of entities and relations. However, these embedding-based methods do not explicitly capture the compositional logical rules underlying the knowledge graph,  and they are limited to the transductive setting,  where the full set of entities must be known during training. Here, we propose a graph neural network based relation prediction framework,  GraIL, that reasons over local subgraph structures and has a strong inductive bias to learn entity-independent relational semantics.  Unlike embedding-based models, GraIL is naturally inductive and can generalize to unseen entities and graphs after training. We provide theoretical proof and strong empirical evidence that GraIL can rep-resent a useful subset of first-order logic and show that GraIL outperforms existing rule-induction baselines in the inductive setting. We also demonstrate significant gains obtained by ensembling GraIL with various knowledge graph embedding methods in the transductive setting, highlighting the complementary inductive bias of our method.
","['Mila, McGill University', 'McGill', 'McGill University and Mila']"
2020,Individual Calibration with Randomized Forecasting,"Shengjia Zhao, Tengyu Ma, Stefano Ermon",https://icml.cc/Conferences/2020/Schedule?showEvent=6546,"Machine learning applications often require calibrated predictions, e.g. a 90\% credible interval should contain the true outcome 90\% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.  
","['Stanford University', 'Stanford', 'Stanford University']"
2020,Structured Linear Contextual Bandits: A Sharp and Geometric Smoothed Analysis,"Vidyashankar Sivakumar, Steven Wu, Arindam Banerjee",https://icml.cc/Conferences/2020/Schedule?showEvent=6494,"Bandit learning algorithms typically involve the balance of exploration and exploitation. However, in many practical applications, worst-case scenarios needing systematic exploration are seldom encountered. In this work, we consider a smoothed setting for structured linear contextual bandits where the adversarial contexts are perturbed by Gaussian noise and the unknown parameter $\theta^*$ has structure, e.g., sparsity, group sparsity, low rank, etc. We propose simple greedy algorithms for both the single- and multi-parameter (i.e., different parameter for each context) settings and provide a unified regret analysis for $\theta^*$ with any assumed structure. The regret bounds are expressed in terms of geometric quantities such as Gaussian widths associated with the structure of $\theta^*$. We also obtain sharper regret bounds compared to earlier work for the unstructured $\theta^*$ setting as a consequence of our improved analysis. We show there is implicit exploration in the smoothed setting where a simple greedy algorithm works.","['Walmart Labs', 'University of Minnesota', 'University of Minnesota']"
2020,On Coresets for Regularized Regression,"Rachit Chhaya, Supratim Shit, Anirban Dasgupta",https://icml.cc/Conferences/2020/Schedule?showEvent=6377,"We study the effect of norm based regularization on the size of coresets for regression problems. Specifically, given a matrix $ \mathbf{A} \in {\mathbb{R}}^{n \times d}$ with $n\gg d$ and a vector $\mathbf{b} \in \mathbb{R} ^ n $ and $\lambda > 0$, we analyze the size of coresets for regularized versions of regression of the form $\|\mathbf{Ax}-\mathbf{b}\|_p^r + \lambda\|{\mathbf{x}}\|_q^s$. Prior work has shown that for ridge regression (where $p,q,r,s=2$) we can obtain a coreset that is smaller than the coreset for the unregularized counterpart i.e. least squares regression~\cite{avron2017sharper}. We show that when $r \neq s$, no coreset for regularized regression can have size smaller than the optimal coreset of the unregularized version. The well known lasso problem falls under this category and hence does not allow a coreset smaller than the one for least squares regression. We propose a modified version of the lasso problem and obtain for it a coreset of size smaller than the least square regression. We empirically show that the modified version of lasso also induces sparsity in solution, similar to the original lasso. We also obtain smaller coresets for $\ell_p$ regression with $\ell_p$ regularization. We extend our methods to multi response regularized regression. Finally, we empirically demonstrate the coreset performance for the modified lasso and the $\ell_1$ regression with $\ell_1$  regularization.","['IIT Gandhinagar', 'IIT Gandhinagar', 'IIT Gandhinagar']"
2020,Optimal transport mapping via input convex neural networks,"Ashok Vardhan Makkuva, Amirhossein Taghvaei, Sewoong Oh, Jason Lee",https://icml.cc/Conferences/2020/Schedule?showEvent=6233,"In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework to estimate the optimal transport mapping as the gradient of a convex function that is trained via minimax optimization. Numerical experiments confirm the accuracy of the learned transport map. Our approach can be readily used to train a deep generative model. When trained between a simple distribution in the latent space and a target distribution, the learned optimal transport map acts as a deep generative model. Although scaling this to a large dataset is challenging, we demonstrate two important strengths over standard adversarial training: robustness and discontinuity. As we seek the optimal transport, the learned generative model provides the same mapping regardless of how we initialize the neural networks. Further, a gradient of a neural network can easily represent discontinuous mappings, unlike standard neural networks that are constrained to be continuous. This allows the learned transport map to match any target distribution with many discontinuous supports and achieve sharp boundaries.
","['UIUC', 'University of Illinois at Urbana-Champaign', 'University of Washington', 'Princeton']"
2020,Approximation Capabilities of Neural ODEs and Invertible Residual Networks,"Han Zhang, Xi Gao, Jacob Unterman, Tomasz Arodz",https://icml.cc/Conferences/2020/Schedule?showEvent=5911,"Recent interest in invertible models and normalizing flows has resulted in new architectures that ensure invertibility of the network model. Neural ODEs and i-ResNets are two recent techniques for constructing models that are invertible, but it is unclear if they can be used to approximate any continuous invertible mapping. Here, we show that out of the box, both of these architectures are limited in their approximation capabilities. We then show how to overcome this limitation: we prove that any homeomorphism on a $p$-dimensional Euclidean space can be approximated by a Neural ODE or an i-ResNet operating on a $2p$-dimensional Euclidean space. We conclude by showing that capping a Neural ODE or an i-ResNet with a single linear layer is sufficient to turn the model into a universal approximator for non-invertible continuous functions.","['Virginia Commonwealth University', 'Virginia Commonwealth University', 'Virginia Commonwealth University', 'Virginia Commonwealth University']"
2020,Optimization Theory for ReLU Neural Networks Trained with Normalization Layers,"Yonatan Dukler, Quanquan Gu, Guido Montufar",https://icml.cc/Conferences/2020/Schedule?showEvent=6769,"The current paradigm of deep neural networks has been successful in part due to the use of normalization layers. Normalization layers like Batch Normalization, Layer Normalization and Weight Normalization are ubiquitous in practice as they improve the generalization performance and training speed of neural networks significantly. Nonetheless, the vast majority of current deep learning theory and non-convex optimization literature focuses on the un-normalized setting. 
We bridge this gap by providing the first global convergence result for 2 layer non-linear neural networks with ReLU activations trained with a normalization layer, namely Weight Normalization. The analysis shows how the introduction of normalization layers changes the optimization landscape and in some settings enables faster convergence as compared with un-normalized neural networks. 
","['UCLA', 'University of California, Los Angeles', 'UCLA']"
2020,Margin-aware Adversarial Domain Adaptation with Optimal Transport,"Sofien Dhouib, Ievgen Redko, Carole Lartizien",https://icml.cc/Conferences/2020/Schedule?showEvent=6194,"In this paper, we propose a new theoretical analysis of unsupervised domain adaptation that relates notions of large margin separation, adversarial learning and optimal transport. This analysis generalizes previous work on the subject by providing a bound on the target margin violation rate, thus reflecting a better control of the quality of separation between classes in the target domain than bounding the misclassification rate. The bound also highlights the benefit of a large margin separation on the source domain for adaptation and introduces an optimal transport (OT) based distance between domains that has the virtue of being task-dependent, contrary to other approaches. From the obtained theoretical results, we derive a novel algorithmic solution for domain adaptation that introduces a novel shallow OT-based adversarial approach and outperforms other OT-based DA baselines on several simulated and real-world classification tasks.
","['CREATIS Laboratory INSA Lyon', 'Laboratoire Hubert Curien', 'CREATIS']"
2020,Flexible and Efficient Long-Range Planning Through Curious Exploration,"Aidan Curtis, Minjian Xin, Dilip Arumugam, Kevin Feigelis, Daniel Yamins",https://icml.cc/Conferences/2020/Schedule?showEvent=6776,"Identifying algorithms that flexibly and efficiently discover temporally-extended multi-phase plans is an essential step for the advancement of robotics and model-based reinforcement learning. The core problem of long-range planning is finding an efficient way to search through the tree of possible action sequences. Existing non-learned planning solutions from the Task and Motion Planning (TAMP) literature rely on the existence of logical descriptions for the effects and preconditions for actions. This constraint allows TAMP methods to efficiently reduce the tree search problem but limits their ability to generalize to unseen and complex physical environments. In contrast, deep reinforcement learning (DRL) methods use flexible neural-network-based function approximators to discover policies that generalize naturally to unseen circumstances. However, DRL methods struggle to handle the very sparse reward landscapes inherent to long-range multi-step planning situations. Here, we propose the Curious Sample Planner (CSP), which fuses elements of TAMP and DRL by combining a curiosity-guided sampling strategy with imitation learning to accelerate planning. We show that CSP can efficiently discover interesting and complex temporally-extended plans for solving a wide range of physically realistic 3D tasks. In contrast, standard planning and learning methods often fail to solve these tasks at all or do so only with a huge and highly variable number of training samples. We explore the use of a variety of curiosity metrics with CSP and analyze the types of solutions that CSP discovers. Finally, we show that CSP supports task transfer so that the exploration policies learned during experience with one task can help improve efficiency on related tasks.
","['Rice University', 'Shanghai Jiao Tong University', 'Stanford University', 'Stanford University', 'Stanford University']"
2020,Implicit Euler Skip Connections: Enhancing Adversarial Robustness via Numerical Stability,"Mingjie Li, Lingshen He, Zhouchen Lin",https://icml.cc/Conferences/2020/Schedule?showEvent=5818,"Deep neural networks have achieved great success in various areas, but recent works have found that neural networks are vulnerable to adversarial attacks, which 
leads to a hot topic nowadays. 
Although many approaches have been proposed to enhance the robustness of neural networks, few of them explored robust architectures for neural networks. 
On this account, we try to address such an issue from the perspective of dynamic system in this work. 
By viewing ResNet as an explicit Euler discretization of an ordinary differential equation~(ODE), for the first time, we find that the adversarial robustness of ResNet is connected to the numerical stability of the corresponding dynamic system, i.e., more stable numerical schemes may correspond to more robust deep networks. 
Furthermore, inspired by the implicit Euler method for solving numerical ODE problems, we propose Implicit Euler skip connections~(IE-Skips) by modifying the original skip connection in ResNet or its variants. 
Then we theoretically prove its advantages under the adversarial attack and the experimental results show that our ResNet with IE-Skips can largely improve the robustness and the generalization ability under adversarial attacks when compared with the vanilla ResNet of the same parameter size. 
","['Peking University', 'Peking University', 'Peking University']"
2020,Structural Language Models of Code,"Uri Alon, Roy Sadaka, Omer Levy, Eran Yahav",https://icml.cc/Conferences/2020/Schedule?showEvent=5952,"We address the problem of any-code completion - generating a missing piece of source code in a given program without any restriction on the vocabulary or structure. We introduce a new approach to any-code completion that leverages the strict syntax of programming languages to model a code snippet as a tree - structural language modeling (SLM).
SLM estimates the probability of the program's abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes.
We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node.
Unlike previous techniques that have severely restricted the kinds of expressions that can be generated in this task, our approach can generate arbitrary code in any programming language.
Our model significantly outperforms both seq2seq and a variety of structured approaches in generating Java and C# code.
Our code, data, and trained models are available at http://github.com/tech-srl/slm-code-generation/.
An online demo is available at http://AnyCodeGen.org.
","['Technion - Israel Institute of Technology', 'Technion', 'Tel Aviv University / Facebook AI Research', 'Technion']"
2020,Cooperative Multi-Agent Bandits with Heavy Tails,"Abhimanyu Dubey, Alex `Sandy' Pentland",https://icml.cc/Conferences/2020/Schedule?showEvent=5806,"We study the heavy-tailed stochastic bandit problem in the cooperative multi-agent setting, where a group of agents interact with a common bandit problem, while communicating on a network with delays. Existing algorithms for the stochastic bandit in this setting utilize confidence intervals arising from an averaging-based communication protocol known as running consensus, that does not lend itself to robust estimation for heavy-tailed settings. We propose MP-UCB, a decentralized multi-agent algorithm for the cooperative stochastic bandit that incorporates robust estimation with a message-passing protocol. We prove optimal regret bounds for MP-UCB for several problem settings, and also demonstrate its superiority to existing methods. Furthermore, we establish the first lower bounds for the cooperative bandit problem, in addition to providing efficient algorithms for robust bandit estimation of location.
","['Massachusetts Institute of Technology', 'MIT']"
2020,Ordinal Non-negative Matrix Factorization for Recommendation,"Olivier Gouvert, Thomas Oberlin, Cedric Fevotte",https://icml.cc/Conferences/2020/Schedule?showEvent=6513,"We introduce a new non-negative matrix factorization (NMF) method for ordinal data, called OrdNMF. Ordinal data are categorical data which exhibit a natural ordering between the categories. In particular, they can be found in recommender systems, either with explicit data (such as ratings) or implicit data (such as quantized play counts). OrdNMF is a probabilistic latent factor model that generalizes Bernoulli-Poisson factorization (BePoF) and Poisson factorization (PF) applied to binarized data. Contrary to these methods, OrdNMF circumvents binarization and can exploit a more informative representation of the data. We design an efficient variational algorithm based on a suitable model augmentation and related to variational PF. In particular, our algorithm preserves the scalability of PF and can be applied to huge sparse datasets. We report recommendation experiments on explicit and implicit datasets, and show that OrdNMF outperforms BePoF and PF applied to binarized data.
","['CNRS, IRIT', 'ISAE-SUPAERO', 'CNRS']"
2020,Adversarial Robustness Against the Union of Multiple Perturbation Models,"Pratyush Maini, Eric Wong, Zico Kolter",https://icml.cc/Conferences/2020/Schedule?showEvent=6411,"Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against $\ell_\infty$, $\ell_2$, and $\ell_1$ attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 46.1% against the union of ($\ell_\infty$, $\ell_2$, $\ell_1$) perturbations with radius  = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6% accuracy. ","['IIT Delhi', 'MIT', 'Carnegie Mellon University / Bosch Center for AI']"
2020,SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates,"Lingkai Kong, Jimeng Sun, Chao Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6065,"Uncertainty quantification is a fundamental yet unsolved problem for deep
learning. The Bayesian framework provides a principled way of uncertainty
estimation but is often not scalable to modern deep neural nets (DNNs) that
have a large number of parameters. Non-Bayesian methods are simple to implement
but often conflate different sources of uncertainties and require huge
computing resources.  We propose a new method for quantifying uncertainties of
DNNs from a dynamical system perspective.  The core of our method is to view
DNN transformations as state evolution of a stochastic dynamical system and
introduce a Brownian motion term for capturing epistemic uncertainty. Based on this
perspective, we propose a neural stochastic differential equation model
(SDE-Net) which consists of (1) a drift net that controls the system to fit the
predictive function; and (2) a diffusion net that captures epistemic uncertainty.
We theoretically analyze the existence and uniqueness of the solution to
SDE-Net. Our experiments demonstrate that the SDE-Net model can outperform
existing uncertainty estimation methods across a series of tasks where
uncertainty plays a fundamental role.
","['Georgia Institute of Technoloy', 'UIUC', 'Georgia Institute of Technology']"
2020,Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search,"Binghong Chen, Chengtao Li, Hanjun Dai, Le Song",https://icml.cc/Conferences/2020/Schedule?showEvent=6732,"Retrosynthetic planning is a critical task in organic chemistry which identifies a series of reactions that can lead to the synthesis of a target product. The vast number of possible chemical transformations makes the size of the search space very big, and retrosynthetic planning is challenging even for experienced chemists. However, existing methods either require expensive return estimation by rollout with high variance, or optimize for search speed rather than the quality.  In this paper, we propose Retro, a neural-based A-like algorithm that finds high-quality synthetic routes efficiently. It maintains the search as an AND-OR tree, and learns a neural search bias with off-policy data. Then guided by this neural network, it performs best-first search efficiently during new planning episodes. Experiments on benchmark USPTO datasets show that, our proposed method outperforms existing state-of-the-art with respect to both the success rate and solution quality, while being more efficient at the same time.
","['Georgia Tech', 'Galixir', 'Google Brain', 'Georgia Institute of Technology']"
2020,Provable Self-Play Algorithms for Competitive Reinforcement Learning,"Yu Bai, Chi Jin",https://icml.cc/Conferences/2020/Schedule?showEvent=6303,"Self-play, where the algorithm learns by playing against itself without requiring any direct supervision, has become the new weapon in modern Reinforcement Learning (RL) for achieving superhuman performance in practice. However, the majority of exisiting theory in reinforcement learning only applies to the setting where the agent plays against a fixed environment; it remains largely open whether self-play algorithms can be provably effective, especially when it is necessary to manage the exploration/exploitation tradeoff. We study self-play in competitive reinforcement learning under the setting of Markov games, a generalization of Markov decision processes to the two-player case. We introduce a self-play algorithm---Value Iteration with Upper/Lower Confidence Bound (VI-ULCB)---and show that it achieves regret $\mathcal{\tilde{O}}(\sqrt{T})$ after playing $T$ steps of the game, where the regret is measured by the agent's performance against a fully adversarial opponent who can exploit the agent's strategy at any step. We also introduce an explore-then-exploit style algorithm, which achieves a slightly worse regret of $\mathcal{\tilde{O}}(T^{2/3})$, but is guaranteed to run in polynomial time even in the worst case. To the best of our knowledge, our work presents the first line of provably sample-efficient self-play algorithms for competitive reinforcement learning.","['Salesforce Research', 'Princeton University']"
2020,Balancing Competing Objectives with Noisy Data: Score-Based Classifiers for Welfare-Aware Machine Learning,"Esther Rolf, Max Simchowitz, Sarah Dean, Lydia T. Liu, Daniel Bjorkegren, Moritz Hardt, Joshua  Blumenstock",https://icml.cc/Conferences/2020/Schedule?showEvent=6694,"While real-world decisions involve many competing objectives, algorithmic decisions are often evaluated with a single objective function. In this paper, we study algorithmic policies which explicitly trade off between a private objective (such as profit) and a public objective (such as social welfare). We analyze a natural class of policies which trace an empirical Pareto frontier based on learned scores, and focus on how such decisions can be made in noisy or data-limited regimes. Our theoretical results characterize the optimal strategies in this class, bound the Pareto errors due to inaccuracies in the scores, and show an equivalence between optimal strategies and a rich class of fairness-constrained profit-maximizing policies. We then present empirical results in two different contexts --- online content recommendation and sustainable abalone fisheries --- to underscore the generality of our approach to a wide range of practical decisions. Taken together, these results shed light on inherent trade-offs in using machine learning for decisions that impact social welfare.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'University of California Berkeley', 'Brown University', 'University of California, Berkeley', 'University of California, Berkeley']"
2020,Mix-n-Match : Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning,"Jize Zhang, Bhavya Kailkhura, T. Yong-Jin Han",https://icml.cc/Conferences/2020/Schedule?showEvent=6603,"This paper studies the problem of post-hoc calibration of machine learning classifiers. We introduce the following desiderata for uncertainty calibration: (a) accuracy-preserving, (b) data-efficient, and (c) high expressive power. We show that none of the existing methods satisfy all three requirements, and demonstrate how Mix-n-Match calibration strategies (i.e., ensemble and composition) can help achieve remarkably better data-efficiency and expressive power while provably maintaining the classification accuracy of the original classifier. Mix-n-Match strategies are generic in the sense that they can be used to improve the performance of any off-the-shelf calibrator. We also reveal potential issues in standard evaluation practices. Popular approaches (e.g., histogram-based expected calibration error (ECE)) may provide misleading results especially in small-data regime. Therefore, we propose an alternative data-efficient kernel density-based estimator for a reliable evaluation of the calibration performance and prove its asymptotically unbiasedness and consistency. Our approaches outperform state-of-the-art solutions on both the calibration as well as the evaluation tasks in most of the experimental settings. Our codes are available at https://github.com/zhang64- llnl/Mix-n-Match-Calibration.
","['Lawrence Livermore National Laboratory', 'Lawrence Livermore National Laboratory', 'LLNL']"
2020,Fast and Consistent Learning of Hidden Markov Models by Incorporating Non-Consecutive Correlations,"Robert Mattila, Cristian R. Rojas, Eric Moulines, Vikram Krishnamurthy, Bo Wahlberg",https://icml.cc/Conferences/2020/Schedule?showEvent=6415,"Can the parameters of a hidden Markov model (HMM) be estimated from a single sweep through the observations -- and additionally, without being trapped at a local optimum in the likelihood surface? That is the premise of recent method of moments algorithms devised for HMMs. In these, correlations between consecutive pair- or triplet-wise observations are empirically estimated and used to compute estimates of the HMM parameters. Albeit computationally very attractive, the main drawback is that by restricting to only low-order correlations in the data, information is being neglected which results in a loss of accuracy (compared to standard maximum likelihood schemes). In this paper, we propose extending these methods (both pair- and triplet-based) by also including non-consecutive correlations in a way which does not significantly increase the computational cost (which scales linearly with the number of additional lags included). We prove strong consistency of the new methods, and demonstrate an improved performance in numerical experiments on both synthetic and real-world financial time-series datasets.
","['KTH Royal Institute of Technology', 'KTH Royal Institute of Technology', 'Ecole Polytechnique', 'Cornell University', 'KTH Royal Institute of Technology']"
2020,Low-Variance and Zero-Variance Baselines for Extensive-Form Games,"Trevor Davis, Martin Schmid, Michael Bowling",https://icml.cc/Conferences/2020/Schedule?showEvent=6002,"Extensive-form games (EFGs) are a common model of multi-agent interactions with imperfect information. State-of-the-art algorithms for solving these games typically perform full walks of the game tree that can prove prohibitively slow in large games. Alternatively, sampling-based methods such as Monte Carlo Counterfactual Regret Minimization walk one or more trajectories through the tree, touching only a fraction of the nodes on each iteration, at the expense of requiring more iterations to converge due to the variance of sampled values. In this paper, we extend recent work that uses baseline estimates to reduce this variance. We introduce a framework of baseline-corrected values in EFGs that generalizes the previous work. Within our framework, we propose new baseline functions that result in significantly reduced variance compared to existing techniques. We show that one particular choice of such a function --- predictive baseline --- is provably optimal under certain sampling schemes. This allows for efficient computation of zero-variance value estimates even along sampled trajectories.
","['University of Alberta', 'DeepMind', 'DeepMind']"
2020,Optimal Sequential Maximization: One Interview is Enough!,"Moein Falahatgar, Alon Orlitsky, Venkatadheeraj Pichapati",https://icml.cc/Conferences/2020/Schedule?showEvent=6273,"Maximum selection under probabilistic queries
\emph{(probabilistic maximization)} is a fundamental algorithmic problem
arising in numerous theoretical and practical contexts. 
We derive the first query-optimal sequential algorithm for
probabilistic-maximization.
Departing from previous assumptions,
the algorithm and performance guarantees
apply even for infinitely many items, hence in particular do
not require a-priori knowledge of the number of items.
The algorithm has linear query complexity,
and is optimal also in the streaming setting.
To derive these results we consider a probabilistic setting where several candidates
for a position are asked multiple questions with the goal of
finding who has the highest probability of answering interview
questions correctly. Previous work minimized the total number
of questions asked by alternating back and forth between the
best performing candidates,
in a sense, inviting them to multiple interviews.  We
show that the same order-wise selection accuracy can be achieved by
querying the candidates sequentially, never returning to a previously
queried candidate. Hence one interview is enough!
","['UCSD', 'UCSD', 'Apple Inc']"
2020,A Mean Field Analysis Of Deep ResNet And Beyond: Towards  Provably Optimization Via Overparameterization From Depth,"Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, Lexing Ying",https://icml.cc/Conferences/2020/Schedule?showEvent=5777,"Training deep neural networks with stochastic gradient descent (SGD) can often achieve zero training loss on real-world tasks although the optimization landscape is known to be highly non-convex. To understand the success of SGD for training deep neural networks, this work presents a mean-field analysis of deep residual networks, based on a line of works which interpret the continuum limit of the deep residual network as an ordinary differential equation as the the network capacity tends to infinity. Specifically, we propose a \textbf{new continuum limit} of deep residual networks, which enjoys a good landscape in the sense that \textbf{every local minimizer is global}. 
This characterization enables us to derive the first global convergence result for multilayer neural networks in the mean-field regime. Furthermore, our proof does not rely on the convexity of the loss landscape, but instead, an assumption on the global minimizer should achieve zero loss which can be achieved when the model shares a universal approximation property. Key to our result is the observation that a deep residual network resembles a shallow network ensemble~\cite{veit2016residual}, \emph{i.e.} a two-layer network. We bound the difference between the shallow network and our ResNet model via the adjoint sensitivity method, which enables us to transfer previous mean-field analysis of two-layer networks to deep networks. Furthermore, we propose several novel training schemes based on our new continuous model, among which one new training procedure introduces the operation of switching the order of the residual blocks and results in strong empirical performance on benchmark datasets. 
","['Stanford University', 'Princeton University', 'Duke University', 'Duke University', 'Stanford University']"
2020,Learning for Dose Allocation in Adaptive Clinical Trials with Safety Constraints,"Cong Shen, Zhiyang Wang, Sofia Villar, Mihaela van der Schaar",https://icml.cc/Conferences/2020/Schedule?showEvent=6446,"Phase I dose-finding trials are increasingly challenging as the relationship between efficacy and toxicity of new compounds (or combination of them) becomes more complex. Despite this, most commonly used methods in practice focus on identifying a Maximum Tolerated Dose (MTD) by learning only from toxicity events. We present a novel adaptive clinical trial methodology, called Safe Efficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the cumulative efficacies while satisfying the toxicity safety constraint with high probability. We evaluate performance objectives that have operational meanings in practical clinical trials, including cumulative efficacy, recommendation/allocation success probabilities, toxicity violation probability, and sample efficiency. An extended SEEDA-Plateau algorithm that is tailored for the increase-then-plateau efficacy behavior of molecularly targeted agents (MTA) is also presented. Through numerical experiments using both synthetic and real-world datasets, we show that SEEDA outperforms state-of-the-art clinical trial designs by finding the optimal dose with higher success rate and fewer patients.
","['University of Virginia', 'University of Pennsylvania', 'University of Cambridge', 'University of Cambridge and UCLA']"
2020,Dual Mirror Descent for Online Allocation Problems,"Santiago Balseiro, Haihao Lu, Vahab Mirrokni",https://icml.cc/Conferences/2020/Schedule?showEvent=6241,"We consider online allocation problems with concave revenue functions and resource constraints, which are central problems in revenue management and online advertising. In these settings, requests arrive sequentially during a finite horizon and, for each request, a decision maker needs to choose an action that consumes a certain amount of resources and generates revenue. The revenue function and resource consumption of each request are drawn independently and at random from a probability distribution that is unknown to the decision maker. The objective is to maximize cumulative revenues subject to a constraint on the total consumption of resources. 
We design a general class of algorithms that achieve sub-linear expected regret compared to the hindsight optimal allocation. Our algorithms operate in the Lagrangian dual space: they maintain a dual multiplier for each resource that is updated using online mirror descent. By choosing the reference function accordingly, we recover dual sub-gradient descent and dual exponential weights algorithm. The resulting algorithms are simple, efficient, and shown to attain the optimal order of regret when the length of the horizon and the initial number of resources are scaled proportionally. We discuss applications to online bidding in repeated auctions with budget constraints and online proportional matching with high entropy.
","['Columbia University', 'MIT', 'Google Research']"
2020,Leveraging Procedural Generation to Benchmark Reinforcement Learning,"Karl Cobbe, Chris Hesse, Jacob Hilton, John Schulman",https://icml.cc/Conferences/2020/Schedule?showEvent=6267,"We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.
","['OpenAI', 'OpenAI', 'OpenAI', 'OpenAI']"
2020,Set Functions for Time Series ,"Max Horn, Michael Moor, Christian Bock, Bastian Rieck, Karsten Borgwardt",https://icml.cc/Conferences/2020/Schedule?showEvent=6545,"Despite the eminent successes of deep neural networks, many
architectures are often hard to transfer to irregularly-sampled and
asynchronous time series that commonly occur in real-world datasets,
especially in healthcare applications. This paper proposes a novel
approach for classifying irregularly-sampled time series with unaligned
measurements, focusing on high scalability and data efficiency. Our
method SeFT (Set Functions for Time Series) is based on recent advances
in differentiable set function learning, extremely parallelizable with
a beneficial memory footprint, thus scaling well to large datasets of
long time series and online monitoring scenarios. Furthermore, our
approach permits quantifying per-observation contributions to the
classification outcome. We extensively compare our method with existing
algorithms on multiple healthcare time series datasets and demonstrate
that it performs competitively whilst significantly reducing runtime.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2020,Accountable Off-Policy Evaluation With Kernel Bellman Statistics,"Yihao Feng, Tongzheng Ren, Ziyang Tang, Qiang Liu",https://icml.cc/Conferences/2020/Schedule?showEvent=6683,"Off-policy evaluation plays an important role in modern reinforcement learning. However, most existing off-policy evaluation algorithms focus on point estimation, without providing an account- able confidence interval, that can reflect the uncertainty caused by limited observed data and algorithmic errors. In this work, we propose a new optimization-based framework, which can find a feasible set that contains the true value function with high probability, by leveraging the statistical properties of the recent proposed kernel Bellman loss (Feng et al., 2019). We further utilize the feasible set to construct accountable confidence intervals for off-policy evaluations, and propose a post-hoc diagnosis for existing estimators. Empirical results show that our methods yield tight yet accountable confidence intervals in different settings, which demonstrate the effectiveness of our method.
","['The University of Texas at Austin', 'UT Austin', 'University of Texas at Austin', 'UT Austin']"
2020,On the consistency of top-k surrogate losses,"Forest Yang, Sanmi Koyejo",https://icml.cc/Conferences/2020/Schedule?showEvent=6668,"The top-$k$ error is often employed to evaluate performance for challenging classification tasks in computer vision as it is designed to compensate for ambiguity in ground truth labels. This practical success motivates our theoretical analysis of consistent top-$k$ classification. To this end, we provide a characterization of Bayes optimality by defining a top-$k$ preserving property, which is new and fixes a non-uniqueness gap in prior work. Then, we define top-$k$ calibration and show it is necessary and sufficient for consistency.  Based on the top-$k$ calibration analysis, we propose a rich class of top-$k$ calibrated Bregman divergence surrogates. Our analysis continues by showing previously proposed hinge-like top-$k$ surrogate losses are not top-$k$ calibrated and thus inconsistent. On the other hand, we propose two new hinge-like losses, one which is similarly inconsistent, and one which is consistent.
Our empirical results highlight theoretical claims, confirming our analysis of the consistency of these losses.","['Google Brain', 'Illinois / Google']"
2020,Hallucinative Topological Memory for Zero-Shot Visual Planning,"Kara Liu, Thanard Kurutach, Christine Tung, Pieter Abbeel, Aviv Tamar",https://icml.cc/Conferences/2020/Schedule?showEvent=6571,"In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. Most previous works on VP approached the problem by planning in a learned latent space, resulting in low-quality visual plans, and difficult training algorithms. Here, instead, we propose a simple VP method that plans directly in image space and displays competitive performance. 
We build on the semi-parametric topological memory (SPTM) method: image samples are treated as nodes in a graph, the graph connectivity is learned from image sequence data, and planning can be performed using conventional graph search methods. We propose two modifications on SPTM. First, we train an energy-based graph connectivity function using contrastive predictive coding that admits stable training. Second, to allow zero-shot planning in new domains, we learn a conditional VAE model that generates images given a context describing the domain, and use these hallucinated samples for building the connectivity graph and planning. We show that this simple approach significantly outperform the SOTA VP methods, in terms of both plan interpretability and success rate when using the plan to guide a trajectory-following controller. Interestingly, our method can pick up non-trivial visual properties of objects, such as their geometry, and account for it in the plans.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'Technion']"
2020,Learning Representations that Support Extrapolation,"Taylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall O'Reilly, Jonathan Cohen",https://icml.cc/Conferences/2020/Schedule?showEvent=6208,"Extrapolation -- the ability to make inferences that go beyond the scope of one's experiences -- is a hallmark of human intelligence. By contrast, the generalization exhibited by contemporary neural network algorithms is largely limited to interpolation between data points in their training corpora. In this paper, we consider the challenge of learning representations that support extrapolation. We introduce a novel visual analogy benchmark that allows the graded evaluation of extrapolation as a function of distance from the convex domain defined by the training data. We also introduce a simple technique, context normalization, that encourages representations that emphasize the relations between objects. We find that this technique enables a significant improvement in the ability to extrapolate, considerably outperforming a number of competitive techniques.
","['UCLA Psychology Department', 'Princeton University', 'Princeton University', 'The Ohio State University', 'University of California Davis', 'Princeton University']"
2020,Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks,"Mert Pilanci, Tolga Ergen",https://icml.cc/Conferences/2020/Schedule?showEvent=6192,"We develop exact representations of two-layer neural networks with rectified linear units in terms of a single convex program with number of variables polynomial in the number of training samples and number of hidden neurons. Our theory utilizes semi-infinite duality and minimum norm regularization. Moreover, we show that certain standard convolutional linear networks are equivalent to $\ell_1$ regularized linear models in a polynomial sized discrete Fourier feature space.","['Stanford', 'Stanford University']"
2020,"Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?","Angelos Filos, Panagiotis Tigas, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, Yarin Gal",https://icml.cc/Conferences/2020/Schedule?showEvent=6266,"Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called \emph{robust imitative planning} (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model's uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term \emph{adaptive robust imitative planning} (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes \emph{prediction} challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess \emph{control}, we introduce an autonomous car novel-scene benchmark, \texttt{CARNOVEL}, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.
","['University of Oxford', 'Oxford University', 'UC Berkeley', 'Carnegie Mellon University', 'UC Berkeley', 'University of Oxford']"
2020,Time-Consistent Self-Supervision for Semi-Supervised Learning,"Tianyi Zhou, Shengjie Wang, Jeff Bilmes",https://icml.cc/Conferences/2020/Schedule?showEvent=6671,"Semi-supervised learning (SSL) leverages unlabeled data when training a model with insufficient labeled data. A common strategy for SSL is to enforce the consistency of model outputs between similar samples, e.g., neighbors or data augmentations of the same sample. However, model outputs can vary dramatically on unlabeled data over different training stages, e.g., when using large learning rates. This can introduce harmful noises and inconsistent objectives over time that may lead to concept drift and catastrophic forgetting. In this paper, we study the dynamics of neural net outputs in SSL and show that selecting and using first the unlabeled samples with more consistent outputs over the course of training (i.e., ""time-consistency"") can improve the final test accuracy and save computation. Under the time-consistent data selection, we design an SSL objective composed of two self-supervised losses, i.e., a consistency loss between a sample and its augmentation, and a contrastive loss encouraging different samples to have different outputs. Our approach achieves SOTA on several SSL benchmarks with much fewer computations.
","['University of Washington', 'University of Washington', 'UW']"
2020,Efficiently Solving MDPs with Stochastic Mirror Descent,"Yujia Jin, Aaron Sidford",https://icml.cc/Conferences/2020/Schedule?showEvent=6820,"In this paper we present a unified framework based on primal-dual stochastic mirror descent for approximately solving infinite-horizon Markov decision processes (MDPs) given a generative model. When applied to an average-reward MDP with $A_{tot}$ total actions and mixing time bound $t_{mix}$ our method computes an $\epsilon$-optimal policy with an expected $\widetilde{O}(t_{mix}^2 A_{tot} \epsilon^{-2})$ samples from the state-transition matrix, removing the ergodicity dependence of prior art.  When applied to a $\gamma$-discounted MDP with $A_{tot}$ total actions our method computes an $\epsilon$-optimal policy with an expected $\widetilde{O}((1-\gamma)^{-4} A_{tot} \epsilon^{-2})$ samples, improving over the best-known primal-dual methods while matching the state-of-the-art up to a $(1-\gamma)^{-1}$ factor. Both methods are model-free, update state values and policy simultaneously, and run in time linear in the number of samples taken.","['Stanford University', 'Stanford']"
2020,Self-supervised Label Augmentation via Input Transformations,"Hankook Lee, Sung Ju Hwang, Jinwoo Shin",https://icml.cc/Conferences/2020/Schedule?showEvent=6093,"Self-supervised learning, which learns by constructing artificial labels given only the input signals, has recently gained considerable attention for learning representations with unlabeled datasets, i.e., learning without any human-annotated supervision. In this paper, we show that such a technique can be used to significantly improve the model accuracy even under fully-labeled datasets. Our scheme trains the model to learn both original and self-supervised tasks, but is different from conventional multi-task learning frameworks that optimize the summation of their corresponding losses. Our main idea is to learn a single unified task with respect to the joint distribution of the original and self-supervised labels, i.e., we augment original labels via self-supervision. This simple, yet effective approach allows to train models easier by relaxing a certain invariant constraint during learning the original and self-supervised tasks simultaneously. It also enables an aggregated inference which combines the predictions from different augmentations to improve the prediction accuracy. Furthermore, we propose a novel knowledge transfer technique, which we refer to as self-distillation, that has the effect of the aggregated inference in a single (faster) inference. We demonstrate the large accuracy improvement and wide applicability of our framework on various fully-supervised settings, e.g., the few-shot and imbalanced classification scenarios.
","['KAIST', 'KAIST, AITRICS', 'KAIST']"
2020,Fast computation of Nash Equilibria in Imperfect Information Games,"Remi Munos, Julien Perolat, Jean-Baptiste Lespiau, Mark Rowland, Bart De Vylder, Marc Lanctot, Finbarr Timbers, Daniel Hennes, Shayegan Omidshafiei, Audrunas Gruslys, Mohammad Gheshlaghi Azar, Edward Lockhart, Karl Tuyls",https://icml.cc/Conferences/2020/Schedule?showEvent=6450,"We introduce and analyze a class of algorithms, called Mirror Ascent against an Improved Opponent (MAIO), for computing Nash equilibria in two-player zero-sum games, both in normal form and in sequential form with imperfect information. These algorithms update the policy of each player with a mirror-ascent step to maximize the value of playing against an improved opponent. An improved opponent can be a best response, a greedy policy, a policy improved by policy gradient, or by any other reinforcement learning or search techniques. We establish a convergence result of the last iterate to the set of Nash equilibria and show that the speed of convergence depends on the amount of improvement offered by these improved policies. In addition, we show that under some condition, if we use a best response as improved policy, then an exponential convergence rate is achieved. 
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'Deepmind', 'DeepMind', 'DeepMind']"
2020,Learning to Simulate and Design for Structural Engineering,"Kai-Hung Chang, Chin-Yi Cheng",https://icml.cc/Conferences/2020/Schedule?showEvent=6685,"The structural design process for buildings is time-consuming and laborious. To automate this process, structural engineers combine optimization methods with simulation tools to find an optimal design with minimal building mass subject to building regulations. However, structural engineers in practice often avoid optimization and compromise on a suboptimal design for the majority of buildings, due to the large size of the design space, the iterative nature of the optimization methods, and the slow simulation tools. In this work, we formulate the building structures as graphs and create an end-to-end pipeline that can learn to propose the optimal cross-sections of columns and beams by training together with a pre-trained differentiable structural simulator. The performance of the proposed structural designs is comparable to the ones optimized by genetic algorithm (GA), with all the constraints satisfied. The optimal structural design with the reduced the building mass can not only lower the material cost, but also decrease the carbon footprint.
","['Autodesk Research', 'Autodesk Research']"
2020,Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors,"Mike Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, Dustin Tran",https://icml.cc/Conferences/2020/Schedule?showEvent=6680,"Bayesian neural networks (BNNs) demonstrate promising success in improving the robustness and uncertainty quantification of modern deep learning. However, they generally struggle with underfitting at scale and parameter efficiency. On the other hand, deep ensembles have emerged as alternatives for uncertainty quantification that, while outperforming BNNs on certain problems, also suffer from efficiency issues. It remains unclear how to combine the strengths of these two approaches and remediate their common issues. To tackle this challenge, we propose a rank-1 parameterization of BNNs, where each weight matrix involves only a distribution on a rank-1 subspace. We also revisit the use of mixture approximate posteriors to capture multiple modes, where unlike typical mixtures, this approach admits a significantly smaller memory increase (e.g., only a 0.4% increase for a ResNet-50 mixture of size 10). We perform a systematic empirical study on the choices of prior, variational posterior, and methods to improve training. For ResNet-50 on ImageNet, Wide ResNet 28-10 on CIFAR-10/100, and an RNN on MIMIC-III, rank-1 BNNs achieve state-of-the-art performance across log-likelihood, accuracy, and calibration on the test sets and out-of-distribution variants.
","['Google Brain (AI Residency)', 'Duke University / Google Brain / UC Berkeley', 'University of Toronto', 'Google', 'Google Brain', 'Google', 'Google Brain', 'Google']"
2020,A Game Theoretic Framework for Model Based Reinforcement Learning,"Aravind Rajeswaran, Igor Mordatch, Vikash Kumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6314,"Designing stable and efficient algorithms for model-based reinforcement learning (MBRL) with function approximation has remained challenging despite growing interest in the field. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1)~a policy player, which attempts to maximize rewards under the learned model; (2)~a model player, which attempts to fit the real-world data collected by the policy player. We show that a near-optimal policy for the environment can be obtained by finding an approximate equilibrium for aforementioned game, and we develop two families of algorithms to find the game equilibrium by drawing upon ideas from Stackelberg games. Experimental studies suggest that the proposed algorithms achieve state of the art sample efficiency, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation.
","['University of Washington', 'Google Brain', 'Univ. Of Washington']"
2020,How to Solve Fair k-Center in Massive Data Models,"Ashish Chiplunkar, Sagar Kale, Sivaramakrishnan Natarajan Ramamoorthy",https://icml.cc/Conferences/2020/Schedule?showEvent=6406,"Fueled by massive data, important decision making is being automated with the help of algorithms, therefore, fairness in algorithms has become an especially important research topic. In this work, we design new streaming and distributed algorithms for the fair k-center problem that models fair data summarization. The streaming and distributed models of computation have an attractive feature of being able to handle massive data sets that do not fit into main memory. Our main contributions are: (a) the first distributed algorithm; which has provably constant approximation ratio and is extremely parallelizable, and (b) a two-pass streaming algorithm with a provable approximation guarantee matching the best known algorithm (which is not a streaming algorithm). Our algorithms have the advantages of being easy to implement in practice, being fast with linear running times, having very small working memory and communication, and outperforming existing algorithms on several real and synthetic data sets. To complement our distributed algorithm, we also give a hardness result for natural distributed algorithms, which holds for even the special case of k-center.
","['IIT Delhi', 'University of Vienna', 'University of Washington']"
2020,Stochastic bandits with arm-dependent delays,"Anne Gael Manegueu, Claire Vernade, Alexandra Carpentier, Michal Valko",https://icml.cc/Conferences/2020/Schedule?showEvent=6140,"Significant work has been recently dedicated to the stochastic delayed bandit setting because of its relevance in applications. The applicability of existing algorithms is however restricted by the fact that strong assumptions are often made on the delay distributions, such as full observability, restrictive shape constraints, or uniformity over arms. In this work, we weaken them significantly and only assume that there is a bound on the tail of the delay. In particular, we cover the important case where the delay distributions vary across arms, and the case where the delays are heavy-tailed. Addressing these difficulties, we propose a simple but efficient UCB-based algorithm called the PATIENTBANDITS. We provide both problem-dependent and problem-independent bounds on
the regret as well as performance lower bounds.
","['Otto-von-Guerricke University', 'Deepmind', 'Otto-von-Guericke University', 'DeepMind']"
2020,Handling the Positive-Definite Constraint in the Bayesian Learning Rule,"Wu Lin, Mark Schmidt, Mohammad Emtiyaz Khan",https://icml.cc/Conferences/2020/Schedule?showEvent=6821,"The Bayesian learning rule is a natural-gradient variational inference method, which not only contains many existing learning algorithms as special cases but also enables the design of new algorithms.
Unfortunately, when variational parameters lie in an open constraint set, the rule may not satisfy the constraint and requires line-searches which could slow down the algorithm.
In this work, we address this issue for positive-definite constraints by proposing an improved rule that naturally handles the constraints.
Our modification is obtained by using Riemannian gradient methods, and is valid when the approximation attains a block-coordinate natural parameterization (e.g., Gaussian distributions and their mixtures).
Our method outperforms existing methods without any significant increase in computation. 
Our work makes it easier to apply the rule in the presence of positive-definite constraints in parameter spaces.
","['University of British Columbia', 'University of British Columbia', 'RIKEN']"
2020,Batch Stationary Distribution Estimation,"Junfeng Wen, Bo Dai, Lihong Li, Dale Schuurmans",https://icml.cc/Conferences/2020/Schedule?showEvent=6311,"We consider the problem of approximating the stationary distribution of
an ergodic Markov chain given a set of sampled transitions. Classical simulation-based approaches assume access to the underlying process so that trajectories of sufficient length can be gathered to approximate stationary sampling. Instead, we consider an alternative setting where a \emph{fixed} set of transitions has been collected beforehand, by a separate, possibly unknown procedure. The goal is still to estimate properties of the stationary distribution, but without additional access to the underlying system. We propose a consistent estimator that is based on recovering a correction ratio function over the given data. In particular, we develop a variational power method (VPM) that provides provably consistent estimates under general conditions. In addition to unifying a number of existing approaches from different subfields, we also find that VPM yields significantly better estimates across a range of problems, including queueing, stochastic differential equations, post-processing MCMC, and off-policy evaluation.
","['University of Alberta', 'Google Brain', 'Google Research', 'University of Alberta']"
2020,Continuous-time Lower Bounds for Gradient-based Algorithms,"Michael Muehlebach, Michael Jordan",https://icml.cc/Conferences/2020/Schedule?showEvent=6331,"This article derives lower bounds on the convergence rate of continuous-time gradient-based optimization algorithms. The algorithms are subjected to a time-normalization constraint that avoids a reparametrization of time in order to make the discussion of continuous-time convergence rates meaningful. We reduce the multi-dimensional problem to a single dimension, recover well-known lower bounds from the discrete-time setting, and provide insights into why these lower bounds occur. We further explicitly provide algorithms that achieve the proposed lower bounds, even when the function class under consideration includes certain non-convex functions.
","['UC Berkeley', 'UC Berkeley']"
2020,Causal Effect Estimation and Optimal Dose Suggestions in Mobile Health,"Liangyu Zhu, Wenbin Lu, Rui Song",https://icml.cc/Conferences/2020/Schedule?showEvent=5965,"In this article, we propose novel structural nested models to estimate causal effects of continuous treatments based on mobile health data. To find the treatment regime which optimizes the short-term outcomes for the patients, we define the weighted lag K advantage. The optimal treatment regime is then defined to be the one which maximizes this advantage. This method imposes minimal assumptions on the data generating process. Statistical inference can also be provided for the estimated parameters. Simulation studies and an application to the Ohio type 1 diabetes dataset show that our method could provide meaningful insights for dose suggestions with mobile health data. 
","['North Carolina State University', 'North Carolina State University', 'North Carolina State University']"
2020,Hierarchical Verification for Adversarial Robustness,"Cong Han Lim, Raquel Urtasun, Ersin Yumer",https://icml.cc/Conferences/2020/Schedule?showEvent=6055,"We introduce a new framework for the exact point-wise ℓp robustness verification problem that exploits the layer-wise geometric structure of deep feed-forward networks with rectified linear activations (ReLU networks). The activation regions of the network partition the input space, and one can verify the ℓp robustness around a point by checking all the activation regions within the desired radius. The GeoCert algorithm (Jordan et al., NeurIPS 2019) treats this partition as a generic polyhedral complex in order to detect which region to check next. In contrast, our LayerCert framework considers the nested hyperplane arrangement structure induced by the layers of the ReLU network and explores regions in a hierarchical manner. We show that, under certain conditions on the algorithm parameters, LayerCert provably reduces the number and size of the convex programs that one needs to solve compared to GeoCert. Furthermore, our LayerCert framework allows  the incorporation of lower bounding routines based on convex relaxations to further improve performance. Experimental results demonstrate that LayerCert can significantly reduce both the number of convex programs solved and the running time over the state-of-the-art.
","['Uber ATG', 'Uber ATG & University of Toronto', 'Uber ATG']"
2020,AdaScale SGD: A User-Friendly Algorithm for Distributed Training,"Tyler Johnson, Pulkit Agrawal, Haijie Gu, Carlos Guestrin",https://icml.cc/Conferences/2020/Schedule?showEvent=6534,"When using large-batch training to speed up stochastic gradient descent, learning rates must adapt to new batch sizes in order to maximize speed-ups and preserve model quality.  Re-tuning learning rates is resource intensive, while fixed scaling rules often degrade model quality.  We propose AdaScale SGD, an algorithm that reliably adapts learning rates to large-batch training.  By continually adapting to the gradient's variance, AdaScale automatically achieves speed-ups for a wide range of batch sizes.  We formally describe this quality with AdaScale’s convergence bound, which maintains final objective values, even as batch sizes grow large and the number of iterations decreases.  In empirical comparisons, AdaScale trains well beyond the batch size limits of popular “linear learning rate scaling” rules.  This includes large-batch training with no model degradation for machine translation, image classification, object detection, and speech recognition tasks.  AdaScale's qualitative behavior is similar to that of ""warm-up"" heuristics, but unlike warm-up, this behavior emerges naturally from a principled mechanism.  The algorithm introduces negligible computational overhead and no new hyperparameters, making AdaScale an attractive choice for large-scale training in practice.
","['Apple', 'Apple', 'Apple', 'Apple & Univesity of Washington']"
2020,Learning Calibratable Policies using Programmatic Style-Consistency,"Eric Zhan, Albert Tseng, Yisong Yue, Adith Swaminathan, Matthew Hausknecht",https://icml.cc/Conferences/2020/Schedule?showEvent=6229,"We study the problem of controllable generation of long-term sequential behaviors, where the goal is to calibrate to multiple behavior styles simultaneously. In contrast to the well-studied areas of controllable generation of images, text, and speech, there are two questions that pose significant challenges when generating long-term behaviors: how should we specify the factors of variation to control, and how can we ensure that the generated behavior faithfully demonstrates combinatorially many styles? We leverage programmatic labeling functions to specify controllable styles, and derive a formal notion of style-consistency as a learning objective, which can then be solved using conventional policy learning approaches. We evaluate our framework using demonstrations from professional basketball players and agents in the MuJoCo physics environment, and show that existing approaches that do not explicitly enforce style-consistency fail to generate diverse behaviors whereas our learned policies can be calibrated for up to $4^5 (1024)$ distinct style combinations.","['California Institute of Technology', 'Caltech', 'Caltech', 'Microsoft Research', 'Microsoft Research']"
2020,Test-Time Training with Self-Supervision for Generalization under Distribution Shifts,"Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, Moritz Hardt",https://icml.cc/Conferences/2020/Schedule?showEvent=6552,"In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a single unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'University of California, Berkeley', 'UC Berkeley', 'University of California, Berkeley']"
2020,Quantized Decentralized Stochastic Learning over Directed Graphs,"Hossein Taheri, Aryan Mokhtari, Hamed Hassani, Ramtin Pedarsani",https://icml.cc/Conferences/2020/Schedule?showEvent=6342,"We consider a decentralized stochastic learning problem where data points are distributed among computing nodes communicating over a directed graph. As the model size gets large, decentralized learning faces a major bottleneck that is the heavy communication load due to each node transmitting large messages (model updates) to its neighbors. To tackle this bottleneck, we propose the quantized decentralized stochastic learning algorithm over directed graphs that is based on the push-sum algorithm in decentralized consensus optimization. More importantly, we prove that our algorithm achieves the same convergence rates of the decentralized stochastic learning algorithm with exact-communication for both convex and non-convex losses. Furthermore, our numerical results illustrate significant speed-up compared to the exact-communication methods.
","['UC Santa Barbara', 'UT Austin', 'University of Pennsylvania', 'University of California, Santa Barbara']"
2020,Small-GAN: Speeding up GAN Training using Core-Sets ,"Samrath Sinha, Han Zhang, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Augustus Odena",https://icml.cc/Conferences/2020/Schedule?showEvent=5973,"Recent work suggests that Generative Adversarial Networks (GANs) benefit disproportionately from large mini-batch sizes. This finding is interesting but also discouraging -- large batch sizes are slow and expensive to emulate on conventional hardware. Thus, it would be nice if there were some trick by which we could generate batches that were effectively big though small in practice. In this work, we propose such a trick, inspired by the use of Coreset-selection in active learning. When training a GAN, we draw a large batch of samples from the prior and then compress that batch using Coreset-selection. To create effectively large batches of real images, we create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset-selection on those projected embeddings at training time. We conduct experiments showing that this technique substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it helps us use GANs to reach a new state of the art in anomaly detection.
","['University of Toronto', 'Google', 'Université de Montréal', 'Montreal Institute for Learning Algorithms', 'Google Brain', 'Google Brain']"
2020,Single Point Transductive Prediction,"Nilesh Tripuraneni, Lester Mackey",https://icml.cc/Conferences/2020/Schedule?showEvent=6309,"Standard methods in supervised learning separate training and prediction: the model is fit independently of any test points it may encounter. However, can knowledge of the next test point $\mathbf{x}_{\star}$ be exploited to improve prediction accuracy? We address this question in the context of linear prediction, showing how  techniques from semi-parametric inference can be used transductively to combat regularization bias. We first lower bound the $\mathbf{x}_{\star}$ prediction error of ridge regression and the Lasso, showing that they must incur significant bias in certain test directions. We then provide non-asymptotic upper bounds on the $\mathbf{x}_{\star}$ prediction error of two transductive prediction rules. We conclude by showing the efficacy of our methods on both synthetic and real data, highlighting the improvements single point transductive
    prediction can provide in settings with distribution shift.","['UC Berkeley', 'Microsoft Research']"
2020,Hypernetwork approach to generating point clouds,"Przemysław Spurek, Sebastian Winczowski, Jacek Tabor, Maciej Zamorski, Maciej Zieba, Tomasz Trzcinski",https://icml.cc/Conferences/2020/Schedule?showEvent=6323,"In this work, we propose a novel method for generating 3D point clouds that leverage properties of hyper networks. Contrary to the existing methods that learn only the representation of a 3D object, our approach simultaneously finds a representation of the object and its 3D surfaces. The main idea of our HyperCloud method is to build a hyper network that returns weights of a particular neural network (target network) trained to map points from a uniform unit ball distribution into a 3D shape. As a consequence, a particular 3D shape can be generated using point-by-point sampling from the assumed prior distribution and transforming sampled points with the target network. Since the hyper network is based on an auto-encoder architecture trained to reconstruct realistic 3D shapes, the target network weights can be considered a parametrisation of the surface of a 3D shape, and not a standard representation of point cloud usually returned by competitive approaches. The proposed architecture allows to find mesh-based representation of 3D objects in a generative manner, while providing point clouds en pair in quality with the state-of-the-art methods. 
","['Jagiellonian University', 'Jagiellonian University', 'Jagiellonian University', 'Wrocław University of Science and Technology', 'Wroclaw University of Science and Technology, Tooploox', 'Warsaw University of Technology, Tooploox']"
2020,The Sample Complexity of Best-$k$ Items Selection from Pairwise Comparisons,"Wenbo Ren, Jia Liu, Ness Shroff",https://icml.cc/Conferences/2020/Schedule?showEvent=6195,"This paper studies the sample complexity (aka number of comparisons) bounds for the active best-$k$ items selection from pairwise comparisons. From a given set of items, the learner can make pairwise comparisons on every pair of items, and each comparison returns an independent noisy result about the preferred item. At any time, the learner can adaptively choose a pair of items to compare according to past observations (i.e., active learning). The learner's goal is to find the (approximately) best-$k$ items with a given confidence, while trying to use as few comparisons as possible. In this paper, we study two problems: (i) finding the probably approximately correct (PAC) best-$k$ items and (ii) finding the exact best-$k$ items, both under strong stochastic transitivity and stochastic triangle inequality. For PAC best-$k$ items selection, we first show a lower bound and then propose an algorithm whose sample complexity upper bound matches the lower bound up to a constant factor. For the exact best-$k$ items selection, we first prove a worst-instance lower bound. We then propose two algorithms based on our PAC best items selection algorithms: one works for $k=1$ and is sample complexity optimal up to a loglog factor, and the other works for all values of $k$ and is sample complexity optimal up to a log factor. ","['The Ohio State University', 'Iowa State University', 'The Ohio State University']"
2020,The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent,"Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, Tom Goldstein",https://icml.cc/Conferences/2020/Schedule?showEvent=6681,"This paper studies how neural network architecture affects the speed of training. We introduce a simple concept called gradient confusion to help formally analyze this. When gradient confusion is high, stochastic gradients produced by different data samples may be negatively correlated, slowing down convergence. But when gradient confusion is low, data samples interact harmoniously, and training proceeds quickly. Through theoretical and experimental results, we demonstrate how the neural network architecture affects gradient confusion, and thus the efficiency of training. Our results show that, for popular initialization techniques, increasing the width of neural networks leads to lower gradient confusion, and thus faster model training. On the other hand, increasing the depth of neural networks has the opposite effect. Our results indicate that alternate initialization techniques or networks using both batch normalization and skip connections help reduce the training burden of very deep networks.
","['Facebook', 'DeepMind', 'Google Research', 'Google AI', 'University of Maryland']"
2020,Provable Representation Learning for Imitation Learning via Bi-level Optimization,"Sanjeev Arora, Simon Du, Sham Kakade, Yuping Luo, Nikunj Umesh Saunshi",https://icml.cc/Conferences/2020/Schedule?showEvent=6457,"A common strategy in modern learning systems is to learn a representation that is useful for many tasks, a.k.a. representation learning. We study this strategy in the imitation learning setting for Markov decision processes (MDPs) where multiple experts' trajectories are available. We formulate representation learning  as a bi-level optimization problem where the outer"" optimization tries to learn the joint representation and theinner"" optimization encodes the imitation learning setup and tries to learn task-specific parameters. We instantiate this framework for the imitation learning settings of behavior cloning and observation-alone. Theoretically, we show using our framework that representation learning can provide sample complexity benefits for imitation learning in both settings. We also provide proof-of-concept experiments to verify our theory.
","['Princeton University and Institute for Advanced Study', 'Institute for Advanced Study', 'University of Washington', 'Princeton University', 'Princeton University']"
2020,Temporal Phenotyping using Deep Predictive Clustering of Disease Progression,"Changhee Lee, Mihaela van der Schaar",https://icml.cc/Conferences/2020/Schedule?showEvent=6046,"Due to the wider availability of modern electronic health records, patient care data is often being stored in the form of time-series. Clustering such time-series data is crucial for patient phenotyping, anticipating patients’ prognoses by identifying “similar” patients, and designing treatment guidelines that are tailored to homogeneous patient subgroups. In this paper, we develop a deep learning approach for clustering time-series data, where each cluster comprises patients who share similar future outcomes of interest (e.g., adverse events, the onset of comorbidities). To encourage each cluster to have homogeneous future outcomes, the clustering is carried out by learning discrete representations that best describe the future outcome distribution based on novel loss functions. Experiments on two real-world datasets show that our model achieves superior clustering performance over state-of-the-art benchmarks and identifies meaningful clusters that can be translated into actionable information for clinical decision-making.
","['UCLA', 'University of Cambridge and UCLA']"
2020,NetGAN without GAN: From Random Walks to Low-Rank Approximations,"Luca Rendsburg, Holger Heidrich, Ulrike von Luxburg",https://icml.cc/Conferences/2020/Schedule?showEvent=6514,"A graph generative model takes a graph as input and is supposed to generate new graphs that ``look like'' the input graph. While most classical models focus on few, hand-selected graph statistics and are too simplistic to reproduce real-world graphs, NetGAN recently emerged as an attractive alternative: by training a GAN to learn the random walk distribution of the input graph, the algorithm is able to reproduce a large number of important network patterns simultaneously, without explicitly specifying any of them. In this paper, we investigate the implicit bias of NetGAN. We find that the root of its generalization properties does not lie in the GAN architecture, but in an inconspicuous low-rank approximation of the logits random walk transition matrix. Step by step we can strip NetGAN of all unnecessary parts, including the GAN, and obtain a highly simplified reformulation that achieves comparable generalization results, but is orders of magnitudes faster and easier to adapt. Being much simpler on the conceptual side, we reveal the implicit inductive bias of the algorithm  --- an important step towards increasing the interpretability, transparency and acceptance of machine learning systems.
","['University of Tübingen', 'University of Tübingen', 'U Tübingen']"
2020,Correlation Clustering with Asymmetric Classification Errors,"Jafar Jafarov, Sanchit Kalhan, Konstantin Makarychev, Yury Makarychev",https://icml.cc/Conferences/2020/Schedule?showEvent=6642,"In the Correlation Clustering problem, we are given a weighted graph $G$ with its edges labelled as ""similar"" or ""dissimilar"" by a binary classifier. The goal is to produce a clustering that minimizes the weight of ""disagreements"": the sum of the weights of ""similar"" edges across clusters and ""dissimilar"" edges within clusters. We study the correlation clustering problem under the following assumption: Every ""similar"" edge $e$ has weight $w_e \in [ \alpha w, w ]$ and every ""dissimilar"" edge $e$ has weight $w_e \geq \alpha w$ (where $\alpha \leq 1$ and $w > 0$ is a scaling parameter). We give a $(3 + 2 \log_e (1/\alpha))$ approximation algorithm for this problem. This assumption captures well the scenario when classification errors are asymmetric. Additionally, we show an asymptotically matching Linear Programming integrality gap of $\Omega(\log 1/\alpha)$.","['University of Chicago', 'Northwestern University', 'Northwestern University', 'Toyota Technological Institute at Chicago']"
2020,Evolutionary Topology Search for Tensor Network Decomposition,"Chao Li, Zhun Sun",https://icml.cc/Conferences/2020/Schedule?showEvent=6158,"Tensor network (TN) decomposition is a promising  framework  to  represent extremely high-dimensional problems with few parameters. However, it is challenging to search the (near-)optimal topological structure for TN decomposition, since the number of candidate solutions exponentially grows with increasing the order of a tensor. In this paper, we claim that this issue can be practically tackled by evolutionary algorithms in an affordable manner. We encode the complex topological structures into binary strings, and develop a simple genetic meta-algorithm to search the optimal topology on Hamming space. The experimental results by both synthetic and real-world data demonstrate that our method can effectively discover the ground-truth topology or even better structures with few number of generations, and significantly boost the representational power of TN decomposition compared with well-known tensor-train (TT) or tensor-ring (TR) models.
","['RIKEN Center for Advanced Intelligence Project', 'RIKEN']"
2020,Data Valuation using Reinforcement Learning,"Jinsung Yoon, Sercan Arik, Tomas Pfister",https://icml.cc/Conferences/2020/Schedule?showEvent=6276,"Quantifying the value of data is a fundamental problem in machine learning and has multiple important use cases: (1) building insights about the dataset and task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. We propose Data Valuation using Reinforcement Learning (DVRL), to adaptively learn data values jointly with the predictor model. DVRL uses a data value estimator (DVE) to learn how likely each datum is used in training of the predictor model. DVE is trained using a reinforcement signal that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across numerous datasets and application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6% and 10.8%, respectively. 
","['Google', 'Google', 'Google']"
2020,"Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks","Agustinus Kristiadi, Matthias Hein, Philipp Hennig",https://icml.cc/Conferences/2020/Schedule?showEvent=5879,"The point estimates of ReLU classification networks---arguably the most widely used neural network architecture---have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is ``to be a bit Bayesian''. These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations.
","['University of Tübingen', 'University of Tübingen', 'University of Tuebingen']"
2020,PackIt: A Virtual Environment for Geometric Planning,"Ankit Goyal, Jia Deng",https://icml.cc/Conferences/2020/Schedule?showEvent=5771,"The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. We refer to this ability as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. We present PackIt, a virtual environment to evaluate and potentially learn the ability to do geometric planning, where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space. We also construct a set of challenging packing tasks using an evolutionary algorithm. Further, we study various baselines for the task that include model-free learning-based and heuristic-based methods, as well as search-based optimization methods that assume access to the model of the environment.
","['Princeton University', 'Princeton University']"
2020,Multilinear Latent Conditioning for Generating Unseen Attribute Combinations,"Markos Georgopoulos, Grigorios Chrysos, Maja Pantic, Yannis Panagakis",https://icml.cc/Conferences/2020/Schedule?showEvent=5968,"Deep generative models rely on their inductive bias to facilitate generalization, especially for problems with high dimensional data, like images. However, empirical studies have shown that variational autoencoders (VAE) and generative adversarial networks (GAN) lack the generalization ability that occurs naturally in human perception. For example, humans can visualize a woman smiling after only seeing a smiling man. On the contrary, the standard conditional VAE (cVAE) is unable to generate unseen attribute combinations. To this end, we extend cVAE by introducing a multilinear latent conditioning framework that captures the multiplicative interactions between the attributes. We implement two variants of our model and demonstrate their efficacy on MNIST, Fashion-MNIST and CelebA. Altogether, we design a novel conditioning framework that can be used with any architecture to synthesize unseen attribute combinations.
","['Imperial College London', 'Imperial College London', 'Samsung AI Centre Cambridge/ Imperial College London ', 'Imperial College London']"
2020,Fairwashing explanations with off-manifold detergent,"Christopher Anders, Plamen Pasliev, Ann-Kathrin Dombrowski, Klaus-robert Mueller, Pan Kessel",https://icml.cc/Conferences/2020/Schedule?showEvent=6394,"Explanation methods promise to make black-box classifiers more transparent.
As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users.
In this paper, we show both theoretically and experimentally that these hopes are presently unfounded.
Specifically, we show that, for any classifier $g$, one can always construct another classifier $\tilde{g}$ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps.
We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets.
Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust.","['TU Berlin', 'TU Berlin', 'TU Berlin', 'Technische Universität Berlin', 'TU Berlin']"
2020,Multidimensional Shape Constraints,"Maya Gupta, Erez Louidor, Oleksandr Mangylov, Nobu Morioka, Taman Narayan, Sen Zhao",https://icml.cc/Conferences/2020/Schedule?showEvent=5995,"We propose new multi-input shape constraints across four intuitive categories:  complements, diminishers, dominance, and unimodality constraints. We show these shape constraints can be checked and even enforced when training machine-learned models for linear models, generalized additive models, and the nonlinear function class of multi-layer lattice models. Toy examples and real-world experiments illustrate how the different shape constraints can be used to increase interpretability and better regularize machine-learned models. 
","['Google', 'Google', 'Google Research', 'Google Research', 'Google', 'Google Research']"
2020,Revisiting Fundamentals of Experience Replay,"William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark Rowland, Will Dabney",https://icml.cc/Conferences/2020/Schedule?showEvent=6751,"Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay — greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.
","['Google Brain/Mila', 'Google', 'Google Research, Brain Team', 'Montreal Institute for Learning Algorithms', 'Google Brain', 'DeepMind', 'DeepMind']"
2020,Learning Near Optimal Policies with Low Inherent Bellman Error,"Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, Emma Brunskill",https://icml.cc/Conferences/2020/Schedule?showEvent=6784,"We study the exploration problem with approximate linear action-value functions in episodic reinforcement learning under the notion of low inherent Bellman error, a condition normally employed to show convergence of approximate value iteration. First we relate this condition to other common frameworks and show that it is strictly more general than the low rank (or linear) MDP assumption of prior work. Second we provide an algorithm with a high probability regret bound $\widetilde O(\sum_{t=1}^H d_t  \sqrt{K} + \sum_{t=1}^H \sqrt{d_t} \IBE K)$ where $H$ is the horizon, $K$ is the number of episodes, $\IBE$ is the value if the inherent Bellman error and $d_t$ is the feature dimension at timestep $t$. In addition, we show that the result is unimprovable beyond constants and logs by showing a matching lower bound. This has two important consequences: 1) it shows that exploration is possible using only \emph{batch assumptions} with an algorithm that achieves the optimal statistical rate for the setting we consider, which is more general than prior work on low-rank MDPs 2) the lack of closedness (measured by the inherent Bellman error) is only amplified by $\sqrt{d_t}$ despite working in the online setting. Finally, the algorithm reduces to the celebrated \textsc{LinUCB} when $H=1$ but with a different choice of the exploration parameter that allows handling misspecified contextual linear bandits. While computational tractability questions remain open for the MDP setting, this enriches the class of MDPs with a linear representation for the action-value function where statistically efficient reinforcement learning is possible.","['Stanford University', 'Facebook AI Research', 'Stanford University', 'Stanford University']"
2020,Adaptive Gradient Descent without Descent,"Yura Malitsky, Konstantin Mishchenko",https://icml.cc/Conferences/2020/Schedule?showEvent=6239,"We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on smoothness in a neighborhood of a solution. Given that the problem is convex, our method will converge even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.
","['EPFL', 'King Abdullah University of Science & Technology (KAUST)']"
2020,Robust Graph Representation Learning via Neural Sparsification,"Cheng Zheng, Bo Zong, Wei Cheng, Dongjin  Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, Wei Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6180,"Graph representation learning serves as the core of important prediction tasks, ranging from product recommendation to fraud detection. Real-life graphs usually have complex information in the local neighborhood, where each node is described by a rich set of features and connects to dozens or even hundreds of neighbors. Despite the success of neighborhood aggregation in graph neural networks, task-irrelevant information is mixed into nodes' neighborhood, making learned models suffer from sub-optimal generalization performance. In this paper, we present NeuralSparse, a supervised graph sparsification technique that improves generalization power by learning to remove potentially task-irrelevant edges from input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize sparsification processes, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance. Experimental results on both benchmark and private datasets show that NeuralSparse can yield up to 7.2% improvement in testing accuracy when working with existing graph neural networks on node classification tasks.
","['UCLA', 'NEC Labs', 'NEC Laboratories America', 'University of Connecticut', 'NEC Laboratories America, Inc.', 'UCLA', 'NEC Labs', 'UCLA']"
2020,“Other-Play” for Zero-Shot Coordination,"Hengyuan Hu, Alexander Peysakhovich, Adam Lerer, Jakob Foerster",https://icml.cc/Conferences/2020/Schedule?showEvent=6641,"We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g.humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents as well as with human players than SP agents.
","['Facebook AI Research', 'Facebook', 'Facebook AI Research', 'Facebook AI Research']"
2020,Gamification of Pure Exploration for Linear Bandits,"Rémy Degenne, Pierre Menard, Xuedong Shang, Michal Valko",https://icml.cc/Conferences/2020/Schedule?showEvent=6506,"We investigate an active \emph{pure-exploration} setting, that includes \emph{best-arm identification}, in the context of \emph{linear stochastic bandits}.  While asymptotically optimal algorithms exist for standard \emph{multi-armed bandits}, the existence of such algorithms for the best-arm identification in linear bandits has been elusive despite several attempts to address it.  First, we provide a thorough comparison and new insight over different notions of optimality in the linear case, including G-optimality, transductive optimality from optimal experimental design and asymptotic optimality.  Second, we design the first asymptotically optimal algorithm for fixed-confidence pure exploration in linear bandits. As a consequence, our algorithm naturally bypasses the pitfall caused by a simple but difficult instance, that most prior algorithms had to be engineered to deal with explicitly.  Finally, we avoid the need to fully solve an optimal design problem by providing an approach that entails an efficient implementation. 
","['Inria Paris', 'Inria', 'Inria', 'DeepMind']"
2020,Feature Selection using Stochastic Gates,"Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, Yuval Kluger",https://icml.cc/Conferences/2020/Schedule?showEvent=6600,"Feature selection problems have been extensively studied in the setting of linear estimation (e.g. LASSO), but less emphasis has been placed on feature selection for non-linear functions. 
In this study, we propose a method for feature selection in neural network estimation problems. The new procedure is based on probabilistic relaxation of the $\ell_0$ norm of features, or the count of the number of selected features. Our $\ell_0$-based regularization relies on a continuous relaxation of the Bernoulli distribution; such relaxation allows our model to learn the parameters of the approximate Bernoulli distributions via gradient descent. The proposed framework simultaneously learns either a nonlinear regression or classification function while selecting a small subset of features. We provide an information-theoretic justification for incorporating Bernoulli distribution into feature selection. Furthermore, we evaluate our method using synthetic and real-life data to demonstrate that our approach outperforms other commonly used methods in both predictive performance and feature selection.","['Yale University', 'Yale', 'YALE', 'Yale School of Medicine']"
2020,The Shapley Taylor Interaction Index,"Mukund Sundararajan, Kedar Dhamdhere, Ashish Agarwal",https://icml.cc/Conferences/2020/Schedule?showEvent=5853,"The attribution problem, that is the problem of attributing a model's prediction to its base features, is well-studied. We extend the notion of attribution to also apply to feature interactions.

The Shapley value is a commonly used method to attribute a model's prediction to its base features. We propose a generalization of the Shapley value called Shapley-Taylor index that attributes the model's prediction to interactions of subsets of features up to some size $k$. The method is analogous to how the truncated Taylor Series decomposes the function value at a certain point using its  derivatives at a different point. In fact, we show that the Shapley Taylor index is equal to the Taylor Series of the multilinear extension of the set-theoretic behavior of the model.   

We axiomatize this method using the standard Shapley axioms---linearity, dummy, symmetry and efficiency---and an additional axiom that we call the interaction distribution axiom. This new axiom explicitly characterizes how interactions are distributed for a class of functions that model pure interaction. 

We contrast the Shapley-Taylor index against the previously proposed Shapley Interaction index from the cooperative game theory literature. We also apply the Shapley Taylor index to three models and identify interesting qualitative insights.","['Google Inc.', 'Google', 'Google Brain']"
2020,Deep Gaussian Markov Random Fields,"Per Sidén, Fredrik Lindsten",https://icml.cc/Conferences/2020/Schedule?showEvent=6564,"Gaussian Markov random fields (GMRFs) are probabilistic graphical models widely used in spatial statistics and related fields to model dependencies over spatial structures. We establish a formal connection between GMRFs and convolutional neural networks (CNNs). Common GMRFs are special cases of a generative model where the inverse mapping from data to latent variables is given by a 1-layer linear CNN. This connection allows us to generalize GMRFs to multi-layer CNN architectures, effectively increasing the order of the corresponding GMRF in a way which has favorable computational scaling. We describe how well-established tools, such as autodiff and variational inference, can be used for simple and efficient inference and learning of the deep GMRF. We demonstrate the flexibility of the proposed model and show that it outperforms the state-of-the-art on a dataset of satellite temperatures, in terms of prediction and predictive uncertainty.
","['Linköping University', 'Linköping University']"
2020,Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization,"Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, Laurent Massoulié",https://icml.cc/Conferences/2020/Schedule?showEvent=5881,"We consider the setting of distributed empirical risk minimization where multiple machines compute the gradients in parallel and a centralized server updates the model parameters. In order to reduce the number of communications required to reach a given accuracy, we propose a preconditioned accelerated gradient method where the preconditioning is done by solving a local optimization problem over a subsampled dataset at the server. The convergence rate of the method depends on the square root of the relative condition number between the global and local loss functions. We estimate the relative condition number for linear prediction models by studying uniform concentration of the Hessians over a bounded domain, which allows us to derive improved convergence rates for existing preconditioned gradient methods and our accelerated method. Experiments on real-world datasets illustrate the benefits of acceleration in the ill-conditioned regime.
","['INRIA', 'Microsoft Research', 'Microsoft Research', 'INRIA - Ecole Normale Supérieure', 'MSR-INRIA Joint Center']"
2020,A Finite-Time Analysis of  Q-Learning with Neural Network Function Approximation,"Pan Xu, Quanquan Gu",https://icml.cc/Conferences/2020/Schedule?showEvent=6076,"Q-learning with neural network function approximation (neural Q-learning for short) is among the most prevalent deep reinforcement learning algorithms. Despite its empirical success, the non-asymptotic convergence rate of neural Q-learning remains virtually unknown. In this paper, we present a finite-time analysis of a neural Q-learning algorithm, where the data are generated from a Markov decision process, and the action-value function is approximated by a deep ReLU neural network. We prove that neural Q-learning finds the optimal policy with $O(1/\sqrt{T})$ convergence rate if the neural function approximator is sufficiently overparameterized, where $T$ is the number of iterations. To our best knowledge, our result is the first finite-time analysis of neural Q-learning under non-i.i.d. data assumption.","['University of California, Los Angeles', 'University of California, Los Angeles']"
2020,On the Number of Linear Regions of Convolutional Neural Networks,"Huan Xiong, Lei Huang, Mengyang Yu, Li Liu, Fan Zhu, Ling Shao",https://icml.cc/Conferences/2020/Schedule?showEvent=5799,"One fundamental problem in deep learning is understanding the outstanding performance of deep Neural Networks (NNs) in practice. One explanation for the superiority of NNs is that they can realize a large class of complicated functions, i.e., they have powerful expressivity. The expressivity of a ReLU NN can be quantified by the maximal number of linear regions it can separate its input space into. In this paper, we provide several mathematical results needed for studying the linear regions of CNNs, and use them to derive the maximal and average numbers of linear regions for one-layer ReLU CNNs. Furthermore, we obtain upper and lower bounds for the number of linear regions of multi-layer ReLU CNNs. Our results suggest that deeper CNNs have more powerful expressivity than their shallow counterparts, while CNNs have more expressivity than fully-connected NNs per parameter. 
","['Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)', 'Inception Institute of Artificial Intelligence', 'Inception Institute of Artificial Intelligence', 'the inception institute of artificial  intelligence', 'Inception Institute of Artificial Intelligence', 'Inception Institute of Artificial Intelligence']"
2020,The Implicit and Explicit Regularization Effects of Dropout,"Colin Wei, Sham Kakade, Tengyu Ma",https://icml.cc/Conferences/2020/Schedule?showEvent=6763,"Dropout is a widely-used regularization technique, often required to
  obtain state-of-the-art for a number of architectures. This work
  demonstrates that dropout introduces two distinct but entangled
  regularization effects: an explicit effect (also studied in
  prior work) which occurs since dropout modifies the expected 
  training objective, and, perhaps surprisingly, an additional
  implicit effect from the stochasticity in the dropout
  training update. This implicit regularization effect is 
  analogous to the effect of stochasticity in small mini-batch
  stochastic gradient descent.  We disentangle these two effects through 
  controlled experiments. We then derive analytic simplifications which
  characterize each effect in terms of the derivatives of the model
  and the loss, for deep neural networks.  We demonstrate these
  simplified, analytic regularizers accurately capture the important
  aspects of dropout, showing they faithfully replace dropout in
  practice.
","['Stanford University', 'University of Washington', 'Stanford']"
2020,Proving the Lottery Ticket Hypothesis: Pruning is All You Need,"Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, Ohad Shamir",https://icml.cc/Conferences/2020/Schedule?showEvent=6129,"The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. 
    We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training. 
","['Hebrew University Jerusalem Israel', 'Weizmann Institute of Science', 'Hebrew University of Jerusalem', 'Weizmann Institute of Science']"
2020,Option Discovery in the Absence of Rewards with Manifold Analysis,"Amitay Bar, Ronen Talmon, Ron Meir",https://icml.cc/Conferences/2020/Schedule?showEvent=6413,"Options have been shown to be an effective tool in reinforcement learning, facilitating improved exploration and learning. In this paper, we present an approach based on spectral graph theory and derive an algorithm that systematically discovers options without access to a specific reward or task assignment. As opposed to the common practice used in previous methods, our algorithm makes full use of the spectrum of the graph Laplacian.
Incorporating modes associated with higher graph frequencies unravels domain subtleties, which are shown to be useful for option discovery. Using geometric and manifold-based analysis, we present a theoretical justification for the algorithm. In addition, we showcase its performance in several domains, demonstrating clear improvements compared to competing methods.
","['Technion - Israel Institute of Technology', 'Technion - Israel Institute Of Technology', 'Technion Israeli Institute of Technology']"
2020,Double Trouble in Double Descent:  Bias and Variance(s) in the Lazy Regime,"Stéphane d'Ascoli, Maria Refinetti, Giulio Biroli, Florent Krzakala",https://icml.cc/Conferences/2020/Schedule?showEvent=6013,"Deep neural networks can achieve remarkable generalization performances while interpolating the training data. Rather than the U-curve emblematic of the bias-variance trade-off, their test error often follows a ``double descent""---a mark of the beneficial role of overparametrization. In this work, we develop a quantitative theory for this phenomenon in the so-called lazy learning regime of neural networks, by considering the problem of learning a high-dimensional function with random features regression. We obtain a precise asymptotic expression for the bias-variance decomposition of the test error, and show that the bias displays a phase transition at the interpolation threshold, beyond it which it remains constant. We disentangle the variances stemming from the sampling of the dataset, from the additive noise corrupting the labels, and from the initialization of the weights. We demonstrate that the latter two contributions are the crux of the double descent: they lead to the overfitting peak at the interpolation threshold and to the decay of the test error upon overparametrization. We quantify how they are suppressed by ensembling the outputs of $K$ independently initialized estimators. For $K\rightarrow \infty$, the test error is monotonously decreasing and remains constant beyond the interpolation threshold. We further compare the effects of overparametrizing, ensembling and regularizing. Finally, we present numerical experiments on classic deep learning setups to show that our results hold qualitatively in realistic lazy learning scenarios. ","['ENS', 'Laboratoire de Physique de l’Ecole Normale Supérieure Paris', 'ENS', 'ENS']"
2020,A Geometric Approach to Archetypal Analysis via Sparse Projections,"Vinayak Abrol, Pulkit Sharma",https://icml.cc/Conferences/2020/Schedule?showEvent=6390,"Archetypal analysis (AA) aims to extract patterns using self-expressive decomposition of data as convex combinations of extremal points (on the convex hull) of the data. This work presents a computationally efficient greedy AA (GAA) algorithm. GAA leverages the underlying geometry of AA, is scalable to larger datasets, and has significantly faster convergence rate. To achieve this, archetypes are learned via sparse projection of data. In the transformed space, GAA employs an iterative subset selection approach to identify archetypes based on the sparsity of convex representations. The work further presents the use of GAA algorithm for extended AA models such as robust and kernel AA. Experimental results show that GAA is considerably faster while performing comparable to existing methods for tasks such as classification, data visualization/categorization.
","['Mathematical Institute Oxford', 'University of Oxford']"
2020,DeBayes: a Bayesian Method for Debiasing Network Embeddings,"Maarten Buyl, Tijl De Bie",https://icml.cc/Conferences/2020/Schedule?showEvent=6000,"As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.
","['Ghent University', 'Ghent University']"
2020,PolyGen: An Autoregressive Generative Model of 3D Meshes,"Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter Battaglia",https://icml.cc/Conferences/2020/Schedule?showEvent=6851,"Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches for object synthesis have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present PolyGen, a generative model of 3D objects which models the mesh directly, predicting vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.
","['DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2020,Encoding Musical Style with Transformer Autoencoders,"Kristy Choi, Curtis Hawthorne, Ian Simon, Monica  Dinculescu, Jesse Engel",https://icml.cc/Conferences/2020/Schedule?showEvent=5978,"We consider the problem of learning high-level controls over the global structure of generated sequences, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global representation with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and melody. Empirically, we demonstrate the effectiveness of our method on various music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to baselines.
","['Stanford University', 'Google Research', 'Google Brain', 'Google Brain', 'Google Brain']"
2020,From Local SGD to Local Fixed-Point Methods for Federated Learning,"Grigory Malinovsky, Dmitry Kovalev, Elnur Gasanov, Laurent CONDAT, Peter Richtarik",https://icml.cc/Conferences/2020/Schedule?showEvent=6590,"Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed-point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one  based on a fixed number of  local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach.
","['Moscow Institute of Physics and Technology', 'KAUST', 'KAUST', 'KAUST', 'KAUST']"
2020,Adaptive Sketching for Fast and Convergent Canonical Polyadic Decomposition,"Alex Gittens, Kareem Aggour, Bülent Yener",https://icml.cc/Conferences/2020/Schedule?showEvent=6133,"This work considers the canonical polyadic decomposition (CPD) of tensors using proximally regularized sketched alternating least squares algorithms. First, it establishes a sublinear rate of convergence for proximally regularized sketched CPD algorithms under two natural conditions that are known to be satisfied by many popular forms of sketching. Second, it demonstrates that the iterative nature of CPD algorithms can be exploited algorithmically to choose more performant sketching rates. This is accomplished by introducing CPD-MWU, a proximally-regularized sketched alternating least squares algorithm that adaptively selects the sketching rate at each iteration. On both synthetic and real data we observe that for noisy tensors CPD-MWU produces decompositions of comparable accuracy to the standard CPD decomposition in less time, often half the time; for ill-conditioned tensors, given the same time budget, CPD-MWU produces decompositions with an order-of-magnitude lower relative error. For a representative real-world dataset CPD-MWU produces residual errors on average 20% lower than CPRAND-MIX and 44% lower than SPALS, two recent sketched CPD algorithms.
","['Rensselaer Polytechnic Institute', 'GE Research', 'Rensselaer Polytechnic Institute']"
2020,Reserve Pricing in Repeated Second-Price Auctions with Strategic Bidders,Alexey Drutsa,https://icml.cc/Conferences/2020/Schedule?showEvent=5789,"We study revenue optimization learning algorithms for repeated second-price auctions with reserve where a seller interacts with multiple strategic bidders each of which holds a fixed private valuation for a good and seeks to maximize his expected future cumulative discounted surplus.	
We propose a novel algorithm that  has strategic regret upper bound of $O(\log\log T)$ for worst-case valuations.
This pricing is based on our novel transformation that upgrades an algorithm designed for the setup with a single buyer to the multi-buyer case. 
We provide theoretical guarantees on the ability of a transformed algorithm to  learn the valuation of a strategic buyer, which has uncertainty about 
the future due to the presence of rivals.",['Yandex']
2020,DeepMatch: Balancing Deep Covariate Representations for Causal Inference Using Adversarial Training,Nathan Kallus,https://icml.cc/Conferences/2020/Schedule?showEvent=6247,"We study optimal covariate balance for causal inferences from observational data when rich covariates and complex relationships necessitate flexible modeling with neural networks. Standard approaches such as propensity weighting and matching/balancing fail in such settings due to miscalibrated propensity nets and inappropriate covariate representations, respectively. We propose a new method based on adversarial training of a weighting and a discriminator network that effectively addresses this methodological gap. This is demonstrated through new theoretical characterizations and empirical results on both synthetic and clinical data showing how causal analyses can be salvaged in such challenging settings.
",['Cornell University']
2020,T-Basis: a Compact Representation for Neural Networks,"Anton Obukhov, Maxim Rakhuba, Stamatios Georgoulis, Menelaos Kanakis, Dengxin  Dai, Luc Van Gool",https://icml.cc/Conferences/2020/Schedule?showEvent=6594,"We introduce T-Basis, a novel concept for a compact representation of a set of tensors, each of an arbitrary shape, which is often seen in Neural Networks. Each of the tensors in the set is modeled using Tensor Rings, though the concept applies to other Tensor Networks. Owing its name to the T-shape of nodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally shaped three-dimensional tensors, used to represent Tensor Ring nodes. Such representation allows us to parameterize the tensor set with a small number of parameters (coefficients of the T-Basis tensors), scaling logarithmically with each tensor's size in the set and linearly with the dimensionality of T-Basis. We evaluate the proposed approach on the task of neural network compression and demonstrate that it reaches high compression rates at acceptable performance drops. Finally, we analyze memory and operation requirements of the compressed networks and conclude that T-Basis networks are equally well suited for training and inference in resource-constrained environments and usage on the edge devices. Project website: obukhov.io/tbasis.
","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
2020,Scalable Differential Privacy with Certified Robustness in Adversarial Learning,"Hai Phan, My T. Thai, Han Hu, Ruoming Jin, Tong Sun, Dejing Dou",https://icml.cc/Conferences/2020/Schedule?showEvent=6401,"In this paper, we aim to develop a scalable algorithm to preserve differential privacy (DP) in adversarial learning for deep neural networks (DNNs), with certified robustness to adversarial examples. By leveraging the sequential composition theory in DP, we randomize both input and latent spaces to strengthen our certified robustness bounds. To address the trade-off among model utility, privacy loss, and robustness, we design an original adversarial objective function, based on the post-processing property in DP, to tighten the sensitivity of our model. A new stochastic batch training is proposed to apply our mechanism on large DNNs and datasets, by bypassing the vanilla iterative batch-by-batch training in DP DNNs. An end-to-end theoretical analysis and evaluations show that our mechanism notably improves the robustness and scalability of DP DNNs.
","['New Jersey Institute of Technology', 'University of Florida', 'New Jersey Institute of Technology', 'Kent State University', 'Adobe Research', '"" University of Oregon, USA""']"
2020,On Convergence-Diagnostic based Step Sizes for Stochastic Gradient Descent,"Scott Pesme, Aymeric Dieuleveut, Nicolas Flammarion",https://icml.cc/Conferences/2020/Schedule?showEvent=6832,"Constant step-size Stochastic Gradient Descent exhibits two phases: a transient phase during which iterates make fast progress towards the optimum, followed by a stationary phase during which iterates oscillate around the optimal point. In this paper, we show that efficiently detecting this transition and appropriately decreasing the step size can lead to fast convergence rates. We analyse the classical statistical test proposed by Pflug (1983), based on the inner product between consecutive stochastic gradients. Even in the simple case where the objective function is quadratic we show that this test cannot lead to an adequate convergence diagnostic. We then propose a novel and simple statistical procedure that accurately detects stationarity and we provide experimental results showing state-of-the-art performance on synthetic and real-word datasets.
","['EPFL', 'École polytechnique', 'EPFL']"
2020,k-means++:  few more steps yield constant approximation,"Davin Choo, Christoph Grunau, Julian Portmann, Vaclav Rozhon",https://icml.cc/Conferences/2020/Schedule?showEvent=6421,"The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k) approximation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant epsilon > 0, with only epsilon * k additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.
","['ETH Zurich', 'ETH Zürich', 'ETH Zürich', 'ETH']"
2020,Deep Coordination Graphs,"Wendelin Boehmer, Vitaly Kurin, Shimon Whiteson",https://icml.cc/Conferences/2020/Schedule?showEvent=6007,"This paper introduces the deep coordination graph (DCG) for collaborative multi-agent reinforcement learning. DCG strikes a flexible trade-off between representational capacity and generalization by factoring the joint value function of all agents according to a coordination graph into payoffs between pairs of agents. The value can be maximized by local message passing along the graph, which allows training of the value function end-to-end with Q-learning. Payoff functions are approximated with deep neural networks that employ parameter sharing and low-rank approximations to significantly improve sample efficiency. We show that DCG can solve predator-prey tasks that highlight the relative overgeneralization pathology, as well as challenging StarCraft II micromanagement tasks.
","['University of Oxford', 'University of Oxford', 'University of Oxford']"
2020,Scalable Exact Inference in Multi-Output Gaussian Processes,"Wessel Bruinsma, Eric Perim Martins, William Tebbutt, Scott Hosking, Arno Solin, Richard E Turner",https://icml.cc/Conferences/2020/Schedule?showEvent=6430,"Multi-output Gaussian processes (MOGPs) leverage the flexibility and interpretability of GPs while capturing structure across outputs, which is desirable, for example, in spatio-temporal modelling. The key problem with MOGPs is their computational scaling $O(n^3 p^3)$, which is cubic in the number of both inputs $n$ (e.g., time points or locations) and outputs $p$. For this reason, a popular class of MOGPs assumes that the data live around a low-dimensional linear subspace, reducing the complexity to $O(n^3 m^3)$. However, this cost is still cubic in the dimensionality of the subspace $m$, which is still prohibitively expensive for many applications. We propose the use of a sufficient statistic of the data to accelerate inference and learning in MOGPs with orthogonal bases. The method achieves linear scaling in $m$ in practice, allowing these models to scale to large $m$ without sacrificing significant expressivity or requiring approximation. This advance opens up a wide range of real-world tasks and can be combined with existing GP approximations in a plug-and-play way. We demonstrate the efficacy of the method on various synthetic and real-world data sets.","['University of Cambridge and Invenia Labs', 'Invenia Labs', 'University of Cambridge', 'British Antarctic Survey', 'Aalto University', 'University of Cambridge']"
2020,Convergence Rates of Variational Inference in Sparse Deep Learning,Badr-Eddine Chérief-Abdellatif,https://icml.cc/Conferences/2020/Schedule?showEvent=6099,"Variational inference is becoming more and more popular for approximating intractable posterior distributions in Bayesian statistics and machine learning. Meanwhile, a few recent works have provided theoretical justification and new insights on deep neural networks for estimating smooth functions in usual settings such as nonparametric regression. In this paper, we show that variational inference for sparse deep learning retains precisely the same generalization properties than exact Bayesian inference. In particular, we show that a wise choice of the neural network architecture leads to near-minimax rates of convergence for H\""older smooth functions. Additionally, we show that the model selection framework over the architecture of the network via ELBO maximization does not overfit and adaptively achieves the optimal rate of convergence.
",['CREST - ENSAE - Institut Polytechnique de Paris']
2020,Estimating Model Uncertainty of Neural Networks in Sparse Information Form,"Jongseok Lee, Matthias Humt, Jianxiang Feng, Rudolph Triebel",https://icml.cc/Conferences/2020/Schedule?showEvent=6166,"We present a sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. The key insight of our work is that the information matrix, i.e. the inverse of the covariance matrix tends to be sparse in its spectrum. Therefore, dimensionality reduction techniques such as low rank approximations (LRA) can be effectively exploited. To achieve this, we develop a novel sparsification algorithm and derive a cost-effective analytical sampler. As a result, we show that the information form can be scalably applied to represent model uncertainty in DNNs. Our exhaustive theoretical analysis and empirical evaluations on various benchmarks show the competitiveness of our approach over the current methods.
","['German Aerospace Center (DLR)', 'Deutsches Zentrum für Luft- und Raumfahrt (DLR) - German Aerospace Center', 'German Aerospace Center (DLR)', 'German Aerospace Center (DLR)']"
2020,CoMic: Complementary Task Learning & Mimicry for Reusable Skills,"Leonard Hasenclever, Fabio Pardo, Raia Hadsell, Nicolas Heess, Josh Merel",https://icml.cc/Conferences/2020/Schedule?showEvent=6587,"Learning to control complex bodies and reuse learned behaviors is a longstanding challenge in continuous control. We study the problem of learning reusable humanoid skills by imitating motion capture data and joint training with complementary tasks. We show that it is possible to learn reusable skills through reinforcement learning on 50 times more motion capture data than prior work. We systematically compare a variety of different network architectures across different data regimes both in terms of imitation performance as well as transfer to challenging locomotion tasks. Finally we show that it is possible to interleave the motion capture tracking with training on complementary tasks, enriching the resulting skill space, and enabling the reuse of skills not well covered by the motion capture data such as getting up from the ground or catching a ball.
","['DeepMind', 'Imperial College London', 'DeepMind', 'DeepMind', 'DeepMind']"
2020,Equivariant Neural Rendering,"Emilien Dupont, Miguel Angel Bautista Martin, Alex Colburn, Aditya Sankar, Joshua M Susskind, Qi Shan",https://icml.cc/Conferences/2020/Schedule?showEvent=6107,"We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.
","['University of Oxford', 'Apple Inc.', 'Apple Inc.', 'Apple Inc.', 'Apple, Inc.', 'Apple Inc']"
2020,Inductive-bias-driven Reinforcement Learning For Efficient Schedules in Heterogeneous Clusters,"Subho Banerjee, Saurabh Jha, Zbigniew Kalbarczyk, Ravishankar Iyer",https://icml.cc/Conferences/2020/Schedule?showEvent=6484,"The  problem  of  scheduling  of  workloads  onto heterogeneous processors (e.g., CPUs, GPUs, FPGAs)
is of fundamental importance in modern data centers.  Current system schedulers rely on
application/system-specific heuristics that have to be built on a case-by-case basis. Recent work
has demonstrated ML techniques for automating the heuristic search by using black-box approaches
which require significant training data and time, which  make  them  challenging  to  use  in
practice. This paper presents Symphony, a scheduling framework that addresses the challenge in two
ways:  (i)  a  domain-driven  Bayesian  reinforcement learning (RL) model for scheduling, which
inherently models the resource dependencies identified from the system architecture; and (ii) a
sampling-based technique to compute the gradients of a Bayesian model without performing full
probabilistic  inference. Together,  these  techniques reduce both the amount of training data and
the time required to produce scheduling policies that significantly outperform black-box
approaches by up to 2.2×.
","['University of Illinois at Urbana-Champaign', 'UIUC', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']"
2020,Unique Properties of Flat Minima in Deep Networks,"Rotem Mulayoff, Tomer Michaeli",https://icml.cc/Conferences/2020/Schedule?showEvent=5960,"It is well known that (stochastic) gradient descent has an implicit bias towards flat minima. In deep neural network training, this mechanism serves to screen out minima. However, the precise effect that this has on the trained network is not yet fully understood. In this paper, we characterize the flat minima in linear neural networks trained with a quadratic loss. First, we show that linear ResNets with zero initialization necessarily converge to the flattest of all minima. We then prove that these minima correspond to nearly balanced networks whereby the gain from the input to any intermediate representation does not change drastically from one layer to the next. Finally, we show that consecutive layers in flat minima solutions are coupled. That is, one of the left singular vectors of each weight matrix, equals one of the right singular vectors of the next matrix. This forms a distinct path from input to output, that, as we show, is dedicated to the signal that experiences the largest gain end-to-end. Experiments indicate that these properties are characteristic of both linear and nonlinear models trained in practice.
","['Technion - Israel Institute of Technology', 'Technion']"
2020,Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead,"Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh",https://icml.cc/Conferences/2020/Schedule?showEvent=6134,"Differential privacy (DP) is a formal notion for quantifying the privacy loss of algorithms.  Algorithms in the central model of DP achieve high accuracy but make the strongest trust assumptions whereas those in the local DP model make the weakest trust assumptions but incur substantial accuracy loss. The shuffled DP model [Bittau et al 2017, Erlingsson et al 2019, Cheu et al 19] has recently emerged as a feasible middle ground between the central and local models, providing stronger trust assumptions than the former while promising higher accuracies than the latter. In this paper, we obtain practical communication-efficient algorithms in the shuffled DP model for two basic aggregation primitives used in machine learning: 1) binary summation, and 2) histograms over a moderate number of buckets.  Our algorithms achieve accuracy that is arbitrarily close to that of central DP algorithms with an expected communication per user essentially matching what is needed without any privacy constraints! We demonstrate the practicality of our algorithms by experimentally evaluating them and comparing their performance to several widely-used protocols such as Randomized Response [Warner 1965] and RAPPOR [Erlingsson et al. 2014].
","['Google', 'Google', 'Google Research', 'IT University of Copenhagen']"
2020,My Fair Bandit: Distributed Learning of Max-Min Fairness with Multi-player Bandits,"Ilai Bistritz, Tavor Z Baharav, Amir Leshem, Nicholas Bambos",https://icml.cc/Conferences/2020/Schedule?showEvent=5766,"Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, representable
as an NxM matrix. These utilities are unknown to the players.
In each turn players receive noisy observations of their utility for their selected arm. However, if any other players selected the same arm that turn, they will all receive zero utility due to the conflict. No other communication or coordination between the players is possible. Our goal is to design a distributed algorithm that learns the matching between players and arms that achieves max-min fairness
while minimizing the regret. We present an algorithm and prove that
it is regret optimal up to a \log\log T factor. This is the first max-min fairness multi-player bandit algorithm with (near) order optimal regret. 
","['Stanford University', 'Stanford University', 'Bar-Ilan University', 'Stanford University']"
2020,Subspace Fitting Meets Regression: The Effects of Supervision and  Orthonormality Constraints on Double Descent of Generalization Errors,"Yehuda Dar, Paul Mayer, Lorenzo Luzi, Richard Baraniuk",https://icml.cc/Conferences/2020/Schedule?showEvent=6741,"We study the linear subspace fitting problem in the overparameterized setting, where the estimated subspace can perfectly interpolate the training examples. Our scope includes the least-squares solutions to subspace fitting tasks with varying levels of supervision in the training data (i.e., the proportion of input-output examples of the desired low-dimensional mapping) and orthonormality of the vectors defining the learned operator. This flexible family of problems connects standard, unsupervised subspace fitting that enforces strict orthonormality with a corresponding regression task that is fully supervised and does not constrain the linear operator structure. This class of problems is defined over a supervision-orthonormality plane, where each coordinate induces a problem instance with a unique pair of supervision level and softness of orthonormality constraints. We explore this plane and show that the generalization errors of the corresponding subspace fitting problems follow double descent trends as the settings become more supervised and less orthonormally constrained. 
","['Rice University', 'Rice University', 'Rice University', 'OpenStax / Rice University']"
2020,Training Linear Neural Networks: Non-Local Convergence and Complexity Results,Armin Eftekhari,https://icml.cc/Conferences/2020/Schedule?showEvent=5908,"Linear networks provide  valuable insights into the workings of neural networks in general. This paper identifies conditions under which the gradient flow provably trains a linear network, in spite of the non-strict saddle points present in the optimization landscape. This paper also provides the computational complexity of training linear networks with gradient flow. To achieve these results, this work develops a  machinery to provably identify the stable set of gradient flow, which then enables us to improve over the state of the art in the literature of linear networks (Bah et al., 2019;Arora et al., 2018a). Crucially, our results appear to be the first to break away from the lazy training regime which has dominated the literature of neural networks. This work requires the network to have a layer with one neuron, which subsumes the  networks  with a scalar output, but extending the results of this theoretical work to all linear networks remains a challenging open problem.
",['Umea University']
2020,No-Regret Exploration in Goal-Oriented Reinforcement Learning,"Jean Tarbouriech, Evrard Garcelon, Michal Valko, Matteo Pirotta, Alessandro Lazaric",https://icml.cc/Conferences/2020/Schedule?showEvent=5969,"Many popular reinforcement learning problems (e.g., navigation in a maze, some Atari games, mountain car) are instances of the episodic setting under its stochastic shortest path (SSP) formulation, where an agent has to achieve a goal state while minimizing the cumulative cost. Despite the popularity of this setting, the exploration-exploitation dilemma has been sparsely studied in general SSP problems, with most of the theoretical literature focusing on different problems (i.e., fixed-horizon and infinite-horizon) or making the restrictive loop-free SSP assumption (i.e., no state can be visited twice during an episode). In this paper, we study the general SSP problem with no assumption on its dynamics (some policies may actually never reach the goal). We introduce UC-SSP, the first no-regret algorithm in this setting, and prove a regret bound scaling as $\widetilde{\mathcal{O}}( D S \sqrt{ A D K})$ after $K$ episodes for any unknown SSP with $S$ states, $A$ actions, positive costs and SSP-diameter $D$, defined as the smallest expected hitting time from any starting state to the goal. We achieve this result by crafting a novel stopping rule, such that UC-SSP may interrupt the current policy if it is taking too long to achieve the goal and switch to alternative policies that are designed to rapidly terminate the episode.","['Facebook AI Research & Inria', 'Facebook AI Research and ENSAE', 'DeepMind', 'Facebook AI Research', 'Facebook AI Research']"
2020,Doubly Stochastic Variational Inference for Neural Processes with Hierarchical Latent Variables,"Qi Wang, Herke van Hoof",https://icml.cc/Conferences/2020/Schedule?showEvent=6122,"Neural processes (NPs) constitute a family of variational approximate models for stochastic processes with promising properties in computational efficiency and uncertainty quantification. These processes use neural networks with latent variable inputs to induce a predictive distribution.
However, the expressiveness of vanilla NPs is limited as they only use a global latent variable, while target-specific local variation may be crucial sometimes. To address this challenge, we investigate NPs systematically and present a new variant of NP model that we call Doubly Stochastic Variational Neural Process (DSVNP). This model combines the global latent variable and local latent variables for prediction. We evaluate this model in several experiments, and our results demonstrate competitive prediction performance in multi-output regression and uncertainty estimation in classification.
","['AMLab, University of Amsterdam', 'University of Amsterdam']"
2020,Towards a General Theory of Infinite-Width Limits of Neural Classifiers,Eugene Golikov,https://icml.cc/Conferences/2020/Schedule?showEvent=6845,"Obtaining theoretical guarantees for neural networks training appears to be a hard problem in a general case. Recent research has been focused on studying this problem in the limit of infinite width and two different theories have been developed: a mean-field (MF) and a constant kernel (NTK) limit theories. We propose a general framework that provides a link between these seemingly distinct theories. Our framework out of the box gives rise to a discrete-time MF limit which was not previously explored in the literature. We prove a convergence theorem for it, and show that it provides a more reasonable approximation for finite-width nets compared to the NTK limit if learning rates are not very small. Also, our framework suggests a limit model that coincides neither with the MF limit nor with the NTK one. We show that for networks with more than two hidden layers RMSProp training has a non-trivial MF limit but GD training does not have one. Overall, our framework demonstrates that both MF and NTK limits have considerable limitations in approximating finite-sized neural nets, indicating the need for designing more accurate infinite-width approximations for them.
",['Moscow Institute of Physics and Technology']
2020,Optimal Continual Learning has Perfect Memory and is NP-hard,"Jeremias Knoblauch, Hisham Husain, Tom Diethe",https://icml.cc/Conferences/2020/Schedule?showEvent=5796,"Continual Learning (CL) algorithms incrementally learn a predictor or representation across multiple sequentially observed tasks. Designing CL algorithms that perform reliably and avoid so-called catastrophic forgetting has proven a persistent challenge. The current paper develops a theoretical approach that explains why. In particular, we derive the computational properties which CL algorithms would have to possess in order to avoid catastrophic forgetting. Our main finding is that such optimal CL algorithms generally solve an NP-hard problem and will require perfect memory to do so. The findings are of theoretical interest, but also explain the excellent performance of CL algorithms using experience replay, episodic memory and core sets relative to regularization-based approaches.
","['Warwick University & The Alan Turing Institute', 'Australian National University', 'Amazon']"
2020,Efficient Optimistic Exploration in Linear-Quadratic Regulators via Lagrangian Relaxation,"Marc Abeille, Alessandro Lazaric",https://icml.cc/Conferences/2020/Schedule?showEvent=6453,"We study the exploration-exploitation dilemma in the linear quadratic regulator (LQR) setting. Inspired by the extended value iteration algorithm used in optimistic algorithms for finite MDPs, we propose to relax the optimistic optimization of \ofulq and cast it into a constrained \textit{extended} LQR problem, where an additional control variable implicitly selects the system dynamics within a confidence interval. We then move to the corresponding Lagrangian formulation for which we prove strong duality.  As a result, we show that an $\epsilon$-optimistic controller can be computed efficiently by solving at most $O\big(\log(1/\epsilon)\big)$ Riccati equations. Finally, we prove that relaxing the original \ofu problem does not impact the learning performance, thus recovering the $\wt O(\sqrt{T})$ regret of \ofulq. To the best of our knowledge, this is the first computationally efficient confidence-based algorithm for LQR with worst-case optimal regret guarantees.","['Criteo AI Lab', 'Facebook AI Research']"
2020,Dissecting Non-Vacuous Generalization Bounds based on the Mean-Field Approximation,Konstantinos Pitas,https://icml.cc/Conferences/2020/Schedule?showEvent=6503,"Explaining how overparametrized neural networks simultaneously achieve low risk and zero empirical risk on benchmark datasets is an open problem. PAC-Bayes bounds optimized using variational inference (VI) have been recently proposed as a promising direction in obtaining non-vacuous bounds. We show empirically that this approach gives negligible gains when modelling the posterior as a Gaussian with diagonal covariance---known as the mean-field approximation. We investigate common explanations, such as the failure of VI due to problems in optimization or choosing a suboptimal prior. Our results suggest that investigating richer posteriors is the most promising direction forward.
",['Ecole Polytechnique Federale de Lausanne']
2020,Automatic Reparameterisation of Probabilistic Programs,"Maria Gorinova, Dave Moore, Matthew Hoffman",https://icml.cc/Conferences/2020/Schedule?showEvent=5804,"Probabilistic programming has emerged as a powerful paradigm in statistics, applied science, and machine learning: by decoupling modelling from inference, it promises to allow modellers to directly reason about the processes generating data. However, the performance of inference algorithms can be dramatically affected by the parameterisation used to express a model, requiring users to transform their programs in non-intuitive ways. We argue for automating these transformations, and demonstrate that mechanisms available in recent modelling frameworks can implement non-centring and related reparameterisations. This enables new inference algorithms, and we propose two: a simple approach using interleaved sampling and a novel variational formulation that searches over a continuous space of parameterisations. We show that these approaches enable robust inference across a range of models, and can yield more efficient samplers than the best fixed parameterisation.
","['University of Edinburgh', 'Google', 'Google']"
2020,"Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support","Yuan Zhou, Hongseok Yang, Yee-Whye Teh, Tom Rainforth",https://icml.cc/Conferences/2020/Schedule?showEvent=5963,"Universal probabilistic programming systems (PPSs) provide a powerful framework for specifying rich probabilistic models. They further attempt to automate the process of drawing inferences from these models, but doing this successfully is severely hampered by the wide range of non--standard models they can express. As a result, although one can specify complex models in a universal PPS, the provided inference engines often fall far short of what is required. In particular, we show that they produce surprisingly unsatisfactory performance for models where the support varies between executions, often doing no better than importance sampling from the prior. To address this, we introduce a new inference framework: Divide, Conquer, and Combine, which remains efficient for such models, and show how it can be implemented as an automated and generic PPS inference engine. We empirically demonstrate substantial performance improvements over existing approaches on three examples.
","['University of Oxford', 'KAIST', 'Oxford and DeepMind', 'University of Oxford']"
2020,Transformation of ReLU-based recurrent neural networks from discrete-time to continuous-time,"Zahra Monfared, Daniel Durstewitz",https://icml.cc/Conferences/2020/Schedule?showEvent=6095,"Recurrent neural networks (RNN) as used in machine learning are commonly formulated in discrete time, i.e. as recursive maps. This brings a lot of advantages for training models on data, e.g. for the purpose of time series prediction or dynamical systems identification, as powerful and efficient inference algorithms exist for discrete time systems and numerical integration of differential equations is not necessary. On the other hand, mathematical analysis of dynamical systems inferred from data is often more convenient and enables additional insights if these are formulated in continuous time, i.e. as systems of ordinary (or partial) differential equations (ODE). Here we show how to perform such a translation from discrete to continuous time for a particular class of ReLU-based RNN. We prove three theorems on the mathematical equivalence between the discrete and continuous time formulations under a variety of conditions, and illustrate how to use our mathematical results on different machine learning and nonlinear dynamical systems examples.
","['CIMH/ Heidelberg University', 'CIMH/ Heidelberg University']"
2020,Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits,"Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp, Guy Van den Broeck, Kristian Kersting, Zoubin Ghahramani",https://icml.cc/Conferences/2020/Schedule?showEvent=6418,"Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent ``deep-learning-style'' implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs. In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards. At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations. As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation. Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.
","['Eindhoven University of Technology', 'Technical University of Darmstadt', 'University of California, Los Angeles', 'TU Darmstadt', 'TU Darmstadt', 'Graz University of Technology', 'University of California, Los Angeles', 'TU Darmstadt', 'University of Cambridge & Uber']"
2020,Adversarial Nonnegative Matrix Factorization,"lei luo, yanfu Zhang, Heng Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=6634,"Nonnegative Matrix Factorization (NMF) has become an increasingly important research topic in machine learning. 
Despite all the practical success, most of existing NMF models are still vulnerable to adversarial attacks. To overcome this limitation, we propose a novel Adversarial NMF (ANMF) approach in which an adversary can exercise some control over the perturbed data generation process. Different from the traditional NMF models which focus on  either the regular input or certain types of noise, our model considers potential test adversaries that are beyond the pre-defined constraints, which can cope with various noises (or perturbations). We formulate the proposed model as a bilevel optimization problem and use Alternating Direction Method of Multipliers (ADMM) to solve it with convergence analysis. Theoretically, the robustness analysis of ANMF is established under mild conditions dedicating asymptotically unbiased prediction.
Extensive experiments verify that ANMF is robust to a broad categories of perturbations, and achieves state-of-the-art performances on distinct real-world benchmark datasets.
","['JD', 'University of Pittsburgh', 'University of Pittsburgh & JD Finance America Corporation']"
2020,Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions,"Omer Gottesman, Joseph Futoma, Yao Liu, Sonali Parbhoo, Leo Celi, Emma Brunskill, Finale Doshi-Velez",https://icml.cc/Conferences/2020/Schedule?showEvent=6723,"Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares, as well as importance sampling methods. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.
","['Harvard University', 'Harvard University', 'Stanford University', 'Harvard University', 'MIT', 'Stanford University', 'Harvard University']"
2020,"Neuro-Symbolic Visual Reasoning: Disentangling ""Visual"" from ""Reasoning""","Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen Huang, Kazuhito Koishida",https://icml.cc/Conferences/2020/Schedule?showEvent=6760,"Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. However, recent advances in this area are still primarily driven by perception improvements (e.g. scene graph generation) rather than reasoning. Neuro-symbolic models such as Neural Module Networks bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own. To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions
even with imperfect perception. To this end, we introduce a Differentiable First-Order Logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this framework is used to perform in-depth, disentangled comparisons between well-known VQA models leading to informative insights regarding the participating models as well as the task.
","['Microsoft', 'Microsoft Research', 'Microsoft Research', 'Massachusetts Institute of Technology', 'Microsoft']"
2020,Relaxing Bijectivity Constraints with Continuously Indexed Normalising Flows,"Rob Cornish, Anthony Caterini, George Deligiannidis, Arnaud Doucet",https://icml.cc/Conferences/2020/Schedule?showEvent=6505,"We show that normalising flows become pathological when used to model targets whose supports have complicated topologies. In this scenario, we prove that a flow must become arbitrarily numerically noninvertible in order to approximate the target closely. This result has implications for all flow-based models, and especially residual flows (ResFlows), which explicitly control the Lipschitz constant of the bijection used. To address this, we propose continuously indexed flows (CIFs), which replace the single bijection used by normalising flows with a continuously indexed family of bijections, and which can intuitively ""clean up"" mass that would otherwise be misplaced by a single bijection. We show theoretically that CIFs are not subject to the same topological limitations as normalising flows, and obtain better empirical performance on a variety of models and benchmarks.
","['University of Oxford', 'University of Oxford', 'Oxford', 'Oxford University']"
2020,Implicit Regularization of Random Feature Models,"Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, Franck Gabriel",https://icml.cc/Conferences/2020/Schedule?showEvent=6454,"Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with $P$ features, $N$ data points, and a ridge $\lambda$, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an \textit{effective ridge} $\tilde{\lambda}$. We show that $\tilde{\lambda} > \lambda$ and $\tilde{\lambda} \searrow \lambda$ monotonically as $P$ grows, thus revealing the \textit{implicit regularization effect} of finite RF sampling. We then compare the risk (i.e. test error) of the $\tilde{\lambda}$-KRR predictor with the average risk of the $\lambda$-RF predictor and obtain a precise and explicit bound on their difference.
Finally, we empirically find an extremely good agreement between the test errors of the average $\lambda$-RF predictor and $\tilde{\lambda}$-KRR predictor.
","['EPFL', 'EPFL', 'EPFL', 'EPFL', 'EPFL']"
2020,Too Relaxed to Be Fair,"Michael Lohaus, Michaël Perrot, Ulrike von Luxburg",https://icml.cc/Conferences/2020/Schedule?showEvent=5842,"We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.
","['University of Tübingen', 'Université Jean Monnet, Saint-Etienne', 'University of Tübingen']"
2020,A new regret analysis for Adam-type algorithms,"Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, Volkan Cevher",https://icml.cc/Conferences/2020/Schedule?showEvent=6501,"In this paper, we focus on a theory-practice gap for Adam and its variants (AMSGrad, AdamNC, etc.). In practice, these algorithms are used with a constant first-order moment parameter $\beta_{1}$ (typically between $0.9$ and $0.99$). In theory, regret guarantees for online convex optimization require a rapidly decaying $\beta_{1}\to0$ schedule. We show that this is an artifact of the standard analysis, and we propose a novel framework that allows us to derive optimal, data-dependent regret bounds with a constant $\beta_{1}$, without further assumptions. We also demonstrate the flexibility of our analysis on a wide range of different algorithms and settings.","['EPFL', 'EPFL', 'CNRS and Criteo AI Lab', 'EPFL']"
2020,Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization,"Zhize Li, Dmitry Kovalev, Xun Qian, Peter Richtarik",https://icml.cc/Conferences/2020/Schedule?showEvent=6191,"Due to the high communication cost in distributed and federated learning problems, methods relying on compression of communicated messages are becoming increasingly popular. While in other contexts the best performing gradient-type methods invariably rely on some form of acceleration/momentum to reduce the number of iterations, there are no methods which combine the benefits of both gradient compression and acceleration. 
In this paper, we remedy this situation and propose the first {\em accelerated compressed gradient descent (ACGD)} methods. In the single machine regime, we prove that ACGD enjoys the rate $O\Big((1+\omega)\sqrt{\frac{L}{\mu}}\log \frac{1}{\epsilon}\Big)$ for $\mu$-strongly convex problems and $O\Big((1+\omega)\sqrt{\frac{L}{\epsilon}}\Big)$ for convex problems, respectively, where $\omega$ is the compression parameter. 
Our results improve upon the existing non-accelerated rates $O\Big((1+\omega)\frac{L}{\mu}\log \frac{1}{\epsilon}\Big)$ and $O\Big((1+\omega)\frac{L}{\epsilon}\Big)$, respectively, and recover the optimal rates of accelerated gradient descent as a special case when no compression ($\omega=0$) is applied. 
We further propose a distributed variant of ACGD (called ADIANA) and prove the convergence rate $\widetilde{O}\Big(\omega+\sqrt{\frac{L}{\mu}}+\sqrt{\big(\frac{\omega}{n}+\sqrt{\frac{\omega}{n}}\big)\frac{\omega L}{\mu}}\Big)$, where $n$ is the number of devices/workers and $\widetilde{O}$ hides the logarithmic factor $\log \frac{1}{\epsilon}$. This improves upon the previous best result $\widetilde{O}\Big(\omega + \frac{L}{\mu}+\frac{\omega L}{n\mu} \Big)$ achieved by the DIANA method. Finally, we conduct several experiments on real-world datasets which corroborate  our theoretical results and confirm the practical superiority of our accelerated methods.","['King Abdullah University of Science and Technology (KAUST)', 'KAUST', 'KAUST', 'KAUST']"
2020,Radioactive data: tracing through training,"Alexandre Sablayrolles, Douze Matthijs, Cordelia Schmid, Herve Jegou",https://icml.cc/Conferences/2020/Schedule?showEvent=6422,"Data tracing determines whether a particular image dataset has been used to train a model.
We propose a new technique, radioactive data, that makes imperceptible changes to this dataset such that any model trained on it will bear an identifiable mark. 
Given a trained model, our technique detects the use of radioactive data and provides a level of confidence (p-value). 
Experiments on large-scale benchmarks (Imagenet), with standard architectures (Resnet-18, VGG-16, Densenet-121) and training procedures, show that we detect radioactive data with high confidence (p<0.0001) when only 1% of the data used to trained a model is radioactive. 
Our radioactive mark is resilient to strong data augmentations and variations of the model architecture.
As a result, it offers a much higher signal-to-noise ratio than data poisoning and backdoor methods.
","['Facebook AI', 'Facebook AI Research', 'Inria/Google', 'Facebook AI Research']"
2020,Curvature-corrected learning dynamics in deep neural networks,Dongsung Huh,https://icml.cc/Conferences/2020/Schedule?showEvent=6470,"Deep neural networks exhibit complex learning dynamics due to the non-convexity of loss landscapes. Second-order optimization methods  facilitate learning dynamics by compensating for ill-conditioned curvature. We provide analytical description of how curvature-correction changes the learning dynamics in deep linear neural networks. It reveals that curvature-correction preserves the path of parameter dynamics, and thus only modifies the temporal profile along the path. This accelerates the convergence dynamics by reducing the nonlinear effect of depth on the learning dynamics of the input-output map. Our analysis also reveals an undesirable effect of curvature correction that compromises stability of parameters dynamics during learning, especially with block-diagonal approximation of natural gradient. We introduce fractional curvature-correction, which resolves the vanishing/exploding update problem while exhibiting most of the acceleration benefit of full curvature correction.
",['MIT-IBM Watson AI Lab']
2020,Learning to Encode Position for Transformer with Continuous Dynamical Model,"Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, Cho-Jui Hsieh",https://icml.cc/Conferences/2020/Schedule?showEvent=5906,"We introduce a new way of learning to encode position information for non-recurrent models, such as Transformer models. Unlike RNN and LSTM, which contain inductive bias by loading the input tokens sequentially, non-recurrent models are less sensitive to position. The main reason is that position information among input units is not encoded inherently, i.e., they are permutation equivalent, this problem justifies why all of the existing models are accompanied by position encoding/embedding layer at the input. However, this solution has clear limitations: the sinusoidal position encoding is not flexible enough as it is manually designed and does not contain any learnable parameters, whereas the position embedding restricts the maximum length of input sequences. It is thus desirable to design a new position layer that contains learnable parameters to adjust to different datasets and different architectures. At the same time, we would also like it to extrapolate in accordance with the variable length of inputs. In our proposed solution, we borrow from the recent Neural ODE approach, which may be viewed as a versatile continuous version of a ResNet. This model is capable of modeling many kinds of dynamical systems. We model the evolution of encoded results along position index by such a dynamical system, thereby overcoming the above limitations of existing methods. We evaluate our new position layers on a variety of neural machine translation and language understanding tasks, the experimental results show consistent improvements over the baselines.
","['University of California Los Angeles', 'Amazon', 'UT Austin & Amazon', 'UCLA']"
2020,Boosting Frank-Wolfe by Chasing Gradients,"Cyrille W. Combettes, Sebastian Pokutta",https://icml.cc/Conferences/2020/Schedule?showEvent=6608,"The Frank-Wolfe algorithm has become a popular first-order optimization algorithm for it is simple and projection-free, and it has been successfully applied to a variety of real-world problems. Its main drawback however lies in its convergence rate, which can be excessively slow due to naive descent directions. We propose to speed up the Frank-Wolfe algorithm by better aligning the descent direction with that of the negative gradient via a subroutine. This subroutine chases the negative gradient direction in a matching pursuit-style while still preserving the projection-free property. Although the approach is reasonably natural, it produces very significant results. We derive convergence rates $\mathcal{O}(1/t)$ to $\mathcal{O}(e^{-\omega t})$ of our method and we demonstrate its competitive advantage both per iteration and in CPU time over the state-of-the-art in a series of computational experiments.","['Georgia Institute of Technology', 'ZIB']"
2020,Discriminative Adversarial Search for Abstractive Summarization,"Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano",https://icml.cc/Conferences/2020/Schedule?showEvent=6473,"We introduce a novel approach for sequence decoding, Discriminative Adversarial Search (DAS), which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics. Inspired by Generative Adversarial Networks (GANs), wherein a discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is used to drive sequence generation at inference time. 
We investigate the effectiveness of the proposed approach on the task of Abstractive Summarization: the results obtained show that a naive application of DAS improves over the state-of-the-art methods, with further gains obtained via discriminator retraining. Moreover, we show how DAS can be effective for cross-domain adaptation. Finally, all results reported are obtained without additional rule-based filtering strategies, commonly used by the best performing systems available: this indicates that DAS can effectively be deployed without relying on post-hoc modifications of the generated outputs.
","['reciTAL', 'reciTAL', 'LIP6 - Sorbonne Universités', 'Sorbonne Université', 'reciTAL']"
2020,Adding seemingly uninformative labels helps in low data regimes,"Christos Matsoukas, Albert Bou Hernandez, Yue Liu, Karin Dembrower, Gisele Miranda, Emir Konuk, Johan Fredin Haslum, Athanasios Zouzos, Peter Lindholm, Fredrik Strand, Kevin Smith",https://icml.cc/Conferences/2020/Schedule?showEvent=6227,"Evidence suggests that networks trained on large datasets generalize well not solely because of the numerous training examples, but also class diversity which encourages learning of enriched features. This raises the question of whether this remains true when data is scarce - is there an advantage to learning with additional labels in low-data regimes? In this work, we consider a task that requires difficult-to-obtain expert annotations: tumor segmentation in mammography images. We show that, in low-data settings, performance can be improved by complementing the expert annotations with seemingly uninformative labels from non-expert annotators, turning the task into a multi-class problem. We reveal that these gains increase when less expert data is available, and uncover several interesting properties through further studies. We demonstrate our findings on CSAW-S, a new dataset that we introduce here, and confirm them on two public datasets.
","['KTH Royal Institute of Technology', 'Universitat Pompeu Fabra', 'KTH, Royal Institute of Technology', 'Karolinska Institute', 'Science for Life Laboratory', 'KTH', 'KTH Royal Institute of Technology', 'Karolinska Institute', 'Karolinska Institute', 'Karolinska Institutet', 'KTH Royal Institute of Technology']"
2020,Estimating the Error of Randomized Newton Methods: A Bootstrap Approach,"Miles Lopes, Jessie X.T. Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6743,"Randomized Newton methods have recently become the focus of intense research activity in large-scale and distributed optimization. In general, these methods are based on a computation-accuracy trade-off'', which allows the user to gain scalability in exchange for error in the solution. However, the user does not know how much error is created by the randomized approximation, which can be detrimental in two ways: On one hand, the user may try to assess the unknown error with theoretical worst-case error bounds, but this approach is impractical when the bounds involve unknown constants, and it often leads to excessive computation. On the other hand, the user may select thesketch size'' and stopping criteria in a heuristic manner, but this can lead to unreliable results. Motivated by these difficulties, we show how bootstrapping can be used to directly estimate the unknown error, which prevents excessive computation, and offers more confidence about the quality of a randomized solution. Furthermore, we show that the error estimation adds little computational cost to existing randomized Newton methods (e.g. \textsc{newton sketch} and \textsc{giant}), and it performs well empirically.
","['University of California, Davis', 'UC Davis']"
2020,Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization,"Vien Mai, Mikael Johansson",https://icml.cc/Conferences/2020/Schedule?showEvent=6458,"Stochastic gradient methods with momentum are widely used in applications and at the core of optimization subroutines in many popular machine learning libraries. However, their sample complexities have not been obtained for problems beyond those that are convex or smooth. This paper establishes the convergence rate of a stochastic subgradient method with a momentum term of Polyak type for a broad class of non-smooth, non-convex, and constrained optimization problems. Our key innovation is the construction of a special Lyapunov function for which the proven complexity can be achieved without any tuning of the momentum parameter. For smooth problems, we extend the known complexity bound to the constrained case and demonstrate how the unconstrained case can be analyzed under weaker assumptions than the state-of-the-art. Numerical results confirm our theoretical developments.
","['KTH Royal Institute of Technology', 'KTH Royal Institute of Technology']"
2020,Up or Down? Adaptive Rounding for Post-Training Quantization,"Markus Nagel, Rana Ali Amjad, Marinus van Baalen, Christos Louizos, Tijmen Blankevoort",https://icml.cc/Conferences/2020/Schedule?showEvent=6482,"When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1%.
","['Qualcomm AI Research', 'Qualcomm', 'Qualcomm', 'Qualcomm AI Research', 'Qualcomm']"
2020,Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge,"Laura Rieger, Chandan Singh, William Murdoch, Bin Yu",https://icml.cc/Conferences/2020/Schedule?showEvent=5914,"For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany of proposed explainable deep learning methods stop at the first step, providing practitioners with insight into a model, but no way to act on it. In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods to increase the predictive accuracy of a deep learning model. In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by inserting domain knowledge into the model  via explanations. We demonstrate the ability of CDEP to increase performance on an array of toy and real datasets.
","['Technical University of Denmark', 'UC Berkeley', 'UC Berkeley', 'University of California, Berkeley']"
2020,A Unified Theory of Decentralized SGD with Changing Topology and Local Updates,"Anastasiia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, Sebastian Stich",https://icml.cc/Conferences/2020/Schedule?showEvent=6533,"Decentralized stochastic optimization methods have gained a lot of attention recently, mainly because of their cheap per iteration cost, data locality, and their communication-efficiency. In this paper we introduce a unified convergence analysis that covers a large variety of decentralized SGD methods which so far have required different intuitions, have different applications, and which have been developed separately in various communities. 
Our algorithmic framework covers local SGD updates and synchronous and pairwise gossip updates on adaptive network topology. We derive universal convergence rates for smooth (convex and non-convex) problems and the rates interpolate between the heterogeneous (non-identically distributed data) and iid-data settings, recovering linear convergence rates in many special cases, for instance for over-parametrized models. Our proofs rely on weak assumptions (typically improving over prior work in several aspects) and recover (and improve) the best known complexity results for a host of important scenarios, such as for instance coorperative SGD and federated averaging (local SGD).
","['EPFL', 'Mila, Université de Montréal', 'EPFL', 'EPFL', 'EPFL']"
2020,Attentive Group Equivariant Convolutional Networks,"David Romero, Erik Bekkers, Jakub Tomczak, Mark Hoogendoorn",https://icml.cc/Conferences/2020/Schedule?showEvent=6467,"Although group convolutional networks are able to learn powerful representations based on symmetry patterns, they lack explicit means to learn meaningful relationships among them (e.g., relative positions and poses). In this paper, we present attentive group equivariant convolutions, a generalization of the group convolution, in which attention is applied during the course of convolution to accentuate meaningful symmetry combinations and suppress non-plausible, misleading ones. We indicate that prior work on visual attention can be described as special cases of our proposed framework and show empirically that our attentive group equivariant convolutional networks consistently outperform conventional group convolutional networks on benchmark image datasets. Simultaneously, we provide interpretability to the learned concepts through the visualization of equivariant attention maps.
","['Vrije Universiteit Amsterdam', 'University of Amsterdam', 'Vrije Universiteit Amsterdam', 'Vrije Universiteit Amsterdam']"
2020,Generalisation error in learning with random features and the hidden manifold model,"Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, Lenka Zdeborova",https://icml.cc/Conferences/2020/Schedule?showEvent=6414,"We study generalised linear regression and classification for a synthetically generated dataset encompassing different problems of interest, such as learning with random features, neural networks in the lazy training regime, and the hidden manifold model.  We consider the high-dimensional regime and using the replica method from statistical physics, we provide a closed-form expression for the asymptotic generalisation performance in these problems, valid in both the under- and over-parametrised regimes and for a broad choice of generalised linear model loss functions. In particular, we show how to obtain analytically the so-called double descent behaviour for logistic regression with a peak at the interpolation threshold, we illustrate the superiority of orthogonal against random Gaussian projections in learning with random features, and discuss the role played by correlations in the data generated by the hidden manifold model. Beyond the interest in these particular problems, the theoretical formalism introduced in this manuscript provides a path to further extensions to more complex tasks.
","['Institut de Physique Théorique', 'Université de Paris Saclay', 'ENS', 'ENS', 'CNRS']"
2020,Scalable and Efficient Comparison-based Search without Features,"Daniyar Chumbalov, Lucas Maystre, Matthias Grossglauser",https://icml.cc/Conferences/2020/Schedule?showEvent=6160,"We consider the problem of finding a target object t using pairwise comparisons, by asking an oracle questions of the form “Which object from the pair (i,j) is more similar to t?”. Objects live in a space of latent features, from which the oracle generates noisy answers. First, we consider the non-blind setting where these features are accessible. We propose a new Bayesian comparison-based search algorithm with noisy answers; it has low computational complexity yet is efficient in the number of queries. We provide theoretical guarantees, deriving the form of the optimal query and proving almost sure convergence to the target t. Second, we consider the blind setting, where the object features are hidden from the search algorithm. In this setting, we combine our search method and a new distributional triplet embedding algorithm into one scalable learning framework called Learn2Search. We show that the query complexity of our approach on two real-world datasets is on par with the non-blind setting, which is not achievable using any of the current state-of-the-art embedding methods. Finally, we demonstrate the efficacy of our framework by conducting a movie actors search experiment with real users.
","['EPFL', 'Spotify', 'EPFL']"
2020,On the Sample Complexity of Adversarial Multi-Source PAC Learning,"Nikola Konstantinov, Elias Frantar, Dan Alistarh, Christoph H. Lampert",https://icml.cc/Conferences/2020/Schedule?showEvent=6839,"We study the problem of learning from multiple untrusted data sources, a scenario of increasing practical relevance given the recent emergence of crowdsourcing and collaborative learning paradigms. Specifically, we analyze the situation in which a learning system obtains datasets from multiple sources, some of which might be biased or even adversarially perturbed. It is known that in the single-source case, an adversary with the power to corrupt a fixed fraction of the training data can prevent PAC-learnability, that is, even in the limit of infinitely much training data, no learning system can approach the optimal test error. In this work we show that, surprisingly, the same is not true in the multi-source setting, where the adversary can arbitrarily corrupt a fixed fraction of the data sources. Our main results are a generalization bound that provides finite-sample guarantees for this learning setting, as well as corresponding lower bounds. Besides establishing PAC-learnability our results also show that in a cooperative learning setting sharing data with other parties has provable benefits, even if some participants are malicious. 
","['IST Austria', 'TU Vienna', 'IST Austria & NeuralMagic', 'IST Austria']"
2020,Fast Adaptation to New Environments via Policy-Dynamics Value Functions,"Roberta Raileanu, Max Goldstein, Arthur Szlam, Facebook Rob Fergus",https://icml.cc/Conferences/2020/Schedule?showEvent=6424,"Standard RL algorithms assume fixed environment dynamics and require a significant amount of interaction to adapt to new environments. We introduce Policy-Dynamics Value Functions (PD-VF), a novel approach for rapidly adapting to dynamics different from those previously seen in training. PD-VF explicitly estimates the cumulative reward in a space of policies and environments. An ensemble of conventional RL policies is used to gather experience on training environments, from which embeddings of both policies and environments can be learned. Then, a value function conditioned on both embeddings is trained. At test time, a few actions are sufficient to infer the environment embedding, enabling a policy to be selected by maximizing the learned value function (which requires no additional environment interaction). We show that our method can rapidly adapt to new dynamics on a set of MuJoCo domains. 
","['NYU', 'NYU', 'Facebook', 'Facebook AI Research, NYU']"
2020,Self-Attentive Hawkes Process,"Qiang Zhang, Aldo Lipani, Omer Kirnap, Emine Yilmaz",https://icml.cc/Conferences/2020/Schedule?showEvent=5988,"Capturing the occurrence dynamics is crucial to predicting which type of events will happen next and when. A common method to do this is through Hawkes processes. To enhance their capacity, recurrent neural networks (RNNs) have been incorporated due to RNNs' successes in processing sequential data such as languages. Recent evidence suggests that self-attention is more competent than RNNs in dealing with languages. However, we are unaware of the effectiveness of self-attention in the context of Hawkes processes. This study aims to fill the gap by designing a self-attentive Hawkes process (SAHP). SAHP employs self-attention to summarise the influence of history events and compute the probability of the next event. One deficit of the conventional self-attention when applied to event sequences is that its positional encoding only considers the order of a sequence ignoring the time intervals between events. To overcome this deficit, we modify its encoding by translating time intervals into phase shifts of sinusoidal functions. Experiments on goodness-of-fit and prediction tasks show the improved capability of SAHP. Furthermore, SAHP is more interpretable than RNN-based counterparts because the learnt attention weights reveal contributions of one event type to the happening of another type. To the best of our knowledge, this is the first work that studies the effectiveness of self-attention in Hawkes processes. 
","['University College London', 'University College London (UCL)', 'University College London', 'University College London']"
2020,Continuous Time Bayesian Networks with Clocks,"Nicolai Engelmann, Dominik Linzner, Heinz Koeppl",https://icml.cc/Conferences/2020/Schedule?showEvent=6597,"Structured stochastic processes evolving in continuous time present a widely adopted framework to model phenomena occurring in nature and engineering. However, such models are often chosen to satisfy the Markov property to maintain tractability. One of the more popular of such memoryless models are Continuous Time Bayesian Networks (CTBNs). In this work, we lift its restriction to exponential survival times to arbitrary distributions. Current extensions achieve this via auxiliary states, which hinder tractability. To avoid that, we introduce a set of node-wise clocks to construct a collection of graph-coupled semi-Markov chains. We provide algorithms for parameter and structure inference, which make use of local dependencies and conduct experiments on synthetic data and a data-set generated through a benchmark tool for gene regulatory networks. In doing so, we point out advantages compared to current CTBN extensions.
","['Technische Universitat Darmstadt', 'Technische Universität Darmstadt', 'TU Darmstadt']"
2020,Bayesian Learning from Sequential Data using Gaussian Processes with Signature Covariances,"Csaba Toth, Harald Oberhauser",https://icml.cc/Conferences/2020/Schedule?showEvent=6407,"We develop a Bayesian approach to learning from sequential data by using Gaussian processes (GPs) with so-called signature kernels as covariance functions. This allows to make sequences of different length comparable and to rely on strong theoretical results from stochastic analysis.
Signatures capture sequential structure with tensors that can scale unfavourably in sequence length and state space dimension. To deal with this, we introduce a sparse variational approach with inducing tensors.
We then combine the resulting GP with LSTMs and GRUs to build larger models that leverage the strengths of each of these approaches and benchmark the resulting GPs on multivariate time series (TS) classification datasets.
","['University of Oxford', 'University of Oxford']"
2020,Super-efficiency of automatic differentiation for functions defined as a minimum,"Pierre Ablin, Gabriel Peyré, Thomas Moreau",https://icml.cc/Conferences/2020/Schedule?showEvent=6188,"In min-min optimization or max-min optimization, one has to compute the gradient of a function defined as a minimum. In most cases, the minimum has no closed-form, and an approximation is obtained via an iterative algorithm. There are two usual ways of estimating the gradient of the function: using either an analytic formula obtained by assuming exactness of the approximation, or automatic differentiation through the algorithm. In this paper, we study the asymptotic error made by these estimators as a function of the optimization error. We find that the error of the automatic estimator is close to the square of the error of the analytic estimator, reflecting a super-efficiency phenomenon. The convergence of the automatic estimator greatly depends on the convergence of the Jacobian of the algorithm. We analyze it for gradient descent and stochastic gradient descent and derive convergence rates for the estimators in these cases. Our analysis is backed by numerical experiments on toy problems and on Wasserstein barycenter computation. Finally, we discuss the computational complexity of these estimators and give practical guidelines to chose between them.
","['CNRS and ENS', 'CNRS and ENS', 'Inria']"
2020,Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention,"Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret",https://icml.cc/Conferences/2020/Schedule?showEvent=6257,"Transformers achieve remarkable performance in several tasks but due to their
quadratic complexity, with respect to the input's length, they are
prohibitively slow for very long sequences. To address this limitation, we
express the self-attention as a linear dot-product of kernel feature maps and
make use of the associativity property of matrix products to reduce the
complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length.
We show that this formulation permits an iterative implementation that
dramatically accelerates autoregressive transformers and reveals their
relationship to recurrent neural networks. Our \textit{Linear Transformers}
achieve similar performance to vanilla Transformers and they are up to 4000x
faster on autoregressive prediction of very long sequences.","['Idiap & EPFL', 'Idiap Research Institute and EPFL', 'University of Washington', 'University of Geneva']"
2020,State Space Expectation Propagation: Efficient Inference Schemes for Temporal Gaussian Processes,"William Wilkinson, Paul Chang, Michael Andersen, Arno Solin",https://icml.cc/Conferences/2020/Schedule?showEvent=6206,"We formulate approximate Bayesian inference in non-conjugate temporal and spatio-temporal Gaussian process models as a simple parameter update rule applied during Kalman smoothing. This viewpoint encompasses most inference schemes, including expectation propagation (EP), the classical (Extended, Unscented, \etc) Kalman smoothers, and variational inference. We provide a unifying perspective on these algorithms, showing how replacing the power EP moment matching step with linearisation recovers the classical smoothers. EP provides some benefits over the traditional methods via introduction of the so-called cavity distribution, and we combine these benefits with the computational efficiency of linearisation, providing extensive empirical analysis demonstrating the efficacy of various algorithms under this unifying framework. We provide a fast implementation of all methods in JAX.
","['Aalto University', 'Aalto University', 'Technical University of Denmark', 'Aalto University']"
2020,Leveraging Frequency Analysis for Deep Fake Image Recognition,"Joel Frank, Thorsten Eisenhofer, Lea Schönherr, Asja Fischer, Dorothea Kolossa, Thorsten Holz",https://icml.cc/Conferences/2020/Schedule?showEvent=6005,"Deep neural networks can generate images that are astonishingly realistic, so much so that it is often hard for humans to distinguish them from actual  photos.   These  achievements  have  been largely made possible by Generative Adversarial Networks (GANs).  While deep fake images have been thoroughly investigated in the image domain—a classical approach from the area of image  forensics—an  analysis  in  the frequency domain has been missing so far.  In this paper,we address this shortcoming and our results reveal that in frequency space, GAN-generated images  exhibit  severe  artifacts  that  can  be  easily identified. We perform a comprehensive analysis, showing that these artifacts are consistent across different neural network architectures, data sets, and resolutions. In a further investigation, we demonstrate that these artifacts are caused by upsampling operations found in all current GAN architectures, indicating a structural and fundamental problem in the way images are generated via GANs.  Based on this analysis, we demonstrate how the frequency representation can be used to identify deep fake images in an automated way, surpassing state-of-the-art methods.
","['Ruhr University Bochum', 'Ruhr University Bochum', 'Ruhr-Universität Bochum', 'Ruhr University Bochum', 'Ruhr University Bochum', 'Ruhr-Universität Bochum']"
2020,On the Generalization Benefit of Noise in Stochastic Gradient Descent,"Samuel Smith, Erich Elsen, Soham De",https://icml.cc/Conferences/2020/Schedule?showEvent=6135,"It has long been argued that minibatch stochastic gradient descent can generalize better than large batch gradient descent in deep neural networks. However recent papers have questioned this claim, arguing that this effect is simply a consequence of suboptimal hyperparameter tuning or insufficient compute budgets when the batch size is large. In this paper, we perform carefully designed experiments and rigorous hyperparameter sweeps on a range of popular models, which verify that small or moderately large batch sizes can substantially outperform very large batches on the test set. This occurs even when both models are trained for the same number of iterations and large batches achieve smaller training losses. Our results confirm that the noise in stochastic gradients can enhance generalization. We study how the optimal learning rate schedule changes as the epoch budget grows, and we provide a theoretical account of our observations based on the stochastic differential equation perspective of SGD dynamics.
","['DeepMind', 'Google', 'DeepMind']"
2020,Implicit Geometric Regularization for Learning Shapes,"Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman",https://icml.cc/Conferences/2020/Schedule?showEvent=6096,"Representing shapes as level-sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level-sets. 
In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level-set surfaces, avoiding bad zero-loss solutions. 
We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state-of-the-art implicit neural representations with higher level-of-details and fidelity compared to previous methods. 
","['Weizmann Institute of Science', 'Weizmann Institute of Science', 'Weizmann Institute of Science', 'Weizmann Institute of Science', 'Weizmann Institute of Science']"
2020,Student-Teacher Curriculum Learning via Reinforcement Learning: Predicting Hospital Inpatient Admission Location,"Rasheed El-Bouri, David Eyre, Peter Watkinson, Tingting Zhu, David Clifton",https://icml.cc/Conferences/2020/Schedule?showEvent=6412,"Accurate and reliable prediction of hospital admission location is important due to resource-constraints and space availability in a clinical setting, particularly when dealing with patients who come from the emergency department. In this work we propose a student-teacher network via reinforcement learning to deal with this specific problem. A representation of the weights of the student network is treated as the state and is fed as an input to the teacher network. The teacher network's action is to select the most appropriate batch of data to train the student network on from a training set sorted according to entropy. By validating on three datasets, not only do we show that our approach outperforms state-of-the-art methods on tabular data and performs competitively on image recognition, but also that novel curricula are learned by the teacher network. We demonstrate experimentally that the teacher network can actively learn about the student network and guide it to achieve better performance than if trained alone.
","['University of Oxford', 'University of Oxford', 'Oxford University Hospitals NHS Foundation Trust', 'University of Oxford', 'University of Oxford']"
2020,Involutive MCMC: a Unifying Framework,"Kirill Neklyudov, Max Welling, Evgenii Egorov, Dmitry Vetrov",https://icml.cc/Conferences/2020/Schedule?showEvent=6476,"Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. The field has developed a broad spectrum of algorithms, varying in the way they are motivated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a wide range of MCMC algorithms in terms of iMCMC, and formulate a number of ""tricks"" which one can use as design principles for developing new MCMC algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms, which facilitates the derivation of powerful extensions. We demonstrate the latter with two examples where we transform known reversible MCMC algorithms into more efficient irreversible ones.
","['Samsung', 'University of Amsterdam & Qualcomm', 'Skolkovo Institute of Science and Technology', 'Higher School of Economics, Samsung AI Center Moscow']"
2020,Interference and Generalization in Temporal Difference Learning,"Emmanuel Bengio, Joelle Pineau, Doina Precup",https://icml.cc/Conferences/2020/Schedule?showEvent=6586,"We study the link between generalization and interference in temporal-difference (TD) learning. Interference is defined as the inner product of two different gradients, representing their alignment; this quantity emerges as being of interest from a variety of observations about neural networks, parameter sharing and the dynamics of learning. We find that TD easily leads to low-interference, under-generalizing parameters, while the effect seems reversed in supervised learning. We hypothesize that the cause can be traced back to the interplay between the dynamics of interference and bootstrapping. This is supported empirically by several observations: the negative relationship between the generalization gap and interference in TD, the negative effect of bootstrapping on interference and the local coherence of targets, and the contrast between the propagation rate of information in TD(0) versus TD($\lambda$) and regression tasks such as Monte-Carlo policy evaluation. We hope that these new findings can guide the future discovery of better bootstrapping methods.","['McGill University', 'McGill University / Facebook', 'McGill University / DeepMind']"
2020,A Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits,"Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, Radu Grosu",https://icml.cc/Conferences/2020/Schedule?showEvent=6144,"We propose a neural information processing system obtained by re-purposing the function of a biological neural circuit model to govern simulated and real-world control tasks. Inspired by the structure of the nervous system of the soil-worm, C. elegans, we introduce ordinary neural circuits (ONCs), defined as the model of biological neural circuits reparameterized for the control of alternative tasks. We first demonstrate that ONCs realize networks with higher maximum flow compared to arbitrary wired networks. We then learn instances of ONCs to control a series of robotic tasks, including the autonomous parking of a real-world rover robot. For reconfiguration of the purpose of the neural circuit, we adopt a search-based optimization algorithm. Ordinary neural circuits perform on par and, in some cases, significantly surpass the performance of contemporary deep learning models. ONC networks are compact, 77% sparser than their counterpart neural controllers, and their neural dynamics are fully interpretable at the cell-level.
","['MIT', 'IST Austria', 'MIT', 'MIT CSAIL', 'TU Wien']"
2020,Quantum Boosting,"Srinivasan Arunachalam, Reevu Maity",https://icml.cc/Conferences/2020/Schedule?showEvent=6610,"Boosting is a technique that boosts a weak and inaccurate machine learning algorithm into a strong accurate learning algorithm. The AdaBoost algorithm by Freund and Schapire (for which they were awarded the G{\""o}del prize in 2003) is one of the widely used boosting algorithms, with many applications in theory and practice. Suppose we have a gamma-weak learner for a Boolean concept class C that takes time R(C), then the time complexity of AdaBoost scales as VC(C)poly(R(C), 1/gamma), where VC(C) is the VC-dimension of C. In this paper, we show how quantum techniques can improve the time complexity of classical AdaBoost. To this end, suppose we have a gamma-weak quantum learning algorithm for a Boolean concept class C that takes time Q(C), we introduce a quantum boosting algorithm whose complexity scales as sqrt{VC(C)}poly(Q(C),1/gamma); thereby achieving quadratic quantum improvement over classical AdaBoost in terms of  VC(C). 
","['IBM', 'Oxford University']"
2020,Growing Adaptive Multi-hyperplane Machines,"Nemanja Djuric, Zhuang Wang, Slobodan Vucetic",https://icml.cc/Conferences/2020/Schedule?showEvent=6507,"Adaptive Multi-hyperplane Machine (AMM) is an online algorithm for learning Multi-hyperplane Machine (MM), a classification model which allows multiple hyperplanes per class. AMM is based on Stochastic Gradient Descent (SGD), with training time comparable to linear Support Vector Machine (SVM) and significantly higher accuracy. On the other hand, empirical results indicate there is a large accuracy gap between AMM and non-linear SVMs. In this paper we show that this performance gap is not due to limited representability of the MM model, as it can represent arbitrary concepts. We set to explain the connection between the AMM and Learning Vector Quantization (LVQ) algorithms, and introduce a novel Growing AMM (GAMM) classifier motivated by Growing LVQ, that imputes duplicate hyperplanes into the MM model during SGD training. We provide theoretical results showing that GAMM has favorable convergence properties, and analyze the generalization bound of the MM models. Experiments indicate that GAMM achieves significantly improved accuracy on non-linear problems, with only slightly slower training compared to AMM. On some tasks GAMM comes close to non-linear SVM, and outperforms other popular classifiers such as Neural Networks and Random Forests.
","['Uber ATG', 'Facebook, Inc.', 'Temple University']"
2020,Sparse Gaussian Processes with Spherical Harmonic Features,"Vincent Dutordoir, Nicolas Durrande, James Hensman",https://icml.cc/Conferences/2020/Schedule?showEvent=5898,"We introduce a new class of inter-domain variational Gaussian processes (GP) where data is mapped onto the unit hypersphere in order to use spherical harmonic representations. Our inference scheme is comparable to variational Fourier features, but it does not suffer from the curse of dimensionality, and leads to diagonal covariance matrices between inducing variables. This enables a speed-up in inference, because it bypasses the need to invert large covariance matrices. Our experiments show that our model is able to fit a regression model for a dataset with 6 million entries two orders of magnitude faster compared to standard sparse GPs, while retaining state of the art accuracy. We also demonstrate competitive performance on classification with non-conjugate likelihoods.
","['PROWLER.io', 'PROWLER.io', 'PROWLER.io']"
2020,Control Frequency Adaptation via Action Persistence in Batch Reinforcement Learning,"Alberto Maria Metelli, Flavio Mazzolini, Lorenzo Bisi, Luca Sabbioni, Marcello Restelli",https://icml.cc/Conferences/2020/Schedule?showEvent=6146,"The choice of the control frequency of a system has a relevant impact on the ability of reinforcement learning algorithms to learn a highly performing policy. In this paper, we introduce the notion of action persistence that consists in the repetition of an action for a fixed number of decision steps, having the effect of modifying the control frequency. We start analyzing how action persistence affects the performance of the optimal policy, and then we present a novel algorithm, Persistent Fitted Q-Iteration (PFQI), that extends FQI, with the goal of learning the optimal value function at a given persistence. After having provided a theoretical study of PFQI and a heuristic approach to identify the optimal persistence, we present an experimental campaign on benchmark domains to show the advantages of action persistence and proving the effectiveness of our persistence selection method.
","['Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano', 'Politecnico di Milano']"
2020,Variance Reduced Coordinate Descent with Acceleration: New Method With a Surprising Application to Finite-Sum Problems,"Filip Hanzely, Dmitry Kovalev, Peter Richtarik",https://icml.cc/Conferences/2020/Schedule?showEvent=5949,"We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only. However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both  SEGA and SVRCD. As a by-product of our theory, we show that a variant of Katyusha (Allen-Zhu, 2017) is a specific case of ASVRCD, recovering the optimal oracle complexity for the finite sum objective.
","['KAUST', 'KAUST', 'KAUST']"
2020,Lifted Disjoint Paths with Application in Multiple Object Tracking,"Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, Paul Swoboda",https://icml.cc/Conferences/2020/Schedule?showEvent=5878,"We present an extension to the disjoint paths problem in which additional lifted edges are introduced to provide path connectivity priors.
We call the resulting optimization problem the lifted disjoint paths problem.
We show that this problem is NP-hard by reduction from integer multicommodity flow and 3-SAT.
To enable practical global optimization, we propose several classes of linear inequalities that produce a high-quality LP-relaxation.
Additionally, we propose efficient cutting plane algorithms for separating the proposed linear inequalities.
The lifted disjoint path problem is a natural model for multiple object tracking and allows an elegant mathematical formulation for long range temporal interactions.
Lifted edges help to prevent id switches and to re-identify persons. 
Our lifted disjoint paths tracker achieves nearly optimal assignments with respect to input detections. 
As a consequence, it leads on all three main benchmarks of the MOT challenge, improving significantly over state-of-the-art.
","['Max Planck Institute for Informatics', 'Leibniz University of Hannover', 'Leibniz University Hannover', 'MPI fuer Informatik, Saarbruecken']"
2020,Healing Products of Gaussian Process Experts,"samuel cohen, Rendani Mbuvha, Tshilidzi Marwala, Marc Deisenroth",https://icml.cc/Conferences/2020/Schedule?showEvent=6109,"Gaussian processes (GPs) are nonparametric Bayesian models that have been applied to regression and classification problems. One of the approaches to alleviate their cubic training cost is the use of local GP experts trained on subsets of the data. In particular, product-of-expert models combine the predictive distributions of local experts through a tractable product operation. While these expert models allow for massively distributed computation, their predictions typically suffer from erratic behaviour of the mean or uncalibrated uncertainty quantification. By calibrating predictions via a tempered softmax weighting, we provide a solution to these problems for multiple product-of-expert models, including the generalised product of experts and the robust Bayesian committee machine. Furthermore, we leverage the optimal transport literature and propose a new product-of-expert model that combines predictions of local experts by computing their Wasserstein barycenter, which can be applied to both regression and classification.
","['University College London', 'University of Johannesburg', 'University of Johannesburg', 'University College London']"
2020,Let's Agree to Agree: Neural Networks Share Classification Order on Real Datasets,"Guy Hacohen, Leshem Choshen, Daphna Weinshall",https://icml.cc/Conferences/2020/Schedule?showEvent=6404,"We report a series of robust empirical observations, demonstrating that deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries -- models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn benchmark datasets. Thus, when fixing the architecture, we show synthetic datasets where this pattern ceases to exist. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results reflect how neural networks discover structure in natural datasets.
","['Hebrew University of Jerusalem', 'Hebrew University, Jerusalem', 'Hebrew University of Jerusalem, Israel']"
2020,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,"Francesco Croce, Matthias Hein",https://icml.cc/Conferences/2020/Schedule?showEvent=6842,"The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.
","['University of Tuebingen', 'University of Tübingen']"
2020,Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes,"Chen-Yu Wei, Mehdi Jafarnia, Haipeng Luo, Hiteshi Sharma, Rahul Jain",https://icml.cc/Conferences/2020/Schedule?showEvent=6520,"Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm reduces the problem to the discounted-reward version and achieves $\mathcal{O}(T^{2/3})$ regret after $T$ steps, under the minimal assumption of weakly communicating MDPs. To our knowledge, this is the first model-free algorithm for general MDPs in this setting. The second algorithm makes use of recent advances in adaptive algorithms for adversarial multi-armed bandits and improves the regret to $\mathcal{O}(\sqrt{T})$, albeit with a stronger ergodic assumption. This result significantly improves over the $\mathcal{O}(T^{3/4})$ regret achieved by the only existing model-free algorithm by Abbasi-Yadkori et al. (2019) for ergodic MDPs in the infinite-horizon average-reward setting.","['University of Southern California', 'University of Southern California', 'University of Southern California', 'University of Southern California', 'USC']"
2020,"It's Not What Machines Can Learn, It's What We Cannot Teach","Gal Yehuda, Moshe Gabel, Assaf Schuster",https://icml.cc/Conferences/2020/Schedule?showEvent=6434,"Can deep neural networks learn to solve any task, and in particular problems of high complexity?
This question attracts a lot of interest, with recent works tackling computationally hard tasks such as the traveling salesman problem and satisfiability.
In this work we offer a different perspective on this question.
Given the common assumption that NP != coNP we prove that any polynomial-time sample generator for an NP-hard problem samples, in fact, from an easier sub-problem.
We empirically explore a case study, Conjunctive Query Containment, and show how common data generation techniques generate biased data-sets that lead practitioners to over-estimate model accuracy.
Our results suggest that machine learning approaches that require training on a dense uniform sampling from the target distribution cannot be used to solve computationally hard problems, the reason being the difficulty of generating sufficiently large and unbiased training sets.
","['Technion, I.I.T', 'University of Toronto', 'Technion']"
2020,VideoOneNet: Bidirectional Convolutional Recurrent OneNet with Trainable Data Steps for Video Processing,"Zoltán Á. Milacski, Barnabás Póczos, Andras Lorincz",https://icml.cc/Conferences/2020/Schedule?showEvent=6579,"Deep Neural Networks (DNNs) achieve the state-of-the-art results on a wide range of image processing tasks, however, the majority of such solutions are problem-specific, like most AI algorithms. The One Network to Solve Them All (OneNet) procedure has been suggested to resolve this issue by exploiting a DNN as the proximal operator in Alternating Direction Method of Multipliers (ADMM) solvers for various imaging problems. In this work, we make two contributions, both facilitating end-to-end learning using backpropagation. First, we generalize OneNet to videos by augmenting its convolutional prior network with bidirectional recurrent connections; second, we extend the fixed fully connected linear ADMM data step with another trainable bidirectional convolutional recurrent network. In our computational experiments on the Rotated MNIST, Scanned CIFAR-10 and UCF-101 data sets, the proposed modifications improve performance by a large margin compared to end-to-end convolutional OneNet and 3D Wavelet sparsity on several video processing problems: pixelwise inpainting-denoising, blockwise inpainting, scattered inpainting, super resolution, compressive sensing, deblurring, frame interpolation, frame prediction and colorization. Our two contributions are complementary, and using them together yields the best results.
","['ELTE Eötvös Loránd University', 'CMU', 'Eotvos Lorand University']"
2020,Multi-Precision Policy Enforced Training (MuPPET) : A Precision-Switching Strategy for Quantised Fixed-Point Training of CNNs,"Aditya Rajagopal, Diederik Vink, Stylianos Venieris, Christos-Savvas Bouganis",https://icml.cc/Conferences/2020/Schedule?showEvent=6250,"Large-scale convolutional neural networks (CNNs) suffer from very long training times, spanning from hours to weeks, limiting the productivity and experimentation of deep learning practitioners. As networks grow in size and complexity, training time can be reduced through low-precision data representations and computations, however, in doing so the final accuracy suffers due to the problem of vanishing gradients. Existing state-of-the-art methods combat this issue by means of a mixed-precision approach utilising two different precision levels, FP32 (32-bit floating-point) and FP16/FP8 (16-/8-bit floating-point), leveraging the hardware support of recent GPU architectures for FP16 operations to obtain performance gains. This work pushes the boundary of quantised training by employing a multilevel optimisation approach that utilises multiple precisions including low-precision fixed-point representations resulting in a novel training strategy MuPPET; it combines the use of multiple number representation regimes together with a precision-switching mechanism that decides at run time the transition point between precision regimes. Overall, the proposed strategy tailors the training process to the hardware-level capabilities of the target hardware architecture and yields improvements in training time and energy efficiency compared to state-of-the-art approaches. Applying MuPPET on the training of AlexNet, ResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA Turing GPU, MuPPET achieves the same accuracy as standard full-precision training with training-time speedup of up to 1.84x and an average speedup of 1.58x across the networks.
","['Imperial College London', 'Imperial College London', 'Samsung AI', 'Imperial College London']"
2020,DeepCoDA: personalized interpretability for compositional health data,"Thomas Quinn, Dang Nguyen, Santu Rana, Sunil Gupta, Svetha Venkatesh",https://icml.cc/Conferences/2020/Schedule?showEvent=6328,"Abstract Interpretability allows the domain-expert to directly evaluate the model's relevance and reliability, a practice that offers assurance and builds trust. In the healthcare setting, interpretable models should implicate relevant biological mechanisms independent of technical factors like data pre-processing. We define personalized interpretability as a measure of sample-specific feature attribution, and view it as a minimum requirement for a precision health model to justify its conclusions. Some health data, especially those generated by high-throughput sequencing experiments, have nuances that compromise precision health models and their interpretation. These data are compositional, meaning that each feature is conditionally dependent on all other features. We propose the Deep Compositional Data Analysis (DeepCoDA) framework to extend precision health modelling to high-dimensional compositional data, and to provide personalized interpretability through patient-specific weights. Our architecture maintains state-of-the-art performance across 25 real-world data sets, all while producing interpretations that are both personalized and fully coherent for compositional data.
","['A2I2', 'Deakin University', 'Deakin University', 'Deakin University', 'Deakin University']"
2020,Explicit Gradient Learning for Black-Box Optimization,"Elad Sarafian, Mor Sinay, yoram louzoun, Noa Agmon, Sarit Kraus",https://icml.cc/Conferences/2020/Schedule?showEvent=5927,"Black-Box Optimization (BBO) methods can find optimal policies for systems that interact with complex environments with no analytical representation. As such, they are of interest in many Artificial Intelligence (AI) domains. Yet classical BBO methods fall short in high-dimensional non-convex problems. They are thus often overlooked in real-world AI tasks. Here we present a BBO method, termed Explicit Gradient Learning (EGL), that is designed to optimize high-dimensional ill-behaved functions. We derive EGL by finding weak spots in methods that fit the objective function with a parametric Neural Network (NN) model and obtain the gradient signal by calculating the parametric gradient. Instead of fitting the function, EGL trains a NN to estimate the objective gradient directly. We prove the convergence of EGL to a stationary point and its robustness in the optimization of integrable functions. We evaluate EGL and achieve state-of-the-art results in two challenging problems: (1) the COCO test suite against an assortment of standard BBO methods; and (2) in a high-dimensional non-convex image generation task.
","['Bar-Ilan University', 'Bar-Ilan University', 'Bar Ilan University', 'Bar-Ilan University', 'Bar-Ilan University']"
2020,"Small Data, Big Decisions: Model Selection in the Small-Data Regime","Jorg Bornschein, Francesco Visin, Simon Osindero",https://icml.cc/Conferences/2020/Schedule?showEvent=6850,"Highly overparametrized neural networks can display curiously strong generalization performance -- a phenomenon that has recently garnered a wealth of theoretical and empirical research in order to better understand it.
In contrast to most previous work, which typically considers the performance as a function of the model size, in this paper we empirically study the generalization performance as the size of the training set varies over multiple orders of magnitude.
These systematic experiments lead to some interesting and potentially very useful 
observations; perhaps most notably that training on smaller subsets of the
data can lead to more reliable model selection decisions whilst simultaneously enjoying smaller computational overheads.
Our experiments furthermore allow us to estimate Minimum Description Lengths for common datasets given modern neural network architectures, thereby paving the way for principled model selection taking into account Occams-razor.
","['DeepMind', 'Deepmind', 'DeepMind']"
2020,Haar Graph Pooling,"Yuguang Wang, Ming Li, Zheng Ma, Guido Montufar, Xiaosheng Zhuang, Yanan Fan",https://icml.cc/Conferences/2020/Schedule?showEvent=6118,"Deep Graph Neural Networks (GNNs) are useful models for graph classification and graph-based regression tasks. In these tasks, graph pooling is a critical ingredient by which GNNs adapt to input graphs of varying size and structure. We propose a new graph pooling operation based on compressive Haar transforms --- \emph{HaarPooling}. HaarPooling implements a cascade of pooling operations; it is computed by following a sequence of clusterings of the input graph. A HaarPooling layer transforms a given input graph to an output graph with a smaller node number and the same feature dimension; the compressive Haar transform filters out fine detail information in the Haar wavelet domain.
In this way, all the HaarPooling layers together synthesize the features of any given input graph into a feature vector of uniform size. Such transforms provide a sparse characterization of the data and preserve the structure information of the input graph. GNNs implemented with standard graph convolution layers and HaarPooling layers achieve state of the art performance on diverse graph classification and regression problems.
","['UNSW; MPI MIS', 'Zhejiang Normal University', 'Princeton University', 'UCLA Math / Stat; MPI MIS', 'City University of Hong Kong', 'University of New South Wales']"
2020,MetaFun: Meta-Learning with Iterative Functional Updates,"Jin Xu, Jean-Francois Ton, Hyunjik Kim, Adam Kosiorek, Yee-Whye Teh",https://icml.cc/Conferences/2020/Schedule?showEvent=6581,"We develop a functional encoder-decoder approach to supervised meta-learning, where labeled data is encoded into an infinite-dimensional functional representation rather than a finite-dimensional one. Furthermore, rather than directly producing the representation, we learn a neural update rule resembling functional gradient descent which iteratively improves the representation. The final representation is used to condition the decoder to make predictions on unlabeled data. Our approach is the first to demonstrates the success of encoder-decoder style meta-learning methods like conditional neural processes on large-scale few-shot classification benchmarks such as miniImageNet and tieredImageNet, where it achieves state-of-the-art performance.
","['University of Oxford', 'University of Oxford', 'DeepMind', 'DeepMind', 'Oxford and DeepMind']"
2020,LP-SparseMAP: Differentiable Relaxed Optimization for Sparse Structured Prediction,"Vlad Niculae, Andre Filipe Torres Martins",https://icml.cc/Conferences/2020/Schedule?showEvent=6518,"Structured predictors require solving a combinatorial optimization problem over a large number of structures, such as dependency trees or alignments. When embedded as structured hidden layers in a neural net, argmin differentiation and efficient gradient computation are further required. Recently, SparseMAP has been proposed as a differentiable, sparse alternative to maximum a posteriori (MAP) and marginal inference. SparseMAP returns an interpretable combination of a small number of structures; its sparsity being the key to efficient optimization. However, SparseMAP requires access to an exact MAP oracle in the structured model, excluding, e.g., loopy graphical models or logic constraints, which generally require approximate inference. In this paper, we introduce LP-SparseMAP, an extension of SparseMAP addressing this limitation via a local polytope relaxation. LP-SparseMAP uses the flexible and powerful language of factor graphs to define expressive hidden structures, supporting coarse decompositions, hard logic constraints, and higher-order correlations. We derive the forward and backward algorithms needed for using LP-SparseMAP as a structured hidden or output layer. Experiments in three structured tasks show benefits versus SparseMAP and Structured SVM.
","['Instituto de Telecomunicações // NIF 502854200', 'Instituto de Telecomunicacoes']"
2020,Statistically Efficient Off-Policy Policy Gradients,"Nathan Kallus, Masatoshi Uehara",https://icml.cc/Conferences/2020/Schedule?showEvent=5945,"Policy gradient methods in reinforcement learning update policy parameters by taking steps in the direction of an estimated gradient of policy value. In this paper, we consider the efficient estimation of policy gradients from off-policy data, where the estimation is particularly non-trivial. We derive the asymptotic lower bound on the feasible mean-squared error in both Markov and non-Markov decision processes and show that existing estimators fail to achieve it in general settings. We propose a meta-algorithm that achieves the lower bound without any parametric assumptions and exhibits a unique 4-way double robustness property. We discuss how to estimate nuisances that the algorithm relies on. Finally, we establish guarantees at the rate at which we approach a stationary point when we take steps in the direction of our new estimated policy gradient.
","['Cornell University', 'Harvard University']"
2020,DINO: Distributed Newton-Type Optimization Method,"Rixon Crane, Fred Roosta",https://icml.cc/Conferences/2020/Schedule?showEvent=6101,"We present a novel communication-efficient Newton-type algorithm for finite-sum optimization over a distributed computing environment. Our method, named DINO, overcomes both theoretical and practical shortcomings of similar existing methods. Under minimal assumptions, we guarantee global sub-linear convergence of DINO to a first-order stationary point for general non-convex functions and arbitrary data distribution over the network. Furthermore, for functions satisfying Polyak-Lojasiewicz (PL) inequality, we show that DINO enjoys a linear convergence rate. Our proposed algorithm is practically parameter free, in that it will converge regardless of the selected hyper-parameters, which are easy to tune. Additionally, its sub-problems are simple linear least-squares, for which efficient solvers exist, and numerical simulations demonstrate the efficiency of DINO as compared with similar alternatives.
","['The University of Queensland', 'University of Queensland']"
2020,XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning,"Sung Whan Yoon, Do-Yeon Kim, Jun Seo, Jaekyun Moon",https://icml.cc/Conferences/2020/Schedule?showEvent=6852,"Learning novel concepts while preserving prior knowledge is a long-standing challenge in machine learning. The challenge gets greater when a novel task is given with only a few labeled examples, a problem known as incremental few-shot learning. We propose XtarNet, which learns to extract task-adaptive representation (TAR) for facilitating incremental few-shot learning. The method utilizes a backbone network pretrained on a set of base categories while also employing additional modules that are meta-trained across episodes. Given a new task, the novel feature extracted from the meta-trained modules is mixed with the base feature obtained from the pretrained model. The process of combining two different features provides TAR and is also controlled by meta-trained modules. The TAR contains effective information for classifying both novel and base categories. The base and novel classifiers quickly adapt to a given task by utilizing the TAR. Experiments on standard image datasets indicate that XtarNet achieves state-of-the-art incremental few-shot learning performance. The concept of TAR can also be used in conjunction with existing incremental few-shot learning methods; extensive simulation results in fact show that applying TAR enhances the known methods significantly.
","['Ulsan National Institute of Science and Technology (UNIST)', 'Korea Advanced Institute of Science and Technology', 'Korea Advanced Institute of Science and Technology(KAIST)', 'KAIST']"
2020,Agent57: Outperforming the Atari Human Benchmark,"Adrià Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Oleksandr Vitvitskyi, Zhaohan Guo, Charles Blundell",https://icml.cc/Conferences/2020/Schedule?showEvent=6392,"Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.
","['Deepmind', 'DeepMind', 'Deepmind', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind']"
2020,Orthogonalized SGD and Nested Architectures for Anytime Neural Networks,"Chengcheng Wan, Henry (Hank) Hoffmann, Shan Lu, Michael Maire",https://icml.cc/Conferences/2020/Schedule?showEvent=6765,"We propose a novel variant of SGD customized for training network architectures that support anytime behavior: such networks produce a series of increasingly accurate outputs over time. Efficient architectural designs for these networks focus on re-using internal state; subnetworks must produce representations relevant for both imme- diate prediction as well as refinement by subse- quent network stages. We consider traditional branched networks as well as a new class of re- cursively nested networks. Our new optimizer, Orthogonalized SGD, dynamically re-balances task-specific gradients when training a multitask network. In the context of anytime architectures, this optimizer projects gradients from later out- puts onto a parameter subspace that does not in- terfere with those from earlier outputs. Experi- ments demonstrate that training with Orthogonal- ized SGD significantly improves generalization accuracy of anytime networks.
","['University of Chicago', 'The University of Chicago', 'University of Chicago', 'University of Chicago']"
2020,Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders,"Ioana Bica, Ahmed Alaa, Mihaela van der Schaar",https://icml.cc/Conferences/2020/Schedule?showEvent=6131,"The estimation of treatment effects is a pervasive problem in medicine. Existing methods for estimating treatment effects from longitudinal observational data assume that there are no hidden confounders, an assumption that is not testable in practice and, if it does not hold, leads to biased estimates. In this paper, we develop the Time Series Deconfounder, a method that leverages the assignment of multiple treatments over time to enable the estimation of treatment effects in the presence of multi-cause hidden confounders. The Time Series Deconfounder uses a novel recurrent neural network architecture with multitask output to build a factor model over time and infer latent variables that render the assigned treatments conditionally independent; then, it performs causal inference using these latent variables that act as substitutes for the multi-cause unobserved confounders. We provide a theoretical analysis for obtaining unbiased causal effects of time-varying exposures using the Time Series Deconfounder. Using both simulated and real data we show the effectiveness of our method in deconfounding the estimation of treatment responses over time. 
","['University of Oxford', 'UCLA', 'University of Cambridge and UCLA']"
2020,From Chaos to Order: Symmetry and Conservation Laws in Game Dynamics,"Sai Ganesh Nagarajan, David Balduzzi, Georgios Piliouras",https://icml.cc/Conferences/2020/Schedule?showEvent=6825,"Games are an increasingly useful tool for training and testing learning algorithms. Recent examples include GANs, AlphaZero and the AlphaStar league. However, multi-agent learning can be extremely difficult to predict and control.  Learning dynamics even in simple games can yield chaotic behavior. In this paper, we present basic \emph{mechanism design} tools for constructing games with predictable and controllable dynamics. We show that arbitrarily large and complex network games, encoding both cooperation (team play) and competition (zero-sum interaction), exhibit conservation laws when agents use the standard regret-minimizing dynamics known as Follow-the-Regularized-Leader. These laws persist when different agents use different dynamics and encode long-range correlations between agents' behavior, even though the agents may not interact directly. Moreover, we provide sufficient conditions under which the dynamics have multiple, linearly independent, conservation laws. Increasing the number of conservation laws results in more predictable dynamics, eventually making chaotic behavior formally impossible in some cases.
","['SUTD', 'DeepMind', 'Singapore University of Technology and Design']"
2020,Optimization from Structured Samples for Coverage Functions,"Wei Chen, Xiaoming Sun, Jialin Zhang, Zhijie Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6352,"We revisit the optimization from samples (OPS) model, which studies the problem of optimizing objective functions directly from the sample data. Previous results showed that we cannot obtain a constant approximation ratio for the maximum coverage problem using polynomially many independent samples of the form $\{S_i, f(S_i)\}_{i=1}^t$ (Balkanski et al., 2017), even if coverage functions are $(1 - \epsilon)$-PMAC learnable using these samples (Badanidiyuru et al., 2012), which means most of the function values can be approximately learned very well with high probability. In this work, to circumvent the impossibility result of OPS, we propose a stronger model called optimization from structured samples (OPSS) for coverage functions, where the data samples encode the structural information of the functions. We show that under three general assumptions on the sample distributions, we can design efficient OPSS algorithms that achieve a constant approximation for the maximum coverage problem. We further prove a constant lower bound under these assumptions, which is tight when not considering computational efficiency. Moreover, we also show that if we remove any one of the three assumptions, OPSS for the maximum coverage problem has no constant approximation.","['Microsoft', 'Institute of Computing Technology, Chinese Academy of Sciences ', 'Institute of Computing Technology, CAS', 'Institute of Computing Technology, Chinese Academy of Sciences']"
2020,"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback","Michihiro Yasunaga, Percy Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6722,"We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small.
In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 68.2\% full repair rate on DeepFix (+22.9\% over the prior best), and 48.4\% synthesis success rate on SPoC (+3.7\% over the prior best).
","['Stanford University', 'Stanford University']"
2020,Boosted Histogram Transform for Regression,"Yuchao Cai, Hanyuan Hang, Hanfang Yang, Zhouchen Lin",https://icml.cc/Conferences/2020/Schedule?showEvent=6139,"In this paper, we propose a boosting algorithm for regression problems called \textit{boosted histogram transform for regression} (BHTR) based on histogram transforms composed of random rotations, stretchings, and translations. From the theoretical perspective, we first prove fast convergence rates for BHTR under the assumption that the target function lies in the spaces $C^{0,\alpha}$. Moreover, if the target function resides in the subspace $C^{1,\alpha}$, by establishing the upper bound of the convergence rate for the boosted regressor, i.e. BHTR, and the lower bound for base regressors, i.e. histogram transform regressors (HTR), we manage to explain the benefits of the boosting procedure. In the experiments, compared with other state-of-the-art algorithms such as gradient boosted regression tree (GBRT), Breiman's forest, and kernel-based methods, our BHTR algorithm shows promising performance on both synthetic and real datasets.","['AI Lab, Samsung Research China - Beijing', 'AI Lab, Samsung Research China - Beijing', 'Renmin university of China', 'Peking University']"
2020,Spread Divergence,"Mingtian Zhang, Peter Hayes, Thomas Bird, Raza Habib, David Barber",https://icml.cc/Conferences/2020/Schedule?showEvent=6040,"For distributions $\mathbb{P}$ and $\mathbb{Q}$ with different supports or undefined densities, the divergence $\textrm{D}(\mathbb{P}||\mathbb{Q})$ may not exist. We define a Spread Divergence $\tilde{\textrm{D}}(\mathbb{P}||\mathbb{Q})$  on modified $\mathbb{P}$ and $\mathbb{Q}$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a Spread Divergence to train implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).","['UCL', 'University College London', 'UCL', 'UCL', 'University College London']"
2020,Unsupervised Transfer Learning for Spatiotemporal Predictive Networks,"Zhiyu Yao, Yunbo Wang, Mingsheng Long, Jianmin Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6100,"This paper explores a new research problem of unsupervised transfer learning across multiple spatiotemporal prediction tasks. Unlike most existing transfer learning methods that focus on fixing the discrepancy between supervised tasks, we study how to transfer knowledge from a zoo of unsupervisedly learned models towards another predictive network. Our motivation is that models from different sources are expected to understand the complex spatiotemporal dynamics from different perspectives, thereby effectively supplementing the new task, even if the task has sufficient training samples. Technically, we propose a differentiable framework named transferable memory. It adaptively distills knowledge from a bank of memory states of multiple pretrained RNNs, and applies it to the target network via a novel recurrent structure called the Transferable Memory Unit (TMU). Compared with finetuning, our approach yields significant improvements on three benchmarks for spatiotemporal prediction, and benefits the target task even from less relevant pretext ones.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2020,Online Dense Subgraph Discovery via Blurred-Graph Feedback,"Yuko Kuroki, Atsushi Miyauchi, Junya Honda, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=5919,"\emph{Dense subgraph discovery} aims to find a dense component in edge-weighted graphs. This is a fundamental graph-mining task with a variety of applications and thus has received much attention recently. Although most existing methods assume that each individual edge weight is easily obtained, such an assumption is not necessarily valid in practice. In this paper, we introduce a novel learning problem for dense subgraph discovery in which a learner queries edge subsets rather than only single edges and observes a noisy sum of edge weights in a queried subset. For this problem, we first propose a polynomial-time algorithm that obtains a nearly-optimal solution with high probability. Moreover, to deal with large-sized graphs, we design a more scalable algorithm with a theoretical guarantee. Computational experiments using real-world graphs demonstrate the effectiveness of our algorithms.
","['The University of Tokyo /RIKEN', 'University of Tokyo', 'University of Tokyo / RIKEN', 'RIKEN / The University of Tokyo']"
2020,Non-separable Non-stationary random fields,"Kangrui Wang, Oliver Hamelijnck, Theodoros Damoulas, Mark Steel",https://icml.cc/Conferences/2020/Schedule?showEvent=6398,"We describe a framework for constructing nonstationary nonseparable random fields based on an infinite mixture of convolved stochastic processes. When the mixing process is stationary but the convolution function is nonstationary we arrive at nonseparable kernels with constant non-separability that are available in closed form. When the mixing is nonstationary and the convolution function is stationary we arrive at nonseparable random fields that have varying nonseparability and better preserve local structure. These fields have natural interpretations through the spectral representation of stochastic differential equations (SDEs) and are demonstrated on a range of synthetic benchmarks and spatio-temporal applications in geostatistics and machine learning. We show how a single Gaussian process (GP) with these random fields can computationally and statistically outperform both separable and existing nonstationary nonseparable approaches such as treed GPs and deep GP constructions. 
","['The Alan Turing Institute', 'The Alan Turing Institute', 'University of Warwick & The Alan Turing Institute', 'University of Warwick']"
2020,On Lp-norm Robustness of Ensemble Decision Stumps and Trees,"Yihan Wang, Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh",https://icml.cc/Conferences/2020/Schedule?showEvent=6813,"Recent papers have demonstrated that ensemble stumps and trees could be vulnerable to small input perturbations, so robustness verification and defense for those models have become an important research problem. However, due to the structure of decision trees, where each node makes decision purely based on one feature value, all the previous works only consider the $\ell_\infty$ norm perturbation. To study robustness with respect to a general $\ell_p$ norm perturbation, one has to consider the correlation between perturbations on different features, which has not been handled by previous algorithms. In this paper, we study the problem of robustness verification and certified defense with respect to general $\ell_p$ norm perturbations for ensemble decision stumps and trees. For robustness verification of ensemble stumps, we prove that complete verification is NP-complete for $p\in(0, \infty)$ while polynomial time algorithms exist for $p=0$ or $\infty$. For $p\in(0, \infty)$ we develop an efficient dynamic programming based algorithm for sound verification of ensemble stumps. For ensemble trees, we generalize the previous multi-level robustness verification algorithm to $\ell_p$ norm.
We demonstrate the first certified defense method for training ensemble stumps and trees with respect to $\ell_p$ norm perturbations, and verify its effectiveness empirically on real datasets.","['Tsinghua University', 'UCLA', 'MIT', 'MIT', 'UCLA']"
2020,Adversarial Robustness via Runtime Masking and Cleansing,"Yi-Hsuan Wu, Chia-Hung Yuan, Shan-Hung (Brandon) Wu",https://icml.cc/Conferences/2020/Schedule?showEvent=5817,"Deep neural networks are shown to be vulnerable to adversarial attacks. This motivates robust learning techniques, such as the adversarial training, whose goal is to learn a network that is robust against adversarial attacks.  However, the sample complexity of robust learning can be significantly larger than that of “standard” learning. In this paper, we propose improving the adversarial robustness of a network by leveraging the potentially large test data seen at runtime. We devise a new defense method, called runtime masking and cleansing (RMC), that adapts the network at runtime before making a prediction to dynamically mask network gradients and cleanse the model of the non-robust features inevitably learned during the training process due to the size limit of the training set. We conduct experiments on real-world datasets and the results demonstrate the effectiveness of RMC empirically.
","['National Tsing Hua University', 'National Tsing Hua University', 'National Tsing Hua University']"
2020,On Relativistic f-Divergences,Alexia Jolicoeur-Martineau,https://icml.cc/Conferences/2020/Schedule?showEvent=6142,"We take a more rigorous look at Relativistic Generative Adversarial Networks (RGANs) and prove that the objective function of the discriminator is a statistical divergence for any concave function $f$ with minimal properties ($f(0)=0$, $f'(0) \neq 0$, $\sup_x f(x)>0$). We devise additional variants of relativistic $f$-divergences. We show that the Wasserstein distance is weaker than $f$-divergences which are weaker than relativistic $f$-divergences. Given the good performance of RGANs, this suggests that Wasserstein GAN does not performs well primarily because of the weak metric, but rather because of regularization and the use of a relativistic discriminator. We introduce the minimum-variance unbiased estimator (MVUE) for Relativistic paired GANs (RpGANs; originally called RGANs which could bring confusion) and show that it does not perform better. We show that the estimator of Relativistic average GANs (RaGANs) is asymptotically unbiased and that the finite-sample bias is small; removing this bias does not improve performance.",['Mila']
2020,Bidirectional Model-based Policy Optimization,"Hang Lai, Jian Shen, Weinan Zhang, Yong Yu",https://icml.cc/Conferences/2020/Schedule?showEvent=6797,"Model-based reinforcement learning approaches leverage a forward dynamics model to support planning and decision making, which, however, may fail catastrophically if the model is inaccurate. Although there are several existing methods dedicated to combating the model error, the potential of the single forward model is still limited. In this paper, we propose to additionally construct a backward dynamics model to reduce the reliance on accuracy in forward model predictions. We develop a novel method, called Bidirectional Model-based Policy Optimization (BMPO) to utilize both the forward model and backward model to generate short branched rollouts for policy optimization. Furthermore, we theoretically derive a tighter bound of return discrepancy, which shows the superiority of BMPO against the one using merely the forward model. Extensive experiments demonstrate that BMPO outperforms state-of-the-art model-based methods in terms of sample efficiency and asymptotic performance.
","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']"
2020,Safe Reinforcement Learning in Constrained Markov Decision Processes,"Akifumi Wachi, Yanan Sui",https://icml.cc/Conferences/2020/Schedule?showEvent=5904,"Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a step-wise approach for optimizing safety and cumulative reward.  In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openly-available environment named GP-Safety-Gym, and the other simulates Mars surface exploration by using real observation data.
","['IBM Research AI', 'Tsinghua University']"
2020,Feature-map-level Online Adversarial Knowledge Distillation,"Inseop Chung, SeongUk Park, Kim Jangho, NOJUN KWAK",https://icml.cc/Conferences/2020/Schedule?showEvent=5785,"Feature maps contain rich information about image intensity and spatial correlation. However, previous online knowledge distillation methods only utilize the class probabilities. Thus in this paper, we propose an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. We train multiple networks simultaneously by employing discriminators to distinguish the feature map distributions of different networks. Each network has its corresponding discriminator which discriminates the feature map from its own as fake while classifying that of the other network as real. By training a network to fool the corresponding discriminator, it can learn the other network's feature map distribution. We show that our method performs  better than the conventional direct alignment method such as L1 and is more suitable for online distillation. Also, we propose a novel cyclic learning scheme for training more than two networks together. We have applied our method to various network architectures on the classification task and discovered a significant improvement of performance especially in the case of training a pair of a small network and a large one.
","['Seoul National University', 'Seoul National University', 'Seoul National University', 'Seoul National University']"
2020,Automated Synthetic-to-Real Generalization,"Wuyang Chen, Zhiding Yu, Zhangyang Wang, Anima Anandkumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6537,"Models trained on synthetic images often face degraded generalization to real data. As a convention, these models are often initialized with ImageNet pretrained representation. Yet the role of ImageNet knowledge is seldom discussed despite common practices that leverage this knowledge to maintain the generalization ability. An example is the careful hand-tuning of early stopping and layer-wise learning rates, which is shown to improve synthetic-to-real generalization but is also laborious and heuristic. In this work, we explicitly encourage the synthetically trained model to maintain similar representations with the ImageNet pretrained model, and propose a \textit{learning-to-optimize (L2O)} strategy to automate the selection of layer-wise learning rates. We demonstrate that the proposed framework can significantly improve the synthetic-to-real generalization performance without seeing and training on real data, while also benefiting downstream tasks such as domain adaptation. Code is available at: https://github.com/NVlabs/ASG.
","['Texas A&M University', 'NVIDIA', 'University of Texas at Austin', 'Amazon AI & Caltech']"
2020,Rigging the Lottery: Making All Tickets Winners,"Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, Erich Elsen",https://icml.cc/Conferences/2020/Schedule?showEvent=5808,"Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations.  We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.
","['Google', 'Google Brain', 'DeepMind', 'Google Brain', 'Google']"
2020,Input-Sparsity Low Rank Approximation in Schatten Norm,"Yi Li, David Woodruff",https://icml.cc/Conferences/2020/Schedule?showEvent=6798,"We give the first input-sparsity time algorithms for the rank-$k$ low rank approximation problem in every Schatten norm. Specifically, for a given $n\times n$ matrix $A$, our algorithm computes $Y,Z\in \R^{n\times k}$, which, with high probability, satisfy $\|A-YZ^T\|_p \leq (1+\eps)\|A-A_k\|_p$, where $\|M\|_p = \left (\sum_{i=1}^n \sigma_i(M)^p \right )^{1/p}$ is the Schatten $p$-norm of a matrix $M$ with singular values $\sigma_1(M), \ldots, \sigma_n(M)$, and where $A_k$ is the best rank-$k$ approximation to $A$. Our algorithm runs in time $\tilde{O}(\nnz(A) + n^{\alpha_p}\poly(k/\eps))$, where $\alpha_p = 1$ for $p\in [1,2)$ and $\alpha_p = 1 + (\omega-1)(1-2/p)$ for $p>2$ and $\omega \approx 2.374$ is the exponent of matrix multiplication. For the important case of $p = 1$, which corresponds to the more ``robust'' nuclear norm, we obtain $\tilde{O}(\nnz(A) + n \cdot \poly(k/\epsilon))$ time, which was previously only known for the Frobenius norm $(p = 2)$. Moreover, since $\alpha_p < \omega$ for every $p$, our algorithm has a better dependence on $n$ than that in the singular value decomposition for every $p$. Crucial to our analysis is the use of dimensionality reduction for Ky-Fan $p$-norms. ","['Nanyang Technological University', 'Carnegie Mellon University']"
2020,Online Learning with Dependent Stochastic Feedback Graphs,"Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, Ningshan Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6538,"A general framework for online learning with partial information is one where feedback graphs specify which losses can be observed by the learner. We study a challenging scenario where feedback graphs vary stochastically with time and, more importantly, where graphs and losses are dependent. This scenario appears in several real-world applications that we describe where the outcome of actions are correlated.  We devise a new algorithm for this setting that exploits the stochastic properties of the graphs and that benefits from favorable regret guarantees. We present a detailed theoretical analysis of this algorithm, and also report the result of a series of experiments on real-world datasets, which show that our algorithm outperforms standard baselines for online learning with feedback graphs.
","['Google Research', 'Google Research', 'Google Research', 'Google Research and Courant Institute of Mathematical Sciences', 'Hudson River Trading']"
2020,Scalable Differentiable Physics for Learning and Control,"Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, Ming Lin",https://icml.cc/Conferences/2020/Schedule?showEvent=5767,"Differentiable physics is a powerful approach to learning and control problems that involve physical objects and environments. While notable progress has been made, the capabilities of differentiable physics solvers remain limited. We develop a scalable framework for differentiable physics that can support a large number of objects and their interactions. To accommodate objects with arbitrary geometry and topology, we adopt meshes as our representation and leverage the sparsity of contacts for scalable differentiable collision handling. Collisions are resolved in localized regions to minimize the number of optimization variables even when the number of simulated objects is high. We further accelerate implicit differentiation of optimization with nonlinear constraints. Experiments demonstrate that the presented framework requires up to two orders of magnitude less memory and computation in comparison to recent particle-based methods. We further validate the approach on inverse problems and control scenarios, where it outperforms derivative-free and model-free baselines by at least an order of magnitude.
","['University of Maryland, College Park', 'University of Maryland, College Park', 'Intel Labs', 'UMD-CP & UNC-CH']"
2020,Do RNN and LSTM have Long Memory?,"Jingyu Zhao, Feiqing Huang, Jia Lv, Yanjie Duan, Zhen Qin, Guodong Li, Guangjian Tian",https://icml.cc/Conferences/2020/Schedule?showEvent=5907,"The LSTM network was proposed to overcome the difficulty in learning long-term dependence, and has made significant advancements in applications. With its success and drawbacks in mind, this paper raises the question -do RNN and LSTM have long memory? We answer it partially by proving that RNN and LSTM do not have long memory from a statistical perspective. A new definition for long memory networks is further introduced, and it requires the model weights to decay at a polynomial rate. To verify our theory, we convert RNN and LSTM into long memory networks by making a minimal modification, and their superiority is illustrated in modeling longterm dependence of various datasets.
","['The University of Hong Kong', 'University of Hong Kong', ""Huawei Noah's Ark Lab"", 'Huawei Noah’s Ark Lab', ""Huawei Noah's Ark Lab"", 'University of Hong Kong', 'Huawei Noah’s Ark Lab']"
2020,Sparse Convex Optimization via Adaptively Regularized Hard Thresholding,"Kyriakos Axiotis, Maxim Sviridenko",https://icml.cc/Conferences/2020/Schedule?showEvent=6777,"The goal of Sparse Convex Optimization is to optimize a convex function $f$ under a sparsity constraint $s\leq s^*\gamma$, where $s^*$ is the target number of non-zero entries in a feasible solution (sparsity) and $\gamma\geq 1$ is an approximation factor. There has been a lot of work to analyze the sparsity guarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP), Iterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number $\kappa$. The best known algorithms guarantee to find an approximate solution of value $f(x^*)+\epsilon$ with the sparsity bound of $\gamma = O\left(\kappa\min\left\{\log \frac{f(x^0)-f(x^*)}{\epsilon}, \kappa\right\}\right)$, where $x^*$ is the target solution. We present a new Adaptively Regularized Hard Thresholding (ARHT) algorithm that makes significant progress on this problem by bringing the bound down to $\gamma=O(\kappa)$, which has been shown to be tight for a general class of algorithms including LASSO, OMP, and IHT. This is achieved without significant sacrifice in the runtime efficiency compared to the fastest known algorithms. We also provide a new analysis of OMP with Replacement (OMPR) for general $f$, under the condition $s > s^* \frac{\kappa^2}{4}$, which yields Compressed Sensing bounds under the Restricted Isometry Property (RIP). When compared to other Compressed Sensing approaches, it has the advantage of providing a strong tradeoff between the RIP condition and the solution sparsity, while working for any general function $f$ that meets the RIP condition.","['MIT', 'Yahoo! Research']"
2020,Deep Graph Random Process for Relational-Thinking-Based  Speech Recognition,"Huang Hengguan, Fuzhao Xue, Hao Wang, Ye Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6322,"Lying at the core of human intelligence, relational thinking is characterized by initially relying on innumerable unconscious percepts pertaining to relations between new sensory signals and prior knowledge, consequently becoming a recognizable concept or object through coupling and transformation of these percepts. Such mental processes are difficult to model in real-world problems such as in conversational automatic speech recognition (ASR), as the percepts (if they are modelled as graphs indicating relationships among utterances) are supposed to be innumerable and not directly observable. In this paper, we present a Bayesian nonparametric deep learning method called deep graph random process (DGP) that can generate an infinite number of probabilistic graphs representing percepts.  We further provide a closed-form solution for coupling and transformation of these percept graphs for acoustic modeling.  Our approach is able to successfully infer relations among utterances without using any relational data during training. Experimental evaluations on ASR tasks including CHiME-2 and CHiME-5 demonstrate the effectiveness and benefits of our method.
","['NUS', 'National University of Singapore', 'MIT', 'National University of Singapore']"
2020,Linear Convergence of Randomized Primal-Dual Coordinate Method for Large-scale Linear Constrained Convex Programming,"Daoli Zhu, Lei Zhao",https://icml.cc/Conferences/2020/Schedule?showEvent=5813,"Linear constrained convex programming (LCCP) has many practical applications, including support vector machine (SVM) and machine learning portfolio (MLP) problems. We propose the randomized primal-dual coordinate (RPDC) method, a randomized coordinate extension of the first-order primal-dual method by Cohen and Zhu, 1984 and Zhao and Zhu, 2019, to solve LCCP. We randomly choose a block of variables based on the uniform distribution and apply linearization and a Bregman-like function (core function) to the selected block to obtain simple parallel primal-dual decomposition for LCCP. We establish almost surely convergence and expected O(1/t) convergence rate. Under global strong metric subregularity, we establish the linear convergence of RPDC. Finally, we discuss the implementation details of RPDC and present numerical experiments on SVM and MLP problems to verify the linear convergence.
","['Shanghai Jiao Tong University', 'Shanghai Jiao Tong University']"
2020,Multi-Agent Routing Value Iteration Network,"Quinlan Sykora, Mengye Ren, Raquel Urtasun",https://icml.cc/Conferences/2020/Schedule?showEvent=6071,"In this paper we tackle the problem of routing multiple agents in a coordinated manner. This is a complex problem that has a wide range of applications in fleet management to achieve a common goal, such as mapping from a swarm of robots and ride sharing. Traditional methods are typically not designed for realistic environments which contain  sparsely connected graphs and unknown traffic, and are often too slow in runtime to be practical. In contrast, we propose a graph neural network based model that is able to perform multi-agent routing based on learned value iteration in a sparsely connected graph with dynamically changing traffic conditions. Moreover, our learned communication module enables the agents to coordinate online and adapt to changes more effectively. We created a simulated environment to mimic realistic  mapping performed by autonomous vehicles with unknown minimum edge coverage and traffic conditions; our approach significantly outperforms traditional solvers both in terms of total cost and runtime. We also show that our model trained with only two agents on graphs with a maximum of 25 nodes can easily generalize to situations with more agents and/or nodes.
","['Uber ATG', 'Uber ATG / University of Toronto', 'Uber ATG & University of Toronto']"
2020,Task Understanding from Confusing Multi-task Data,"Xin Su, Yizhou Jiang, Shangqi Guo, Feng Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=5847,"Beyond machine learning's success in the specific tasks, research for learning multiple tasks simultaneously is referred to as multi-task learning. However, existing multi-task learning needs manual definition of tasks and manual task annotation. A crucial problem for advanced intelligence is how to understand the human task concept using basic input-output pairs. Without task definition, samples from multiple tasks are mixed together and result in a confusing mapping challenge. We propose Confusing Supervised Learning (CSL) that takes these confusing samples and extracts task concepts by differentiating between these samples. We theoretically proved the feasibility of the CSL framework and designed an iterative algorithm to distinguish between tasks. The experiments demonstrate that our CSL methods could achieve a human-like task understanding without task labeling in multi-function regression problems and multi-task recognition problems.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2020,Lower Complexity Bounds for Finite-Sum Convex-Concave Minimax Optimization Problems,"Guangzeng Xie, Luo Luo, yijiang lian, Zhihua Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6431,"This paper studies the lower bound complexity for minimax optimization problem whose objective function is the average of $n$ individual smooth convex-concave functions. We consider the algorithm which gets access to gradient and proximal oracle for each individual component. For the strongly-convex-strongly-concave case, we prove such an algorithm can not reach an $\varepsilon$-suboptimal point in fewer than $\Omega\left((n+\kappa)\log(1/\varepsilon)\right)$ iterations, where $\kappa$ is the condition number of the objective function. This lower bound matches the upper bound of the existing incremental first-order oracle algorithm stochastic variance-reduced extragradient. We develop a novel construction to show the above result, which partitions the tridiagonal matrix of classical examples into $n$ groups. This construction is friendly to the analysis of incremental gradient and proximal oracle and we also extend the analysis to general convex-concave cases.","['Peking University', 'Hong Kong University of Science and Technology', 'baidu', 'Peking University']"
2020,CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information,"Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, Lawrence Carin",https://icml.cc/Conferences/2020/Schedule?showEvent=6205,"There has been considerable recent interest in mutual information (MI) minimization for various machine learning tasks. However, estimating and minimizing MI in high-dimensional spaces remains a challenging problem, especially when only samples are accessible, rather than the underlying distribution forms. Previous works mainly focus on MI lower bound approximation, which is not applicable to MI minimization problems. In this paper, we propose a novel Contrastive Log-ratio Upper Bound (CLUB) of mutual information. We provide a theoretical analysis of the properties of CLUB and its variational approximation. Based on this upper bound, we introduce an accelerated MI minimization training scheme, that bridges MI minimization with negative sampling. Simulation studies on Gaussian distributions show that CLUB provides reliable estimates. Real-world MI minimization experiments, including domain adaptation and the information bottleneck, further demonstrate the effectiveness of the proposed method.
","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Microsoft', 'Duke']"
2020,Learning Robot Skills with Temporal Variational Inference,"Tanmay Shankar, Abhinav Gupta",https://icml.cc/Conferences/2020/Schedule?showEvent=6238,"In this paper, we address the discovery of robotic options from demonstrations in an unsupervised manner. Specifically, we present a framework to jointly learn low-level control policies and higher-level policies of how to use them from demonstrations of a robot performing various tasks. By representing options as continuous latent variables, we frame the problem of learning these options as latent variable inference. We then present a temporally causal variant of variational inference based on a temporal factorization of trajectory likelihoods, that allows us to infer options in an unsupervised manner. We demonstrate the ability of our framework to learn such options across three robotic demonstration datasets.
","['Facebook AI Research', 'Carnegie Mellon University']"
2020,Transparency Promotion with Model-Agnostic Linear Competitors," Hassan Rafique, Tong Wang, Qihang Lin, Arshia Singhani",https://icml.cc/Conferences/2020/Schedule?showEvent=6060,"We propose a novel type of hybrid model for multi-class classification, which utilizes competing linear models to collaborate with an existing black-box model, promoting transparency in the decision-making process. Our proposed hybrid model, Model-Agnostic Linear Competitors (MALC), brings together the interpretable power of linear models and the good predictive performance of the state-of-the-art black-box models. We formulate the training of a MALC model as a convex optimization problem, optimizing the predictive accuracy and transparency (defined as the percentage of data captured by the linear models) in the objective function. Experiments show that MALC offers more model flexibility for users to balance transparency and accuracy, in contrast to the currently available choice of either a pure black-box model or a pure interpretable model. The human evaluation also shows that more users are likely to choose MALC for this model flexibility compared with interpretable models and black-box models.
","['The University of Iowa', 'University of Iowa', 'University of Iowa', 'BASIS Independent Silicon Valley']"
2020,Amortized Finite Element Analysis for Fast PDE-Constrained Optimization,"Tianju Xue, Alex Beatson, Sigrid Adriaenssens , Ryan P. Adams",https://icml.cc/Conferences/2020/Schedule?showEvent=6574,"Optimizing the parameters of partial differential equations (PDEs), i.e., PDE-constrained optimization (PDE-CO), allows us to model natural systems from observations or perform rational design of structures with complicated mechanical, thermal, or electromagnetic properties.  However, PDE-CO is often computationally prohibitive due to the need to solve the PDE---typically via finite element analysis (FEA)---at each step of the optimization procedure. In this paper we propose amortized finite element analysis (AmorFEA), in which a neural network learns to produce accurate PDE solutions, while preserving many of the advantages of traditional finite element methods. This network is trained to directly minimize the potential energy from which the PDE and finite element method are derived, avoiding the need to generate costly supervised training data by solving PDEs with traditional FEA. As FEA is a variational procedure, AmorFEA is a direct analogue to popular amortized inference approaches in latent variable models, with the finite element basis acting as the variational family. AmorFEA can perform PDE-CO without the need to repeatedly solve the associated PDE, accelerating optimization when compared to a traditional workflow using FEA and the adjoint method.
","['Princeton University', 'Princeton University', 'Princeton University', 'Princeton University']"
2020,Estimation of Bounds on Potential Outcomes For Decision Making,"Maggie Makar, Fredrik Johansson, John Guttag, David Sontag",https://icml.cc/Conferences/2020/Schedule?showEvent=6563,"Estimation of individual treatment effects is commonly used as the basis for contextual decision making in fields such as healthcare, education, and economics. However, it is often sufficient for the decision maker to have estimates of upper and lower bounds on the potential outcomes of decision alternatives to assess risks and benefits. We show that, in such cases, we can improve sample efficiency by estimating simple functions that bound these outcomes instead of estimating their conditional expectations, which may be complex and hard to estimate. Our analysis highlights a trade-off between the complexity of the learning task and the confidence with which the learned bounds hold. Guided by these findings, we develop an algorithm for learning upper and lower bounds on potential outcomes which optimize an objective function defined by the decision maker, subject to the probability that bounds are violated being small. Using a clinical dataset and a well-known causality benchmark, we demonstrate that our algorithm outperforms baselines, providing tighter, more reliable bounds. 
","['MIT', 'Chalmers University of Technology', 'MIT', 'Massachusetts Institute of Technology']"
2020,How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization,"Chris Finlay, Joern-Henrik Jacobsen, Levon Nurbekyan, Adam Oberman",https://icml.cc/Conferences/2020/Schedule?showEvent=6601,"Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.
","['McGill University', 'Apple Inc.', 'UCLA', 'McGill University']"
2020,Interpreting Robust Optimization via Adversarial Influence Functions,"Zhun Deng, Cynthia Dwork, Jialiang Wang, Linjun Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=5967,"Robust optimization has been widely used in nowadays data science, especially in adversarial training. However, little research has been done to quantify how robust optimization changes the optimizers and the prediction losses comparing to standard training.  In this paper, inspired by the influence function in robust statistics, we introduce the Adversarial Influence Function (AIF) as a tool to investigate the solution produced by robust optimization. The proposed AIF enjoys a closed-form and can be calculated efficiently. To illustrate the usage of AIF, we apply it to study model sensitivity -- a quantity defined to capture the change of prediction losses on the natural data after implementing robust optimization. We use AIF to analyze how model complexity and randomized smoothing affect the model sensitivity with respect to specific models.  We further derive AIF for kernel regressions, with a particular application to neural tangent kernels, and experimentally demonstrate the effectiveness of the proposed AIF. Lastly, the theories of AIF will be extended to distributional robust optimization.
","['Harvard', 'Harvard', 'Harvard University', 'Rutgers University']"
2020,Enhancing Simple Models by Exploiting What They Already Know,"Amit Dhurandhar, Karthikeyan Shanmugam, Ronny Luss",https://icml.cc/Conferences/2020/Schedule?showEvent=5779,"There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.
","['IBM Research', 'IBM Research NY', 'IBM Research']"
2020,Analytic Marching: An Analytic Meshing Solution from Deep Implicit Surface Networks,"Jiabao Lei, Kui Jia",https://icml.cc/Conferences/2020/Schedule?showEvent=6169,"This paper studies a problem of learning surface mesh via implicit functions in an emerging field of deep learning surface reconstruction, where implicit functions are popularly implemented as multi-layer perceptrons (MLPs) with rectified linear units (ReLU). To achieve meshing from the learned implicit functions, existing methods adopt the de-facto standard algorithm of marching cubes; while promising, they suffer from loss of precision learned in the MLPs, due to the discretization nature of marching cubes. Motivated by the knowledge that a ReLU based MLP partitions its input space into a number of linear regions, we identify from these regions analytic cells and faces that are associated with zero-level isosurface of the implicit function, and characterize the conditions under which the identified faces are guaranteed to connect and form a closed, piecewise planar surface. We propose a naturally parallelizable algorithm of analytic marching to exactly recover the mesh captured by a learned MLP. Experiments on deep learning mesh reconstruction verify the advantages of our algorithm over existing ones.
","['South China University of Technology', 'South China University of Technology']"
2020,Neural Datalog Through Time: Informed Temporal Modeling via Logical Specification,"Hongyuan Mei, Guanghui Qin, Minjie Xu, Jason Eisner",https://icml.cc/Conferences/2020/Schedule?showEvent=6481,"Learning how to predict future events from patterns of past events is difficult when the set of possible event types is large. Training an unrestricted neural model might overfit to spurious patterns. To exploit domain-specific knowledge of how past events might affect an event's present probability, we propose using a temporal deductive database to track structured facts over time. Rules serve to prove facts from other facts and from past events. Each fact has a time-varying state---a vector computed by a neural net whose topology is determined by the fact's provenance, including its experience of past events. The possible event types at any time are given by special facts, whose probabilities are neurally modeled alongside their states. In both synthetic and real-world domains, we show that neural probabilistic models derived from concise Datalog programs improve prediction by encoding appropriate domain knowledge in their architecture.
","['Johns Hopkins University', 'JOHNS HOPKINS UNIVERSITY', 'Bloomberg LP', 'Microsoft Semantic Machines / Johns Hopkins Univ.']"
2020,Deep Reinforcement Learning with Smooth Policy,"Qianli Shen, Yan Li, Haoming Jiang, Zhaoran Wang, Tuo Zhao",https://icml.cc/Conferences/2020/Schedule?showEvent=6478,"Deep reinforcement learning (RL) has achieved great empirical successes in various domains.  However, the large search space of neural networks requires a large amount of data, which makes the current RL algorithms not sample efficient. 
Motivated by the fact that many environments with continuous state space have smooth transitions, we propose to learn a smooth policy that behaves smoothly with respect to states.  We develop a new framework --- \textbf{S}mooth \textbf{R}egularized \textbf{R}einforcement \textbf{L}earning ($\textbf{SR}^2\textbf{L}$), where the policy is trained with smoothness-inducing regularization. Such regularization effectively constrains the search space, and enforces smoothness in the learned policy. Moreover, our proposed framework can also improve the robustness of policy against measurement error in the state space, and can be naturally extended to distribubutionally robust setting.  We apply the proposed framework to both on-policy (TRPO) and off-policy algorithm (DDPG). Through extensive experiments, we demonstrate that our method  achieves improved sample efficiency and robustness.","['Peking University', 'Georgia Tech', 'Georgia Tech', 'Northwestern', 'Georgia Tech']"
2020,GraphOpt: Learning Optimization Models of Graph Formation,"Rakshit Trivedi, Jiachen Yang, Hongyuan Zha",https://icml.cc/Conferences/2020/Schedule?showEvent=6196,"Formation mechanisms are fundamental to the study of complex networks, but learning them  from observations is challenging. In real-world domains, one often has access only to the final constructed graph, instead of the full construction process, and observed graphs exhibit complex structural properties. In this work, we propose GraphOpt, an end-to-end framework that jointly learns an implicit model of graph structure formation and discovers an underlying optimization mechanism in the form of a latent objective function. The learned objective can serve as an explanation for the observed graph properties, thereby lending itself to transfer across different graphs within a domain. GraphOpt poses link formation in graphs as a sequential decision-making process and solves it using maximum entropy inverse reinforcement learning algorithm. Further, it employs a novel continuous latent action space that aids scalability. Empirically, we demonstrate that GraphOpt discovers a latent objective transferable across graphs with different characteristics. GraphOpt also learns a robust stochastic policy that achieves competitive link prediction performance without being explicitly trained on this task and further enables construction of graphs with properties similar to those of the observed graph.
","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']"
2020,"Calibration, Entropy Rates, and Memory in Language Models","Mark Braverman, Xinyi Chen, Sham Kakade, Karthik Narasimhan, Cyril Zhang, Yi Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6034,"Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that state-of-the-art language models, including LSTMs and Transformers, are miscalibrated: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.
","['Princeton University', '', 'University of Washington', 'Princeton', 'Princeton University', 'Princeton University']"
2020,Refined bounds for algorithm configuration: The knife-edge of dual class approximability,"Nina Balcan, Tuomas Sandholm, Ellen Vitercik",https://icml.cc/Conferences/2020/Schedule?showEvent=5912,"Automating algorithm configuration is growing increasingly necessary as algorithms come with more and more tunable parameters. It is common to tune parameters using machine learning, optimizing algorithmic performance (runtime or solution quality, for example) using a training set of problem instances from the specific domain at hand. We investigate a fundamental question about these techniques: how large should the training set be to ensure that a parameter’s average empirical performance over the training set is close to its expected, future performance? We answer this question for algorithm configuration problems that exhibit a widely-applicable structure: the algorithm's performance as a function of its parameters can be approximated by a “simple” function. We show that if this approximation holds under the L∞-norm, we can provide strong sample complexity bounds, but if the approximation holds only under the Lp-norm for p < ∞, it is not possible to provide meaningful sample complexity bounds in the worst case. We empirically evaluate our bounds in the context of integer programming, obtaining sample complexity bounds that are up to 700 times smaller than the previously best-known bounds.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2020,Constrained Markov Decision Processes via Backward Value Functions,"Harsh Satija, Philip Amortila, Joelle Pineau",https://icml.cc/Conferences/2020/Schedule?showEvent=6226,"Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world, undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight the computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks. 
","['McGill University', 'McGill University', 'McGill University / Facebook']"
2020,A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton,"Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=5938,"In recent years, a variety of gradient-based bi-level optimization methods have been developed for learning tasks. However, theoretical guarantees of these existing approaches often heavily rely on the simplification that for each fixed upper-level variable, the lower-level solution must be a singleton (a.k.a., Lower-Level Singleton, LLS). In this work, by formulating bi-level models from the optimistic viewpoint and aggregating hierarchical objective information, we establish Bi-level Descent Aggregation (BDA), a flexible and modularized algorithmic framework for bi-level programming. Theoretically, we derive a new methodology to prove the convergence of BDA without the LLS condition. Furthermore, we improve the convergence properties of conventional first-order bi-level schemes (under the LLS simplification) based on our proof recipe. Extensive experiments justify our theoretical results and demonstrate the superiority of the proposed BDA for different tasks, including hyper-parameter optimization and meta learning.
","['Dalian University of Technology', 'Dalian University of Technology', 'The University of Hong Kong', 'The University of Hong Kong', 'Southern University of Science and Technology']"
2020,Differentiable Product Quantization for End-to-End Embedding Compression,"Ting Chen, Lala Li, Yizhou Sun",https://icml.cc/Conferences/2020/Schedule?showEvent=6429,"Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints. In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238X) at negligible or no performance cost on 10 datasets across three different language tasks.
","['Google Brain', 'Google', 'UCLA']"
2020,On the Expressivity of Neural Networks for Deep Reinforcement Learning,"Kefan Dong, Yuping Luo, Tianhe (Kevin) Yu, Chelsea Finn, Tengyu Ma",https://icml.cc/Conferences/2020/Schedule?showEvent=6381,"We compare the model-free reinforcement learning with the model-based approaches through the lens of the expressive power of neural networks for policies, Q-functions, and dynamics. We show, theoretically and empirically, that even for one-dimensional continuous state space, there are many MDPs whose optimal Q-functions and policies are much more complex than the dynamics. For these MDPs, model-based planning is a favorable algorithm, because the resulting policies can approximate the optimal policy significantly better than a neural network parameterization can, and model-free or model-based policy optimization rely on policy parameterization. Motivated by the theory, we apply a simple multi-step model-based bootstrapping planner (BOOTS) to bootstrap a weak Q-function into a stronger policy. Empirical results show that applying BOOTS on top of model-based or model-free policy optimization algorithms at the test time improves the performance on benchmark tasks.
","['Tsinghua University', 'Princeton University', 'Stanford University', 'Stanford', 'Stanford']"
2020,FR-Train: A Mutual Information-Based Approach to Fair and Robust Training,"Yuji Roh, Kangwook Lee, Steven Whang, Changho Suh",https://icml.cc/Conferences/2020/Schedule?showEvent=6025,"Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias to be fixed, resulting in severe performance degradation. To address this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.
","['KAIST', 'UW Madison', 'KAIST', 'KAIST']"
2020,Does label smoothing mitigate label noise?,"Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, Sanjiv Kumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6128,"Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors.
Empirically, smoothing has been shown to improve both predictive performance and model calibration.
In this paper, we study whether label smoothing is also effective as a means of coping with label noise.
While label smoothing apparently amplifies this problem --- being equivalent to injecting symmetric noise to the labels --- we show how it relates to a general family of loss-correction techniques from the label noise literature.
Building on this connection, we show that label smoothing is competitive with loss-correction under label noise.
Further, we show that when distilling models from noisy data, label smoothing of the teacher is beneficial;
this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.
","['Google Research', 'Google AI', 'Google Research', 'Google Research, NY']"
2020,Mapping natural-language problems to formal-language solutions using structured neural representations,"Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Ken Forbus, Jianfeng Gao",https://icml.cc/Conferences/2020/Schedule?showEvent=6094,"Generating formal-language programs represented by relational tuples, such as Lisp programs or mathematical operations, to solve problems stated in natural language is a challenging task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural sequence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neural representation, Tensor Product Representations (TPRs), for mapping Natural-language problems to Formal-language solutions, called TPN2F. The encoder of TP-N2F employs TPR ‘binding’ to encode natural-language symbolic structure in vector space and the decoder uses TPR ‘unbinding’ to generate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art results. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Analysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F.
","['Northwestern University', 'Microsoft Research, Redmond', 'Microsoft Research', 'Microsoft Research', 'Northwestern University', 'Microsoft Research AI']"
2020,New Oracle-Efficient Algorithms for Private Synthetic Data Release,"Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke, Steven Wu",https://icml.cc/Conferences/2020/Schedule?showEvent=5814,"We present three new algorithms for constructing differentially private synthetic data---a sanitized version of a sensitive dataset that approximately preserves the answers to a large collection of statistical queries. All three algorithms are \emph{oracle-efficient} in the sense that they are computationally efficient when given access to an optimization oracle. Such an oracle can be implemented using many existing (non-private) optimization tools such as sophisticated integer program solvers. While the accuracy of the synthetic data is contingent on the oracle's optimization performance, the algorithms satisfy differential privacy even in the worst case. For all three algorithms, we provide theoretical guarantees for both accuracy and privacy. Through empirical evaluation, we demonstrate that our methods scale well with both the dimensionality of the data and the number of queries. Compared to the state-of-the-art method High-Dimensional Matrix Mechanism (McKenna et al.~VLDB 2018), our algorithms provide better accuracy in the large workload and high privacy regime (corresponding to low privacy loss $\eps$).
","['University of Minnesota', 'Harvard', 'Princeton University', 'IBM, Almaden', 'University of Minnesota']"
2020,Universal Equivariant Multilayer Perceptrons,Siamak Ravanbakhsh,https://icml.cc/Conferences/2020/Schedule?showEvent=6245,"Group invariant and equivariant Multilayer Perceptrons (MLP), also known as Equivariant Networks, have achieved remarkable success in learning on a variety of data structures, such as sequences, images, sets, and graphs. Using tools from group theory, this paper proves the universality of a broad class of equivariant MLPs with a single hidden layer.  In particular, it is shown that having a hidden layer on which the group acts regularly is sufficient for universal equivariance (invariance). A corollary is unconditional universality of equivariant MLPs for Abelian groups, such as CNNs with a single hidden layer. A second corollary is the universality of equivariant MLPs with a high-order hidden layer, where we give both group-agnostic bounds and means for calculating group-specific bounds on the order of hidden layer that guarantees universal equivariance (invariance). 
",['McGill - Mila']
2020,Countering Language Drift with Seeded Iterated Learning,"Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, Olivier Pietquin",https://icml.cc/Conferences/2020/Schedule?showEvent=6047,"Supervised learning methods excel at capturing statistical properties of language when trained over large text corpora. Yet, these models often produce inconsistent outputs in goal-oriented language settings as they are not trained to complete the underlying task. Moreover, as soon as the agents are finetuned to maximize task completion, they suffer from the so-called language drift phenomenon: they slowly lose syntactic and semantic properties of language as they only focus on solving the task. In this paper, we propose a generic approach to counter language drift called Seeded iterated learning(SIL). We periodically refine a pretrained student agent by imitating data sampled from a newly generated teacher agent. At each time step, the teacher is created by copying the student agent, before being finetuned to maximize task completion.SIL does not require external syntactic constraint nor semantic knowledge, making it a valuable task-agnostic finetuning protocol. We evaluate SIL in a toy-setting Lewis Game, and then scale it up to the translation game with natural language. In both settings, SIL helps counter language drift as well as it improves the task completion compared to baselines.
","['Mila & University of Montreal', 'Mila, University of Montreal', 'DeepMind', 'Université de Montréal', 'Google Brain']"
2020,"BINOCULARS for efficient, nonmyopic sequential experimental design","Shali Jiang, Henry Chai, Javier Gonzalez, Roman Garnett",https://icml.cc/Conferences/2020/Schedule?showEvent=6056,"Finite-horizon sequential experimental design (SED) arises naturally in many contexts, including hyperparameter tuning in machine learning among more traditional settings. Computing the optimal policy for such problems requires solving Bellman equations, which are generally intractable. Most existing work resorts to severely myopic approximations by limiting the decision horizon to only a single time-step, which can underweight exploration in favor of exploitation. We present BINOCULARS: Batch-Informed NOnmyopic Choices, Using Long-horizons for Adaptive, Rapid SED, a general framework for deriving efficient, nonmyopic approximations to the optimal experimental policy. Our key idea is simple and surprisingly effective: we first compute a one-step optimal batch of experiments, then select a single point from this batch to evaluate. We realize BINOCULARS for Bayesian optimization and Bayesian quadrature -- two notable example problems with radically different objectives -- and demonstrate that BINOCULARS significantly outperforms significantly outperforms myopic alternatives in real-world scenarios.
","['Washington University in St. Louis', 'Washington University in St. Louis', 'Microsoft Research', 'Washington University in St. Louis']"
2020,Error-Bounded Correction of Noisy Labels,"Songzhu Zheng, Pengxiang Wu, Aman Goswami, Mayank Goswami, Dimitris Metaxas, Chao Chen",https://icml.cc/Conferences/2020/Schedule?showEvent=6161,"To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. To be robust against label noise, many successful methods rely on the noisy classifiers (i.e., models trained on the noisy training data) to determine whether a label is trustworthy. However, it remains unknown why this heuristic works well in practice. In this paper, we provide the first theoretical explanation for these methods. We prove that the prediction of a noisy classifier can indeed be a good indicator of whether the label of a training data is clean. Based on the theoretical result, we propose a novel algorithm that corrects the labels based on the noisy classifier prediction. The corrected labels are consistent with the true Bayesian optimal classifier with high probability. We incorporate our label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets.
","['Stony Brook University', 'Rutgers University', 'Bain and Company', 'Queens College of CUNY', 'Rutgers', 'Stony Brook University']"
2020,Sequence Generation with Mixed Representations,"Lijun Wu, Shufang Xie, Yingce Xia, Yang Fan, Jian-Huang Lai, Tao Qin, Tie-Yan Liu",https://icml.cc/Conferences/2020/Schedule?showEvent=6391,"Tokenization is the first step of many natural language processing (NLP) tasks and plays an important role for neural NLP models. Tokenizaton method such as byte-pair encoding (BPE), which can greatly reduce the large vocabulary and deal with out-of-vocabulary words, has shown to be effective and is widely adopted for sequence generation tasks. While various tokenization methods exist, there is no common acknowledgement which is the best. In this work, we propose to leverage the mixed representations from different tokenization methods for sequence generation tasks, in order to boost the model performance with unique characteristics and advantages of individual tokenization methods. Specifically, we introduce a new model architecture to incorporate mixed representations and a co-teaching algorithm to better utilize the diversity of different tokenization methods. Our approach  achieves significant improvements on neural machine translation (NMT) tasks with six language pairs (e.g., English$\leftrightarrow$German, English$\leftrightarrow$Romanian), as well as an abstractive summarization task.","['Microsoft Research; Sun Yat-sen University', 'Microsoft Research Asia', 'Microsoft Research Asia', 'University of Science and Technology of China', 'Sun Yat-sen University', 'Microsoft Research Asia', 'Microsoft Research Asia']"
2020,Sparsified Linear Programming for Zero-Sum Equilibrium Finding,"Brian Zhang, Tuomas Sandholm",https://icml.cc/Conferences/2020/Schedule?showEvent=6015,"Computational equilibrium finding in large zero-sum extensive-form imperfect-information games has led to significant recent AI breakthroughs. The fastest algorithms for the problem are new forms of counterfactual regret minimization (Brown & Sandholm, 2019). In this paper we present a totally different approach to the problem, which is competitive and often orders of magnitude better than the prior state of the art. The equilibrium-finding problem can be formulated as a linear program (LP) (Koller et al., 1994), but solving it as an LP has not been scalable due to the memory requirements of LP solvers, which can often be quadratically worse than CFR-based algorithms. We give an efficient practical algorithm that factors a large payoff matrix into a product of two matrices that are typically dramatically sparser. This allows us to express the equilibrium-finding problem as a linear program with size only a logarithmic factor worse than CFR, and thus allows linear program solvers to run on such games. With experiments on poker endgames, we demonstrate in practice, for the first time, that modern linear program solvers are competitive against even game-specific modern variants of CFR in solving large extensive-form games, and can be used to compute exact solutions unlike iterative algorithms like CFR.
","['Carnegie Mellon University', 'Carnegie Mellon University']"
2020,Latent Variable Modelling with Hyperbolic Normalizing Flows,"Joey Bose, Ariella Smofsky, Renjie Liao, Prakash Panangaden, Will Hamilton",https://icml.cc/Conferences/2020/Schedule?showEvent=6036,"The choice of approximate posterior distributions plays a central role in stochastic variational inference (SVI). One effective solution is the use of normalizing flows \cut{defined on Euclidean spaces} to construct flexible posterior distributions. 
However, one key limitation of existing normalizing flows is that they are restricted to the Euclidean space and are ill-equipped to model data with an underlying hierarchical structure.
To address this fundamental limitation, we present the first extension of normalizing flows to hyperbolic spaces. 
We first elevate normalizing flows to hyperbolic spaces using coupling transforms defined on the tangent bundle, termed Tangent Coupling ($\mathcal{TC}$). 
We further introduce Wrapped Hyperboloid Coupling ($\mathcal{W}\mathbb{H}C$), a fully invertible and learnable transformation that explicitly utilizes the geometric structure of hyperbolic spaces, allowing for expressive posteriors while being efficient to sample from. We demonstrate the efficacy of our novel normalizing flow over hyperbolic VAEs and Euclidean normalizing flows. 
Our approach achieves improved performance on density estimation, as well as reconstruction of real-world graph data, which exhibit a hierarchical structure. 
Finally, we show that our approach can be used to power a generative model over hierarchical data using hyperbolic latent variables. ","['McGill/Mila', 'McGill University and Mila', 'University of Toronto', 'McGill University and Mila', 'McGill University and Mila']"
2020,Fast Deterministic CUR Matrix Decomposition with Accuracy Assurance,"Yasutoshi Ida, Sekitoshi Kanai, Yasuhiro Fujiwara, Tomoharu Iwata, Koh Takeuchi, Hisashi Kashima",https://icml.cc/Conferences/2020/Schedule?showEvent=5996,"The deterministic CUR matrix decomposition is a low-rank approximation method to analyze a data matrix.
It has attracted considerable attention due to its high interpretability, which results from the fact that the decomposed matrices consist of subsets of the original columns and rows of the data matrix.
The subset is obtained by optimizing an objective function with sparsity-inducing norms via coordinate descent.
However, the existing algorithms for optimization incur high computation costs.
This is because coordinate descent iteratively updates all the parameters in the objective until convergence.
This paper proposes a fast deterministic CUR matrix decomposition.
Our algorithm safely skips unnecessary updates by efficiently evaluating the optimality conditions for the parameters to be zeros.
In addition, we preferentially update the parameters that must be nonzeros.
Theoretically, our approach guarantees the same result as the original approach.
Experiments demonstrate that our algorithm speeds up the deterministic CUR while achieving the same accuracy.
","['NTT', 'NTT Software Innovation Center', 'NTT Communication Science Laboratories', 'NTT', 'NTT', 'Kyoto University/RIKEN Center for AIP']"
2020,PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions,"Zhengyang Shen, Lingshen He, Zhouchen Lin, Jinwen Ma",https://icml.cc/Conferences/2020/Schedule?showEvent=6749,"Recent research has shown that incorporating equivariance into neural network architectures is very helpful, and there have been some works investigating the equivariance of networks under group actions. However, as digital images and feature maps are on the discrete meshgrid, corresponding equivariance-preserving transformation groups are very limited. 

In this work, we deal with this issue from the connection between convolutions and partial differential operators (PDOs). In theory, assuming inputs to be smooth, we transform PDOs and propose a system which is equivariant to a much more general continuous group, the $n$-dimension Euclidean group. In implementation, we discretize the system using the numerical schemes of PDOs, deriving approximately equivariant convolutions (PDO-eConvs). Theoretically, the approximation error of PDO-eConvs is of the quadratic order. It is the first time that the error analysis is provided when the equivariance is approximate. Extensive experiments on rotated MNIST and natural image classification show that PDO-eConvs perform competitively yet use parameters much more efficiently. Particularly, compared with Wide ResNets, our methods result in better results using only 12.6% parameters.","['Peking University', 'Peking University', 'Peking University', 'Peking University']"
2020,Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE,"Juntang Zhuang, Nicha Dvornek, Xiaoxiao Li, Sekhar Tatikonda, Xenophon Papademetris, James Duncan",https://icml.cc/Conferences/2020/Schedule?showEvent=5900,"The empirical performance of neural ordinary differential equations (NODEs) is significantly inferior to discrete-layer models on benchmark tasks (e.g. image classification). We demonstrate an explanation is the inaccuracy of existing gradient estimation methods: the adjoint method has numerical errors in reverse-mode integration; the naive method suffers from a redundantly deep computation graph. We propose the Adaptive Checkpoint Adjoint (ACA) method: ACA applies a trajectory checkpoint strategy which records the forward- mode trajectory as the reverse-mode trajectory to guarantee accuracy; ACA deletes redundant components for shallow computation graphs; and ACA supports adaptive solvers. On image classification tasks, compared with the adjoint and naive method, ACA achieves half the error rate in half the training time; NODE trained with ACA outperforms ResNet in both accuracy and test-retest reliability. On time-series modeling, ACA outperforms competing methods. Furthermore, NODE with ACA can incorporate physical knowledge to achieve better accuracy.
","['Yale University', 'Yale University', 'Yale University', 'Yale', 'Yale University', 'Yale University']"
2020,T-GD: Transferable GAN-generated Images Detection Framework,"Hyeonseong Jeon, Young Oh Bang, Junyaup Kim, Simon Woo",https://icml.cc/Conferences/2020/Schedule?showEvent=6372,"Recent advancements in Generative Adversarial Networks (GANs) enable the generation of highly realistic images, raising concerns about their misuse for malicious purposes. Detecting these GAN-generated images (GAN-images) becomes increasingly challenging due to the significant reduction of underlying artifacts and specific patterns. The absence of such traces can hinder detection algorithms from identifying GAN-images and transferring knowledge to identify other types of GAN-images as well. In this work, we present the Transferable GAN-images Detection framework T-GD, a robust transferable framework for an effective detection of GAN-images. T-GD is composed of a teacher and a student model that can iteratively teach and evaluate each other to improve the detection performance. First, we train the teacher model on the source dataset and use it as a starting point for learning the target dataset. To train the student model, we inject noise by mixing up the source and target datasets, while constraining the weight variation to preserve the starting point. Our approach is a self-training method, but distinguishes itself from prior approaches by focusing on improving the transferability of GAN-image detection. T-GD achieves high performance on the source dataset by overcoming catastrophic forgetting and effectively detecting state-of-the-art GAN-images with only a small volume of data without any metadata information.
","['Sungkyunkwan University', 'Sungkyunkwan University', 'Sungkyunkwan University', 'SKKU']"
2020,Bandits for BMO Functions,"Tianyu Wang, Cynthia Rudin",https://icml.cc/Conferences/2020/Schedule?showEvent=6082,"We study the bandit problem where the underlying expected reward is a Bounded Mean Oscillation (BMO) function. BMO functions are allowed to be discontinuous and unbounded, and are useful in modeling signals with singularities in the domain.  We develop a toolset for BMO bandits, and provide an algorithm that can achieve poly-log $\delta$-regret -- a regret measured against an arm that is optimal after removing a $\delta$-sized portion of the arm space. ","['Duke University', 'Duke']"
2020,On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems,"Darren Lin, Chi Jin, Michael Jordan",https://icml.cc/Conferences/2020/Schedule?showEvent=5790,"We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in \mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding light on its superior practical performance in training generative adversarial networks (GANs) and other real applications. ","['UC Berkeley', 'Princeton University', 'UC Berkeley']"
2020,Stochastic Gauss-Newton Algorithms for Nonconvex Compositional Optimization,"Quoc Tran-Dinh, Nhan H Pham, Lam Nguyen",https://icml.cc/Conferences/2020/Schedule?showEvent=6048,"We develop two new stochastic Gauss-Newton algorithms for solving a class of non-convex  stochastic compositional optimization problems frequently arising in practice. We consider both the expectation and finite-sum settings under standard assumptions, and use both classical stochastic and SARAH estimators for approximating function values and Jacobians. In the expectation case, we establish $\BigO{\varepsilon^{-2}}$ iteration-complexity to achieve a stationary point in expectation and estimate the total number of stochastic oracle calls for both function value and its Jacobian, where $\varepsilon$ is a desired accuracy. In the finite sum case, we also estimate $\BigO{\varepsilon^{-2}}$ iteration-complexity and the total oracle calls with high probability. To our best knowledge, this is the first time such global stochastic oracle complexity is established for stochastic Gauss-Newton methods. Finally, we illustrate our theoretical results via two numerical examples on both synthetic and real datasets.","['The University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill', 'IBM Research, Thomas J. Watson Research Center']"
2020,Multi-Task Learning with User Preferences: Gradient Descent with Controlled Ascent in Pareto Optimization,"Debabrata Mahapatra, Vaibhav Rajan",https://icml.cc/Conferences/2020/Schedule?showEvent=6375,"Multi-Task Learning (MTL) is a well established paradigm for jointly learning models for multiple correlated tasks. Often the tasks conflict, requiring trade-offs between them during optimization. In such cases, multi-objective optimization based MTL methods can be used to find one or more Pareto optimal solutions. A common requirement in MTL applications, that cannot be addressed by these methods, is to find a solution satisfying userspecified preferences with respect to task-specific losses. We advance the state-of-the-art by developing the first gradient-based multi-objective MTL algorithm to solve this problem. Our unique approach combines multiple gradient descent with carefully controlled ascent to traverse the Pareto
front in a principled manner, which also makes it robust to initialization. The scalability of our algorithm enables its use in large-scale deep networks for MTL. Assuming only differentiability of the task-specific loss functions, we provide theoretical guarantees for convergence. Our experiments show that our algorithm outperforms the best competing methods on benchmark datasets.
","['National University of Singapore', 'National University of Singapore']"
2020,Improved Bounds on Minimax Regret under Logarithmic Loss via Self-Concordance,"Blair Bilodeau, Dylan Foster, Daniel Roy",https://icml.cc/Conferences/2020/Schedule?showEvent=6768,"We consider the classical problem of sequential probability assignment under logarithmic loss while competing against an arbitrary, potentially nonparametric class of experts. We obtain improved bounds on the minimax regret via a new approach that exploits the self-concordance property of the logarithmic loss. We show that for any expert class with (sequential) metric entropy $\mathcal{O}(\gamma^{-p})$ at scale $\gamma$, the minimax regret is $\mathcal{O}(n^{\frac{p}{p+1}})$, and that this rate cannot be improved without additional assumptions on the expert class under consideration. As an application of our techniques, we resolve the minimax regret for nonparametric Lipschitz classes of experts.","['University of Toronto', 'MIT', 'University of Toronto; Vector Institute']"
2020,Empirical Study of the Benefits of Overparameterization in Learning Latent Variable Models,"Rares-Darius Buhai, Yoni Halpern, Yoon Kim, Andrej Risteski, David Sontag",https://icml.cc/Conferences/2020/Schedule?showEvent=6678,"One of the most surprising and exciting discoveries in supervised learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it was observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). We perform an empirical study of different aspects of overparameterization in unsupervised learning of latent variable models via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that across a variety of models (noisy-OR networks, sparse coding, probabilistic context-free grammars) and training algorithms (variational inference, alternating minimization, expectation-maximization), overparameterization can significantly increase the number of ground truth latent variables recovered.
","['Massachusetts Institute of Technology', 'Google', 'Harvard University', 'CMU', 'Massachusetts Institute of Technology']"
2020,Strategic Classification is Causal Modeling in Disguise,"John Miller, Smitha Milli, Moritz Hardt",https://icml.cc/Conferences/2020/Schedule?showEvent=6779,"Consequential decision-making incentivizes individuals to strategically adapt
their behavior to the specifics of the decision rule. While a long line of
work has viewed strategic adaptation as gaming and attempted to mitigate its
effects, recent work has instead sought to design classifiers that incentivize
individuals to improve a desired quality. Key to both accounts is a cost
function that dictates which adaptations are rational to undertake. In this
work, we develop a causal framework for strategic adaptation. Our causal
perspective clearly distinguishes between gaming and improvement and reveals
an important obstacle to incentive design. We prove any procedure for
designing classifiers that incentivize improvement must inevitably solve a
non-trivial causal inference problem. We show a similar result holds for
designing cost functions that satisfy the requirements of previous work.
With the benefit of hindsight, our results show much of the prior work on
strategic classification is causal modeling in disguise.
","['University of California, Berkeley', 'UC Berkeley', 'University of California, Berkeley']"
2020,Sharp Statistical Guaratees for Adversarially Robust Gaussian Classification,"Chen Dan, Yuting Wei, Pradeep Ravikumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6312,"Adversarial robustness has become a fundamental requirement in modern machine learning applications. Yet, there has been surprisingly little statistical understanding so far. In this paper, we provide the first result of the \emph{optimal} minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model proposed by \cite{schmidt2018adversarially}. The results are stated in terms of the \emph{Adversarial Signal-to-Noise Ratio (AdvSNR)}, which
generalizes a similar notion for standard linear classification to the adversarial setting. For the Gaussian mixtures with AdvSNR value of $r$, we prove an excess risk lower bound of order $\Theta(e^{-(\frac{1}{2}+o(1)) r^2} \frac{d}{n})$ and design a computationally efficient estimator that achieves this optimal rate. Our results built upon minimal assumptions while cover a wide spectrum of adversarial perturbations including $\ell_p$ balls for any $p \ge 1$.","['Carnegie Mellon University', 'CMU', 'Carnegie Mellon University']"
2020,Born-again Tree Ensembles,"Thibaut Vidal, Maximilian Schiffer",https://icml.cc/Conferences/2020/Schedule?showEvent=6682,"The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles, in particular, offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble in its entire feature space. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise.
","['Pontifical Catholic University of Rio de Janeiro', 'TUM School of Management, Technical University of Munich']"
2020,A Pairwise Fair and Community-preserving Approach to k-Center Clustering,"Brian Brubach, Darshan Chakrabarti, John P Dickerson, Samir Khuller, Aravind Srinivasan, Leonidas Tsepenekas",https://icml.cc/Conferences/2020/Schedule?showEvent=6673,"Clustering is a foundational problem in machine learning with numerous applications. As machine learning increases in ubiquity as a backend for automated systems, concerns about fairness arise. Much of the current literature on fairness deals with discrimination against protected classes in supervised learning (group fairness). We define a different notion of fair clustering wherein the probability that two points (or a community of points) become separated is bounded by an increasing function of their pairwise distance (or community diameter). We capture the situation where data points represent people who gain some benefit from being clustered together. Unfairness arises when certain points are deterministically separated, either arbitrarily or by someone who intends to harm them as in the case of gerrymandering election districts. In response, we formally define two new types of fairness in the clustering setting, pairwise fairness and community preservation. To explore the practicality of our fairness goals, we devise an approach for extending existing $k$-center algorithms to satisfy these fairness constraints. Analysis of this approach proves that reasonable approximations can be achieved while maintaining fairness. In experiments, we compare the effectiveness of our approach to classical $k$-center algorithms/heuristics and explore the tradeoff between optimal clustering and fairness.","['Wellesley College', 'Carnegie Mellon University', 'University of Maryland', 'Northwestern University', 'Amazon', 'University of Maryland, College Park']"
2020,Piecewise Linear Regression via a Difference of Convex Functions,"Ali Siahkamari, Aditya Gangrade, Brian Kulis, Venkatesh Saligrama",https://icml.cc/Conferences/2020/Schedule?showEvent=6667,"We present a new piecewise linear regression methodology that utilises fitting a \emph{difference of convex} functions (DC functions) to the data. These are functions $f$ that may be represented as the difference $\phi_1 - \phi_2$ for a choice of \emph{convex} functions $\phi_1, \phi_2$. The method proceeds by estimating piecewise-liner convex functions, in a manner similar to max-affine regression, whose difference approximates the data. The choice of the function is regularised by a new seminorm over the class of DC functions that controls the $\ell_\infty$ Lipschitz constant of the estimate. The resulting methodology can be efficiently implemented via Quadratic programming \emph{even in high dimensions}, and is shown to have close to minimax statistical risk. We empirically validate the method, showing it to be practically implementable, and to outperform existing regression methods in accuracy on real-world datasets. ","['Boston University', 'Boston University', 'Boston University', 'Boston University']"
2020,Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere,"Tongzhou Wang, Phillip Isola",https://icml.cc/Conferences/2020/Schedule?showEvent=6657,"Contrastive representation learning has been outstandingly successful in practice.
In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks.
Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning.
","['MIT', 'MIT']"
2020,Stochastic Coordinate Minimization with Progressive Precision for Stochastic Convex Optimization,"Sudeep Salgia, Qing Zhao, Sattar Vakili",https://icml.cc/Conferences/2020/Schedule?showEvent=6652,"A framework based on iterative coordinate minimization (CM) is developed for stochastic convex optimization. Given that exact coordinate minimization is impossible due to the unknown stochastic nature of the objective function, the crux of the proposed optimization algorithm is an optimal control of the minimization precision in each iteration.  We establish the optimal precision control and the resulting order-optimal regret performance for strongly convex and separably nonsmooth functions.  An interesting finding is that the optimal progression of precision across iterations is independent of the low-dimension CM routine employed, suggesting a general framework for extending low-dimensional optimization routines to high-dimensional problems. The proposed algorithm is amenable to online implementation and inherits the scalability and parallelizability  properties of CM for large-scale optimization. Requiring only a sublinear order of message exchanges,  it also lends itself well to distributed computing as compared with the alternative approach of coordinate gradient descent.
","['Cornell University', 'Cornell University', 'Prowler.io']"
2020,Almost Tune-Free Variance Reduction,"Bingcong Li, Lingda Wang, Georgios B. Giannakis",https://icml.cc/Conferences/2020/Schedule?showEvent=5825,"The variance reduction class of algorithms including the representative ones, SVRG and SARAH, have well documented merits for empirical risk minimization problems. However, they require grid search to tune parameters (step size and the number of iterations per inner loop) for optimal performance. This work introduces almost tune-free' SVRG and SARAH schemes equipped with i) Barzilai-Borwein (BB) step sizes; ii) averaging; and, iii) the inner loop length adjusted to the BB step sizes. In particular, SVRG, SARAH, and their BB variants are first reexamined through anestimate sequence' lens to enable new averaging methods that tighten their convergence rates theoretically, and improve their performance empirically when the step size or the inner loop length is chosen large. Then a simple yet effective means to adjust the number of iterations per inner loop is developed to enhance the merits of the proposed averaging schemes and BB step sizes. Numerical tests corroborate the proposed methods.
","['University of Minnesota', 'University of Illinois at Urbana-Champaign', 'University of Minnesota']"
2020,Provably Efficient Model-based Policy Adaptation,"Yuda Song, Aditi Mavalankar, Wen Sun, Sicun Gao",https://icml.cc/Conferences/2020/Schedule?showEvent=6588,"The high sample complexity of reinforcement learning challenges its use in practice. A promising approach is to quickly adapt pre-trained policies to new environments. Existing methods for this policy adaptation problem typically rely on domain randomization and meta-learning, by sampling from some distribution of target environments during pre-training, and thus face difficulty on out-of-distribution target environments. We propose new model-based mechanisms that are able to make online adaptation in unseen target environments, by combining ideas from no-regret online learning and adaptive control. We prove that the approach learns policies in the target environment that can quickly recover trajectories from the source environment, and establish the rate of convergence in general settings. We demonstrate the benefits of our approach for policy adaptation in a diverse set of continuous control tasks, achieving the performance of state-of-the-art methods with much lower sample complexity. Our project website, including code, can be found at https://yudasong.github.io/PADA.
","['University of California, San Diego', 'University of California San Diego', 'Microsoft Research', 'University of California, San Diego']"
2020,Causal Inference using Gaussian Processes with Structured Latent Confounders,"Sam Witty, Kenta Takatsu, David Jensen, Vikash Mansinghka",https://icml.cc/Conferences/2020/Schedule?showEvent=6593,"Latent confounders---unobserved variables that influence both treatment and outcome---can bias estimates of causal effects. In some cases, these confounders are shared across observations, e.g. all students in a school are influenced by the school's culture in addition to any educational interventions they receive individually. This paper shows how to model latent confounders that have this structure and thereby improve estimates of causal effects. The key innovations are a hierarchical Bayesian model, Gaussian processes with structured latent confounders (GP-SLC), and a Monte Carlo inference algorithm for this model based on elliptical slice sampling. GP-SLC provides principled Bayesian uncertainty estimates of individual treatment effect with minimal assumptions about the functional forms relating confounders, covariates, treatment, and outcomes. This paper also proves that, for linear functional forms, accounting for the structure in latent confounders is sufficient for asymptotically consistent estimates of causal effect. Finally, this paper shows GP-SLC is competitive with or more accurate than widely used causal inference techniques such as multi-level linear models and Bayesian additive regression trees. Benchmark datasets include the Infant Health and Development Program and a dataset showing the effect of changing temperatures on state-wide energy consumption across New England.
","['University of Massachusetts, Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'Massachusetts Institute of Technology']"
2020,Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation,"Yaqi Duan, Zeyu Jia, Mengdi Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6543,"This paper studies the statistical theory of off-policy evaluation with function approximation in batch data reinforcement learning problem. We consider a regression-based fitted Q-iteration method, show that it is equivalent to a model-based method that estimates a conditional mean embedding of the transition operator, and prove that this method is information-theoretically optimal and has nearly minimal estimation error. In particular, by leveraging contraction property of Markov processes and martingale concentration, we establish a finite-sample instance-dependent error upper bound and a nearly-matching minimax lower bound. The policy evaluation error depends sharply on a restricted $\chi^2$-divergence over the function class between the long-term distribution of target policy and the distribution of past data. This restricted $\chi^2$-divergence characterizes the statistical limit of off-policy evaluation and is both instance-dependent and function-class-dependent. Further, we provide an easily computable confidence bound for the policy evaluator, which may be useful for optimistic planning and safe policy improvement.","['Princeton University', 'Peking University', 'Princeton University']"
2020,Continuous Graph Neural Networks,"Louis-Pascal Xhonneux, Meng Qu, Jian Tang",https://icml.cc/Conferences/2020/Schedule?showEvent=6441,"This  paper  builds  on  the  connection  between graph  neural  networks  and  traditional  dynamical systems. We propose continuous graph neural networks (CGNN), which generalise existing graph neural networks with discrete dynamics in that they can be viewed as a specific discretisation scheme. The key idea is how to characterise the continuous dynamics of node representations, i.e. the derivatives of node representations, w.r.t. time.Inspired by existing diffusion-based methods on graphs (e.g. PageRank and epidemic models on social networks), we define the derivatives as a combination of the current node representations,the representations of neighbors, and the initial values of the nodes. We propose and analyse two possible dynamics on graphs—including each dimension of node representations (a.k.a. the feature channel) change independently or interact with each other—both with theoretical justification. The proposed continuous graph neural net-works are robust to over-smoothing and hence allow us to build deeper networks, which in turn are able to capture the long-range dependencies between nodes. Experimental results on the task of node classification demonstrate the effectiveness of our proposed approach over competitive baselines.
","['Mila / Université de Montréal', 'Mila', 'HEC Montreal & MILA']"
2020,Learning with Bounded Instance- and Label-dependent Label Noise,"Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, Dacheng Tao",https://icml.cc/Conferences/2020/Schedule?showEvent=6059,"Instance- and Label-dependent label Noise (ILN) widely exists in real-world datasets but has been rarely studied. In this paper, we focus on Bounded Instance- and Label-dependent label Noise (BILN), a particular case of ILN where the label noise rates---the probabilities that the true labels of examples flip into the corrupted ones---have upper bound less than $1$. Specifically, we introduce the concept of distilled examples, i.e. examples whose labels are identical with the labels assigned for them by the Bayes optimal classifier, and prove that under certain conditions classifiers learnt on distilled examples will converge to the Bayes optimal classifier. Inspired by the idea of learning with distilled examples, we then propose a learning algorithm with theoretical guarantees for its robustness to BILN. At last, empirical evaluations on both synthetic and real-world datasets show effectiveness of our algorithm in learning with BILN.","['University of California, San Diego', 'The University of Sydney', 'The University of Melbourne', 'The University of Sydney']"
2020,Skew-Fit: State-Covering Self-Supervised Reinforcement Learning,"Vitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, Sergey Levine",https://icml.cc/Conferences/2020/Schedule?showEvent=6516,"Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. We prove that, under regularity conditions, Skew-Fit converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that combining Skew-Fit for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'Carnegie Mellon University', 'UC Berkeley']"
2020,Low-Rank Bottleneck in Multi-head Attention Models,"Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Jakkam Reddi, Sanjiv Kumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6288,"Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation. We further validate this in our experiments. As a solution we propose to set the head size of an attention unit to input sequence length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and with better performance scaling.
","['Google AI', 'MIT', 'Google', 'Google', 'Google Research, NY']"
2020,"Estimating Q(s,s') with Deep Deterministic Dynamics Gradients","Ashley Edwards, Himanshu Sahni, Rosanne Liu, Jane Hung, Ankit Jain, Rui Wang, Adrien Ecoffet, Thomas Miconi, Charles Isbell, Jason Yosinski",https://icml.cc/Conferences/2020/Schedule?showEvent=6719,"In this paper, we introduce a novel form of value function, $Q(s, s')$, that expresses the utility of transitioning from a state $s$ to a neighboring state $s'$ and then acting optimally thereafter. In order to derive an optimal policy, we develop a forward dynamics model that learns to make next-state predictions that maximize this value. This formulation decouples actions from values while still learning off-policy. We highlight the benefits of this approach in terms of value function transfer, learning within redundant action spaces, and learning off-policy from state observations generated by sub-optimal or completely random policies. Code and videos are available at http://sites.google.com/view/qss-paper.","['Uber AI', 'Georgia Institute of Technology', 'ML Collective', 'Uber', 'Uber AI', 'Uber AI', 'OpenAI', 'Uber AI Labs', 'Georgia Institute of Technology', 'Deep Collective']"
2020,FACT: A Diagnostic for Group Fairness Trade-offs,"Joon Kim, Jiahao Chen, Ameet Talwalkar",https://icml.cc/Conferences/2020/Schedule?showEvent=6636,"Group fairness, a class of fairness notions that measure how different groups of individuals are treated differently according to their protected attributes, has been shown to conflict with one another, often with a necessary cost in loss of model's predictive performance. We propose a general diagnostic that enables systematic characterization of these trade-offs in group fairness. We observe that the majority of group fairness notions can be expressed via the fairness-confusion tensor, which is the confusion matrix split according to the protected attribute values. We frame several optimization problems that directly optimize both accuracy and fairness objectives over the elements of this tensor, which yield a general perspective for understanding multiple trade-offs including group fairness incompatibilities. It also suggests an alternate post-processing method for designing fair classifiers. On synthetic and real datasets, we demonstrate the use cases of our diagnostic, particularly on understanding the trade-off landscape between accuracy and fairness. 
","['Carnegie Mellon University', 'JPMorgan Chase', 'Carnegie Mellon University']"
2020,Improving Robustness of Deep-Learning-Based Image Reconstruction,"Ankit Raj, Yoram Bresler, Bo Li",https://icml.cc/Conferences/2020/Schedule?showEvent=6782,"Deep-learning-based methods for various applications have been shown vulnerable to adversarial examples.  Here we address the use of deep-learning networks as inverse problem solvers, which has generated much excitement and even adoption efforts by the main equipment vendors for medical imaging including computed tomography (CT) and MRI. However, the recent demonstration that such networks suffer from a similar vulnerability to adversarial attacks potentially undermines their future.  We propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. To this end, we introduce an auxiliary net-work to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we argue that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of in the signal-space used in previous work. We show for a linear reconstruction scheme that our min-max formulation results in a singular-value filter regularized solution, which suppresses the effect of adversarial examples.  Numerical experiments using the proposed min-max scheme confirm convergence to this solution.  We complement the theory by experiments on non-linear Compressive Sensing(CS) reconstruction by a deep neural network on two standard datasets, and, using anonymized clinical data, on a state-of-the-art published algorithm for low-dose x-ray CT reconstruction. We show a significant improvement in robustness over other methods for deep network-based reconstruction, by using the proposed approach.
","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'UIUC']"
2020,Adaptive Adversarial Multi-task Representation Learning,"YUREN MAO, Weiwei Liu, Xuemin Lin",https://icml.cc/Conferences/2020/Schedule?showEvent=5936,"Adversarial Multi-task Representation Learning (AMTRL) methods are able to boost the performance of Multi-task Representation Learning (MTRL) models. However, the theoretical mechanism behind AMTRL is less investigated. To fill this gap, we study the generalization error bound of AMTRL through the lens of Lagrangian duality . Based on the duality, we proposed an novel adaptive AMTRL algorithm which improves the performance of original AMTRL methods. The extensive experiments back up our theoretical analysis and validate the superiority of our proposed algorithm.
","['School of Computer Science and Engineering, University of New South Wales', 'Wuhan University', 'University of New South Wales']"
2020,Towards Understanding the Regularization of Adversarial Robustness on Neural Networks,"Yuxin Wen, Shuai Li, Kui Jia",https://icml.cc/Conferences/2020/Schedule?showEvent=5922,"The problem of adversarial examples has shown that modern Neural Network (NN) models could be rather fragile.  Among the more established techniques to solve the problem, one is to require the model to be {\it $\epsilon$-adversarially robust} (AR); that is, to require the model not to change predicted labels when any given input examples are perturbed within a certain range.  However, it is observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples.  In this work, we study the degradation through the regularization perspective.  We identify quantities from generalization analysis of NNs; with the identified quantities we empirically find that AR is achieved by regularizing/biasing NNs towards less confident solutions by making the changes in the feature space (induced by changes in the instance space) of most layers smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r.t.  perturbations.  However, the end result of such smoothing concentrates samples around decision boundaries, resulting in less confident solutions, and leads to worse standard performance.  Our studies suggest that one might consider ways that build AR into NNs in a gentler way to avoid the problematic regularization.
","['South China University of Technology', 'South China University of Technology', 'South China University of Technology']"
2020,Optimizing Dynamic Structures with Bayesian Generative Search,"Minh Hoang, Carleton Kingsford",https://icml.cc/Conferences/2020/Schedule?showEvent=6573,"Kernel selection for kernel-based methods is prohibitively expensive due to the NP-hard nature of discrete optimization. Since gradient-based optimizers are not applicable due to the lack of a differentiable objective function, many state-of-the-art solutions resort to heuristic search or gradient-free optimization. These approaches, however, require imposing restrictive assumptions on the explorable space of structures such as limiting the active candidate pool, thus depending heavily on the intuition of domain experts. This paper instead proposes \textbf{DTERGENS}, a novel generative search framework that constructs and optimizes a high-performance composite kernel expressions generator. \textbf{DTERGENS} does not restrict the space of candidate kernels and is capable of obtaining flexible length expressions by jointly optimizing a generative termination criterion. We demonstrate that our framework explores more diverse kernels and obtains better performance than state-of-the-art approaches on many real-world predictive tasks.
","['Carnegie Mellon University', 'Carnegie Mellon University']"
2020,A Flexible Framework for Nonparametric Graphical Modeling that Accommodates Machine Learning,"Yunhua Xiang, Noah Simon",https://icml.cc/Conferences/2020/Schedule?showEvent=6143,"Graphical modeling has been broadly useful for exploring the dependence structure among features in a dataset. However, the strength of graphical modeling hinges on our ability to encode and estimate conditional dependencies. In particular, commonly used measures such as partial correlation are only meaningful under strongly parametric (in this case, multivariate Gaussian) assumptions. These assumptions are unverifiable, and there is often little reason to believe they hold in practice. In this paper, we instead consider 3 non-parametric measures of conditional dependence. These measures are meaningful without structural assumptions on the multivariate distribution of the data. In addition, we show that for 2 of these measures there are simple, strong plug-in estimators that require only the estimation of a conditional mean. These plug-in estimators (1) are asymptotically linear and non-parametrically efficient, (2) allow incorporation of flexible machine learning techniques for conditional mean estimation, and (3) enable the construction of valid Wald-type confidence intervals. In addition, by leveraging the influence function of these estimators, one can obtain intervals with simultaneous coverage guarantees for all pairs of features.
","['University of Washington', 'University of Washington']"
2020,Concise Explanations of Neural Networks using Adversarial Training,"Prasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Xi Wu, Somesh Jha",https://icml.cc/Conferences/2020/Schedule?showEvent=6609,"We show new connections between adversarial learning and explainability for deep neural networks (DNNs). One form of explanation of the output of a neural network model in terms of its input features, is a vector of feature-attributions, which can be generated by various techniques such as Integrated Gradients (IG), DeepSHAP, LIME, and CXPlain. Two desirable characteristics of an attribution-based explanation are: (1) \textit{sparseness}: the attributions of irrelevant or weakly relevant features should be negligible, thus resulting in \textit{concise} explanations in terms of the significant features, and (2) \textit{stability}: it should not vary significantly within a small local neighborhood of the input. Our first contribution is a theoretical exploration of how these two properties (when using IG-based attributions) are related to adversarial training, for a class of 1-layer networks (which includes logistic regression models for binary and multi-class classification); for these networks we show that (a) adversarial training using an $\ell_\infty$-bounded adversary produces models with sparse attribution vectors, and (b) natural model-training while encouraging stable explanations (via an extra term in the loss function), is equivalent to adversarial training.  Our second contribution is an empirical verification of phenomenon (a), which we show, somewhat surprisingly, occurs \textit{not only in 1-layer networks, but also DNNs trained on standard image datasets}, and extends beyond IG-based attributions,  to those based on DeepSHAP:  adversarial training with $\linf$-bounded perturbations yields significantly sparser attribution vectors, with little degradation in performance on natural test data, compared to natural training. Moreover, the sparseness of the attribution vectors is significantly better than that achievable via $\ell_1$-regularized natural training.
","['XaiPient', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'Google', 'University of Wisconsin, Madison']"
2020,Reward-Free Exploration for Reinforcement Learning,"Chi Jin, Akshay Krishnamurthy, Max Simchowitz, Tiancheng Yu",https://icml.cc/Conferences/2020/Schedule?showEvent=6285,"Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose the following ``reward-free RL'' framework. In the exploration phase, the agent first collects trajectories from an MDP $M$ without a pre-specified reward function. After exploration, it is tasked with computing a near-policies under the transitions of $\mathcal{M}$ for a collection of given reward functions.  This framework is particularly suitable where there are many reward functions of interest, or where the reward function is shaped by an external agent to elicit desired behavior. 

We give an efficient algorithm that conducts  $\widetilde{O}(S^2A\mathrm{poly}(H)/\epsilon^2)$ episodes of exploration, and returns $\epsilon$-suboptimal policies for an arbitrary number of reward functions. We achieve this by finding exploratory policies that jointly visit each ``significant'' state with probability proportional to its maximum visitation probability under any possible policy. Moreover, our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. Finally, we give a nearly-matching $\Omega(S^2AH^2/\epsilon^2)$ lower bound, demonstrating the near-optimality of our algorithm in this setting. ","['Princeton University', 'Microsoft Research', 'UC Berkeley', 'MIT']"
2020,Convex Representation Learning for Generalized Invariance in Semi-Inner-Product Space,"Yingyi Ma, Vignesh Ganapathiraman, Yaoliang Yu, Xinhua Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6355,"Invariance (defined in a general sense) has been one of the most effective priors for representation learning.  Direct factorization of parametric models is feasible only for a small range of invariances, while regularization approaches, despite improved generality, lead to nonconvex optimization.  In this work, we develop a \emph{convex} representation learning algorithm for a variety of generalized invariances that can be modeled as semi-norms.  Novel Euclidean embeddings are introduced for kernel representers in a semi-inner-product space, and approximation bounds are established.  This allows invariant representations to be learned efficiently and effectively as confirmed in our experiments, along with accurate predictions.
","['UIC', 'University of Illinois at Chicago', 'University of Waterloo', 'University of Illinois at Chicago (UIC)']"
2020,Do GANs always have Nash equilibria?,"Farzan Farnia, Asuman Ozdaglar",https://icml.cc/Conferences/2020/Schedule?showEvent=6066,"Generative adversarial networks (GANs) represent a zero-sum game between two machine players, a generator and a discriminator, designed to learn the distribution of data. While GANs have achieved state-of-the-art performance in several benchmark learning tasks, GAN minimax optimization still poses great theoretical and empirical challenges. GANs trained using first-order optimization methods commonly fail to converge to a stable solution where the players cannot improve their objective, i.e., the Nash equilibrium of the underlying game. Such issues raise the question of the existence of Nash equilibria in GAN zero-sum games. In this work, we show through theoretical and numerical results that indeed GAN zero-sum games may have no Nash equilibria. To characterize an equilibrium notion applicable to GANs, we consider the equilibrium of a new zero-sum game with an objective function given by a proximal operator applied to the original objective, a solution we call the proximal equilibrium. Unlike the Nash equilibrium, the proximal equilibrium captures the sequential nature of GANs, in which the generator moves first followed by the discriminator. We prove that the optimal generative model in Wasserstein GAN problems provides a proximal equilibrium. Inspired by these results, we propose a new approach, which we call proximal training, for solving GAN problems. We perform several numerical experiments indicating the existence of proximal equilibria in GANs.
","['Massachusetts Institute of Technology', 'MIT']"
2020,Stochastic Flows and Geometric Optimization on the Orthogonal Group,"Krzysztof Choromanski, David Cheikhi, Jared Quincy Davis, Valerii Likhosherstov, Achille Nazaret, Achraf Bahamou, Xingyou Song, Mrugank Akarte, Jack Parker-Holder, Jacob Bergquist, Yuan Gao, Aldo Pacchiano, Tamas Sarlos, Adrian Weller, Vikas Sindhwani",https://icml.cc/Conferences/2020/Schedule?showEvent=5770,"We present a new class of stochastic, geometrically-driven optimization algorithms on the orthogonal group O(d) and naturally reductive homogeneous manifolds obtained from the action of the rotation group SO(d). We theoretically and experimentally demonstrate that our methods can be applied in various fields of machine learning including deep, convolutional and recurrent neural networks, reinforcement learning, normalizing flows and metric learning. We show an intriguing connection between efficient stochastic optimization on the orthogonal group and graph theory (e.g. matching problem, partition functions over graphs, graph-coloring). We leverage the theory of Lie groups and provide theoretical results for the designed class of algorithms. We demonstrate broad applicability of our methods by showing strong performance on the seemingly unrelated tasks of learning world models to obtain stable policies for the most difficult Humanoid agent from OpenAI Gym and improving convolutional neural networks.
","['Google Brain Robotics', 'Columbia University', 'DeepMind & Stanford University', 'University of Cambridge', 'Columbia University', 'Columbia University', 'Google Brain', 'Columbia University', 'University of Oxford', 'Columbia University', 'Columbia University', 'UC Berkeley', 'Google', 'University of Cambridge, Alan Turing Institute', 'Google']"
2020,Robustifying Sequential Neural Processes,"Jaesik Yoon, Gautam Singh, Sungjin Ahn",https://icml.cc/Conferences/2020/Schedule?showEvent=6572,"When tasks change over time, meta-transfer learning seeks to improve the efficiency of learning a new task via both meta-learning and transfer-learning. While the standard attention has been effective in a variety of settings, we question its effectiveness in improving meta-transfer learning since the tasks being learned are dynamic and the amount of context can be substantially smaller. In this paper, using a recently proposed meta-transfer learning model, Sequential Neural Processes (SNP), we first empirically show that it suffers from a similar underfitting problem observed in the functions inferred by Neural Processes. However, we further demonstrate that unlike the meta-learning setting, the standard attention mechanisms are not effective in meta-transfer setting. To resolve, we propose a new attention mechanism, Recurrent Memory Reconstruction (RMR), and demonstrate that providing an imaginary context that is recurrently updated and reconstructed with interaction is crucial in achieving effective attention for meta-transfer learning. Furthermore, incorporating RMR into SNP, we propose Attentive Sequential Neural Processes-RMR (ASNP-RMR) and demonstrate in various tasks that ASNP-RMR significantly outperforms the baselines. 
","['SAP', 'Rutgers Univerity', 'Rutgers University']"
2020,Black-Box Variational Inference as a Parametric Approximation to Langevin Dynamics,"Matthew Hoffman, Yian Ma",https://icml.cc/Conferences/2020/Schedule?showEvent=6629,"Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms that are often said to have complementary strengths, with VI being fast but biased and MCMC being slower but asymptotically unbiased. In this paper, we analyze gradient-based MCMC and VI procedures and find theoretical and empirical evidence that these procedures are not as different as one might think. In particular, a close examination of the Fokker-Planck equation that governs the Langevin dynamics (LD) MCMC procedure reveals that LD implicitly follows a gradient flow that corresponds to an VI procedure based on optimizing a nonparametric normalizing flow. The evolution under gradient descent of real-world VI approximations that use tractable, parametric flows can thus be seen as an approximation to the evolution of a population of LD-MCMC chains. This result suggests that the transient bias of LD (due to the Markov chain not having burned in) may track that of VI (due to the optimizer not having converged), up to differences due to VI’s asymptotic bias and parameter geometry. Empirically, we find that the transient biases of these algorithms (and their momentum-accelerated counterparts) do evolve similarly. This suggests that practitioners with a limited time budget may get more accurate results by running an MCMC procedure (even if it is stopped before fully burning in) than a VI procedure, as long as the variance of the MCMC estimator can be dealt with (e.g., by running many parallel chains).
","['Google', 'Google']"
2020,Tails of Lipschitz Triangular Flows,"Priyank Jaini, Ivan Kobyzev, Yaoliang Yu, Marcus Brubaker",https://icml.cc/Conferences/2020/Schedule?showEvent=6006,"We investigate the ability of popular flow models to capture tail-properties of a target density by studying the increasing triangular maps used in these flow methods acting on a tractable source density. We show that the density quantile functions of the source and target density provide a precise characterization of the slope of transformation required to capture tails in a target density. We further show that any Lipschitz-continuous transport map acting on a source density will result in a density with similar tail properties as the source, highlighting the trade-off between the importance of choosing a complex source density and a sufficiently expressive transformation to capture desirable properties of a target density. Subsequently, we illustrate that flow models like Real-NVP, MAF, and Glow as implemented lack the ability to capture a distribution with non-Gaussian tails. We circumvent this problem by proposing tail-adaptive flows consisting of a source distribution that can be learned simultaneously with the triangular map to capture tail-properties of a target density. We perform several synthetic and real-world experiments to complement our theoretical findings. 
","['University of Amsterdam', 'Borealis AI', 'University of Waterloo', 'Borealis AI']"
2020,Adaptive Region-Based Active Learning,"Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, Ningshan Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=5991,"We present a new active learning algorithm that adaptively partitions the input space into a finite number of regions, and subsequently seeks a distinct predictor for each region, while actively requesting labels. We prove theoretical guarantees for both the generalization error and the label complexity of our algorithm, and analyze the number of regions defined by the algorithm under some mild assumptions. We also report the results of an extensive suite of experiments on several real-world datasets demonstrating substantial empirical benefits over existing single-region and non-adaptive region-based active learning baselines.
","['Google Research', 'Google Research', 'Google Research', 'Google Research and Courant Institute of Mathematical Sciences', 'Hudson River Trading']"
2020,Meta Variance Transfer: Learning to Augment from the Others,"Seong-Jin Park, Seungju Han, Ji-won Baek, Insoo Kim, Juhwan Song, Hae Beom Lee, Jae-Joon Han, Sung Ju Hwang",https://icml.cc/Conferences/2020/Schedule?showEvent=6113,"Humans have the ability to robustly recognize objects with various factors of variations such as nonrigid transformations, background noises, and changes in lighting conditions. However, training deep learning models generally require huge amount of data instances under diverse variations, to ensure its robustness. To alleviate the need of collecting large amount of data and better learn to generalize with scarce data instances, we propose a novel meta-learning method which learns to transfer factors of variations from one class to another, such that it can improve the classification performance on unseen examples. Transferred variations generate virtual samples that augment the feature space of the target class during training, simulating upcoming query samples with similar variations. By sharing the factors of variations across different classes, the model becomes more robust to variations in the unseen examples and tasks using small number of examples per class. We validate our model on multiple benchmark datasets for few-shot classification and face recognition, on which our model significantly improves the performance of the base model, outperforming relevant baselines.
","['Samsung Advanced Institute of Technology', 'Samsung Advanced Institute of Technology', 'Samsung Advanced Institute of Technology', 'Samsung Advanced Institute of Technology', 'Samsung Advanced Institute of Technology', 'KAIST', 'Samsung Advanced Institute of Technology', 'KAIST, AITRICS']"
2020,Towards Accurate Post-training Network Quantization via Bit-Split and Stitching,"Peisong Wang, Qiang Chen, Xiangyu He, Jian Cheng",https://icml.cc/Conferences/2020/Schedule?showEvent=5787,"Network quantization is essential for deploying deep models to IoT devices due to its high efficiency. Most existing quantization approaches rely on the full training datasets and the time-consuming fine-tuning to retain accuracy. Post-training quantization does not have these problems, however, it has mainly been shown effective for 8-bit quantization due to the simple optimization strategy. In this paper, we propose a Bit-Split and Stitching framework (Bit-split) for lower-bit post-training quantization with minimal accuracy degradation. The proposed framework is validated on a variety of computer vision tasks, including image classification, object detection, instance segmentation, with various network architectures. Specifically, Bit-split can achieve near-original model performance even when quantizing FP32 models to INT3 without fine-tuning.
","['Institute of Automation, Chinese Academy of Sciences', 'CASIA', 'CASIA', '""Chinese Academy of Sciences, China""']"
2020,Improving Transformer Optimization Through Better Initialization ,"Xiao Shi Huang, Felipe Perez, Jimmy Ba, Maksims Volkovs",https://icml.cc/Conferences/2020/Schedule?showEvent=6684,"The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with 
attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without difficulty. Code for this work is available here:~\url{https://github.com/layer6ai-labs/T-Fixup}.
","['Layer 6', 'Layer6 AI', 'University of Toronto', 'Layer6 AI']"
2020,Learning Selection Strategies in Buchberger’s Algorithm,"Dylan Peifer, Michael Stillman, Daniel Halpern-Leistner",https://icml.cc/Conferences/2020/Schedule?showEvent=6742,"Studying the set of exact solutions of a system of polynomial equations largely depends on a single iterative algorithm, known as Buchberger’s algorithm. Optimized versions of this algorithm are crucial for many computer algebra systems (e.g., Mathematica, Maple, Sage). We introduce a new approach to Buchberger’s algorithm that uses reinforcement learning agents to perform S-pair selection, a key step in the algorithm. We then study how the difficulty of the problem depends on the choices of domain and distribution of polynomials, about which little is known. Finally, we train a policy model using proximal policy optimization (PPO) to learn S-pair selection strategies for random systems of binomial equations. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent developments in machine learning have the potential to improve performance of algorithms in symbolic computation.
","['Cornell University', 'Cornell University', 'Cornell University']"
2020,Improved Sleeping Bandits with Stochastic Action Sets and Adversarial Rewards,"Aadirupa Saha, Pierre Gaillard, Michal Valko",https://icml.cc/Conferences/2020/Schedule?showEvent=5830,"In this paper, we consider the problem of sleeping bandits with stochastic action sets and adversarial rewards. In this setting, in contrast to most work in bandits, the actions may not be available at all times. For instance, some products might be out of stock in item recommendation. The best existing efficient (i.e., polynomial-time) algorithms for this problem only guarantee a $O(T^{2/3})$ upper-bound on the regret. Yet, inefficient algorithms based on EXP4 can achieve $O(\sqrt{T})$.  In this paper, we provide a new computationally efficient algorithm inspired by EXP3 satisfying a regret of order  $O(\sqrt{T})$ when the availabilities of each action $i \in \cA$ are independent. We then study the most general version of the problem where at each round available sets are generated from some unknown arbitrary distribution (i.e., without the independence assumption) and propose an efficient algorithm with $O(\sqrt {2^K T})$ regret guarantee. Our theoretical results are corroborated with experimental evaluations.","['Indian Institute of Science (IISc), Bangalore', '', 'DeepMind']"
2020,Black-Box Methods for Restoring Monotonicity,"Evangelia Gergatsouli, Brendan Lucier, Christos Tzamos",https://icml.cc/Conferences/2020/Schedule?showEvent=6775,"In many practical applications, heuristic or approximation algorithms
are used to efficiently solve the task at hand. However their
solutions frequently do not satisfy natural monotonicity properties
expected to hold in the optimum. In this work we develop algorithms that are
able to restore monotonicity in the parameters of interest.
Specifically, given oracle access to a possibly non monotone function, 
we provide an algorithm that restores monotonicity while
degrading the expected value of the function by at most $\epsilon$. The
number of queries required is at most logarithmic in $1/\epsilon$ and
exponential in the number of parameters. We also give a lower bound
showing that this exponential dependence is necessary.
Finally, we obtain improved query complexity bounds for restoring the
weaker property of $k$-marginal monotonicity. Under this property, every
$k$-dimensional projection of the function is required to be
monotone. The query complexity we obtain only scales exponentially with $k$ and is polynomial in the number of parameters.
","['UW-Madison', 'Microsoft Research New England', 'UW-Madison']"
2020,Graph Homomorphism Convolution,"Hoang NT, Takanori Maehara",https://icml.cc/Conferences/2020/Schedule?showEvent=6747,"In this paper, we study the graph classification problem from the graph homomorphism perspective. We consider the homomorphisms from $F$ to $G$, where $G$ is a graph of interest (e.g. molecules or social networks) and $F$ belongs to some family of graphs (e.g. paths or non-isomorphic trees). We show that graph homomorphism numbers provide a natural invariant (isomorphism invariant and $\mathcal{F}$-invariant) embedding maps which can be used for graph classification. Viewing the expressive power of a graph classifier by the $\mathcal{F}$-indistinguishable concept, we prove the universality property of graph homomorphism vectors in approximating $\mathcal{F}$-invariant functions. In practice, by choosing $\mathcal{F}$ whose elements have bounded tree-width, we show that the homomorphism method is efficient compared with other methods.","['RIKEN AIP & Tokyo Tech', 'RIKEN AIP']"
2020,Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth Expansion,"Qinqing Zheng, Jinshuo Dong, Qi Long, Weijie Su",https://icml.cc/Conferences/2020/Schedule?showEvent=6734,"Datasets containing sensitive information are often sequentially analyzed by many algorithms and, accordingly, a fundamental question in differential privacy is concerned with how the overall privacy bound degrades under composition. To address this question, we introduce a family of analytical and sharp privacy bounds under composition using the Edgeworth expansion in the framework of the recently proposed $f$-differential privacy. In short, whereas the existing composition theorem, for example, relies on the central limit theorem, our new privacy bounds under composition gain improved tightness by leveraging the refined approximation accuracy of the Edgeworth expansion. Our approach is easy to implement and computationally efficient for any number of compositions. The superiority of these new bounds is confirmed by an asymptotic error analysis and an application to quantifying the overall privacy guarantees of noisy stochastic gradient descent used in training private deep neural networks.","['University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania', 'University of Pennsylvania']"
2020,Kernelized Stein Discrepancy Tests of Goodness-of-fit  for Time-to-Event Data,"Tamara Fernandez, Arthur Gretton, Nicolas Rivera, Wenkai Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=5893,"Survival Analysis and Reliability Theory are concerned with the analysis of time-to-event data, in which observations correspond to waiting times until an event of interest, such as death from a particular disease or failure of a component in a mechanical system. This type of data is unique due to the presence of censoring, a type of missing data that occurs when we do not observe the actual time of the event of interest but instead we have access to an approximation for it given by random interval in which the observation is known to belong.
Most traditional methods are not designed to deal with censoring, and thus we need to adapt them to censored time-to-event data. In this paper, we focus on non-parametric Goodness-of-Fit testing procedures based on combining the Stein's method and kernelized discrepancies. While for uncensored data, there is a natural way of implementing a kernelized Stein discrepancy test, for censored data there are several options, each of them with different advantages and disadvantages. In this paper we propose a collection of kernelized Stein discrepancy tests for time-to-event data, and we study each of them theoretically and empirically. Our experimental results show that our proposed methods perform better than existing tests, including previous tests based on a kernelized maximum mean discrepancy.
","['University College London', 'Gatsby Computational Neuroscience Unit', 'University of Cambridge', 'Gatsby Unit，UCL']"
2020,Performative Prediction,"Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dünner, Moritz Hardt",https://icml.cc/Conferences/2020/Schedule?showEvent=5810,"When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.
","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley']"
2020,Acceleration through spectral density estimation,"Fabian Pedregosa, Damien Scieur",https://icml.cc/Conferences/2020/Schedule?showEvent=5794,"We develop a framework for the average-case analysis of random quadratic problems and derive algorithms that are optimal under this analysis. This yields a new class of methods that achieve acceleration given a model of the Hessian's eigenvalue distribution. We develop explicit algorithms for the uniform, Marchenko-Pastur, and exponential distributions. These methods are momentum-based algorithms, whose hyper-parameters can be estimated without knowledge of the Hessian's smallest singular value, in contrast with classical accelerated methods like Nesterov acceleration and Polyak momentum. Through empirical benchmarks on quadratic and logistic regression problems, we identify regimes in which the the proposed methods improve over classical (worst-case) accelerated methods.
","['Google', 'Samsung - SAIT AI Lab, Montreal']"
2020,Bridging the Gap Between f-GANs and Wasserstein GANs,"Jiaming Song, Stefano Ermon",https://icml.cc/Conferences/2020/Schedule?showEvent=6640,"Generative adversarial networks (GANs) variants approximately minimize divergences between the model and the data distribution using a discriminator. Wasserstein GANs (WGANs) enjoy superior empirical performance, however, unlike in f-GANs, the discriminator does not provide an estimate for the ratio between model and data densities, which is useful in applications such as inverse reinforcement learning. To overcome this limitation, we propose an new training objective where we additionally optimize over a set of importance weights over the generated samples. By suitably constraining the feasible set of importance weights, we obtain a family of objectives which includes and generalizes the original f-GAN and WGAN objectives. We show that a natural extension outperforms WGANs while providing density ratios as in f-GAN, and demonstrate empirical success on distribution modeling, density ratio estimation and image generation.
","['Stanford', 'Stanford University']"
2020,Reducing Sampling Error in Batch Temporal Difference Learning,"Brahma Pavse, Ishan Durugkar, Josiah Hanna, Peter Stone",https://icml.cc/Conferences/2020/Schedule?showEvent=6626,"Temporal difference (TD) learning is one of the main foundations of modern reinforcement learning. This paper studies the use of TD(0), a canonical TD algorithm, to estimate the value function of a given policy from a batch of data. In this batch setting, we show that TD(0) may converge to an inaccurate value function because the update following an action is weighted according to the number of times that action occurred in the batch -- not the true probability of the action under the given policy. To address this limitation, we introduce \textit{policy sampling error corrected}-TD(0) (PSEC-TD(0)). PSEC-TD(0) first estimates the empirical distribution of actions in each state in the batch and then uses importance sampling to correct for the mismatch between the empirical weighting and the correct weighting for updates following each action. We refine the concept of a certainty-equivalence estimate and argue that PSEC-TD(0) is a more data efficient estimator than TD(0) for a fixed batch of data. Finally, we conduct an empirical evaluation of PSEC-TD(0) on three batch value function learning tasks, with a hyperparameter sensitivity analysis, and show that PSEC-TD(0) produces value function estimates with lower mean squared error than TD(0).
","['University of Texas at Austin', 'University of Texas at Austin', 'University of Edinburgh', 'University of Texas at Austin']"
2020,Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning,"Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, Jimmy Ba",https://icml.cc/Conferences/2020/Schedule?showEvent=6622,"What goals should a multi-goal reinforcement learning agent pursue during training in long-horizon tasks? When the desired (test time) goal distribution is too distant to offer a useful learning signal, we argue that the agent should not pursue unobtainable goals. Instead, it should set its own intrinsic goals that maximize the entropy of the historical achieved goal distribution. We propose to optimize this objective by having the agent pursue past achieved goals in sparsely explored areas of the goal space, which focuses exploration on the frontier of the achievable goal set.
We show that our strategy achieves an order of magnitude better sample efficiency than the prior state of the art on long-horizon multi-goal tasks including maze navigation and block stacking.
","['University of Toronto', 'University of Toronto, Vector Institute', 'University of Toronto', 'Vector Institute', 'University of Toronto']"
2020,Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks,"Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, Tom Goldstein",https://icml.cc/Conferences/2020/Schedule?showEvent=6591,"Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification.  While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well.  We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically.  In doing so, we introduce and verify several hypotheses for why meta-learned models perform better.  Furthermore, we develop a regularizer which boosts the performance of standard training routines for few-shot classification.  In many cases, our routine outperforms meta-learning while simultaneously running an order of magnitude faster.
","['University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland', 'University of Maryland']"
2020,Universal Asymptotic Optimality of Polyak Momentum,"Damien Scieur, Fabian Pedregosa",https://icml.cc/Conferences/2020/Schedule?showEvent=5816,"Polyak momentum (PM), also known as the heavy-ball method, is a widely used optimization method that enjoys an asymptotic optimal worst-case complexity on quadratic objectives. However, its remarkable empirical success  is not fully explained by this optimality, as the worst-case analysis --contrary to the average-case-- is not representative of the expected complexity of an algorithm. In this work we establish a novel link between PM and the average-case analysis. Our main contribution is to prove that \emph{any} optimal average-case method converges in the number of iterations to PM, under mild assumptions. This brings a new perspective on this classical method, showing that PM is asymptotically both worst-case and average-case optimal.
","['Samsung - SAIT AI Lab, Montreal', 'Google']"
2020,Upper bounds for Model-Free Row-Sparse Principal Component Analysis,"Guanyi Wang, Santanu  Dey",https://icml.cc/Conferences/2020/Schedule?showEvent=6283,"Sparse principal component analysis (PCA) is a widely-used dimensionality reduction tool in statistics and machine learning. Most methods mentioned in literature are either heuristics for good primal feasible solutions under statistical assumptions or ADMM-type algorithms with stationary/critical points convergence property for the regularized reformulation of sparse PCA. However, none of these methods can efficiently verify the quality of the solutions via comparing current objective values with their dual bounds, especially in model-free case. We propose a new framework that finds out upper (dual) bounds for the sparse PCA within polynomial time via solving a convex integer program (IP). We show that, in the worst-case, the dual bounds provided by the convex IP is within an affine function of the global optimal value. Moreover, in contrast to the semi-definition relaxation, this framework is much easier to scale on large cases. Numerical results on both artificial and real cases are reported to demonstrate the advantages of our method.
","['Georgia Institute of Technology', 'Georgia Institute of Technology']"
2020,Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks,"Blake Bordelon, Abdulkadir Canatar, Cengiz Pehlevan",https://icml.cc/Conferences/2020/Schedule?showEvent=6524,"We derive analytical expressions for the  generalization performance of kernel regression as a function of the number of training samples using theoretical methods from Gaussian processes and statistical physics. Our expressions apply to  wide neural networks due to an equivalence between training them and kernel regression with the Neural Tangent Kernel (NTK). By computing the decomposition of the total generalization error due to different spectral components of the kernel, we identify a new spectral principle: as the size of the training set grows, kernel machines and neural networks fit successively higher spectral modes of the target function. When data are sampled from a uniform distribution on a high-dimensional hypersphere, dot product kernels, including NTK, exhibit learning stages where different frequency modes of the target function are learned. We verify our theory with simulations on synthetic data and MNIST dataset.
","['Harvard University', 'Harvard University', 'Harvard University']"
2020,Invariant Rationalization,"Shiyu Chang, Yang Zhang, Mo Yu, Tommi Jaakkola",https://icml.cc/Conferences/2020/Schedule?showEvent=6451,"Selective rationalization improves neural network interpretability by identifying a small subset of input features — the rationale — that best explains or supports the prediction. A typical rationalization criterion, i.e. maximum mutual information (MMI), finds the rationale that maximizes the prediction performance based only on the rationale. However, MMI can be problematic because it picks up spurious correlations between the input features and the output.  Instead, we introduce a game-theoretic invariant rationalization criterion where the rationales are constrained to enable the same predictor to be optimal across different environments. We show both theoretically and empirically that the proposed rationales can rule out spurious correlations and generalize better to different test scenarios. The resulting explanations also align better with human judgments. 
Our implementations are publicly available at https://github.com/code-terminator/invariant_rationalization.
","['MIT-IBM Watson AI Lab', 'MIT-IBM Watson AI Lab', 'IBM T. J. Watson', 'MIT']"
2020,History-Gradient Aided Batch Size Adaptation for Variance Reduced Algorithms,"Kaiyi Ji, Zhe Wang, Bowen Weng, Yi Zhou, Wei  Zhang, Yingbin LIANG",https://icml.cc/Conferences/2020/Schedule?showEvent=6310,"Variance-reduced algorithms, although achieve great theoretical performance, can run slowly in practice due to the periodic gradient estimation with a large batch of data. Batch-size adaptation thus arises as a promising approach to accelerate such algorithms. However, existing schemes either apply prescribed batch-size adaption rule or exploit the information along optimization path via additional backtracking and condition verification steps. In this paper, we propose a novel scheme, which eliminates backtracking line search but still exploits the information along optimization path by adapting the batch size via history stochastic gradients. We further theoretically show that such a scheme substantially reduces the overall complexity for popular variance-reduced algorithms SVRG and SARAH/SPIDER for both conventional nonconvex optimization and reinforcement learning problems. To this end, we develop a new convergence analysis framework to handle the dependence of the batch size on history stochastic gradients. Extensive experiments validate the effectiveness of the proposed batch-size adaptation scheme. 
","['The Ohio State University', 'Ohio State University', 'Ohio State University', 'University of Utah', 'Southern University of Science and Technology', 'The Ohio State University']"
2020,Linear Lower Bounds and Conditioning of Differentiable Games,"Adam Ibrahim, Waïss Azizian, Gauthier Gidel, Ioannis Mitliagkas",https://icml.cc/Conferences/2020/Schedule?showEvent=6357,"Recent successes of game-theoretic formulations in ML have caused a resurgence of research interest in differentiable games. Overwhelmingly, that research focuses on methods and upper bounds on their speed of convergence. In this work, we approach the question of fundamental iteration complexity by providing lower bounds to complement the linear (i.e. geometric) upper bounds observed in the literature on a wide class of problems. We cast saddle-point and min-max problems as 2-player games. We leverage tools from single-objective convex optimisation to propose new linear lower bounds for convex-concave games. Notably, we give a linear lower bound for $n$-player differentiable games, by using the spectral properties of the update operator. We then propose a new definition of the condition number arising from our lower bound analysis. Unlike past definitions, our condition number captures the fact that linear rates are possible in games, even in the absence of strong convexity or strong concavity in the variables.","['Mila, Université de Montréal', 'Ecole Normale Supérieure de Paris', 'MILA', 'MILA, UdeM']"
2020,Fiedler Regularization: Learning Neural Networks with Graph Sparsity,"Edric Tam, David Dunson",https://icml.cc/Conferences/2020/Schedule?showEvent=6349,"We introduce a novel regularization approach for deep learning that incorporates and respects the underlying graphical structure of the neural network. Existing regularization methods often focus on penalizing weights in a global/uniform manner that ignores the connectivity structure of the neural network. We propose to use the Fiedler value of the neural network's underlying graph as a tool for regularization. We provide theoretical support for this approach via spectral graph theory. We show several useful properties of the Fiedler value that makes it suitable for regularization. We provide an approximate, variational approach for faster computation during training. We provide bounds on such approximations. We provide an alternative formulation of this framework in the form of a structurally weighted L1 penalty, thus linking our approach to sparsity induction. We performed experiments on datasets that compare Fiedler regularization with traditional regularization methods such as Dropout and weight decay. Results demonstrate the efficacy of Fiedler regularization.
","['Duke University', 'Duke University']"
2020,On a projective ensemble approach to two sample test for equality of distributions,"Zhimei Li, Yaowu Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6324,"In this work, we propose a robust test for the multivariate two-sample problem through projective ensemble, which is a generalization of the Cramer-von Mises statistic. The proposed test statistic has a simple closed-form expression without any tuning parameters involved, it is easy to implement can be computed in quadratic time. Moreover, our test is insensitive to the dimension and consistent against all fixed alternatives, it does not require the moment assumption and is robust to the presence of outliers. We study the asymptotic behaviors of the test statistic under the null and two kinds of alternative hypotheses. We also suggest a permutation procedure to approximate critical values and employ its consistency. We demonstrate the effectiveness of our test through extensive simulation studies and a real data application.
","['Shanghai University of Finance and Economics', 'Shanghai University of Finance and Economics']"
2020,Active Learning on Attributed Graphs via Graph   Cognizant Logistic Regression and Preemptive Query Generation,"Florence Regol, Soumyasundar Pal, Yingxue Zhang, Mark Coates",https://icml.cc/Conferences/2020/Schedule?showEvent=5809,"Node classification in attributed graphs is an important task in multiple practical settings, but it can often be difficult or expensive to obtain labels. Active learning can improve the achieved classification performance for a given budget on the number of queried labels. The best existing methods are based on graph neural networks, but they often perform poorly unless a sizeable validation set of labelled nodes is available in order to choose good hyperparameters. We propose a novel graph-based active learning algorithm for the task of node classification in attributed graphs; our algorithm uses graph cognizant logistic regression, equivalent to a linearized graph-convolutional neural network
(GCN), for the prediction phase and maximizes the expected error
reduction in the query phase. To reduce the delay experienced by a labeller interacting with the system, we derive a preemptive querying system that calculates a new query during the labelling process, and to address the setting where learning starts with almost no labelled data, we also develop a hybrid algorithm that performs adaptive model averaging of label propagation and linearized GCN inference. We conduct experiments on five public benchmark datasets, demonstrating a significant improvement over state-of-the-art approaches and illustrate the practical value of the method by applying it to a
private microwave link network dataset.
","['McGill University', 'McGill University', 'Huawei Technologies Canada', 'McGill University']"
2020,Convex Calibrated Surrogates for the Multi-Label F-Measure,"Mingyuan Zhang, Harish Guruprasad Ramaswamy, Shivani Agarwal",https://icml.cc/Conferences/2020/Schedule?showEvent=6237,"The F-measure is a widely used performance measure for multi-label classification, where multiple labels can be active in an instance simultaneously (e.g. in image tagging, multiple tags can be active in any image). In particular, the F-measure explicitly balances recall (fraction of active labels predicted to be active) and precision (fraction of labels predicted to be active that are actually so), both of which are important in evaluating the overall performance of a multi-label classifier.  As with most discrete prediction problems, however, directly optimizing the F-measure is computationally hard. In this paper, we explore the question of designing convex surrogate losses that are calibrated for the F-measure -- specifically, that have the property that minimizing the surrogate loss yields (in the limit of sufficient data) a Bayes optimal multi-label classifier for the F-measure. We show that the F-measure for an $s$-label problem, when viewed as a $2^s \times 2^s$ loss matrix, has rank at most $s^2+1$, and apply a result of Ramaswamy et al. (2014) to design a family of convex calibrated surrogates for the F-measure. The resulting surrogate risk minimization algorithms can be viewed as decomposing the multi-label F-measure learning problem into $s^2+1$ binary class probability estimation problems. We also provide a quantitative regret transfer bound for our surrogates, which allows any regret guarantees for the binary problems to be transferred to regret guarantees for the overall F-measure problem, and discuss a connection with the algorithm of Dembczynski et al. (2013). Our experiments confirm our theoretical findings.  ","['University of Pennsylvania', 'IIT Madras', 'University of Pennsylvania']"
2020,Learning the Valuations of a $k$-demand Agent,"Hanrui Zhang, Vincent Conitzer",https://icml.cc/Conferences/2020/Schedule?showEvent=5807,"We study problems where a learner aims to learn the valuations of an agent by observing which goods he buys under varying price vectors.  More specifically, we consider the case of a $k$-demand agent, whose valuation over the goods is additive when receiving up to $k$ goods, but who has no interest in receiving more than $k$ goods.  We settle the query complexity for the active-learning (preference elicitation) version, where the learner chooses the prices to post, by giving a {\em biased binary search} algorithm, generalizing the classical binary search procedure.
We complement our query complexity upper bounds by lower bounds that match up to lower-order terms.  We also study the passive-learning version in which the learner does not control the prices, and instead they are sampled from some distribution.  We show that in the PAC model for passive learning, any {\em empirical risk minimizer} has a sample complexity that is optimal up to a factor of $\widetilde{O}(k)$.","['Duke University', 'Duke']"
2020,Learning Optimal Tree Models under Beam Search,"Jingwei Zhuo, Ziru Xu, Wei Dai, Han Zhu, HAN LI, Jian Xu, Kun Gai",https://icml.cc/Conferences/2020/Schedule?showEvent=6164,"Retrieving relevant targets from an extremely large target set under computational limits is a common challenge for information retrieval and recommendation systems. Tree models, which formulate targets as leaves of a tree with trainable node-wise scorers, have attracted a lot of interests in tackling this challenge due to their logarithmic computational complexity in both training and testing. Tree-based deep models (TDMs) and probabilistic label trees (PLTs) are two representative kinds of them. Though achieving many practical successes, existing tree models suffer from the training-testing discrepancy, where the retrieval performance deterioration caused by beam search in testing is not considered in training. This leads to an intrinsic gap between the most relevant targets and those retrieved by beam search with even the optimally trained node-wise scorers. We take a first step towards understanding and analyzing this problem theoretically, and develop the concept of Bayes optimality under beam search and calibration under beam search as general analyzing tools for this purpose. Moreover, to eliminate the discrepancy, we propose a novel algorithm for learning optimal tree models under beam search. Experiments on both synthetic and real data verify the rationality of our theoretical analysis and demonstrate the superiority of our algorithm compared to state-of-the-art methods.
","['Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group', 'Alibaba Group']"
2020,p-Norm Flow Diffusion for Local Graph Clustering,"Kimon Fountoulakis, Di Wang, Shenghao Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6287,"Local graph clustering and the closely related seed set expansion problem are primitives on graphs that are central to a wide range of analytic and learning tasks such as local clustering, community detection, semi-supervised learning, nodes ranking and feature inference. Prior work on local graph clustering mostly falls into two categories with numerical and combinatorial roots respectively, in this work we draw inspiration from both fields and propose a family of convex optimization formulations based on the idea of diffusion with $p$-norm network flow for $p\in (1,\infty)$. In the context of local clustering, we characterize the optimal solutions for these optimization problems and show their usefulness in finding low conductance cuts around input seed set. In particular, we achieve quadratic approximation of conductance in the case of $p=2$ similar to the Cheeger-type bounds of spectral methods, constant factor approximation when $p\rightarrow\infty$ similar to max-flow based methods, and a smooth transition for general $p$ values in between. Thus, our optimization formulation can be viewed as bridging the numerical and combinatorial approaches, and we can achieve the best of both worlds in terms of speed and noise robustness. We show that the proposed problem can be solved in strongly local running time for $p\ge 2$ and conduct empirical evaluations on both synthetic and real-world graphs to illustrate our approach compares favorably with existing methods.","['University of Waterloo', 'Google Research', 'University of Waterloo']"
2020,Understanding the Impact of Model Incoherence on Convergence of Incremental SGD with Random Reshuffle,"Shaocong Ma, Yi Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=6033,"Although SGD with random reshuffle has been widely-used in machine learning applications, there is a limited understanding of how model characteristics affect the convergence of the algorithm. In this work, we introduce model incoherence to characterize the diversity of model characteristics and study its impact on convergence of SGD with random reshuffle \shaocong{under weak strong convexity}. Specifically, {\em minimizer incoherence} measures the discrepancy between the global minimizers of a sample loss and those of the total loss and affects the convergence error of SGD with random reshuffle.  In particular, we show that the variable sequence generated by SGD with random reshuffle converges to a certain global minimizer of the total loss under full minimizer coherence. The other {\em curvature incoherence} measures the quality of condition numbers of the sample losses and determines the convergence rate of SGD.  With model incoherence, our results show that SGD has a faster convergence rate and smaller convergence error under random reshuffle than those under random sampling, and hence provide justifications to the superior practical performance of SGD with random reshuffle.
","['University of Utah', 'University of Utah']"
2020,Privately detecting changes in unknown distributions,"Rachel Cummings, Sara Krehbiel, Yuliia Lut, Wanrong Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=5854,"The change-point detection problem seeks to identify distributional changes in streams of data. Increasingly, tools for change-point detection are applied in settings where data may be highly sensitive and formal privacy guarantees are required, such as identifying disease outbreaks based on hospital records, or IoT devices detecting activity within a home. Differential privacy has emerged as a powerful technique for enabling data analysis while preventing information leakage about individuals. Much of the prior work on change-point detection---including the only private algorithms for this problem---requires complete knowledge of the pre-change and post-change distributions, which is an unrealistic assumption for many practical applications of interest. This work develops differentially private algorithms for solving the change-point detection problem when the data distributions are unknown. Additionally, the data may be sampled from distributions that change smoothly over time, rather than fixed pre-change and post-change distributions.  We apply our algorithms to detect changes in the linear trends of such data streams.  Finally, we also provide experimental results to empirically validate the performance of our algorithms.
","['Georgia Tech', 'Santa Clara University', 'Georgia Institute of Technology', 'Georgia Institute of Technology']"
2020,On Breaking Deep Generative Model-based Defenses and Beyond,"Yanzhi Chen, Renjie Xie, Zhanxing Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=6115,"Deep neural networks have been proven to be vulnerable to the so-called adversarial attacks. Recently there have been efforts to defend such attacks with deep generative models. These defense often predict by inverting the deep generative models rather than simple feedforward propagation. Such defenses are difficult to attack due to obfuscated gradient. In this work, we develop a new gradient approximation attack to break these defenses. The idea is to view the inversion phase as a dynamical system, through which we extract the gradient w.r.t the input by tracing its recent trajectory. An amortized strategy is further developed to accelerate the attack. Experiments show that our attack breaks state-of-the-art defenses (e.g DefenseGAN, ABS) much more effectively than other attacks. Additionally, our empirical results provide insights for understanding the weaknesses of deep generative model-based defenses.
","['University of Edinburgh', 'Southeast University', 'Peking University']"
2020,Robust Pricing in Dynamic Mechanism Design,"Yuan Deng, Sébastien Lahaie, Vahab Mirrokni",https://icml.cc/Conferences/2020/Schedule?showEvent=6105,"Motivated by the repeated sale of online ads via auctions, optimal pricing in repeated auctions has attracted a large body of research. While dynamic mechanisms offer powerful techniques to improve on both revenue and efficiency by optimizing auctions across different items, their reliance on exact distributional information of buyers' valuations (present and future) limits their use in practice. In this paper, we propose robust dynamic mechanism design. We develop a new framework to design dynamic mechanisms that are robust to both estimation errors in value distributions and strategic behavior. We apply the framework in learning environments, leading to the first policy that achieves provably low regret against the optimal dynamic mechanism in contextual auctions, where the dynamic benchmark has full and accurate distributional information.
","['Duke University', 'Google', 'Google Research']"
2020,Graph Convolutional Network for Recommendation with Low-pass Collaborative Filters,"Wenhui Yu, Zheng Qin",https://icml.cc/Conferences/2020/Schedule?showEvent=5840,"\textbf{G}raph \textbf{C}onvolutional \textbf{N}etwork (\textbf{GCN}) is widely used in graph data learning tasks such as recommendation. However, when facing a large graph, the graph convolution is very computationally expensive thus is simplified in all existing GCNs, yet is seriously impaired due to the oversimplification. To address this gap, we leverage the \textit{original graph convolution} in GCN and propose a \textbf{L}ow-pass \textbf{C}ollaborative \textbf{F}ilter (\textbf{LCF}) to make it applicable to the large graph. LCF is designed to remove the noise caused by exposure and quantization in the observed data, and it also reduces the complexity of graph convolution in an unscathed way. Experiments show that LCF improves the effectiveness and efficiency of graph convolution and our GCN outperforms existing GCNs significantly. Codes are available on \url{https://github.com/Wenhui-Yu/LCFN}.
","['Tsinghua University', 'Tsinghua University']"
2020,Robustness to Programmable String Transformations via Augmented Abstract Training,"Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni",https://icml.cc/Conferences/2020/Schedule?showEvent=5820,"Deep neural networks for natural language processing tasks are vulnerable to adversarial input perturbations. In this paper, we present a versatile language for programmatically specifying string transformations---e.g., insertions, deletions, substitutions, swaps, etc.---that are relevant to the task at hand. We then present an approach to adversarially training models that are robust to such user-defined string transformations. Our approach combines the advantages of search-based techniques for adversarial training with abstraction-based techniques. Specifically, we show how to decompose a set of user-defined string transformations into two component specifications, one that benefits from search and another from abstraction. We use our technique to train models on the AG and SST2 datasets and show that the resulting models are robust to combinations of user-defined transformations mimicking spelling mistakes and other meaning-preserving transformations.
","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin-Madison']"
2020,From ImageNet to Image Classification: Contextualizing Progress on Benchmarks,"Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, Aleksander Madry",https://icml.cc/Conferences/2020/Schedule?showEvent=6803,"Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit  to take such misalignment into account.
","['MIT', 'MIT', 'MIT', 'Massachusetts Institute of Technology', 'MIT']"
2020,Graph Structure of Neural Networks,"Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie",https://icml.cc/Conferences/2020/Schedule?showEvent=5795,"Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) graph structure of neural networks matters; (2) a “sweet spot” of relational graphs lead to neural networks with significantly improved predictive performance; (3) neural network’s performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (4) our findings are consistent across many different tasks and datasets; (5) top architectures can be identified efficiently; (6) well-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.
","['Stanford University', 'Stanford University', 'Facebook AI Research', 'Facebook AI Research']"
2020,Online Learning with Imperfect Hints,"Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit",https://icml.cc/Conferences/2020/Schedule?showEvent=6350,"We consider a variant of the classical online linear optimization problem in which at every step, the online player receives a ``hint'' vector before choosing the action for that round. Rather surprisingly, it was shown that if the hint vector is guaranteed to have a positive correlation with the cost vector, then the online player can achieve a regret of $O(\log T)$, thus significantly improving over the $O(\sqrt{T})$ regret in the general setting. However, the result and analysis require the correlation property at \emph{all} time steps, thus raising the natural question: can we design online learning algorithms that are resilient to bad hints? 

In this paper we develop algorithms and nearly matching lower bounds for online learning with imperfect hints.  Our algorithms are oblivious to the quality of the hints, and the regret bounds interpolate between the always-correlated hints case and the no-hints case.  Our results also generalize, simplify, and improve upon previous results on optimistic regret bounds, which can be viewed as an additive version of hints.","['University of Utah', 'Boston University', 'Google', 'Google Research']"
2020,Spectral Frank-Wolfe Algorithm: Strict Complementarity and Linear Convergence,"Lijun Ding, Yingjie Fei, Qiantong Xu, Chengrun Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6737,"We develop a novel variant of the classical Frank-Wolfe algorithm, which we call spectral Frank-Wolfe, for convex optimization over a spectrahedron. The spectral Frank-Wolfe algorithm has a novel ingredient: it computes a few eigenvectors of the gradient and solves a small-scale subproblem in each iteration. Such a procedure overcomes the slow convergence of the classical Frank-Wolfe algorithm due to ignoring eigenvalue coalescence. We demonstrate that strict complementarity of the optimization problem is key to proving linear convergence of various algorithms, such as the spectral Frank-Wolfe algorithm as well as the projected gradient method and its accelerated version. We showcase that the strict complementarity is equivalent to the eigengap assumption on the gradient at the optimal solution considered in the literature. As a byproduct of this observation, we also develop a generalized block Frank-Wolfe algorithm and prove its linear convergence.
","['Cornell University', 'Cornell University', 'Facebook AI Research', 'Cornell University']"
2020,Semi-Supervised StyleGAN for Disentanglement Learning,"Weili Nie, Tero Karras, Animesh Garg, Shoubhik Debnath, Anjul Patney, Ankit Patel, Anima Anandkumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6305,"Disentanglement learning is crucial for obtaining disentangled representations and controllable generation. Current disentanglement methods face several inherent limitations: difficulty with high-resolution images, primarily focusing on learning disentangled representations, and non-identifiability due to the unsupervised setting. To alleviate these limitations, we design new architectures and loss functions based on StyleGAN (Karras et al., 2019), for semi-supervised high-resolution disentanglement learning. We create two complex high-resolution synthetic datasets for systematic testing. We investigate the impact of limited supervision and find that using only 0.25%~2.5% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We propose new metrics to quantify generator controllability, and observe there may exist a crucial trade-off between disentangled representation learning and controllable generation. We also consider semantic fine-grained image editing to achieve better generalization to unseen images. 
","['Rice University', 'NVIDIA', 'University of Toronto, Vector Institute, Nvidia', 'Nvidia', 'Nvidia', 'Rice University, Baylor College of Medicine', 'Amazon AI & Caltech']"
2020,Goodness-of-Fit Tests for Inhomogeneous Random Graphs,"Soham Dan, Bhaswar B. Bhattacharya",https://icml.cc/Conferences/2020/Schedule?showEvent=5934,"Hypothesis testing of random networks is an emerging area of modern research, especially in the high-dimensional regime, where the  number of samples is smaller or comparable to the size of the graph.  In this paper we consider the goodness-of-fit testing problem for large inhomogeneous random (IER) graphs, where given a (known) reference symmetric matrix $Q \in [0, 1]^{n \times n}$ and $m$ independent samples from an IER graph given by an unknown  symmetric matrix $P \in [0, 1]^{n \times n}$, the goal is to test the hypothesis $P=Q$ versus $||P-Q|| \geq \varepsilon$, where $||\cdot||$ is some specified norm on symmetric matrices. Building on recent related work on two-sample testing for IER graphs, we derive the optimal minimax sample complexities for the goodness-of-fit problem in various natural norms, such as the Frobenius norm and the operator norm. We also propose practical implementations of natural test statistics, using their asymptotic distributions and through the parametric bootstrap. We compare the performances of the different tests in simulations, and show that the proposed  tests outperform the baseline tests across various natural random graphs models.","['University of Pennsylvania', 'University of Pennsylvania']"
2020,Time-aware Large Kernel Convolutions,"Vasileios Lioutas, Yuhong Guo",https://icml.cc/Conferences/2020/Schedule?showEvent=6695,"To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.","['Carleton University', 'Carleton University']"
2020,Efficient Continuous Pareto Exploration in Multi-Task Learning,"Pingchuan Ma, Tao Du, Wojciech Matusik",https://icml.cc/Conferences/2020/Schedule?showEvent=5856,"Tasks in multi-task learning often correlate, conflict, or even compete with each other. As a result, a single solution that is optimal for all tasks rarely exists. Recent papers introduced the concept of Pareto optimality to this field and directly cast multi-task learning as multi-objective optimization problems, but solutions returned by existing methods are typically finite, sparse, and discrete. We present a novel, efficient method that generates locally continuous Pareto sets and Pareto fronts, which opens up the possibility of continuous analysis of Pareto optimal solutions in machine learning problems. We scale up theoretical results in multi-objective optimization to modern machine learning problems by proposing a sample-based sparse linear system, for which standard Hessian-free solvers in machine learning can be applied. We compare our method to the state-of-the-art algorithms and demonstrate its usage of analyzing local Pareto sets on various multi-task classification and regression problems. The experimental results confirm that our algorithm reveals the primary directions in local Pareto sets for trade-off balancing, finds more solutions with different trade-offs efficiently, and scales well to tasks with millions of parameters.
","['MIT', 'MIT', 'MIT']"
2020,Learning Discrete Structured Representations by Adversarially Maximizing Mutual Information,"Karl Stratos, Sam Wiseman",https://icml.cc/Conferences/2020/Schedule?showEvent=6223,"We propose learning discrete structured representations from unlabeled data by maximizing the mutual information between a structured latent variable and a target variable. Calculating mutual information is intractable in this setting. Our key technical contribution is an adversarial objective that can be used to tractably estimate mutual information assuming only the feasibility of cross entropy calculation. We develop a concrete realization of this general formulation with Markov distributions over binary encodings. We report critical and unexpected findings on practical aspects of the objective such as the choice of variational priors. We apply our model on document hashing and show that it outperforms current best baselines based on discrete and vector quantized variational autoencoders. It also yields highly compressed interpretable representations.
","['Rutgers University', 'TTIC']"
2020,On Leveraging Pretrained GANs for Generation with Limited Data,"Miaoyun Zhao, Yulai Cong, Lawrence Carin",https://icml.cc/Conferences/2020/Schedule?showEvent=5942,"Recent work has shown generative adversarial networks (GANs) can generate highly realistic images, that are often indistinguishable (by humans) from real images. Most images so generated are not contained in the training dataset, suggesting potential for augmenting training sets with GAN-generated data. While this scenario is of particular relevance when there are limited data available, there is still the issue of training the GAN itself based on that limited data. To facilitate this, we leverage existing GAN models pretrained on large-scale datasets (like ImageNet) to introduce additional knowledge (which may not exist within the limited data), following the concept of transfer learning. Demonstrated by natural-image generation, we reveal that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be transferred to facilitate generation in a perceptually-distinct target domain with limited training data. To further adapt the transferred filters to the target domain, we propose adaptive filter modulation (AdaFM). An extensive set of experiments is presented to demonstrate the effectiveness of the proposed techniques on generation with limited data.
","['Duke University', 'Duke University', 'Duke']"
2020,Uniform Convergence of Rank-weighted Learning ,"Justin Khim, Liu Leqi, Adarsh Prasad, Pradeep Ravikumar",https://icml.cc/Conferences/2020/Schedule?showEvent=5826,"The decision-theoretic foundations of classical machine learning models have largely focused on estimating model parameters that minimize the expectation of a given loss function. However, as machine learning models are deployed in varied contexts, such as in high-stakes decision-making and societal settings, it is clear that these models are not just evaluated by their average performances. In this work, we study a novel notion of L-Risk based on the classical idea of rank-weighted learning. These L-Risks, induced by rank-dependent weighting functions with bounded variation, is a unification of popular risk measures such as conditional value-at-risk and those defined by cumulative prospect theory. We give uniform convergence bounds of this broad class of risk measures and study their consequences on a logistic regression example.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2020,ACFlow: Flow Models for Arbitrary Conditional Likelihoods,"Yang Li, Shoaib Akbar, Junier Oliva",https://icml.cc/Conferences/2020/Schedule?showEvent=6265,"Understanding the dependencies among features of a dataset is at the core of most unsupervised learning tasks. However, a majority of generative modeling approaches are focused solely on the joint distribution $p(x)$ and utilize models where it is intractable to obtain the conditional distribution of some arbitrary subset of features $x_u$ given the rest of the observed covariates $x_o$: $p(x_u \mid x_o)$. Traditional conditional approaches provide a model for a \emph{fixed} set of covariates conditioned on another \emph{fixed} set of observed covariates. Instead, in this work we develop a model that is capable of yielding \emph{all} conditional distributions $p(x_u \mid x_o)$ (for arbitrary $x_u$) via tractable conditional likelihoods. We propose a novel extension of (change of variables based) flow generative models, arbitrary conditioning flow models (ACFlow). ACFlow can be conditioned on arbitrary subsets of observed covariates, which was previously infeasible. We further extend ACFlow to model the joint distributions $p(x)$ and arbitrary marginal distributions $p(x_u)$. We also apply ACFlow to the imputation of features, and develop a unified platform for both multiple and single imputation by introducing an auxiliary objective that provides a principled single ``best guess'' for flow models. Extensive empirical evaluations show that our model achieves state-of-the-art performance in modeling arbitrary conditional likelihoods in addition to both single and multiple imputation in synthetic and real-world datasets.","['University of North Carolina at Chapel Hill', 'North Carolina state university', 'UNC-Chapel Hill']"
2020,Fractal Gaussian Networks: A sparse random graph model based on Gaussian Multiplicative Chaos,"Subhroshekhar Ghosh, Krishna Balasubramanian, Xiaochuan Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6370,"We propose a novel stochastic network model, called Fractal Gaussian Network (FGN), that embodies well-defined and analytically tractable fractal structures. Such fractal structures have been empirically observed in diverse applications. FGNs interpolate continuously between the popular purely random geometric graphs (a.k.a. the Poisson Boolean network), and random graphs with increasingly fractal behavior. In fact, they form a parametric family of sparse random geometric graphs that are parametrised by a fractality parameter $\nu$ which governs the strength of the fractal structure. FGNs are driven by the latent spatial geometry of Gaussian Multiplicative Chaos (GMC), a canonical model of fractality in its own right. We explore the natural question of detecting the presence of fractality and the problem of parameter estimation based on observed network data. Finally, we explore fractality in community structures by unveiling a natural stochastic block model in the setting of FGNs.
","['National University of Singapore', 'University of California, Davis', 'Université du Luxembourg']"
2020,Adversarial Attacks on Copyright Detection Systems,"Parsa Saadatpanah, Ali Shafahi, Tom Goldstein",https://icml.cc/Conferences/2020/Schedule?showEvent=6072,"It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. As proof of concept, we describe a well-known music identification method and implement this system in the form of a neural net. We then attack this system using simple gradient methods and show that it is easily broken with white-box attacks. By scaling these perturbations up, we can create transfer attacks on industrial systems, such as the AudioTag copyright detector and YouTube's Content ID system, using perturbations that are audible but significantly smaller than a random baseline. Our goal is to raise awareness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.
","['University of Maryland', 'University of Maryland', 'University of Maryland']"
2020,Graph Optimal Transport for Cross-Domain Alignment,"Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, Jingjing Liu",https://icml.cc/Conferences/2020/Schedule?showEvent=5910,"Cross-domain alignment between two sets of entities (e.g., objects in an image, words in a sentence) is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, where no training signals are provided to explicitly encourage alignment. Plus, the learned attention matrices are often dense and difficult to interpret. We propose Graph Optimal Transport (GOT), a principled framework that builds upon recent advances in Optimal Transport (OT). In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities as a dynamically-constructed graph. Two types of OT distances are considered: (i) Wasserstein distance (WD) for node (entity) matching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure) matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer. 
The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization. 
","['Duke University', 'Microsoft', 'Microsoft', 'Microsoft', 'Duke', 'Microsoft']"
2020,Online Learned Continual Compression with Adaptive Quantization Modules,"Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Joelle Pineau",https://icml.cc/Conferences/2020/Schedule?showEvent=6338,"We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoder in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning.  This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression.  Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks with images, LiDAR, and reinforcement learning agents.
","['McGIll', 'Mila', 'MILA', 'McGill University / Facebook']"
2020,Variational Inference for Sequential Data with Future Likelihood Estimates,"Geon-Hyeong Kim, Youngsoo Jang, Hongseok Yang, Kee-Eung Kim",https://icml.cc/Conferences/2020/Schedule?showEvent=6402,"The recent development of flexible and scalable variational inference algorithms has popularized the use of deep probabilistic models in a wide range of applications. However, learning and reasoning about high-dimensional models with nondifferentiable densities are still a challenge. For such a model, inference algorithms struggle to estimate the gradients of variational objectives accurately, due to high variance in their estimates. To tackle this challenge, we present a novel variational inference algorithm for sequential data, which performs well even when the density from the model is not differentiable, for instance, due
to the use of discrete random variables. The key feature of our algorithm is that it estimates future likelihoods at all time steps. The estimated future likelihoods form the core of our new low-variance gradient estimator. We formally analyze our gradient estimator from the perspective of variational objective, and show the effectiveness of our algorithm with synthetic and real datasets.
","['KAIST', 'KAIST', '', 'KAIST']"
2020,When Demands Evolve Larger and Noisier: Learning and Earning in a Growing Environment,"Feng Zhu, Zeyu Zheng",https://icml.cc/Conferences/2020/Schedule?showEvent=6361,"We consider a single-product dynamic pricing problem under a specific non-stationary setting, where the underlying demand process grows over time in expectation and also possibly in the level of random fluctuation. The decision maker sequentially sets price in each time period and learns the unknown demand model, with the goal of maximizing expected cumulative revenue over a time horizon $T$. We prove matching upper and lower bounds on regret and provide near-optimal pricing policies, showing how the growth rate of random fluctuation over time affects the best achievable regret order and the near-optimal policy design. In the analysis, we show that whether the seller knows the length of time horizon $T$ in advance or not surprisingly render different optimal regret orders. We then extend the demand model such that the optimal price may vary with time and present a novel and near-optimal policy for the extended model. Finally, we consider an analogous non-stationary setting in the canonical multi-armed bandit problem, and points out that knowing or not knowing the length of time horizon $T$ render the same optimal regret order, in contrast to the non-stationary dynamic pricing problem.","['Peking University', 'University of California, Berkeley']"
2020,Converging to Team-Maxmin Equilibria in Zero-Sum Multiplayer Games,"Youzhi Zhang, Bo An",https://icml.cc/Conferences/2020/Schedule?showEvent=6003,"Efficiently computing equilibria for multiplayer games is still an open challenge in computational game theory. This paper focuses on computing Team-Maxmin Equilibria (TMEs), which is an important solution concept for zero-sum multiplayer
games where players in a team having the same utility function play against an adversary independently. Existing algorithms are inefficient to compute TMEs in large games, especially when the strategy space is too large to be represented due to limited memory. In two-player games, the Incremental Strategy Generation (ISG) algorithm is an efficient approach to avoid enumerating all pure strategies. However, the study of ISG for computing TMEs is completely unexplored. To fill this gap, we first study the properties of ISG for multiplayer games, showing that ISG converges to a Nash equilibrium (NE) but may not converge to a TME. Second, we design an ISG variant for TMEs (ISGT) by exploiting that a TME is an NE maximizing the team’s utility and show that ISGT converges to a TME and the impossibility of relaxing conditions in ISGT. Third, to further improve the scalability, we design an ISGT variant (CISGT) by using the strategy space for computing an equilibrium that is close to a TME but is easier to be computed as the initial strategy space of ISGT. Finally, extensive experimental results show that CISGT is orders of magnitude faster than ISGT and the state-of-the-art algorithm to compute TMEs in large games.
","['Nanyang Technological University', 'Nanyang Technological University']"
2020,Normalized Loss Functions for Deep Learning with Noisy Labels,"Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, James Bailey",https://icml.cc/Conferences/2020/Schedule?showEvent=5998,"Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: \emph{any loss can be made robust to noisy labels}. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of {\em underfitting}. To address this, we propose a framework to build robust loss functions called \emph{Active Passive Loss} (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60\% or 80\% incorrect labels.
","['Deakin University', 'University of Melbourne', 'Shanghai Jiao Tong University', 'University of Melbourne', 'University of Melbourne', 'The University of Melbourne']"
2020,Educating Text Autoencoders: Latent Representation Guidance via Denoising,"Tianxiao Shen, Jonas Mueller, Regina Barzilay, Tommi Jaakkola",https://icml.cc/Conferences/2020/Schedule?showEvent=6616,"Generative autoencoders offer a promising approach for controllable text generation by leveraging their learned sentence representations.
However, current models struggle to maintain coherent latent spaces required to
perform meaningful text manipulations via latent vector operations.
Specifically, we demonstrate by example that neural encoders do not necessarily map similar sentences to nearby latent vectors. A theoretical explanation for this phenomenon establishes that high-capacity autoencoders can learn an arbitrary mapping between sequences and associated latent representations.
To remedy this issue, we augment adversarial autoencoders with a denoising objective where original sentences are reconstructed from perturbed versions (referred to as DAAE).
We prove that this simple modification guides the latent space geometry of the resulting model by encouraging the encoder to map similar texts to similar latent representations.
In empirical comparisons with various types of autoencoders, our model provides the best trade-off between generation quality and reconstruction capacity.
Moreover, the improved geometry of the DAAE latent space enables \textit{zero-shot} text style transfer via simple latent vector arithmetic.
","['MIT', 'Amazon Web Services', 'MIT CSAIL', 'MIT']"
2020,A Nearly-Linear Time Algorithm for Exact Community Recovery in Stochastic Block Model,"Peng Wang, Zirui Zhou, Anthony Man-Cho So",https://icml.cc/Conferences/2020/Schedule?showEvent=6810,"Learning community structures in graphs that are randomly generated by stochastic block models (SBMs) has received much attention lately. In this paper, we focus on the problem of exactly recovering the communities in a binary symmetric SBM, where a graph of $n$ vertices is partitioned into two equal-sized communities and the vertices are connected with probability $p = \alpha\log(n)/n$ within communities and $q = \beta\log(n)/n$ across communities for some $\alpha>\beta>0$. We propose a two-stage iterative algorithm for solving this problem, which employs the power method with a random starting point in the first-stage and turns to a generalized power method that can identify the communities in a finite number of iterations in the second-stage. It is shown that for any fixed $\alpha$ and $\beta$ such that $\sqrt{\alpha} - \sqrt{\beta} > \sqrt{2}$, which is known to be the information-theoretical limit for exact recovery, the proposed algorithm exactly identifies the underlying communities in $\tilde{O}(n)$ running time with probability tending to one as $n\rightarrow\infty$. As far as we know, this is the first algorithm with nearly-linear running time that achieves exact recovery at the information-theoretical limit. We also present numerical results of the proposed algorithm to support and complement our theoretical development.","['The Chinese University of Hong Kong', 'Huawei Technologies Canada', 'The Chinese University of Hong Kong']"
2020,Doubly robust off-policy evaluation with shrinkage ,"Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, Miroslav Dudik",https://icml.cc/Conferences/2020/Schedule?showEvent=6423,"We propose a new framework for designing estimators for off-policy evaluation in contextual bandits. Our approach is based on the asymptotically optimal doubly robust estimator, but we shrink the importance weights to minimize a bound on the mean squared error, which results in a better bias-variance tradeoff in finite samples. We use this optimization-based framework to obtain three estimators: (a) a weight-clipping estimator, (b) a new weight-shrinkage estimator, and (c) the first shrinkage-based estimator for combinatorial action sets. Extensive experiments in both standard and combinatorial bandit benchmark problems show that our estimators are highly adaptive and typically outperform state-of-the-art methods.
","['Cornell University', 'Stanford University', 'Microsoft Research', 'Microsoft Research']"
2020,Variance Reduction and Quasi-Newton for Particle-Based Variational Inference,"Michael Zhu, Chang Liu, Jun Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=6650,"Particle-based Variational Inference methods (ParVIs), like Stein Variational Gradient Descent, are nonparametric variational inference methods that optimize a set of particles to best approximate a target distribution. ParVIs have been proposed as efficient approximate inference algorithms and as potential alternatives to MCMC methods. However, to our knowledge, the quality of the posterior approximation of particles from ParVIs has not been examined before for large-scale Bayesian inference problems. We conduct this analysis and evaluate the sample quality of particles produced by ParVIs, and we find that existing ParVI approaches using stochastic gradients converge insufficiently fast under sample quality metrics. We propose a novel variance reduction and quasi-Newton preconditioning framework for ParVIs, by leveraging the Riemannian structure of the Wasserstein space and advanced Riemannian optimization algorithms. Experimental results demonstrate the accelerated convergence of variance reduction and quasi-Newton methods for ParVIs for accurate posterior inference in large-scale and ill-conditioned problems.
","['Stanford University', 'Microsoft Research', 'Tsinghua University']"
2020,Understanding and Stabilizing GANs' Training Dynamics Using Control Theory,"Kun Xu, Chongxuan Li, Jun Zhu, Bo Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6077,"Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable.
There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods.
To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the frequency domain and provide simple yet effective methods to stabilize GAN's training.
We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, which can be implemented as an L2 regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.
","['Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University']"
2020,Optimal Differential Privacy Composition for Exponential Mechanisms,"Jinshuo Dong, David Durfee, Ryan Rogers",https://icml.cc/Conferences/2020/Schedule?showEvent=6687,"Composition is one of the most important properties of differential privacy (DP), as it allows algorithm designers to build complex private algorithms from DP primitives. We consider precise composition bounds of the overall privacy loss for exponential mechanisms, one of the fundamental classes of mechanisms in DP. Exponential mechanism has also become a fundamental building block in private machine learning, e.g. private PCA and hyper-parameter selection. We give explicit formulations of the optimal privacy loss for both the adaptive and non-adaptive composition of exponential mechanism. For the non-adaptive setting in which each mechanism has the same privacy parameter, we give an efficiently computable formulation of the optimal privacy loss. In the adaptive case, we derive a recursive formula and an efficiently computable upper bound. These precise understandings about the problem lead to a 40\% saving of the privacy budget in a practical application. Furthermore, the algorithm-specific analysis shows a difference in privacy parameters of adaptive and non-adaptive composition, which was widely believed to not exist based on the evidence from general analysis.
","['University of Pennsylvania', 'Georgia Tech', 'LinkedIn']"
2020,Model-Based Reinforcement Learning with Value-Targeted Regression,"Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, Lin Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6705,"This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model P belongs to a known family of models, a special case of which is when models in the model class take the form of linear mixtures. We propose a model based RL algorithm that is based on the optimism principle: In each episode, the set of models that are `consistent' with the data collected is constructed. The criterion of consistency is based on the total squared error that the model incurs on the task of predicting state values as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, takes the form O(d (H^3 T)^(1/2) ), where H, T and d are the horizon, the total number of steps and the dimension of the parameter vector, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound Omega( (HdT)^(1/2) ). For a general model family, the regret bound is derived based on the Eluder dimension.
","['University of Alberta', 'Peking University', 'DeepMind/University of Alberta', 'Princeton University', 'UCLA']"
2020,Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning,"Dipendra Kumar Misra, Mikael Henaff, Akshay Krishnamurthy, John Langford",https://icml.cc/Conferences/2020/Schedule?showEvent=6535,"We present an algorithm, HOMER, for exploration and reinforcement learning in rich observation environments that are summarizable by an unknown latent state space. The algorithm interleaves representation learning to identify a new notion of kinematic state abstraction with strategic exploration to reach new states using the learned abstraction. The algorithm provably explores the environment with sample complexity scaling polynomially in the number of latent states and the time horizon, and, crucially, with no dependence on the size of the observation space, which could be infinitely large. This exploration guarantee further enables sample-efficient global policy optimization for any reward function. On the computational side, we  show that the algorithm can be implemented efficiently whenever certain supervised learning problems are tractable.  Empirically, we evaluate HOMER on a challenging exploration problem, where we show that the algorithm is more sample efficient than standard reinforcement learning baselines.
","['Microsoft Research', 'Microsoft', 'Microsoft Research', 'Microsoft Research']"
2020,No-Regret and Incentive-Compatible Online Learning,"Rupert Freeman, David Pennock, Chara Podimata, Jennifer Wortman Vaughan",https://icml.cc/Conferences/2020/Schedule?showEvent=6530,"We study online learning settings in which experts act strategically to maximize their influence on the learning algorithm's predictions by potentially misreporting their beliefs about a sequence of binary events. Our goal is twofold. First, we want the learning algorithm to be no-regret with respect to the best fixed expert in hindsight. Second, we want incentive compatibility, a guarantee that each expert's best strategy is to report his true beliefs about the realization of each event. To achieve this goal, we build on the literature on wagering mechanisms, a type of multi-agent scoring rule. We provide algorithms that achieve no regret and incentive compatibility for myopic experts for both the full and partial information settings. In experiments on datasets from FiveThirtyEight, our algorithms have regret comparable to classic no-regret algorithms, which are not incentive-compatible. Finally, we identify an incentive-compatible algorithm for forward-looking strategic agents that exhibits diminishing regret in practice.
","['Microsoft Research', 'Rutgers University', 'Harvard University', 'Microsoft Research']"
2020,R2-B2: Recursive Reasoning-Based Bayesian Optimization for No-Regret Learning in Games,"Zhongxiang Dai, Yizhou Chen, Bryan Kian Hsiang Low, Patrick Jaillet , Teck-Hua Ho",https://icml.cc/Conferences/2020/Schedule?showEvent=6746,"This paper presents a recursive reasoning formalism of Bayesian optimization (BO) to model the reasoning process in the interactions between boundedly rational, self-interested agents with unknown, complex, and costly-to-evaluate payoff functions in repeated games, which we call Recursive Reasoning-Based BO (R2-B2). Our R2-B2 algorithm is general in that it does not constrain the relationship among the payoff functions of different agents and can thus be applied to various types of games such as constant-sum, general-sum, and common-payoff games. We prove that by reasoning at level 2 or more and at one level higher than the other agents, our R2-B2 agent can achieve faster asymptotic convergence to no regret than that without utilizing recursive reasoning. We also propose a computationally cheaper variant of R2-B2 called R2-B2-Lite at the expense of a weaker convergence guarantee. The performance and generality of our R2-B2 algorithm are empirically demonstrated using synthetic games, adversarial machine learning, and multi-agent reinforcement learning.
","['National University of Singapore', 'National University of Singapore', 'National University of Singapore', 'MIT', 'National University of Singapore']"
2020,Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label Rates,"Jeff Calder, Brendan Cook, Matthew Thorpe, Dejan Slepcev",https://icml.cc/Conferences/2020/Schedule?showEvent=6566,"We propose a new framework, called Poisson learning, for graph based semi-supervised learning at very low label rates. Poisson learning is motivated by the need to address the degeneracy of Laplacian semi-supervised learning in this regime. The method replaces the assignment of label values at training points with the placement of sources and sinks, and solves the resulting Poisson equation on the graph. The outcomes are  provably more stable and informative than those of Laplacian learning. Poisson learning is efficient and simple to implement, and we present numerical experiments showing the method is superior to other recent approaches to semi-supervised learning at low label rates on MNIST, FashionMNIST, and Cifar-10. We also propose a graph-cut enhancement of Poisson learning, called Poisson MBO, that gives higher accuracy and can incorporate prior knowledge of relative class sizes.
","['University of Minnesota', 'University of Minnesota', 'University of Manchester', 'Carnegie Mellon University']"
2020,More Data Can Expand The Generalization Gap Between Adversarially Robust and Standard Models,"Lin Chen, Yifei Min, Mingrui Zhang, Amin Karbasi",https://icml.cc/Conferences/2020/Schedule?showEvent=5943,"Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under $\ell_\infty$ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.","['Yale University', 'Yale University', 'Yale University', 'Yale']"
2020,Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation,"Xiang Jiang, Qicheng Lao, Stan Matwin, Mohammad Havaei",https://icml.cc/Conferences/2020/Schedule?showEvent=6585,"We present an approach for unsupervised domain adaptation—with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift—from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels. Instead, we present a sampling-based implicit alignment approach, where the sample selection is implicitly guided by the pseudo-labels. Theoretical analysis reveals the existence of a domain-discriminator shortcut in misaligned classes, which is addressed by the proposed approach to facilitate domain-adversarial learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach, especially in the presence of within-domain class imbalance and between-domain class distribution shift.
","['Imagia, Dalhousie University', 'Mila; Imagia', 'Dalhousie University', 'Imagia']"
2020, Recht-Re Noncommutative Arithmetic-Geometric Mean Conjecture is False,"Zehua Lai, Lek-Heng Lim",https://icml.cc/Conferences/2020/Schedule?showEvent=6623,"Stochastic optimization algorithms have become indispensable in modern machine learning. An unresolved foundational question in this area is the difference between with-replacement sampling and without-replacement sampling — does the latter have superior convergence rate compared to the former? A groundbreaking result of Recht and Re reduces the problem to a noncommutative analogue of the arithmetic-geometric mean inequality where n positive numbers are replaced by n positive definite matrices. If this inequality holds for all n, then without-replacement sampling indeed outperforms with-replacement sampling in some important optimization problems. The conjectured Recht–Re inequality has so far only been established for n = 2 and a special case of n = 3. We will show that the Recht–Re conjecture is false for general n. Our approach relies on the noncommutative Positivstellensatz, which allows us to reduce the conjectured inequality to a semidefinite program and the validity of the conjecture to certain bounds for the optimum values, which we show are false as soon as n = 5.
","['University of Chicago', 'University of Chicago']"
2020,The Intrinsic Robustness of Stochastic Bandits to Strategic Manipulation,"Zhe Feng, David Parkes, Haifeng Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=6347,"Motivated by economic applications such as recommender systems, we study the behavior of  stochastic bandits algorithms under \emph{strategic behavior} conducted by rational actors, i.e.,  the arms. Each arm is a \emph{self-interested} strategic player who can modify its own reward whenever pulled, subject to a cross-period budget constraint, in order to maximize its own expected number of times of being pulled. We analyze the robustness of three popular  bandit algorithms: UCB, $\varepsilon$-Greedy, and Thompson Sampling. We prove that all three algorithms achieve a regret upper bound $\mathcal{O}(\max \{ B, K\ln T\})$  where $B$ is the total budget across arms, $K$ is the total number of arms and $T$ is the running time of the algorithms.  This regret guarantee holds for \emph{arbitrary  adaptive} manipulation strategy of  arms.  Our second set of main results shows that this regret bound is \emph{tight}--- in fact, for UCB, it is tight even when we restrict the arms' manipulation strategies to form a \emph{Nash equilibrium}. We do so by characterizing the Nash equilibrium of the game induced by arms' strategic manipulations and show a regret lower bound of $\Omega(\max \{ B, K\ln T\})$ at the equilibrium.   ","['Harvard University', 'Harvard University', 'University of Virginia']"
2020,Lorentz Group Equivariant Neural Network for Particle Physics,"Alexander Bogatskiy, Brandon Anderson, Jan T Offermann, Marwah Roussi, David Miller, Risi Kondor",https://icml.cc/Conferences/2020/Schedule?showEvent=5843,"We present a neural network architecture that is fully equivariant with respect to transformations under the Lorentz group, a fundamental symmetry of space and time in physics. The architecture is based on the theory of the finite-dimensional representations of the Lorentz group and the equivariant nonlinearity involves the tensor product. For classification tasks in particle physics, we show that such an equivariant architecture leads to drastically simpler models that have relatively few learnable parameters and are much more physically interpretable than leading approaches that use CNNs and point cloud approaches. The performance of the network is tested on a public classification dataset [https://zenodo.org/record/2603256] for tagging top quark decays given energy-momenta of jet constituents produced in proton-proton collisions.
","['University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago', 'University of Chicago', 'The University of Chicago']"
2020,Provable Smoothness Guarantees for Black-Box Variational Inference,Justin Domke,https://icml.cc/Conferences/2020/Schedule?showEvent=5778,"Black-box variational inference tries to approximate a complex target distribution through a gradient-based optimization of the parameters of a simpler distribution. Provable convergence guarantees require structural properties of the objective. This paper shows that for location-scale family approximations, if the target is M-Lipschitz smooth, then so is the “energy” part of the variational objective. The key proof idea is to describe gradients in a certain inner-product space, thus permitting the use of Bessel’s inequality. This result gives bounds on the location of the optimal parameters, and is a key ingredient for convergence guarantees.
","['University of Massachusetts, Amherst']"
2020,Adaptive Reward-Poisoning Attacks against Reinforcement Learning,"Xuezhou Zhang, Yuzhe Ma, Adish Singla, Jerry Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=6555,"In reward-poisoning attacks against reinforcement learning (RL), an attacker can perturb the environment reward $r_t$ into $r_t+\delta_t$ at each step, with the goal of forcing the RL agent to learn a nefarious policy. 
We categorize such attacks by the infinity-norm constraint on $\delta_t$: We provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe; we provide a corresponding upper threshold above which the attack is feasible. 
Feasible attacks can be further categorized as non-adaptive where $\delta_t$ depends only on $(s_t,a_t, s_{t+1})$, or adaptive where $\delta_t$ depends further on the RL agent's learning process at time $t$. Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$, whereas non-adaptive attacks require exponential steps.
We provide a constructive proof that a Fast Adaptive Attack strategy achieves the polynomial rate. Finally, we show that empirically an attacker can find effective reward-poisoning attacks using state-of-the-art deep RL techniques.","['UW-Madison', 'Univ. of Wisconsin-Madison', 'Max Planck Institute (MPI-SWS)', 'University of Wisconsin-Madison']"
2020,Goal-Aware Prediction: Learning to Model What Matters,"Suraj Nair, Silvio Savarese, Chelsea Finn",https://icml.cc/Conferences/2020/Schedule?showEvent=6271,"Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning.
","['Stanford University', 'Stanford University', 'Stanford']"
2020,Deep Molecular Programming: A Natural Implementation of Binary-Weight ReLU Neural Networks,"Marko Vasic, Cameron Chalk, Sarfraz Khurshid, David Soloveichik",https://icml.cc/Conferences/2020/Schedule?showEvent=6738,"Embedding computation in molecular contexts incompatible with traditional electronics is expected to have wide ranging impact in synthetic biology, medicine, nanofabrication and other fields.  A key remaining challenge lies in developing programming paradigms for molecular computation that are well-aligned with the underlying chemical hardware and do not attempt to shoehorn ill-fitting electronics paradigms.  We discover a surprisingly tight connection between a popular class of neural networks (binary-weight ReLU aka BinaryConnect) and a class of coupled chemical reactions that are absolutely robust to reaction rates.  The robustness of rate-independent chemical computation makes it a promising target for bioengineering implementation.  We show how a BinaryConnect neural network trained in silico using well-founded deep learning optimization techniques, can be compiled to an equivalent chemical reaction network, providing a novel molecular programming paradigm.  We illustrate such translation on the paradigmatic IRIS and MNIST datasets.  Toward intended applications of chemical computation, we further use our method to generate a chemical reaction network that can discriminate between different virus types based on gene expression levels.  Our work sets the stage for rich knowledge transfer between neural network and molecular programming communities.
","['The University of Texas at Austin', 'University of Texas at Austin', '', 'The University of Texas at Austin']"
2020,ConQUR: Mitigating Delusional Bias in Deep Q-Learning ,"DiJia Su, Jayden Ooi, Tyler Lu, Dale Schuurmans, Craig Boutilier",https://icml.cc/Conferences/2020/Schedule?showEvent=5980,"Delusional bias is a fundamental source of error in approximate Q-learning. To date, the only techniques that explicitly address delusion require comprehensive search using tabular value estimates. In this paper, we develop efficient methods to mitigate delusional bias by training Q-approximators with labels that are ""consistent"" with the underlying greedy policy class. We introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class. We also propose a search framework that allows multiple Q-approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-learning in a variety of Atari games, sometimes dramatically.
","['Princeton University', 'Google', 'Google', 'Google / University of Alberta', 'Google']"
2020,On Differentially Private Stochastic Convex Optimization  with Heavy-tailed Data,"Di Wang, Hanshen Xiao, Srinivas Devadas, Jinhui Xu",https://icml.cc/Conferences/2020/Schedule?showEvent=5948,"In this paper, we consider the problem of designing 
Differentially Private (DP) algorithms for Stochastic Convex Optimization (SCO) on heavy-tailed data. The irregularity of such data violates some key 
assumptions used in almost all existing DP-SCO and DP-ERM methods, resulting in failure to provide the DP guarantees. To better understand this type of challenges, we provide in this paper a comprehensive study of DP-SCO under various settings. First, we consider the case where the loss function is strongly convex and smooth. For this case, we propose a method based on the sample-and-aggregate framework, which has an excess population risk of 
$\tilde{O}(\frac{d^3}{n\epsilon^4})$ (after omitting other factors), where $n$ is the sample size and $d$ is the dimensionality of the data. Then, we show that with some additional assumptions on the loss functions, it is possible to reduce the 
\textit{expected} excess population risk to $\tilde{O}(\frac{ d^2}{ n\epsilon^2 })$. To lift these additional conditions, we also  
 provide a gradient smoothing and trimming based scheme to achieve  excess population risks of   
$\tilde{O}(\frac{ d^2}{n\epsilon^2})$ and $\tilde{O}(\frac{d^\frac{2}{3}}{(n\epsilon^2)^\frac{1}{3}})$ for strongly convex and general convex loss functions, respectively, \textit{with high probability}.
Experiments on both synthetic and real-world datasets suggest that our algorithms can effectively deal with the challenges caused by data irregularity.
","['State University of New York at Buffalo', 'MIT CSAIL', 'MIT', 'SUNY Buffalo']"
2020,Generalization via Derandomization,"Jeffrey Negrea, Gintare Karolina Dziugaite, Daniel Roy",https://icml.cc/Conferences/2020/Schedule?showEvent=6617,"We propose to study the generalization error of a learned predictor h^ in terms of that of a surrogate (potentially randomized) classifier that is coupled to h^ and designed to trade empirical risk for control of generalization error. In the case where h^ interpolates the data, it is interesting to consider theoretical surrogate classifiers that are partially derandomized or rerandomized, e.g., fit to the training data but with modified label noise. We show that replacing h^ by its conditional distribution with respect to an arbitrary sigma-field is a viable method to derandomize. We give an example, inspired by the work of Nagarajan and Kolter (2019), where the learned classifier h^ interpolates the training data with high probability, has small risk, and, yet, does not belong to a nonrandom class with a tight uniform bound on two-sided generalization error. At the same time, we bound the risk of h^ in terms of a surrogate that is constructed by conditioning and shown to belong to a nonrandom class with uniformly small generalization error. 
","['University of Toronto', 'Element AI', 'University of Toronto; Vector Institute']"
2020,DRWR: A Differentiable Renderer without Rendering for Unsupervised 3D Structure Learning from Silhouette Images,"Zhizhong Han, Chao Chen, Yushen Liu, Matthias Zwicker",https://icml.cc/Conferences/2020/Schedule?showEvent=5786,"Differentiable renderers have been used successfully for unsupervised 3D structure learning from 2D images because they can bridge the gap between 3D and 2D. To optimize 3D shape parameters, current renderers rely on pixel-wise losses between rendered images of 3D reconstructions and ground truth images from corresponding viewpoints. Hence they require interpolation of the recovered 3D structure at each pixel, visibility handling, and optionally evaluating a shading model. In contrast, here we propose a Differentiable Renderer Without Rendering (DRWR) that omits these steps. DRWR only relies on a simple but effective loss that evaluates how well the projections of reconstructed 3D point clouds cover the ground truth object silhouette. Specifically, DRWR employs a smooth silhouette loss to pull the projection of each individual 3D point inside the object silhouette, and a structure-aware repulsion loss to push each pair of projections that fall inside the silhouette far away from each other. Although we omit surface interpolation, visibility handling, and shading, our results demonstrate that DRWR achieves state-of-the-art accuracies under widely used benchmarks, outperforming previous methods both qualitatively and quantitatively. In addition, our training times are significantly lower due to the simplicity of DRWR.
","['University of Maryland, College Park', 'Tsinghua University', 'Tsinghua University', 'University of Maryland']"
2020,Computational and Statistical Tradeoffs in Inferring Combinatorial Structures of Ising Model,"Ying Jin, Zhaoran Wang, Junwei Lu",https://icml.cc/Conferences/2020/Schedule?showEvent=5798,"We study the computational and statistical tradeoffs in inferring combinatorial structures of high dimensional simple zero-field ferromagnetic Ising model. Under the framework of oracle computational model where an algorithm interacts with an oracle that discourses a randomized version of truth, we characterize the computational lower bounds of learning combinatorial structures in polynomial time, under which no algorithms within polynomial-time can distinguish between graphs with and without certain structures. This hardness of learning with limited computational budget is shown to be characterized by a novel quantity called vertex overlap ratio. Such quantity is universally valid for many specific graph structures including cliques and nearest neighbors. On the other side, we attain the optimal rates for testing these structures against empty graph by proposing the quadratic testing statistics to match the lower bounds. We also investigate the relationship between computational bounds and information-theoretic bounds for such problems, and found gaps between the two boundaries in inferring some particular structures, especially for those with dense edges.
","['Stanford University', 'Northwestern', '']"
2020,"Go Wide, Then Narrow: Efficient Training of Deep Thin Networks","Denny Zhou, Mao Ye, Chen Chen, Tianjian Meng, Mingxing Tan, Xiaodan Song, Quoc Le, Qiang Liu, Dale Schuurmans",https://icml.cc/Conferences/2020/Schedule?showEvent=6752,"For deploying a deep learning model into production, it needs to be both accurate and compact to meet the latency and memory constraints. This usually results in a network that is deep (to ensure performance) and yet  thin (to improve computational efficiency). In this paper, we propose an efficient method to train a deep thin network with a theoretic guarantee. Our method is motivated by model compression. It consists of three stages. In the first stage, we sufficiently widen the deep thin network and train it until convergence. In the second stage, we use this well-trained deep wide network to warm up (or initialize) the original deep thin network. This is achieved by letting the thin network imitate the immediate outputs of the wide network from layer to layer.  In the last stage, we further fine tune this well initialized deep thin network. The theoretical guarantee is established by using mean field analysis. It shows the advantage of layerwise imitation over traditional training deep thin networks from scratch by backpropagation.  We also conduct large-scale empirical experiments to validate our approach. By training with our method, ResNet50 can outperform  ResNet101, and BERTBASE can be comparable with BERTLARGE,  where both the latter models are trained via the standard training procedures as in the literature. 
","['Google Brain', 'UT Austin', 'Google', 'Google Brain', 'Google Brain', 'Google Brain', 'Google Brain', 'UT Austin', 'Google / University of Alberta']"
2020,Perceptual Generative Autoencoders,"Zijun Zhang, Ruixiang ZHANG, Zongpeng Li, Yoshua Bengio, Liam Paull",https://icml.cc/Conferences/2020/Schedule?showEvent=5921,"Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimension of data can be much lower than the ambient dimension. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. Specifically, we enforce the consistency in both the data space and the latent space with theoretically justified data and latent reconstruction losses. The resulting generative model, which we call a perceptual generative autoencoder (PGA), is then trained with a maximum likelihood or variational autoencoder (VAE) objective. With maximum likelihood, PGAs generalize the idea of reversible generative models to unrestricted neural network architectures and arbitrary number of latent dimensions. When combined with VAEs, PGAs substantially improve over the baseline VAEs in terms of sample quality. Compared to other autoencoder-based generative models using simple priors, PGAs achieve state-of-the-art FID scores on CIFAR-10 and CelebA.
","['University of Calgary', 'Mila/UdeM', 'Wuhan University', 'Montreal Institute for Learning Algorithms', 'Université de Montréal']"
2020,"Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound","Lin Yang, Mengdi Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6224,"Exploration in reinforcement learning (RL) suffers from the curse of dimensionality when the state-action space is large. A common practice is to parameterize the high-dimensional value and policy functions using given features. However existing methods either have no theoretical guarantee or suffer a regret that is exponential in the planning horizon $H$.In this paper, we propose an online RL algorithm, namely the MatrixRL, that leverages ideas from linear bandit  to learn a low-dimensional representation of the probability  transition model while carefully balancing the exploitation-exploration tradeoff. We show that MatrixRL achieves a regret bound ${O}\big(H^2d\log T\sqrt{T}\big)$ where $d$ is the number of features, independent with the number of state-action pairs. MatrixRL has an equivalent kernelized version, which is able to work with an arbitrary kernel Hilbert space without using explicit features. In this case, the kernelized MatrixRL satisfies a regret bound ${O}\big(H^2\wt{d}\log T\sqrt{T}\big)$, where $\wt{d}$ is the effective dimension of the kernel space.","['UCLA', 'Princeton University']"
2020,On hyperparameter tuning in general clustering problemsm,"Xinjie Fan, Yuguang Yue, Purnamrita Sarkar, Y. X. Rachel Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=5989,"Tuning hyperparameters for unsupervised learning problems is difficult in general due to the lack of ground truth for validation. However, the success of most clustering methods depends heavily on the correct choice of the involved hyperparameters. Take for example the Lagrange multipliers of penalty terms in semidefinite programming (SDP) relaxations of community detection in networks, or the bandwidth parameter needed in the Gaussian kernel used to construct similarity matrices for spectral clustering. Despite the popularity of these clustering algorithms, there are not many provable methods for tuning these hyperparameters. In this paper, we provide an overarching framework with provable guarantees for tuning hyperparameters in the above class of problems under two different models. Our framework can be augmented with a cross validation procedure to do model selection as well. In a variety of simulation and real data experiments, we show that our framework outperforms other widely used tuning procedures in a broad range of parameter settings.
","['UT Austin', 'University of Texas at Austin', 'UT Austin', 'University of Sydney']"
2020,Clinician-in-the-Loop Decision Making: Reinforcement Learning with Near-Optimal Set-Valued Policies,"Shengpu Tang, Aditya Modi, Michael Sjoding, Jenna Wiens",https://icml.cc/Conferences/2020/Schedule?showEvent=5797,"Standard reinforcement learning (RL) aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective -- learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free algorithm based on temporal difference learning and a near-greedy heuristic for action selection. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits good convergence properties and discovers meaningful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician/human-in-the-loop decision making, in which humans (e.g., clinicians, patients) can incorporate additional knowledge (e.g., side effects, patient preference) when selecting among near-equivalent actions. 
","['University of Michigan', 'University of Michigan', 'University of Michigan', 'University of Michigan']"
2020,Stochastic Hamiltonian Gradient Methods for Smooth Games,"Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien, Ioannis Mitliagkas",https://icml.cc/Conferences/2020/Schedule?showEvent=6794,"The success of adversarial formulations in machine learning has brought renewed motivation for smooth games. In this work, we focus on the class of stochastic Hamiltonian methods and provide the first convergence guarantees for certain classes of stochastic smooth games. We propose a novel unbiased estimator for the stochastic Hamiltonian gradient descent (SHGD) and highlight its benefits. Using tools from the optimization literature we show that SHGD converges linearly to the neighbourhood of a stationary point. To guarantee convergence to the exact solution, we analyze SHGD with a decreasing step-size and we also present the first stochastic variance reduced Hamiltonian method. Our results provide the first global non-asymptotic last-iterate convergence guarantees for the class of stochastic unconstrained bilinear games and for the more general class of stochastic games that satisfy a ``sufficiently bilinear"" condition, notably including some non-convex non-concave problems. We supplement our analysis with experiments on stochastic bilinear and sufficiently bilinear games, where our theory is shown to be tight, and on simple adversarial machine learning formulations.
","['Mila, Université de Montréal', 'Université de Montreal', 'Mila', 'U Montreal', 'Mila, University of Montreal & Samsung SAIL Montreal', 'MILA, UdeM']"
2020,On Efficient Constructions of Checkpoints,"Yu Chen, Zhenming LIU, Bin Ren, Xin Jin",https://icml.cc/Conferences/2020/Schedule?showEvent=6599,"Efficient construction of checkpoints/snapshots is a critical tool for training and diagnosing deep learning models. In this paper, we propose a lossy compression scheme for checkpoint constructions (called LC-Checkpoint). LC-Checkpoint simultaneously maximizes the compression rate and optimizes the recovery speed, under the assumption that SGD is used to train the model. LC-Checkpoint uses quantization and priority promotion to store the most crucial information for SGD to recover, and then uses a Huffman coding to leverage the non-uniform distribution of the gradient scales. Our extensive experiments show that LC-Checkpoint achieves a compression rate up to 28× and recovery speedup up to 5.77× over a state-of-the-art algorithm (SCAR).
","['College of William and Mary', 'College of William & Mary', 'College of William and Mary', 'Johns Hopkins University']"
2020,Simultaneous Inference for Massive Data: Distributed Bootstrap,"Yang Yu, Shih-Kang Chao, Guang Cheng",https://icml.cc/Conferences/2020/Schedule?showEvent=6606,"In this paper, we propose a bootstrap method applied to massive data processed distributedly in a large number of machines. This new method is computationally efficient in that we bootstrap on the master machine without over-resampling, typically required by existing methods \cite{kleiner2014scalable,sengupta2016subsampled}, while provably achieving optimal statistical efficiency with minimal communication. Our method does not require repeatedly re-fitting the model but only applies multiplier bootstrap in the master machine on the gradients received from the worker machines. Simulations validate our theory.
","['Purdue University', 'University of Missouri', 'Purdue University']"
2020,Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing,"Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, Kush Varshney",https://icml.cc/Conferences/2020/Schedule?showEvent=6236,"A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that reflects bias, and instead, we should be considering accuracy with respect to ideal, unbiased data.
","['Carnegie Mellon University', 'IBM Research', 'IBM Research', 'IBM Research AI', 'MIT-IBM Watson AI Lab', 'IBM Research AI']"
2020,Momentum Improves Normalized SGD,"Ashok Cutkosky, Harsh Mehta",https://icml.cc/Conferences/2020/Schedule?showEvent=6230,"We provide an improved analysis of normalized SGD showing that adding momentum provably removes the need for large batch sizes on non-convex objectives. Then, we consider the case of objectives with bounded second derivative and show that in this case a small tweak to the momentum formula allows normalized SGD with momentum to find an $\epsilon$-critical point in $O(1/\epsilon^{3.5})$ iterations, matching the best-known rates without accruing any logarithmic factors or dependence on dimension. We provide an adaptive learning rate schedule that automatically improves convergence rates when the variance in the gradients is small. Finally, we show that our method is effective when employed on popular large scale tasks such as ResNet-50 and BERT pretraining, matching the performance of the disparate methods used to get state-of-the-art results on both tasks.","['Boston University', 'Google Research']"
2020,Better depth-width trade-offs for neural networks through the lens of dynamical systems,"Evangelos Chatziafratis, Sai Ganesh Nagarajan, Ioannis Panageas",https://icml.cc/Conferences/2020/Schedule?showEvent=6651,"The expressivity of neural networks as a function of their depth, width and type of activation units has been an important question in deep learning theory. Recently, depth separation results for ReLU networks were obtained via a new connection with dynamical systems, using a generalized notion of fixed points of a continuous map $f$, called periodic points. In this work, we strengthen the connection with dynamical systems and we improve the existing width lower bounds along several aspects. Our first main result is period-specific width lower bounds that hold under the stronger notion of $L^1$-approximation error, instead of the weaker classification error. Our second contribution is that we provide sharper width lower bounds, still yielding meaningful exponential depth-width separations, in regimes where previous results wouldn't apply. A byproduct of our results is that there exists a universal constant characterizing the depth-width trade-offs, as long as $f$ has odd periods. Technically, our results follow by unveiling a tighter connection between the following three quantities of a given function: its period, its Lipschitz constant and the growth rate of the number of oscillations arising under compositions of the function $f$ with itself.","['Stanford University', 'SUTD', 'Singapore University of Technology and Design']"
2020,Multi-Objective Molecule Generation using Interpretable Substructures,"Wengong Jin, Regina Barzilay, Tommi Jaakkola",https://icml.cc/Conferences/2020/Schedule?showEvent=6214,"Drug discovery aims to find novel compounds with specified chemical property profiles. In terms of generative modeling, the goal is to learn to sample molecules in the intersection of multiple property constraints. This task becomes increasingly challenging when there are many property constraints. We propose to offset this complexity by composing molecules from a vocabulary of substructures that we call molecular rationales. These rationales are identified from molecules as substructures that are likely responsible for each property of interest. We then learn to expand rationales into a full molecule using graph generative models. Our final generative model composes molecules as mixtures of multiple rationale completions, and this mixture is fine-tuned to preserve the properties of interest. We evaluate our model on various drug design tasks and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy, diversity, and novelty of generated compounds.
","['MIT', 'MIT CSAIL', 'MIT']"
2020,Breaking the Curse of Many Agents: Provable Mean Embedding Q-Iteration for Mean-Field Reinforcement Learning,"Lingxiao Wang, Zhuoran Yang, Zhaoran Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=6058,"Multi-agent reinforcement learning (MARL) achieves significant empirical successes. However, MARL suffers from the curse of many agents. In this paper, we exploit the symmetry of agents in MARL. In the most generic form, we study a mean-field MARL problem. Such a mean-field MARL is defined on mean-field states, which are distributions that are supported on continuous space. Based on the mean embedding of the distributions, we propose MF-FQI algorithm, which solves the mean-field MARL and establishes a non-asymptotic analysis for MF-FQI algorithm. We highlight that MF-FQI algorithm enjoys a ``blessing of many agents'' property in the sense that a larger number of observed agents improves the performance of MF-FQI algorithm.
","['Northwestern University', 'Princeton University', 'Northwestern U']"
2020,Data-Dependent Differentially Private Parameter Learning for Directed Graphical Models,"Amrita Roy Chowdhury, Theodoros Rekatsinas, Somesh Jha",https://icml.cc/Conferences/2020/Schedule?showEvent=6262,"Directed graphical models (DGMs) are a class of probabilistic models that are widely used for predictive analysis in sensitive domains such as medical diagnostics. In this paper, we present an algorithm for differentially-private learning of the parameters of a DGM. Our solution optimizes for the utility of inference queries over the DGM and \textit{adds noise that is customized to the properties of the private input dataset and the graph structure of the DGM}. To the best of our knowledge, this is the first explicit data-dependent privacy budget allocation algorithm in the context of DGMs. We compare our algorithm with a standard data-independent approach over a diverse suite of  benchmarks and demonstrate that our solution requires a privacy budget that is roughly $3\times$ smaller to obtain the same or higher utility.","['University of Wisconsin-Madison', 'University of Wisconsin-Madison', 'University of Wisconsin, Madison']"
2020,Semi-Supervised Learning with Normalizing Flows,"Pavel Izmailov, Polina Kirichenko, Marc Finzi, Andrew Wilson",https://icml.cc/Conferences/2020/Schedule?showEvent=6343,"Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions.
","['New York University', 'New York University', 'New York University', 'New York University']"
2020,Distribution Augmentation for Generative Modeling,"Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, Ilya Sutskever",https://icml.cc/Conferences/2020/Schedule?showEvent=6748,"We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmentation functions to data and, importantly, conditions the generative model on the specific function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures and problem domains.
","['OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI']"
2020,Learning Task-Agnostic Embedding of Multiple Black-Box Experts for Multi-Task Model Fusion,"Nghia Hoang, Thanh Lam, Bryan Kian Hsiang Low, Patrick Jaillet ",https://icml.cc/Conferences/2020/Schedule?showEvent=6317,"Model fusion is an emerging study in collective learning where heterogeneous experts with private data and learning architectures need to combine their black-box knowledge for better performance. Existing literature achieves this via a local knowledge distillation scheme that transfuses the predictive patterns of each pre-trained expert onto a white-box imitator model, which can be incorporated efficiently into a global model. This scheme however does not extend to multi-task scenarios where different experts were trained to solve different tasks and only part of their distilled knowledge is relevant to a new task. To address this multi-task challenge, we develop a new fusion paradigm that represents each expert as a distribution over a spectrum of predictive prototypes, which are isolated from task-specific information encoded within the prototype distribution. The task-agnostic prototypes can then be reintegrated to generate a new model that solves a new task encoded with a different prototype distribution. The fusion and adaptation performance of the proposed framework is demonstrated empirically on several real-world benchmark datasets.
","['MIT-IBM Watson AI Lab, IBM Research', 'National University of Singapore', 'National University of Singapore', 'MIT']"
2020,Improving generalization by controlling label-noise information in neural network weights,"Hrayr Harutyunyan, Kyle Reing, Greg Ver Steeg, Aram Galstyan",https://icml.cc/Conferences/2020/Schedule?showEvent=6246,"In the presence of noisy or incorrect labels, neural networks have the undesirable tendency to memorize information about the noise.
Standard regularization techniques such as dropout, weight decay or data augmentation sometimes help, but do not prevent this behavior.
If one considers neural network weights as random variables that depend on the data and stochasticity of training, the amount of memorized information can be quantified with the Shannon mutual information between weights and the vector of all training labels given inputs, $I(w; \Y \mid \X)$.
We show that for any training algorithm, low values of this term correspond to reduction in memorization of label-noise and better generalization bounds.
To obtain these low values, we propose training algorithms that employ an auxiliary network that predicts gradients in the final layers of a classifier without accessing labels.
We illustrate the effectiveness of our approach on versions of MNIST, CIFAR-10, and CIFAR-100 corrupted with various noise models, and on a large-scale dataset Clothing1M that has noisy labels.","['University of Southern California', 'USC Information Sciences Institute', 'University of Southern California', 'USC Information Sciences Institute']"
2020,High-dimensional Robust Mean Estimation via Gradient Descent,"Yu Cheng, Ilias Diakonikolas, Rong Ge, Mahdi Soltanolkotabi",https://icml.cc/Conferences/2020/Schedule?showEvent=6824,"We study the problem of high-dimensional robust mean estimation in the presence of a constant fraction of adversarial outliers. A recent line of work has provided sophisticated polynomial-time algorithms for this problem with dimension-independent error guarantees for a range of natural distribution families. In this work, we show that a natural non-convex formulation of the problem can be solved directly by gradient descent. Our approach leverages a novel structural lemma, roughly showing that any approximate stationary point of our non-convex objective gives a near-optimal solution to the underlying robust estimation task. Our work establishes an intriguing connection between algorithmic high-dimensional robust statistics and non-convex optimization, which may have broader applications to other robust estimation tasks.
","['University of Illinois at Chicago', 'University of Wisconsin-Madison', 'Duke University', 'University of Southern California']"
2020,Cost-Effective Interactive Attention Learning with Neural Attention Processes,"Jay Heo, Junhyeon Park, Hyewon Jeong, Kwang Joon Kim, Juho Lee, Eunho Yang, Sung Ju Hwang",https://icml.cc/Conferences/2020/Schedule?showEvent=5848,"We propose a novel interactive learning framework which we refer to as Interactive Attention Learning (IAL), in which the human supervisors interactively manipulate the allocated attentions, to correct the model's behaviour by updating the attention-generating network. However, such a model is prone to overfitting due to scarcity of human annotations, and requires costly retraining. Moreover, it is almost infeasible for the human annotators to examine attentions on tons of instances and features. We tackle these challenges by proposing a sample-efficient attention mechanism and a cost-effective reranking algorithm for instances and features. First, we propose Neural Attention Processes (NAP), which is an attention generator that can update its behaviour by incorporating new attention-level supervisions without any retraining. Secondly, we propose an algorithm which prioritizes the instances and the features by their negative impacts, such that the model can yield large improvements with minimal human feedback. We validate IAL on various time-series datasets from multiple domains (healthcare, real-estate, and computer vision) on which it significantly outperforms baselines with conventional attention mechanisms, or without cost-effective reranking, with substantially less retraining and human-model interaction cost.
","['KAIST', 'KAIST', 'KAIST', 'Yonsei University College of Medicine', 'AITRICS', 'KAIST,AITRICS', 'KAIST, AITRICS']"
2020,Communication-Efficient Distributed Stochastic AUC Maximization with Deep Neural Networks,"Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li Shen, Wei Liu, Tianbao Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=5990,"In this paper, we study distributed algorithms for large-scale AUC maximization with a deep neural network as a predictive model. 
Although distributed learning techniques have been investigated extensively  in deep learning, they are not directly applicable to stochastic AUC maximization with deep neural networks due to its striking differences from standard loss minimization problems (e.g., cross-entropy).  Towards addressing this challenge,  we propose and analyze a communication-efficient distributed optimization algorithm  based on a {\it non-convex concave} reformulation of the AUC maximization, in which the communication of both the primal variable and the dual variable between each worker and the parameter server only occurs after multiple steps of gradient-based updates in each worker.  Compared with the naive parallel version of an existing algorithm that computes stochastic gradients at individual machines and averages them for updating the model parameter, our algorithm requires a much less number of communication rounds and still achieves linear speedup in theory. To the best of our knowledge, this is the \textbf{first} work that solves the {\it non-convex concave min-max} problem for  AUC maximization with deep neural networks in a communication-efficient distributed manner while still maintaining the linear speedup property in theory.   Our experiments on several benchmark datasets show the effectiveness of our algorithm and also confirm our theory. 
","['The University of Iowa', 'The University of Iowa', 'The University of Iowa', 'Tencent AI Lab', 'Tencent AI Lab', 'The University of Iowa']"
2020,Domain Aggregation Networks for Multi-Source Domain Adaptation,"Junfeng Wen, Russell Greiner, Dale Schuurmans",https://icml.cc/Conferences/2020/Schedule?showEvent=6781,"In many real-world applications, we want to exploit multiple source datasets to build a model for a different but related target dataset. Despite the recent empirical success, most existing research has used ad-hoc methods to combine multiple sources, leading to a gap between theory and practice. In this paper, we develop a finite-sample generalization bound based on domain discrepancy and accordingly propose a theoretically justified optimization procedure. Our algorithm, Domain AggRegation Network (DARN), can automatically and dynamically balance between including more data to increase effective sample size and excluding irrelevant data to avoid negative effects during training. We find that DARN can significantly outperform the state-of-the-art alternatives on multiple real-world tasks, including digit/object recognition and sentiment analysis.
","['University of Alberta', 'U Alberta', 'University of Alberta']"
2020,Progressive Identification of True Labels for Partial-Label Learning,"Jiaqi Lv, Miao Xu, LEI FENG, Gang Niu, Xin Geng, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6745,"Partial-label learning (PLL) is a typical weakly supervised learning problem, where each training instance is equipped with a set of candidate labels among which only one is the true label. Most existing methods elaborately designed learning objectives as constrained optimizations that must be solved in specific manners, making their computational complexity a bottleneck for scaling up to big data. The goal of this paper is to propose a novel framework of PLL with flexibility on the model and optimization algorithm. More specifically, we propose a novel estimator of the classification risk, theoretically analyze the classifier-consistency, and establish an estimation error bound. Then we propose a progressive identification algorithm for approximately minimizing the proposed risk estimator, where the update of the model and identification of true labels are conducted in a seamless manner. The resulting algorithm is model-independent and loss-independent, and compatible with stochastic optimization. Thorough experiments demonstrate it sets the new state of the art.
","['Southeast University', 'University of Queensland/ RIKEN AIP', 'Nanyang Technological University', 'RIKEN', 'Southeast University', 'RIKEN / The University of Tokyo']"
2020,One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control,"Wenlong Huang, Igor Mordatch, Deepak Pathak",https://icml.cc/Conferences/2020/Schedule?showEvent=6549,"Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single policy that generalizes to controlling a wide variety of agent morphologies -- ones in which even dimensionality of state and action spaces changes. Such a policy would distill general and modular sensorimotor patterns that can be applied to control arbitrary agents. We propose a policy expressed as a collection of identical modular neural networks for each of the agent's actuators. Every module is only responsible for controlling its own actuator and receives information from its local sensors. In addition, messages are passed between modules, propagating information between distant modules. A single modular policy can successfully generate locomotion behaviors for over 20 planar agents with different skeletal structures such as monopod hoppers, quadrupeds, bipeds, and generalize to variants not seen during training -- a process that would normally require training and manual hyperparameter tuning for each morphology. We observe a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerging via message passing between decentralized modules purely from the reinforcement learning objective. Video and code: https://huangwl18.github.io/modular-rl/
","['UC Berkeley', 'Google Brain', 'CMU, FAIR']"
2020,Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems,"Zhe Dong,  Bryan Seybold, Kevin Murphy, Hung Bui",https://icml.cc/Conferences/2020/Schedule?showEvent=6669,"We propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with stochastic gradient descent. We show that the proposed method can successfully segment time series data, including videos and 3D human pose, into meaningful ``regimes'' by using the piece-wise nonlinear dynamics. 
","['Google', 'Google', 'Google Brain', 'VinAI Research']"
2020,An Imitation Learning Approach for Cache Replacement,"Evan Liu, Milad Hashemi, Kevin Swersky, Parthasarathy Ranganathan, Junwhan Ahn",https://icml.cc/Conferences/2020/Schedule?showEvent=6044,"Program execution speed critically depends on increasing cache hits, as cache hits are orders of magnitude faster than misses. To increase cache hits, we focus on the problem of cache replacement: choosing which cache line to evict upon inserting a new line. This is challenging because it requires planning far ahead and currently there is no known practical solution. As a result, current replacement policies typically resort to heuristics designed for specific common access patterns, which fail on more diverse and complex access patterns. In contrast, we propose an imitation learning approach to automatically learn cache access patterns by leveraging Belady’s, an oracle policy that computes the optimal eviction decision given the future cache accesses. While directly applying Belady’s is infeasible since the future is unknown, we train a policy conditioned only on past accesses that accurately approximates Belady’s even on diverse and complex access patterns, and call this approach Parrot. When evaluated on 13 of the most memory-intensive SPEC applications, Parrot increases cache miss rates by 20% over the current state of the art. In addition, on a large-scale web search benchmark, Parrot increases cache hit rates by 61% over a conventional LRU policy. We release a Gym environment to facilitate research in this area, as data is plentiful, and further advancements can have significant real-world impact.
","['Stanford University, Google Research', 'Google', 'Google Brain', 'Google, USA', 'Google']"
2020,The Performance Analysis of Generalized Margin Maximizers on Separable Data,"Fariborz Salehi, Ehsan Abbasi, Babak Hassibi",https://icml.cc/Conferences/2020/Schedule?showEvent=6833,"Logistic models are commonly used for binary classification tasks. The success of such models has often been attributed to their connection to maximum-likelihood estimators. It has been shown that SGD algorithms , when applied on the logistic loss, converge to the max-margin classifier (a.k.a. hard-margin SVM). The performance of hard-margin SVM has been recently analyzed in~\cite{montanari2019generalization, deng2019model}. Inspired by these results, in this paper, we present and study a more general setting, where the underlying parameters of the logistic model possess certain structures (sparse, block-sparse, low-rank, etc.) and introduce a more general framework (which is referred to as “Generalized Margin Maximizer”, GMM). While classical max-margin classifiers minimize the 2-norm of the parameter vector subject to linearly separating the data, GMM minimizes any arbitrary convex function of the parameter vector. We provide a precise performance analysis of the generalization error of such methods and show improvement over the max-margin method which does not take into account the structure of the model. In future work we show that mirror descent algorithms, with a properly tuned step size, can be exploited to achieve GMM classifiers. Our theoretical results are validated by extensive simulation results across a range of parameter values, problem instances, and model structures.
","['California Institute of Technology', 'California Institute of Technology', 'Caltech']"
2020,Momentum-Based Policy Gradient Methods,"Feihu Huang, Shangqian Gao, Jian Pei, Heng Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=6136,"In the paper, we propose a class of efficient momentum-based policy gradient methods for the model-free reinforcement learning,
which use adaptive learning rates and do not require any large batches.
Specifically, we propose a fast important-sampling momentum-based policy gradient (IS-MBPG) method based on a new momentum-based variance reduced technique and the importance sampling technique.
We also propose a fast Hessian-aided momentum-based policy gradient (HA-MBPG) method based on the momentum-based variance reduced technique and the Hessian-aided technique.
Moreover, we prove that both the IS-MBPG and HA-MBPG methods
reach the best known sample complexity of $O(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of the nonconcave performance function, which only require one trajectory at each iteration. 
In particular, we present a non-adaptive version of IS-MBPG method, i.e., IS-MBPG*, which also reaches the best known sample complexity of  $O(\epsilon^{-3})$ without any large batches. 
In the experiments, we apply four benchmark tasks to demonstrate the effectiveness of our algorithms.","['University of Pittsburgh', 'University of Pittsburgh', 'Simon Fraser University', 'University of Pittsburgh & JD Finance America Corporation']"
2020,Few-shot Relation Extraction via Bayesian Meta-learning on Relation Graphs,"Meng Qu, Tianyu Gao, Louis-Pascal Xhonneux, Jian Tang",https://icml.cc/Conferences/2020/Schedule?showEvent=6686,"This paper studies few-shot relation extraction, which aims at predicting the relation for a pair of entities in a sentence by training with a few labeled examples in each relation. To more effectively generalize to new relations, in this paper we study the relationships between different relations and propose to leverage a global relation graph. We propose a novel Bayesian meta-learning approach to effectively learn the posterior distribution of the prototype vectors of relations, where the initial prior of the prototype vectors is parameterized with a graph neural network on the global relation graph. Moreover, to effectively optimize the posterior distribution of the prototype vectors, we propose to use the stochastic gradient Langevin dynamics, which is related to the MAML algorithm but is able to handle the uncertainty of the prototype vectors. The whole framework can be effectively and efficiently optimized in an end-to-end fashion. Experiments on two benchmark datasets prove the effectiveness of our proposed approach against competitive baselines in both the few-shot and zero-shot settings. 
","['Mila', 'Tsinghua University', 'Mila / Université de Montréal', 'HEC Montreal & MILA']"
2020,Visual Grounding of Learned Physical Models,"Yunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel Yamins, Jiajun Wu, Josh Tenenbaum, Antonio Torralba",https://icml.cc/Conferences/2020/Schedule?showEvent=6550,"Humans intuitively recognize objects' physical properties and predict their motion, even when the objects are engaged in complicated interactions. The abilities to perform physical reasoning and to adapt to new environments, while intrinsic to humans, remain challenging to state-of-the-art computational models. In this work, we present a neural model that simultaneously reasons about physics and makes future predictions based on visual and dynamics priors. The visual prior predicts a particle-based representation of the system from visual observations. An inference module operates on those particles, predicting and refining estimates of particle locations, object states, and physical parameters, subject to the constraints imposed by the dynamics prior, which we refer to as visual grounding. We demonstrate the effectiveness of our method in environments involving rigid objects, deformable materials, and fluids. Experiments show that our model can infer the physical properties within a few observations, which allows the model to quickly adapt to unseen scenarios and make accurate predictions into the future.
","['MIT', 'MIT', 'Harvard University', 'Stanford University', 'Stanford University', 'Stanford University', 'MIT', 'MIT']"
2020,Learning Human Objectives by Evaluating Hypothetical Behavior,"Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, Jan Leike",https://icml.cc/Conferences/2020/Schedule?showEvent=5862,"We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. We propose an algorithm that safely and efficiently learns a model of the user's reward function by posing 'what if?' questions about hypothetical agent behavior. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.
","['University of California, Berkeley', 'University of California, Berkeley', 'UC Berkeley', 'DeepMind', 'U Oxford']"
2020,One-shot Distributed Ridge Regression in High Dimensions,"Yue Sheng, Edgar Dobriban",https://icml.cc/Conferences/2020/Schedule?showEvent=5844,"To scale up data analysis, distributed and parallel computing approaches are increasingly needed. Here we study a fundamental problem in this area: How to do ridge regression in a distributed computing environment? We study one-shot methods constructing weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high dimensional model where each predictor has a small effect, we discover several new phenomena including that the efficiency depends strongly on the signal strength, but does not degrade with many workers, the risk decouples over machines, and the unexpected consequence that the optimal weights do not sum to unity. We also propose a new optimally weighted one-shot ridge regression algorithm. Our results are supported by simulations and real data analysis.
","['University of Pennsylvania', 'University of Pennsylvania']"
2020,Generalization and Representational Limits of Graph Neural Networks,"Vikas K Garg, Stefanie Jegelka, Tommi Jaakkola",https://icml.cc/Conferences/2020/Schedule?showEvent=6249,"We address two fundamental questions about graph neural networks (GNNs).  First, we prove that several important graph properties cannot be discriminated by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node.  Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs.  This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.
","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'MIT']"
2020,Principled learning method for Wasserstein distributionally robust optimization with local perturbations,"Yongchan Kwon, Wonyoung Kim, Joong-Ho (Johann) Won, Myunghee Cho Paik",https://icml.cc/Conferences/2020/Schedule?showEvent=6001,"Wasserstein distributionally robust optimization (WDRO) attempts to learn a model that minimizes the local worst-case risk in the vicinity of the empirical data distribution defined by Wasserstein ball. While WDRO has received attention as a promising tool for inference since its introduction, its theoretical understanding has not been fully matured. Gao et al. (2017) proposed a minimizer based on a tractable approximation of the local worst-case risk, but without showing risk consistency. In this paper, we propose a minimizer based on a novel approximation theorem and provide the corresponding risk consistency results. Furthermore, we develop WDRO inference for locally perturbed data that include the Mixup (Zhang et al., 2017) as a special case. We show that our approximation and risk consistency results naturally extend to the cases when data are locally perturbed. Numerical experiments demonstrate robustness of the proposed method using image classification datasets. Our results show that the proposed method achieves significantly higher accuracy than baseline models on noisy datasets.
","['Stanford University', 'Seoul National University', 'Seoul National University', 'Seoul National University']"
2020,SGD Learns One-Layer Networks in WGANs,"Qi Lei, Jason Lee, Alexandros Dimakis, Constantinos Daskalakis",https://icml.cc/Conferences/2020/Schedule?showEvent=6584,"Generative adversarial networks (GANs) are a widely used framework for learning generative models. Wasserstein GANs (WGANs), one of the most successful variants of GANs, require solving a minmax optimization problem to global optimality, but are in practice successfully trained using stochastic gradient descent-ascent. In this paper, we show that, when the generator is a one-layer network, stochastic gradient descent-ascent converges to a global solution with polynomial time and sample complexity.
","['University of Texas at Austin', 'Princeton', 'UT Austin', 'MIT']"
2020,AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation,"Jae Hyun Lim, Aaron Courville, Christopher Pal, Chin-Wei Huang",https://icml.cc/Conferences/2020/Schedule?showEvent=6639,"Entropy is ubiquitous in machine learning, but it is in general intractable to compute the entropy of the distribution of an arbitrary continuous random variable. In this paper, we propose the amortized residual denoising autoencoder (AR-DAE) to approximate the gradient of the log density function, which can be used to estimate the gradient of entropy. Amortization allows us to significantly reduce the error of the gradient approximator by approaching asymptotic optimality of a regular DAE, in which case the estimation is in theory unbiased. We conduct theoretical and experimental analyses on the approximation error of the proposed method, as well as extensive studies on heuristics to ensure its robustness. Finally, using the proposed gradient approximator to estimate the gradient of entropy, we demonstrate state-of-the-art performance on density estimation with variational autoencoders and continuous control with soft actor-critic.
","['Mila', 'Université de Montréal', 'École Polytechnique de Montréal', 'MILA']"
2020,Model Fusion with Kullback--Leibler Divergence,"Sebastian Claici, Mikhail Yurochkin, Soumya Ghosh, Justin Solomon",https://icml.cc/Conferences/2020/Schedule?showEvent=6156,"We propose a method to fuse posterior distributions learned from heterogeneous datasets. Our algorithm relies on a mean field assumption for both the fused model and the individual dataset posteriors and proceeds using a simple assign-and-average approach. The components of the dataset posteriors are assigned to the proposed global model components by solving a regularized variant of the assignment problem. The global components are then updated based on these assignments by their mean under a KL divergence. For exponential family variational distributions, our formulation leads to an efficient non-parametric algorithm for computing the fused model. Our algorithm is easy to describe and implement, efficient, and competitive with state-of-the-art on motion capture analysis, topic modeling, and federated learning of Bayesian neural networks.
","['MIT', 'IBM Research AI', 'IBM Research', 'MIT']"
2020,Emergence of Separable Manifolds in Deep Language Representations,"Jonathan Mamou, Hang Le, Miguel A del Rio Fernandez, Cory Stephenson, Hanlin Tang, Yoon Kim, SueYeon Chung",https://icml.cc/Conferences/2020/Schedule?showEvent=6793,"Deep neural networks (DNNs) have shown much empirical success in solving perceptual tasks across various cognitive modalities. While they are only loosely inspired by the biological brain, recent studies report considerable similarities between representations extracted from task-optimized DNNs and neural populations in the brain. DNNs have subsequently become a popular model class to infer computational principles underlying complex cognitive functions, and in turn, they have also emerged as a natural testbed for applying methods originally developed to probe information in neural populations. In this work, we utilize mean-field theoretic manifold analysis, a recent technique from computational neuroscience that connects geometry of feature representations with linear separability of classes, to analyze language representations from large-scale contextual embedding models. We explore representations from different model families (BERT, RoBERTa, GPT, etc.) and find evidence for emergence of linguistic manifolds across layer depth (e.g.,
manifolds for part-of-speech tags), especially in ambiguous data (i.e, words with multiple part-of-speech tags, or part-of-speech classes including many words). In addition, we find that the emergence of linear separability in these manifolds is driven by a combined reduction of manifolds’ radius, dimensionality and inter-manifold correlations.
","['Intel Labs', 'MIT', 'MIT', 'Intel Corporation', 'Intel AI', 'Harvard University', 'Columbia University']"
2020,Optimizing Data Usage via Differentiable Rewards,"Xinyi Wang, Hieu Pham, Paul Michel, Antonios  Anastasopoulos, Jaime Carbonell, Graham Neubig",https://icml.cc/Conferences/2020/Schedule?showEvent=5889,"To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that ``adapts'' to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently  updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.
","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
2020,"Parameter-free, Dynamic, and Strongly-Adaptive Online Learning",Ashok Cutkosky,https://icml.cc/Conferences/2020/Schedule?showEvent=6231,"We provide a new online learning algorithm that for the first time combines several disparate notions of adaptivity. First, our algorithm obtains a ``parameter-free'' regret bound that adapts to the norm of the comparator and the squared norm of the size of the gradients it observes. Second, it obtains a ``strongly-adaptive'' regret bound, so that for any given interval of length $N$, the regret over the interval is $\tilde O(\sqrt{N})$. Finally, our algorithm obtains an optimal ``dynamic'' regret bound: for any sequence of comparators with path-length $P$, our algorithm obtains regret $\tilde O(\sqrt{PN})$ over intervals of length $N$. Our primary technique for achieving these goals is a new method of combining constrained online learning regret bounds that does not rely on an expert meta-algorithm to aggregate learners.",['Boston University']
2020,Non-convex Learning via Replica Exchange Stochastic Gradient MCMC,"Wei Deng, Qi Feng, Liyao Gao, Faming Liang, Guang Lin",https://icml.cc/Conferences/2020/Schedule?showEvent=6023,"Replica exchange Monte Carlo (reMC), also known as parallel tempering, is an important technique for accelerating the convergence of the conventional Markov Chain Monte Carlo (MCMC) algorithms. However, such a method requires the evaluation of the energy function based on the full dataset and is not scalable to big data. The na\""ive implementation of reMC in mini-batch settings introduces large biases, which cannot be directly extended to the stochastic gradient MCMC (SGMCMC), the standard sampling method for simulating from deep neural networks (DNNs). In this paper, we propose an adaptive replica exchange SGMCMC (reSGMCMC) to automatically correct the bias and study the corresponding properties. The analysis implies an acceleration-accuracy trade-off in the numerical discretization of a Markov jump process in a stochastic environment. Empirically, we test the algorithm through extensive experiments on various setups and obtain the state-of-the-art results on CIFAR10, CIFAR100, and SVHN in both supervised learning and semi-supervised learning tasks.
","['Purdue University', 'University of Southern California', 'Purdue University', 'Purdue University', 'Purdue University']"
2020,Certified Data Removal from Machine Learning Models,"Chuan Guo, Tom Goldstein, Awni Hannun, Laurens van der Maaten",https://icml.cc/Conferences/2020/Schedule?showEvent=5895,"Good data stewardship requires removal of data at the request of the data's owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it possible to ``remove'' data from a machine-learning model? We study this problem by defining certified removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certified-removal mechanism for linear classifiers and empirically study learning settings in which this mechanism is practical.
","['Cornell University', 'University of Maryland', 'Facebook AI Research', 'Facebook']"
2020,UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training,"Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, Hsiao-Wuen Hon",https://icml.cc/Conferences/2020/Schedule?showEvent=6417,"We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of language understanding and generation tasks across several widely used benchmarks. The code and pre-trained models are available at https://github.com/microsoft/unilm.
","['Harbin Institute of Technology', 'Microsoft Research', 'Microsoft Research Asia', 'Microsoft Research', 'Microsoft Research Asia', 'Microsoft Research', 'Microsoft Research', 'Microsoft Research AI', 'Harbin Institute of Technology', 'Microsoft Research', 'Microsoft Research']"
2020,Minimax Pareto Fairness: A Multi Objective Perspective,"Natalia Martinez Gil, Martin Bertran, Guillermo Sapiro",https://icml.cc/Conferences/2020/Schedule?showEvent=5925,"In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.
","['Duke University', 'Duke University', 'Duke University']"
2020,Defense Through Diverse Directions,"Christopher Bender, Yang Li, Yifeng Shi, Michael K. Reiter, Junier Oliva",https://icml.cc/Conferences/2020/Schedule?showEvent=6557,"In this work we develop a novel Bayesian neural network methodology to achieve strong adversarial robustness without the need for online adversarial training. Unlike previous efforts in this direction, we do not rely solely on the stochasticity of network weights by minimizing the divergence between the learned parameter distribution and a prior. Instead, we additionally require that the model maintain some expected uncertainty with respect to all input covariates. We demonstrate that by encouraging the network to distribute evenly across inputs, the network becomes less susceptible to localized, brittle features which imparts a natural robustness to targeted perturbations. We show empirical robustness on several benchmark datasets.
","['The University of North Carolina Computer Science Department', 'University of North Carolina at Chapel Hill', 'University of North Carolina at Chapel Hill', 'UNC at Chapel Hill', 'UNC-Chapel Hill']"
2020,Sparse Sinkhorn Attention,"Yi Tay, Dara Bahri, Liu Yang, Don Metzler, Da-Cheng Juan",https://icml.cc/Conferences/2020/Schedule?showEvent=6539,"We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.
","['Google', 'Google Research', 'Google', 'Google', 'Google']"
2020,Learning Mixtures of Graphs from Epidemic Cascades ,"Jessica Hoffmann, Soumya Basu, Surbhi Goel, Constantine Caramanis",https://icml.cc/Conferences/2020/Schedule?showEvent=6061,"We consider the problem of learning the weighted edges of a balanced mixture of two undirected graphs from epidemic cascades. While mixture models are popular modeling tools, algorithmic development with rigorous guarantees has lagged. Graph mixtures are apparently no exception: until now, very little is known about whether this problem is solvable. 
 To the best of our knowledge, we establish the first necessary and sufficient conditions for this problem to be solvable in polynomial time on edge-separated graphs. When the conditions are met, i.e., when the graphs are connected with at least three edges, we give an efficient algorithm for learning the weights of both graphs with optimal sample complexity (up to log factors). 

 We give complementary results and provide sample-optimal (up to log factors) algorithms for mixtures of directed graphs of out-degree at least three, and for mixture of undirected graphs of unbalanced and/or unknown priors.

","['University of Texas at Austin', 'University of Texas at Austin', 'University of Texas at Austin', 'University of Texas']"
2020,Generalized and Scalable Optimal Sparse Decision Trees,"Jimmy Lin, Chudi Zhong, Diane Hu, Cynthia Rudin, Margo Seltzer",https://icml.cc/Conferences/2020/Schedule?showEvent=6340,"Decision tree optimization is notoriously difficult from a computational perspective but essential for the field of interpretable machine learning. Despite efforts over the past 40 years, only recently have optimization breakthroughs been made that have allowed practical algorithms to find optimal decision trees. These new techniques have the potential to trigger a paradigm shift, where, it is possible to construct sparse decision trees to efficiently optimize a variety of objective functions, without relying on greedy splitting and pruning heuristics that often lead to suboptimal solutions. The contribution in this work is to provide a general framework for decision tree optimization that addresses the two significant open problems in the area: treatment of imbalanced data and fully optimizing over continuous variables. We present techniques that produce optimal decision trees over variety of objectives including F-score, AUC, and partial area under the ROC convex hull. We also introduce a scalable algorithm that produces provably optimal results in the presence of continuous variables and speeds up decision tree construction by several order of magnitude relative to  the state-of-the art.
","['University of British Columbia', 'Duke University', 'Duke University', 'Duke', 'University of British Columbia']"
2020,Gradient Temporal-Difference Learning with Regularized Corrections,"Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, Martha White",https://icml.cc/Conferences/2020/Schedule?showEvent=6067,"It is still common to use Q-learning and temporal difference (TD) learning—even though they have divergence issues and sound Gradient TD alternatives exist—because divergence seems rare and they typically perform well. However, recent work with large neural network learning systems reveals that instability is more common than previously thought. Practitioners face a difficult dilemma: choose an easy to use and performant TD method, or a more complex algorithm that is more sound but harder to tune and all but unexplored with non-linear function approximation or control. In this paper, we introduce a new method called TD with Regularized Corrections (TDRC), that attempts to balance ease of use, soundness, and performance. It behaves as well as TD, when TD performs well, but is sound in cases where TD diverges. We empirically investigate TDRC across a range of problems, for both prediction and control, and for both linear and non-linear function approximation, and show, potentially for the first time, that gradient TD methods could be a better alternative to TD and Q-learning.
","['University of Alberta', 'University of Alberta', 'University of alberta', 'University of Alberta', 'University of Alberta, DeepMind', 'University of Alberta']"
2020,Coresets for Data-efficient Training of Machine Learning Models,"Baharan Mirzasoleiman, Jeff Bilmes, Jure Leskovec",https://icml.cc/Conferences/2020/Schedule?showEvent=6325,"Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.
","['Stanford University', 'UW', 'Stanford University']"
2020,Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels,"Lu Jiang, Di Huang, Mason Liu, Weilong Yang",https://icml.cc/Conferences/2020/Schedule?showEvent=6558,"Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings.
","['Google Research', 'Google', 'Cornell', 'Google Inc. ']"
2020,Provably Convergent Two-Timescale Off-Policy Actor-Critic with Function Approximation,"Shangtong Zhang, Bo Liu, Hengshuai Yao, Shimon Whiteson",https://icml.cc/Conferences/2020/Schedule?showEvent=5837,"We present the first provably convergent two-timescale off-policy actor-critic algorithm (COF-PAC) with function approximation.
Key to COF-PAC is the introduction of a new critic, the emphasis critic, 
which is trained via Gradient Emphasis Learning (GEM), 
a novel combination of the key ideas of Gradient Temporal Difference Learning and Emphatic Temporal Difference Learning.
With the help of the emphasis critic and the canonical value function critic, 
we show convergence for COF-PAC,
where the critics are linear and the actor can be nonlinear. 
","['University of Oxford', 'Auburn University', 'Huawei Technologies', 'University of Oxford']"
2020,Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection,"Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, Qiang Liu",https://icml.cc/Conferences/2020/Schedule?showEvent=6053,"Recent empirical works show that large deep neural networks are often highly redundant and one can find much smaller subnetworks without a significant drop of accuracy. However, most existing methods of network pruning are empirical and heuristic, leaving it open whether good subnetworks provably exist, how to find them efficiently, and if network pruning can be provably better than direct training using gradient descent. We answer these problems positively by proposing a simple greedy  selection approach for finding good subnetworks, which starts from an empty network and greedily adds important neurons from the large network. This differs from the existing methods based on backward elimination, which remove redundant neurons from the large network. Theoretically, applying the greedy selection strategy on sufficiently large {pre-trained} networks guarantees to find small subnetworks with lower loss than networks directly trained with gradient descent. Our results also apply to pruning randomly weighted networks. Practically, we improve prior arts of network pruning on learning compact neural architectures on ImageNet, including ResNet, MobilenetV2/V3, and ProxylessNet. Our theory and empirical results on MobileNet suggest  that we should fine-tune the pruned subnetworks to leverage the information from the large model, instead of re-training from new random initialization as suggested in \citet{liu2018rethinking}. 
","['UT Austin', 'university of texas at austin', 'The University of Chicago', 'Google Brain', 'University of Texas at Austin', 'UT Austin']"
2020,Implicit competitive regularization in GANs,"Florian Schaefer, Hongkai Zheng, Anima Anandkumar",https://icml.cc/Conferences/2020/Schedule?showEvent=6721,"The success of GANs is usually attributed to properties of the divergence obtained by an optimal discriminator.
In this work we show that this approach has a fundamental flaw:\
If we do not impose regularity of the discriminator, it can exploit visually imperceptible errors of the generator to always achieve the maximal generator loss.
In practice, gradient penalties are used to regularize the discriminator.
However, this needs a metric on the space of images that captures visual similarity.
Such a metric is not known, which explains the limited success of gradient penalties in stabilizing GANs.\
Instead, we argue that the implicit competitive regularization (ICR) arising from the simultaneous optimization of generator and discriminator enables GANs performance.
We show that opponent-aware modelling of generator and discriminator, as present in competitive gradient descent (CGD), can significantly strengthen ICR and thus stabilize GAN training without explicit regularization.
In our experiments, we use an existing implementation of WGAN-GP and show that by training it with CGD without any explicit regularization, we can improve the inception score (IS) on CIFAR10, without any hyperparameter tuning.
","['Caltech', 'Shanghai Jiao Tong University', 'Amazon AI & Caltech']"
2020,Improved Optimistic Algorithms for Logistic Bandits,"Louis Faury, Marc Abeille, Clément Calauzènes, Olivier Fercoq",https://icml.cc/Conferences/2020/Schedule?showEvent=6154,"The generalized linear bandit framework has attracted a lot of attention in recent years by extending the well-understood linear setting and allowing to model richer reward structures. It notably covers the logistic model, widely used when rewards are binary. For logistic bandits, the frequentist regret guarantees of existing algorithms are $\tilde{\mathcal{O}}(\kappa \sqrt{T})$, where $\kappa$ is a problem-dependent constant. Unfortunately, $\kappa$ can be arbitrarily large as it scales exponentially with the size of the decision set. This may lead to significantly loose regret bounds and poor empirical performance. In this work, we study the logistic bandit with a focus on the prohibitive dependencies introduced by $\kappa$. We propose a new optimistic algorithm based on a finer examination of the non-linearities of the reward function. We show that it enjoys a $\tilde{\mathcal{O}}(\sqrt{T})$ regret with no dependency in $\kappa$, but for a second order term. Our analysis is based on a new tail-inequality for self-normalized martingales, of independent interest.","['Criteo AI Lab', 'Criteo AI Lab', 'Criteo AI Lab', 'Telecom Paris']"
2020,Generalization Error of Generalized Linear Models in High Dimensions,"Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, Alyson Fletcher",https://icml.cc/Conferences/2020/Schedule?showEvent=6565,"At the heart of machine learning lies the question of generalizability of learned rules over previously unseen data. 
While over-parameterized models based on neural networks are now ubiquitous in machine learning applications, our understanding of their generalization capabilities is incomplete and this task is made harder by the non-convexity of the underlying learning problems. 
We provide a general framework to characterize the asymptotic generalization error for single-layer neural networks (i.e., generalized linear models) with arbitrary non-linearities,
making it applicable to regression as well as classification problems.
This framework enables analyzing the effect of (i) over-parameterization and non-linearity during modeling; (ii) choices of loss function, initialization, and regularizer during learning; and (iii) mismatch between training and test distributions.  As examples, we analyze a few special cases, namely linear regression and logistic regression.  We are also able to rigorously and analytically explain the \emph{double descent} phenomenon in generalized linear models. 
","['University of California Los Angeles', 'UCLA', 'UCLA', 'NYU', 'UCLA']"
2020,Unsupervised Discovery of Interpretable Directions in the GAN Latent Space,"Andrey Voynov, Artem Babenko",https://icml.cc/Conferences/2020/Schedule?showEvent=6091,"The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover.
In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection. The implementation of our method is available online.
","['Yandex', 'Yandex']"
2020,Inferring DQN structure for high-dimensional continuous control,"Andrey Sakryukin, Chedy Raissi, Mohan Kankanhalli",https://icml.cc/Conferences/2020/Schedule?showEvent=6740,"Despite recent advancements in the field of Deep Reinforcement Learning, Deep Q-network (DQN) models still show lackluster performance on problems with high-dimensional action spaces.
The problem is even more pronounced for cases with high-dimensional continuous action spaces due to a combinatorial increase in the number of the outputs. 
Recent works approach the problem by dividing the network into multiple parallel or sequential (action) modules responsible for different discretized actions. 
However, there are drawbacks to both the parallel and the sequential approaches.
Parallel module architectures lack coordination between action modules, leading to extra complexity in the task, while a sequential structure can result in the vanishing gradients problem and exploding parameter space. 
In this work, we show that the compositional structure of the action modules has a significant impact on model performance.
We propose a novel approach to infer the network structure for DQN models operating with high-dimensional continuous actions.
Our method is based on the uncertainty estimation techniques introduced in the paper.
Our approach achieves state-of-the-art performance on MuJoCo environments with high-dimensional continuous action spaces.
Furthermore, we demonstrate the improvement of the introduced approach on a realistic AAA sailing simulator game.
","['National University of Singapore', 'INRIA', 'National University of Singapore,']"
2020,Bounding the fairness and accuracy of classifiers from population statistics,"Sivan Sabato, Elad Yom-Tov",https://icml.cc/Conferences/2020/Schedule?showEvent=6108,"We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations.  We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds.  We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.
","['Ben-Gurion University of the Negev', 'Microsoft Research']"
2020,WaveFlow: A Compact Flow-based Model for Raw Audio,"Wei Ping, Kainan Peng, Kexin Zhao, Zhao Song",https://icml.cc/Conferences/2020/Schedule?showEvent=5857,"In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15× smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6× faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels.
","['Baidu Research', 'Baidu Research', 'Baidu', 'Baidu Research']"
2020,ROMA: Multi-Agent Reinforcement Learning with Emergent Roles,"Tonghan Wang, Heng Dong, Victor Lesser, Chongjie Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=5924,"The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning and to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn specialized, dynamic, and identifiable roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at https://sites.google.com/view/romarl/.
","['Tsinghua University', 'Tsinghua', 'UMASS', 'Tsinghua University']"
2020,Accelerated Message Passing for Entropy-Regularized MAP Inference,"Jonathan Lee, Aldo Pacchiano, Peter Bartlett, Michael Jordan",https://icml.cc/Conferences/2020/Schedule?showEvent=6502,"Maximum a posteriori (MAP) inference in discrete-valued Markov random fields is a fundamental problem in machine learning that involves identifying the most likely configuration of random variables given a distribution.  Due to the difficulty of this combinatorial problem, linear programming (LP) relaxations are commonly used to derive specialized message passing algorithms that are often interpreted as coordinate descent on the dual LP. To achieve more desirable computational properties, a number of methods regularize the LP with an entropy term, leading to a class of smooth message passing algorithms with convergence guarantees. In this paper, we present randomized methods for accelerating these algorithms by leveraging techniques that underlie classical accelerated gradient methods. The proposed algorithms incorporate the familiar steps of standard smooth message passing algorithms, which can be viewed as coordinate minimization steps. We show that these accelerated variants achieve faster rates for finding $\epsilon$-optimal points of the unregularized problem, and, when the LP is tight, we prove that the proposed algorithms recover the true MAP solution in fewer iterations than standard message passing algorithms.","['Stanford University', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley']"
2020,Evaluating Machine Accuracy on ImageNet,"Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, Ludwig Schmidt",https://icml.cc/Conferences/2020/Schedule?showEvent=6766,"We evaluate a wide range of ImageNet models with five trained human labelers.
In our year-long experiment, trained humans first annotated 40,000 images from the ImageNet and ImageNetV2 test sets with multi-class labels to enable a semantically coherent evaluation.
Then we measured the classification accuracy of the five trained humans on the full task with 1,000 classes.
Only the latest models from 2020 are on par with our best human labeler, and human accuracy on the 590 object classes is still 4% and 10% higher than the best model on ImageNet and ImageNetV2, respectively.
Moreover, humans achieve the same accuracy on ImageNet and ImageNetV2, while all models see a consistent accuracy drop.
Overall, our results show that there is still substantial room for improvement on ImageNet and direct accuracy comparisons between humans and machines may overstate machine performance.
","['UC Berkeley', 'Google', 'UC Berkeley', 'UC Berkeley', 'Berkeley', 'University of California, Berkeley']"
2020,Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions,"Michael Chang, Sid Kaushik, S. Matthew Weinberg, Thomas Griffiths, Sergey Levine",https://icml.cc/Conferences/2020/Schedule?showEvent=6293,"This paper seeks to establish a framework for directing a society of simple, specialized, self-interested agents to solve what traditionally are posed as monolithic single-agent sequential decision problems. What makes it challenging to use a decentralized approach to collectively optimize a central objective is the difficulty in characterizing the equilibrium strategy profile of non-cooperative games. To overcome this challenge, we design a mechanism for defining the learning environment of each agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. The society functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We derive a class of decentralized reinforcement learning algorithms that are broadly applicable not only to standard reinforcement learning but also for selecting options in semi-MDPs and dynamically composing computation graphs. Lastly, we demonstrate the potential advantages of a society's inherent modular structure for more efficient transfer learning.
","['UC Berkeley', 'UCB', 'Princeton University', 'Princeton University', 'UC Berkeley']"
2020,Multiclass Neural Network Minimization via Tropical Newton Polytope Approximation,"Georgios Smyrnis, Petros Maragos",https://icml.cc/Conferences/2020/Schedule?showEvent=6843,"The field of tropical algebra is closely linked with the domain of neural networks with piecewise linear activations, since their output can be described via tropical polynomials in the max-plus semiring.  In this work, we attempt to make use of methods stemming from a form of approximate division of such polynomials, which relies on the approximation of their Newton Polytopes, in order to minimize networks trained for multiclass classification problems. We make theoretical contributions in this domain, by proposing and analyzing methods which seek to reduce the size of such networks. In addition, we make experimental evaluations on the MNIST and Fashion-MNIST datasets, with our results demonstrating a significant reduction in network size, while retaining adequate performance.
","['National Technical University of Athens', 'National Technical University of Athens']"
2020,From PAC to Instance-Optimal Sample Complexity in the Plackett-Luce Model,"Aadirupa Saha, Aditya Gopalan",https://icml.cc/Conferences/2020/Schedule?showEvent=5831,"We consider PAC learning a good item from $k$-subsetwise feedback sampled from a Plackett-Luce probability model, with instance-dependent sample complexity performance. In the setting where subsets of a fixed size can be tested and top-ranked feedback is made available to the learner, we give an optimal instance-dependent algorithm with a  sample complexity bound for PAC best arm identification algorithm of 
$O\bigg(\frac{\Theta_{[k]}}{k}\sum_{i = 2}^n\max\Big(1,\frac{1}{\Delta_i^2}\Big) \ln\frac{k}{\delta}\Big(\ln \frac{1}{\Delta_i}\Big)\bigg)$, $\Delta_i$ being the Plackett-Luce parameter gap between the best and the $i^{th}$ best item, and $\Theta_{[k]}$ is the sum of the Plackett-Luce parameters for top-$k$ items. The algorithm is based on a wrapper around a PAC winner-finding algorithm with weaker performance guarantees to adapt to the hardness of the input instance. The sample complexity is also shown to be multiplicatively better depending on the length of rank-ordered feedback available in each subset-wise play. We show optimality of our algorithms with matching sample complexity lower bounds. We next address the winner-finding problem in Plackett-Luce models in the fixed-budget setting with instance dependent upper and lower bounds on the misidentification probability, of $\Omega\left(\exp(-2 \tilde \Delta Q) \right)$ for a given budget $Q$, where $\tilde \Delta$ is an explicit instance-dependent problem complexity parameter. Numerical performance results are also reported for the algorithms. ","['Indian Institute of Science (IISc), Bangalore', 'Indian Institute of Science']"
2020,A Distributional Framework For Data Valuation,"Amirata Ghorbani, Michael Kim, James Zou",https://icml.cc/Conferences/2020/Schedule?showEvent=6637,"Shapley value is a classic notion from game theory, historically used to quantify the contributions of individuals within groups, and more recently applied to assign values to data points when training machine learning models. Despite its foundational role, a key limitation of the data Shapley framework is that it only provides valuations for points within a fixed data set. It does not account for statistical aspects of the data and does not give a way to reason about points outside the data set. To address these limitations, we propose a novel framework -- distributional Shapley -- where the value of a point is defined in the context of an underlying data distribution. We prove that distributional Shapley has several desirable statistical properties; for example, the values are stable under perturbations to the data points themselves and to the underlying data distribution. We leverage these properties to develop a new algorithm for estimating values from data, which comes with formal guarantees and runs two orders of magnitude faster than state-of-the-art algorithms for computing the (non-distributional) data Shapley values. We apply distributional Shapley to diverse data sets and demonstrate its utility in a data market setting.
","['Stanford', 'Stanford University', 'Stanford University']"
2020,Decision Trees for Decision-Making under the Predict-then-Optimize Framework,"Adam Elmachtoub, Jason Cheuk Nam Liang, Ryan McNellis",https://icml.cc/Conferences/2020/Schedule?showEvent=6758,"We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error.
","['Columbia University', 'MIT', 'Amazon']"
2020,Concept Bottleneck Models,"Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6816,"We seek to learn models that we can interact with using high-level concepts: would the model predict severe arthritis if it thinks there is a bone spur in the x-ray? State-of-the-art models today do not typically support the manipulation of concepts like ""the existence of bone spurs'', as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models
by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (""bone spurs"") or bird attributes (""wing color""). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.
","['Stanford University', 'Google', 'Stanford University', 'Stanford University', 'Stanford', 'Google', 'Stanford University']"
2020,Symbolic Network: Generalized Neural Policies for Relational MDPs,"Sankalp Garg, Aniket Bajpai, Mausam ",https://icml.cc/Conferences/2020/Schedule?showEvent=5784,"A Relational Markov Decision Process (RMDP) is a first-order representation to express all instances of a single probabilistic planning domain with possibly unbounded number of objects. Early work in RMDPs outputs generalized (instance-independent) first-order policies or value functions as a means to solve all instances of a domain at once. Unfortunately, this line of work met with limited success due to inherent limitations of the representation space used in such policies or value functions. Can neural models provide the missing link by easily representing more complex generalized policies, thus making them effective on all instances of a given domain?
We present SymNet, the first neural approach for solving RMDPs that are expressed in the probabilistic planning language of RDDL. SymNet trains a set of shared parameters for an RDDL domain using training instances from that domain. For each instance, SymNet first converts it to an instance graph and then uses relational neural models to compute node embeddings. It then scores each ground action as a function over the first-order action symbols and node embeddings related to the action. Given a new test instance from the same domain, SymNet architecture with pre-trained parameters scores each ground action and chooses the best action.  This can be accomplished in a single forward pass without any retraining on the test instance, thus implicitly representing a neural generalized policy for the whole domain. Our experiments on nine RDDL domains from IPPC demonstrate that SymNet policies are significantly better than random and sometimes even more effective than training a state-of-the-art deep reactive policy from scratch.
","['Indian Institute of Technology Delhi', 'Indian Institute of Technology, Delhi', 'IIT Delhi']"
2020,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,"Michael Laskin, Aravind Srinivas, Pieter Abbeel",https://icml.cc/Conferences/2020/Schedule?showEvent=6729,"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://www.github.com/MishaLaskin/curl.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley & Covariant']"
2020,Active World Model Learning in Agent-rich Environments with Progress Curiosity,"Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, Daniel Yamins",https://icml.cc/Conferences/2020/Schedule?showEvent=6064,"World models are self-supervised predictive models of how the world evolves. Humans learn world models by curiously exploring their environment, in the process acquiring compact abstractions of high bandwidth sensory inputs, the ability to plan across long temporal horizons, and an understanding of the behavioral patterns of other agents. In this work, we study how to design such a curiosity-driven Active World Model Learning (AWML) system. To do so, we construct a curious agent building world models while visually exploring a 3D physical environment rich with distillations of representative real-world agents. We propose an AWML system driven by $\gamma$-Progress: a scalable and effective learning progress-based curiosity signal and show that $\gamma$-Progress naturally gives rise to an exploration policy that directs attention to complex but learnable dynamics in a balanced manner, as a result overcoming the ``white noise problem''. As a result, our $\gamma$-Progress-driven controller achieves significantly higher AWML performance than baseline controllers equipped with state-of-the-art exploration strategies such as Random Network Distillation and Model Disagreement. ","['Stanford University', 'Stanford University', 'Harvard University', 'Stanford University', 'Stanford University']"
2020,Aligned Cross Entropy for Non-Autoregressive Machine Translation,"Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, Omer Levy",https://icml.cc/Conferences/2020/Schedule?showEvent=6635,"Non-autoregressive machine translation models significantly speed up decoding by allowing for parallel prediction of the entire target sequence. However, modeling word order is more challenging due to the lack of autoregressive factors in the model. This difficultly is compounded during training with cross entropy loss, which can highly penalize small shifts in word order. In this paper, we propose aligned cross entropy (AXE) as an alternative loss function for training of non-autoregressive models. AXE uses a differentiable dynamic program to assign loss based on the best possible monotonic alignment between target tokens and model predictions. AXE-based training of conditional masked language models (CMLMs) substantially improves performance on major WMT benchmarks, while setting a new state of the art for non-autoregressive models.
","['Facebook AI Research (FAIR)', 'Facebook AI Research', 'Facebook', 'Facebook']"
2020,Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation,"Jian Liang, Dapeng Hu, Jiashi Feng",https://icml.cc/Conferences/2020/Schedule?showEvent=5793,"Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain.
Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data.
This work tackles a novel setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems.
We propose a simple yet generic representation learning framework, named Source HypOthesis Transfer (SHOT).
SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. 
To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation.
Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.
","['NUS', 'NUS', 'National University of Singapore']"
2020,Improving the Sample and Communication Complexity for Decentralized Non-Convex Optimization: Joint Gradient Estimation and Tracking,"Haoran Sun, Songtao Lu, Mingyi Hong",https://icml.cc/Conferences/2020/Schedule?showEvent=6030,"Many modern large-scale machine learning problems benefit from decentralized and stochastic optimization. Recent works have shown that utilizing both decentralized computing and local stochastic gradient estimates can outperform state-of-the-art centralized algorithms, in applications involving highly non-convex problems, such as training deep neural networks. 	
In this work, we propose a decentralized stochastic algorithm to deal with certain smooth non-convex problems where there are $m$ nodes in the system, and each node has a large number of samples (denoted as $n$). Differently from the majority of the existing decentralized learning algorithms for either stochastic or finite-sum problems, our focus is given to {\it both} reducing the total communication rounds among the nodes, while accessing the minimum number of local data samples. In particular, we propose an algorithm named D-GET (decentralized gradient estimation and tracking), which jointly performs decentralized gradient estimation (which estimates the local gradient using a subset of local samples) {\it and} gradient tracking (which tracks the global full gradient using local estimates). We show that to achieve certain $\epsilon$  stationary solution of the deterministic finite sum problem, the proposed algorithm achieves an $\mathcal{O}(mn^{1/2}\epsilon^{-1})$ sample complexity and an $\mathcal{O}(\epsilon^{-1})$ communication complexity. These bounds significantly improve upon the best existing bounds of $\mathcal{O}(mn\epsilon^{-1})$ and $\mathcal{O}(\epsilon^{-1})$, respectively. Similarly, for online problems, the proposed method achieves an $\mathcal{O}(m \epsilon^{-3/2})$ sample complexity and  an $\mathcal{O}(\epsilon^{-1})$ communication complexity.","['University of Minnesota', 'IBM Research', 'University of Minnesota']"
2020,Energy-Based Processes for Exchangeable Data,"Mengjiao Yang, Bo Dai, Hanjun Dai, Dale Schuurmans",https://icml.cc/Conferences/2020/Schedule?showEvent=6254,"Recently there has been growing interest in modeling sets with exchangeability such as point clouds. A shortcoming of current approaches is that they restrict the cardinality of the sets considered or can only express limited forms of distribution over unobserved data. To overcome these limitations, we introduce Energy-Based Processes (EBPs), which extend energy based models to exchangeable data while allowing neural network parameterizations of the energy function. A key advantage of these models is the ability to express more flexible distributions over sets without restricting their cardinality. We develop an efficient training procedure for EBPs that demonstrates state-of-the-art performance on a variety of tasks such as point cloud generation, classification, denoising, and image completion
","['Google Brain', 'Google Brain', 'Google Brain', 'Google / University of Alberta']"
2020,Learning to Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning,"Sai Krishna Gottipati, Boris Sattarov, Sufeng Niu, Yashaswi Pathak, Haoran Wei, Shengchao Liu, Shengchao Liu, Simon Blackburn, Karam Thomas, Connor Coley, Jian Tang, Sarath Chandar, Yoshua Bengio",https://icml.cc/Conferences/2020/Schedule?showEvent=6767,"Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in generative modeling of novel chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel reinforcement learning (RL) setup for de novo drug design: Policy Gradient for Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and logP. Moreover, we put to test PGFS in an in-silico proof-of-concept associated with three HIV targets, and the candidates generated with PGFS outperformed the existing benchmarks in optimizing the activity against the biological targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.
","['99andBeyond', '99andBeyond', 'Linkedin', 'International Institute of Information Technology,Hyderabad', 'University of Delaware', 'MILA-UdeM', 'Mila, Université de Montréal', 'Mila', '99andBeyond', 'MIT', 'HEC Montreal & MILA', 'Mila / École Polytechnique de Montréal', 'Montreal Institute for Learning Algorithms']"
2020,On the Global Convergence Rates of Softmax Policy Gradient Methods,"Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, Dale Schuurmans",https://icml.cc/Conferences/2020/Schedule?showEvent=6712,"We make three contributions toward better understanding policy gradient methods in the tabular setting.
First, we show that with the true gradient, policy gradient with a softmax parametrization converges at a $O(1/t)$ rate, with constants depending on the problem and initialization.
This result significantly expands the recent asymptotic convergence results. 
The analysis relies on two findings:
that the softmax policy gradient satisfies a \L{}ojasiewicz inequality, and the minimum probability of an optimal action during optimization can be bounded in terms of its initial value.
Second, we analyze entropy regularized policy gradient and show that it enjoys a significantly faster linear convergence rate $O(e^{-t})$ toward softmax optimal policy.
This result resolves an open question in the recent literature.
Finally, combining the above two results and additional new $\Omega(1/t)$ lower bound results, we explain how entropy regularization improves policy optimization, even with the true gradient, from the perspective of convergence rate. The separation of rates is further explained using the notion of non-uniform \L{}ojasiewicz degree. 
These results provide a theoretical understanding of the impact of entropy and corroborate existing empirical studies.","['University of Alberta / Google Brain', 'Google / University of Alberta', 'DeepMind/University of Alberta', 'University of Alberta']"
2020,Video Prediction via Example Guidance,"Jingwei Xu, Harry (Huazhe) Xu, Bingbing Ni, Xiaokang Yang, Trevor Darrell",https://icml.cc/Conferences/2020/Schedule?showEvent=6222,"In video prediction tasks, one major challenge is to capture the multi-modal nature of future contents and dynamics. In this work, we propose a simple yet effective framework that can efficiently predict plausible future states. The key insight is that the potential distribution of a sequence could be approximated with analogous ones in a repertoire of training pool, namely, expert examples. By further incorporating a novel optimization scheme into the training procedure, plausible predictions can be sampled efficiently from distribution constructed from the retrieved examples. Meanwhile, our method could be seamlessly integrated with existing stochastic predictive models; significant enhancement is observed with comprehensive experiments in both quantitative and qualitative aspects. We also demonstrate the generalization ability to predict the motion of unseen class, i.e., without access to corresponding data during training phase.
","['Shanghai Jiao Tong University', 'UC Berkeley', 'Shanghai Jiao Tong University', 'Shanghai Jiao Tong University of China', 'University of California at Berkeley']"
2020,Bio-Inspired Hashing for Unsupervised Similarity Search,"Chaitanya Ryali, John Hopfield, Leopold Grinberg, Dmitry Krotov",https://icml.cc/Conferences/2020/Schedule?showEvent=6580,"The fruit fly Drosophila's olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes, FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However, FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology, our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a  data-driven manner. We show that  BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule, our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance.  From the perspective of computer science, BioHash and BioConvHash are  fast, scalable and yield compressed binary representations that are useful for similarity search.
","['UC San Diego', 'Princeton University', 'IBM Research', 'IBM Research']"
2020,ECLIPSE: An Extreme-Scale Linear Program Solver for Web-Applications,"Kinjal Basu, Amol Ghoting, Rahul Mazumder, Yao Pan",https://icml.cc/Conferences/2020/Schedule?showEvent=6728,"Key problems arising in web applications (with millions of users and thousands of items) can be formulated as Linear Programs (LP) involving billions to trillions of decision variables and constraints. Despite the appeal of LP formulations, solving problems at these scales appears to be well beyond the capabilities of existing LP solvers. Often ad-hoc decomposition rules are used to approximately solve these LPs, which have limited optimality guarantees and lead to a sub-optimal performance in practice. In this work, we propose a distributed solver that solves a perturbation of the LP problems at scale via a gradient-based algorithm on the smooth dual of the perturbed LP with computational guarantees. The main workhorses of our algorithm are distributed matrix-vector multiplications (with load balancing) and efficient projection operations on distributed machines. Experiments on real-world data show that our proposed LP solver, ECLIPSE, can solve problems with $10^{12}$ decision variables -- well beyond the capabilities of current solvers.","['LinkedIn Corporation', 'LinkedIn', 'Massachusetts Institute of Technology', 'LinkedIn Corporation']"
2020,When Does Self-Supervision Help Graph Convolutional Networks?,"Yuning You, Tianlong Chen, Zhangyang Wang, Yang Shen",https://icml.cc/Conferences/2020/Schedule?showEvent=5947,"Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into  GCNs. We first elaborate three mechanisms to incorporate
self-supervision into GCNs, analyze the limitations of pretraining & finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel self-supervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https://github.com/Shen-Lab/SS-GCNs.
","['Texas A&M University', 'Texas A&M University', 'University of Texas at Austin', 'Texas A&M University']"
2020,The Implicit Regularization of Stochastic Gradient Flow for Least Squares,"Alnur Ali, Edgar Dobriban, Ryan Tibshirani",https://icml.cc/Conferences/2020/Schedule?showEvent=6211,"We study the implicit regularization of mini-batch stochastic gradient descent, when applied to the fundamental problem of least squares regression.  We leverage a continuous-time stochastic differential equation having the same moments as stochastic gradient descent, which we call stochastic gradient flow.  We give a bound on the excess risk of stochastic gradient flow at time $t$, over ridge regression with tuning parameter $\lambda = 1/t$.  The bound may be computed from explicit constants (e.g., the mini-batch size, step size, number of iterations), revealing precisely how these quantities drive the excess risk.  Numerical examples show the bound can be small, indicating a tight relationship between the two estimators.  We give a similar result relating the coefficients of stochastic gradient flow and ridge.  These results hold under no conditions on the data matrix $X$, and across the entire optimization path (not just at convergence).","['Stanford University', 'University of Pennsylvania', 'Carnegie Mellon University']"
2020,PowerNorm: Rethinking Batch Normalization in Transformers,"Sheng Shen, Zhewei Yao, Amir Gholaminejad, Michael Mahoney, Kurt Keutzer",https://icml.cc/Conferences/2020/Schedule?showEvent=6189,"The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN).This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident.
In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented.
To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN,
(ii) incorporating a running quadratic mean instead of per batch statistics to stabilize
fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. 
In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at https://github.com/sIncerass/powernorm.
","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'UC Berkeley', 'UC Berkeley']"
2020,Randomized Block-Diagonal Preconditioning for Parallel Learning,"Celestine Mendler-Dünner, Aurelien Lucchi",https://icml.cc/Conferences/2020/Schedule?showEvent=5769,"We study preconditioned gradient-based optimization methods where the preconditioning matrix has block-diagonal form. Such a structural constraint comes with the advantage that the update computation can be parallelized across multiple independent tasks. Our main contribution is to demonstrate that the convergence of these methods can significantly be improved by a randomization technique which corresponds to repartitioning coordinates across tasks during the optimization procedure. We provide a theoretical analysis that accurately characterizes the expected convergence gains of repartitioning and validate our findings empirically on various traditional machine learning tasks. From an implementation perspective, block-separable models are well suited for parallelization and, when shared memory is available, randomization can be implemented on top of existing methods very efficiently to improve convergence.
","['University of California, Berkeley', 'ETH Zurich']"
2020,Real-Time Optimisation for Online Learning in Auctions,"Lorenzo Croissant, Marc Abeille, Clément Calauzènes",https://icml.cc/Conferences/2020/Schedule?showEvent=6026,"In display advertising, a small group of sellers and bidders face each other in up to 10^{12} auctions a day. In this context, revenue maximisation via monopoly price learning is a high-value problem for sellers. By nature, these auctions are online and produce a very high frequency stream of data. This results in a computational strain that requires algorithms be real-time. Unfortunately, existing methods, inherited from the batch setting, suffer O(\sqrt(t)) time/memory complexity at each update, prohibiting their use. In this paper, we provide the first algorithm for online learning of monopoly prices in online auctions whose update is constant in time and memory.
","['Criteo AI Lab', 'Criteo AI Lab', 'Criteo AI Lab']"
2020,Neural Kernels Without Tangents,"Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan Ragan-Kelley, Ludwig Schmidt, Benjamin Recht",https://icml.cc/Conferences/2020/Schedule?showEvent=6356,"We investigate the connections between neural networks and simple building blocks in kernel space. In particular, using well established feature space tools such as direct sum, averaging, and moment lifting, we present an algebra for creating “compositional” kernels from bags of features. We show that these operations correspond to many of the building blocks of “neural tangent kernels (NTK)”. Experimentally, we show that there is a correlation in test error between neural network architectures and the associated kernels. We construct a simple neural network architecture using only 3x3 convolutions, 2x2 average pooling, ReLU, and optimized with SGD and MSE loss that achieves 96% accuracy on CIFAR10, and whose corresponding compositional kernel achieves 90% accuracy. We also use our constructions to investigate the relative performance of neural networks, NTKs, and compositional kernels in the small dataset regime. In particular, we find that compositional kernels outperform NTKs and neural networks outperform both kernel methods.
","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'University of California, Berkeley', 'Berkeley']"
2020,Gradient-free Online Learning in Continuous Games with Delayed Rewards,"Amélie Héliou, Panayotis Mertikopoulos, Zhengyuan Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=6416,"Motivated by applications to online advertising and recommender systems, we consider a game-theoretic model with delayed rewards and asynchronous, payoff-based feedback. In contrast to previous work on delayed multi-armed bandits, we focus on games with continuous action spaces, and we examine the long-run behavior of strategic agents that follow a no-regret learning policy (but are otherwise oblivious to the game being played, the objectives of their opponents, etc.). To account for the lack of a consistent stream of information (for instance, rewards can arrive out of order and with an a priori unbounded delay), we introduce a gradient-free learning policy where payoff information is placed in a priority queue as it arrives. Somewhat surprisingly, we find that under a standard diagonal concavity assumption, the induced sequence of play converges to Nash Equilibrium (NE) with probability 1, even if the delay between choosing an action and receiving the corresponding reward is unbounded.
","['Criteo', 'CNRS and Criteo AI Lab', 'Stanford University']"
2020,Learning Flat Latent Manifolds with VAEs,"Nutan Chen, Alexej Klushyn, Francesco Ferroni, Justin Bayer, Patrick van der Smagt",https://icml.cc/Conferences/2020/Schedule?showEvent=6374,"Measuring the similarity between data points often requires domain knowledge, which can in parts be compensated by relying on unsupervised methods such as latent-variable models, where similarity/distance is estimated in a more compact latent space. Prevalent is the use of the Euclidean metric, which has the drawback of ignoring information about similarity of data stored in the decoder, as captured by the framework of Riemannian geometry. We propose an extension to the framework of variational auto-encoders allows learning flat latent manifolds, where the Euclidean metric is a proxy for the similarity between data points. This is achieved by defining the latent space as a Riemannian manifold and by regularising the metric tensor to be a scaled identity matrix. Additionally, we replace the compact prior typically used in variational auto-encoders with a recently presented, more expressive hierarchical one---and formulate the learning problem as a constrained optimisation problem. We evaluate our method on a range of data-sets, including a video-tracking benchmark, where the performance of our unsupervised approach nears that of state-of-the-art supervised approaches, while retaining the computational efficiency of straight-line-based approaches.
","['Volkswagen AG', 'ML Research, VW Group', 'Argo AI', 'Volkswagen Group', 'Volkswagen Group']"
2020,"Efficient Robustness Certificates for Discrete Data: Sparsity-Aware Randomized Smoothing for Graphs, Images and More","Aleksandar Bojchevski, Johannes Klicpera, Stephan Günnemann",https://icml.cc/Conferences/2020/Schedule?showEvent=6848,"Existing techniques for certifying the robustness of models for discrete data either work only for a small class of models or are general at the expense of efficiency or tightness. Moreover, they do not account for sparsity in the input which, as our findings show, is often essential for obtaining non-trivial guarantees. We propose a model-agnostic certificate based on the randomized smoothing framework which subsumes earlier work and is tight, efficient, and sparsity-aware. Its computational complexity does not depend on the number of discrete categories or the dimension of the input (e.g. the graph size), making it highly scalable. We show the effectiveness of our approach on a wide variety of models, datasets, and tasks -- specifically highlighting its use for Graph Neural Networks. So far, obtaining provable guarantees for GNNs has been difficult due to the discrete and non-i.i.d. nature of graph data. Our method can certify any GNN and handles perturbations to both the graph structure and the node attributes.
","['Technical University of Munich', 'Technical University Munich', 'Technical University of Munich']"
2020,Bisection-Based Pricing for Repeated Contextual Auctions against Strategic Buyer,"Anton Zhiyanov, Alexey Drutsa",https://icml.cc/Conferences/2020/Schedule?showEvent=6117,"We are interested in learning algorithms that optimize revenue in repeated contextual posted-price auctions where a single seller faces a single strategic buyer.
In our setting, the buyer  maximizes his expected cumulative discounted surplus, and his valuation of a good is assumed to be a fixed function of a $d$-dimensional context (feature) vector. 
We introduce a novel deterministic learning algorithm that is based on ideas of the Bisection method and has strategic regret upper bound of $O(\log^2 T)$.
Unlike previous works, our algorithm does not require any assumption on the distribution of context information, and the regret guarantee holds for any realization of feature vectors (adversarial upper bound).
To construct our algorithm we non-trivially adopted techniques of integral geometry to act against buyer strategicness and improved the penalization trick to work in contextual auctions.","['Yandex Research, MSU im. Lomonosova', 'Yandex']"
2020,Data-Efficient Image Recognition with Contrastive Predictive Coding,Olivier Henaff,https://icml.cc/Conferences/2020/Schedule?showEvent=6385,"Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers. 
",['DeepMind']
2020,Revisiting Training Strategies and Generalization Performance in Deep Metric Learning,"Karsten Roth, Timo Milbich, Samrath Sinha, Prateek Gupta, Bjorn Ommer, Joseph Paul Cohen",https://icml.cc/Conferences/2020/Schedule?showEvent=6045,"Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets; code and a publicly accessible WandB-repo are available at https://github.com/Confusezius/RevisitingDeepMetricLearningPyTorch.
","['Heidelberg University, Mila', 'Heidelberg University', 'University of Toronto', 'University of Oxford', 'Heidelberg University', 'Mila, University of Montreal']"
2020,Learning to Rank Learning Curves,"Martin Wistuba, Tejaswini Pedapati",https://icml.cc/Conferences/2020/Schedule?showEvent=6155,"Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model configurations. In this work, we present a new method that saves computational budget by terminating poor configurations early on in the training. In contrast to existing methods, we consider this task as a ranking and transfer learning problem. We qualitatively show that by optimizing a pairwise ranking loss and leveraging learning curves from other data sets, our model is able to effectively rank learning curves without having to observe many or very long learning curves. We further demonstrate that our method can be used to accelerate a neural architecture search by a factor of up to 100 without a significant performance degradation of the discovered architecture. In further experiments we analyze the quality of ranking, the influence of different model components as well as the predictive behavior of the model.
","['IBM Research', 'IBM Research']"
2020,Naive Exploration is Optimal for Online LQR,"Max Simchowitz, Dylan Foster",https://icml.cc/Conferences/2020/Schedule?showEvent=5930,"We consider the problem of online adaptive control of the linear quadratic regulator, where the true system parameters are unknown. We prove new upper and lower bounds demonstrating that the optimal regret scales as $\tilde{\Theta ({\sqrt{d_{\mathbf{u}}^2 d_{\mathbf{x}} T}})$, where $T$ is the number of time steps, $d_{\mathbf{u}}$ is the dimension of the input space, and $d_{\mathbf{x}}$ is the dimension of the system state.  Notably, our lower bounds rule out the possibility of a $\mathrm{poly}(\log{}T)$-regret algorithm, which had been conjectured due to the apparent strong convexity of the problem. Our upper bound is attained by a simple variant of certainty equivalent control, where the learner selects control inputs according to  the optimal controller for their estimate of the system while injecting exploratory random noise. While this approach was shown to achieve $\sqrt{T}$ regret by Mania et al. (2019),  we show that if the learner continually refines their estimates of the system matrices, the method attains optimal dimension dependence as well.

Central to our upper and lower bounds is a new approach for controlling perturbations of Riccati equations called the self-bounding ODE method, which we use to derive suboptimality bounds for the certainty equivalent controller synthesized from estimated system dynamics. This in turn enables regret upper bounds which hold for any stabilizable instance and scale with natural control-theoretic quantities.","['UC Berkeley', 'MIT']"
2020,Online Continual Learning from Imbalanced Data,"Aristotelis Chrysakis, Marie-Francine Moens",https://icml.cc/Conferences/2020/Schedule?showEvent=6540,"A well-documented weakness of neural networks is the fact that they suffer from catastrophic forgetting when trained on data provided by a non-stationary distribution. Recent work in the field of continual learning attempts to understand and overcome this issue. Unfortunately, the majority of relevant work embraces the implicit assumption that the distribution of observed data is perfectly balanced, despite the fact that, in the real world, humans and animals learn from observations that are temporally correlated and severely imbalanced. Motivated by this remark, we aim to evaluate memory population methods that are used in online continual learning, when dealing with highly imbalanced and temporally correlated streams of data. More importantly, we introduce a new memory population approach, which we call class-balancing reservoir sampling (CBRS). We demonstrate that CBRS outperforms the state-of-the-art memory population algorithms in a considerably challenging learning setting, over a range of different datasets, and for multiple architectures.
","['KU Leuven', 'KU Leuven']"
2020,Near-optimal Regret Bounds for Stochastic Shortest Path,"Aviv Rosenberg, Alon Cohen, Yishay Mansour, Haim Kaplan",https://icml.cc/Conferences/2020/Schedule?showEvent=6432,"Stochastic shortest path (SSP) is a well-known problem in planning and control, in which an agent has to reach a goal state in minimum total expected cost.
In the learning formulation of the problem, the agent is unaware of the environment dynamics (i.e., the transition function) and has to repeatedly play for a given number of episodes, while learning  the problem's optimal solution. 
Unlike other well-studied models in reinforcement learning (RL), the length of an episode is not predetermined (or bounded) and is influenced by the agent's actions. 
Recently, \cite{tarbouriech2019noregret} studied this problem in the context of regret minimization, and provided an algorithm whose regret bound is inversely proportional to the square root of the minimum instantaneous cost.
In this work we  remove this dependence on the minimum cost---we give an algorithm that guarantees a regret bound of $\widetilde{O}(B^{3/2} S \sqrt{A K})$, where $B$ is an upper bound on the expected cost of the optimal policy, $S$ is the number of states, $A$ is the number of actions and $K$  is the total number of episodes. We additionally show that any learning algorithm must have at least $\Omega(B \sqrt{S A K})$ regret in the worst case.","['Tel Aviv University', 'Technion and Google', 'Google and Tel Aviv University', 'TAU, GOOGLE']"
2020,Double-Loop Unadjusted Langevin Algorithm,"Paul Rolland, Armin Eftekhari, Ali Kavis, Volkan Cevher",https://icml.cc/Conferences/2020/Schedule?showEvent=6167,"A well-known first-order method for sampling from  log-concave probability distributions is the Unadjusted Langevin Algorithm (ULA). This work 
proposes a new annealing step-size schedule for ULA, which allows to prove new convergence guarantees for sampling from a smooth log-concave distribution, which are not covered by existing state-of-the-art convergence guarantees. To establish this result, we derive a new theoretical bound that relates the Wasserstein distance to total variation distance between any two log-concave distributions that complements the reach of Talagrand $T_2$ inequality. Moreover, applying this new step size schedule to an existing constrained sampling algorithm, we show state-of-the-art convergence rates for sampling from a constrained log-concave distribution, as well as improved dimension dependence.  ","['Ecole Polytechnique Fédérale de Lausanne', 'Umea University', 'EPFL', 'EPFL']"
2020,On Thompson Sampling with Langevin Algorithms,"Eric Mazumdar, Aldo Pacchiano, Yian Ma, Michael Jordan, Peter Bartlett",https://icml.cc/Conferences/2020/Schedule?showEvent=6778,"Thompson sampling for multi-armed bandit problems is known to enjoy favorable performance in both theory and practice, though it suffers from a significant limitation computationally arising from the need for samples from posterior distributions at every iteration. To address this issue, we propose two Markov Chain Monte Carlo (MCMC) methods tailored to Thompson sampling. We construct quickly converging Langevin algorithms to generate approximate samples that have accuracy guarantees, and leverage novel posterior concentration rates to analyze the regret of the resulting approximate Thompson sampling algorithm. Further, we specify the necessary hyperparameters for the MCMC procedure to guarantee optimal instance-dependent frequentist regret while having low computational complexity. In particular, our algorithms take advantage of both posterior concentration and a sample reuse mechanism to ensure that only a constant number of iterations and a constant amount of data is needed in each round. The resulting approximate Thompson sampling algorithm has logarithmic regret and its computational complexity does not scale with the time horizon of the algorithm.
","['University of California Berkeley', 'UC Berkeley', 'Google', 'UC Berkeley', 'UC Berkeley']"
2020,Soft Threshold Weight Reparameterization for Learnable Sparsity,"Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, Ali Farhadi",https://icml.cc/Conferences/2020/Schedule?showEvent=5772,"Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at https://github.com/RAIVNLab/STR.
","['University of Washington', 'Allen Institute for Artificial Intelligence', 'University of Washington', 'University of Washington', 'Microsoft Research', 'University of Washington', 'University of Washington, Allen Institue for AI']"
2020,Supervised Quantile Normalization for Low Rank Matrix Factorization,"Marco Cuturi, Olivier Teboul, Jonathan Niles-Weed, Jean-Philippe Vert",https://icml.cc/Conferences/2020/Schedule?showEvent=6638,"Low rank matrix factorization is a fundamental building block in machine learning, used for instance to summarize gene expression profile data or word-document counts. To be robust to outliers and differences in scale across features, a matrix factorization step is usually preceded by ad-hoc feature normalization steps, such as tf-idf scaling or data whitening. We propose in this work to learn these normalization operators jointly with the factorization itself. More precisely, given a $d\times n$ matrix $X$ of $d$ features measured on $n$ individuals, we propose to learn the parameters of quantile normalization operators that can operate row-wise on the values of $X$ and/or of its factorization $UV$  to improve the quality of the low-rank representation of $X$ itself. This optimization is facilitated by the introduction of differentiable quantile normalization operators derived using regularized optimal transport algorithms.","['Google', 'Google Brain', 'NYU', 'Google']"
2020,Composable Sketches for Functions of Frequencies: Beyond the Worst Case,"Edith Cohen, Ofir Geri, Rasmus Pagh",https://icml.cc/Conferences/2020/Schedule?showEvent=6124,"Recently there has been increased interest in using machine learning techniques to improve classical algorithms. In this paper we study when it is possible to construct compact, composable sketches for weighted sampling and statistics estimation according to functions of data frequencies. Such structures are now central components of large-scale data analytics and machine learning pipelines. However, many common functions, such as thresholds and $p$th frequency moments with $p>2$, are known to require polynomial size sketches in the worst case. We explore performance beyond the worst case under two different types of assumptions. The first is having access to noisy \emph{advice} on item frequencies. This continues the line of work of Hsu et al. (ICLR 2019), who assume predictions are provided by a machine learning model. The second is providing guaranteed performance on a restricted class of input frequency distributions that are better aligned with what is observed in practice. This extends the work on heavy hitters under Zipfian distributions in a seminal paper of Charikar et al. (ICALP 2002). Surprisingly, we show analytically and empirically that ""in practice"" small polylogarithmic-size sketches provide accuracy for ""hard"" functions.","['Google Research and Tel Aviv University', 'Stanford University', 'IT University of Copenhagen']"
2020,Topologically Densified Distributions,"Christoph Hofer, Florian Graf, Marc Niethammer, Roland Kwitt",https://icml.cc/Conferences/2020/Schedule?showEvent=6426,"We study regularization in the context of small sample-size learning with over-parametrized neural networks. Specifically, we shift focus from architectural properties, such as norms on the network weights, to properties of the internal representations before a linear classifier. Specifically, we impose a topological constraint on samples drawn from the probability measure induced in that space. This provably leads to mass concentration effects around the representations of training instances, i.e., a property beneficial for generalization. By leveraging previous work to impose topological constrains in a neural network setting, we provide empirical evidence (across various vision benchmarks) to support our claim for better generalization.
","['University of Salzburg', 'University of Salzburg', 'UNC', '""University of Salzburg, Austria""']"
2020,Frequency Bias in Neural Networks for Input of Non-Uniform Density,"Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, Shira Kritchman",https://icml.cc/Conferences/2020/Schedule?showEvent=6225,"Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias -- networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency $\kappa$, convergence at a point $x \in \S^{d-1}$ occurs in time $O(\kappa^d/p(x))$ where $p(x)$ denotes the local density at $x$. Specifically, for data in $\S^1$ we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.","['Weizmann Institute of Science', 'Weizmann Institute of Science', 'Weizmann Institute', 'University of Maryland, USA', 'Weizmann Institute', 'Weizmann Institute']"
2020,Optimistic Bounds for Multi-output Learning,"Henry Reeve, Ata Kaban",https://icml.cc/Conferences/2020/Schedule?showEvent=6702,"We investigate the challenge of multi-output learning, where the goal is to learn a vector-valued function based on a supervised data set. This includes a range of important problems in Machine Learning including multi-target regression, multi-class classification and multi-label classification. We begin our analysis by introducing the self-bounding Lipschitz condition for multi-output loss functions, which interpolates continuously between a classical Lipschitz condition and a multi-dimensional analogue of a smoothness condition. We then show that the self-bounding Lipschitz condition gives rise to optimistic bounds for multi-output learning, which attain the minimax optimal rate up to logarithmic factors. The proof exploits local Rademacher complexity combined with a powerful minoration inequality due to Srebro, Sridharan and Tewari. As an application we derive a state-of-the-art generalisation bound for multi-class gradient boosting.
","['University of Birmingham', 'University of Birmingham']"
2020,Conditional gradient methods for stochastically constrained convex minimization,"Maria-Luiza Vladarean, Ahmet Alacaoglu, Ya-Ping Hsieh, Volkan Cevher",https://icml.cc/Conferences/2020/Schedule?showEvent=5974,"We propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints. Instances of this template naturally arise from SDP-relaxations of combinatorial problems, which involve a number of constraints that is polynomial in the problem dimension. The most important feature of our framework is that only a subset of the constraints is processed at each iteration, thus gaining a computational advantage over prior works that require full passes. Our algorithms rely on variance reduction and smoothing used in conjunction with conditional gradient steps, and are accompanied by rigorous convergence guarantees. Preliminary numerical experiments are provided for illustrating the practical performance of the methods.
","['EPFL', 'EPFL', 'EPFL', 'EPFL']"
2020,Low Bias Low Variance Gradient Estimates for Hierarchical Boolean Stochastic Networks,"Adeel Pervez, Taco Cohen, Efstratios Gavves",https://icml.cc/Conferences/2020/Schedule?showEvent=6373,"Stochastic neural networks with discrete random variables are an important class of models for their expressiveness and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation techniques are a popular alternative. Efficient stochastic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow stochastic models. Their performance, however, suffers with hierarchical, more complex models.
We focus on hierarchical stochastic networks with multiple layers of Boolean latent variables. To analyze such networks, we introduce the framework of harmonic analysis for Boolean functions to derive an analytic formulation for the bias and variance in the Straight-Through estimator. Exploiting these formulations, we propose \emph{FouST}, a low-bias and low-variance gradient estimation algorithm that is just as efficient. Extensive experiments show that FouST performs favorably compared to state-of-the-art biased estimators and is much faster than unbiased ones.
","['University of Amsterdam', 'Qualcomm', 'University of Amsterdam']"
2020,Information Particle Filter Tree: An Online Algorithm for POMDPs with Belief-Based Rewards on Continuous Domains,"Johannes Fischer, Ömer Sahin Tas",https://icml.cc/Conferences/2020/Schedule?showEvent=6098,"Planning in Partially Observable Markov Decision Processes (POMDPs) inherently gathers the information necessary to act optimally under uncertainties. The framework can be extended to model pure information gathering tasks by considering belief-based rewards. This allows us to use reward shaping to guide POMDP planning to informative beliefs by using a weighted combination of the original reward and the expected information gain as the objective. In this work we propose a novel online algorithm, Information Particle Filter Tree (IPFT), to solve problems with belief-dependent rewards on continuous domains. It simulates particle-based belief trajectories in a Monte Carlo Tree Search (MCTS) approach to construct a search tree in the belief space. The evaluation shows that the consideration of information gain greatly improves the performance in problems where information gathering is an essential part of the optimal policy.
","['Karlsruhe Institute of Technology (KIT)', 'FZI Research Center for Information Technology']"
2020,The Complexity of Finding Stationary Points with Stochastic Gradient Descent,"Yoel Drori, Ohad Shamir",https://icml.cc/Conferences/2020/Schedule?showEvent=5821,"We study the iteration complexity of stochastic gradient descent (SGD) for minimizing the gradient norm of smooth, possibly nonconvex functions. We provide several results, implying that the classical $\mathcal{O}(\epsilon^{-4})$ upper bound (for making the average gradient norm less than $\epsilon$) cannot be improved upon, unless a combination of additional assumptions is made. Notably, this holds even if we limit ourselves to convex quadratic functions. We also show that for nonconvex functions, the feasibility of minimizing gradients with SGD is surprisingly sensitive to the choice of optimality criteria.","['Google Research', 'Weizmann Institute of Science']"
2020,Aggregation of Multiple Knockoffs,"Tuan-Binh Nguyen, Jerome-Alexis Chevalier, Thirion Bertrand, Sylvain Arlot",https://icml.cc/Conferences/2020/Schedule?showEvent=5954,"We develop an extension of the knockoff inference procedure, introduced by Barber & Candes (2015). This new method, called Aggregation of Multiple Knockoffs (AKO), addresses the instability inherent to the random nature of knockoff-based inference. Specifically, AKO improves both the stability and power compared with the original knockoff algorithm while still maintaining guarantees for false discovery rate control. We provide a new inference procedure, prove its core properties, and demonstrate its benefits in a set of experiments on synthetic and real datasets.
","['INRIA Saclay Ile-de-France', 'INRIA Saclay Ile-de-France', 'inria', 'University Paris Sud']"
2020,Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules,"Sarthak  Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie, Michael Mozer, Yoshua Bengio",https://icml.cc/Conferences/2020/Schedule?showEvent=6039,"Robust perception relies on both bottom-up and top-down signals.  Bottom-up signals consist of what's directly observed through sensation.  Top-down signals consist of beliefs and expectations based on past experience and the current reportable short-term memory, such as how the phrase `peanut butter and ...' will be completed.  The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow.  We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention.  Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data.  We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emph{bidirectional} information flow can improve results over strong baselines.  
","['Uber ATG', 'Universite de Montreal', 'Université de Montréal', 'Mila, University of Montreal', 'DeepMind / Imperial College London', 'Mila, Université de Montréal', 'Google Research / University of Colorado', 'Montreal Institute for Learning Algorithms']"
2020,Extra-gradient with player sampling for faster convergence in n-player games,"Samy Jelassi, Carles Domingo-Enrich, Damien Scieur, Arthur Mensch, Joan Bruna",https://icml.cc/Conferences/2020/Schedule?showEvent=6016,"Data-driven modeling increasingly requires to find a Nash equilibrium in multi-player games, e.g. when training GANs. In this paper, we analyse a new extra-gradient method for Nash equilibrium finding, that performs gradient extrapolations and updates on a random subset of players at each iteration. This approach provably exhibits a better rate of convergence than full extra-gradient for non-smooth convex games with noisy gradient oracle. We propose an additional variance reduction mechanism to obtain speed-ups in smooth convex games. Our approach makes extrapolation amenable to massive multiplayer settings, and brings empirical speed-ups, in particular when using a heuristic cyclic sampling scheme. Most importantly, it allows to train faster and better GANs and mixtures of GANs.
","['Princeton University', 'NYU', 'Samsung - SAIT AI Lab, Montreal', 'ENS', 'New York University']"
2020,A Flexible Latent Space Model for Multilayer Networks,"Xuefei Zhang, Songkai Xue, Ji Zhu",https://icml.cc/Conferences/2020/Schedule?showEvent=6562,"Entities often interact with each other through multiple types of relations, which are often represented as multilayer networks.  Multilayer networks among the same set of nodes usually share common structures, while each layer can possess its distinct node connecting behaviors. This paper proposes a flexible latent space model for multilayer networks for the purpose of capturing such characteristics. Specifically, the proposed model embeds each node with a latent vector shared among layers and a layer-specific effect for each layer; both elements together with a layer-specific connectivity matrix determine edge formations. To fit the model, we develop a projected gradient descent algorithm for efficient parameter estimation.  We also establish theoretical properties of the maximum likelihood estimators and show that the upper bound of the common latent structure's estimation error is inversely proportional to the number of layers under mild conditions. The superior performance of the proposed model is demonstrated through simulation studies and applications to two real-world data examples.  
","['University of Michigan', 'University of Michigan', 'University of Michigan']"
2020,Predictive Coding for Locally-Linear Control,"Rui Shu, Tung Nguyen, Yinlam Chow, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano Ermon, Hung Bui",https://icml.cc/Conferences/2020/Schedule?showEvent=6789,"High-dimensional observations and unknown dynamics are major challenges when applying optimal control to many real-world decision making tasks. The Learning Controllable Embedding (LCE) framework addresses these challenges by embedding the observations into a lower dimensional latent space, estimating the latent dynamics, and then performing control directly in the latent space. To ensure the learned latent dynamics are predictive of next-observations, all existing LCE approaches decode back into the observation space and explicitly perform next-observation prediction---a challenging high-dimensional task that furthermore introduces a large number of nuisance parameters (i.e., the decoder) which are discarded during control. In this paper, we propose a novel information-theoretic LCE approach and show theoretically that explicit next-observation prediction can be replaced with predictive coding. We then use predictive coding to develop a decoder-free LCE model whose latent dynamics are amenable to locally-linear control. Extensive experiments on benchmark tasks show that our model reliably learns a controllable latent space that leads to superior performance when compared with state-of-the-art LCE baselines.
","['Stanford University', 'VinAI Research, Vietnam', 'Google', 'VinAI Research', 'VinAI & HUST', 'Google Research', 'Stanford University', 'VinAI Research']"
2020,Optimizer Benchmarking Needs to Account for Hyperparameter Tuning,"Prabhu Teja Sivaprasad, Florian Mai, Thijs Vogels, Martin Jaggi, François Fleuret",https://icml.cc/Conferences/2020/Schedule?showEvent=6589,"The performance of optimizers, particularly in deep learning, depends considerably on their chosen hyperparameter configuration. The efficacy of optimizers is often studied under near-optimal problem-specific hyperparameters, and finding these settings may be prohibitively costly for practitioners. In this work, we argue that a fair assessment of optimizers' performance must take the computational cost of hyperparameter tuning into account, i.e., how easy it is to find good hyperparameter configurations using an automatic hyperparameter search. Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, our results indicate that Adam is the most practical solution, particularly in low-budget scenarios.
","['Idiap Research Institute', 'Idiap Research Institute', 'EPFL', 'EPFL', 'University of Geneva']"
2020,Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions,"Ahmed Alaa, Mihaela van der Schaar",https://icml.cc/Conferences/2020/Schedule?showEvent=6179,"Deep learning models achieve high predictive accuracy across a broad spectrum of tasks, but rigorously quantifying their predictive uncertainty remains challenging. Usable estimates of predictive uncertainty should (1) cover the true prediction targets with high probability, and (2) discriminate between high- and low confidence prediction instances. Existing methods for uncertainty quantification are based predominantly on Bayesian neural networks; these may fall short of (1) and (2) — i.e., Bayesian credible intervals do not guarantee frequentist coverage, and approximate posterior inference undermines discriminative accuracy. In this paper, we develop the discriminative jackknife (DJ), a frequentist procedure that utilizes influence functions of a model’s loss functional to construct a jackknife (or leave one-out) estimator of predictive confidence intervals. The DJ satisfies (1) and (2), is applicable to a wide range of deep learning models, is easy to implement, and can be applied in a post-hoc fashion without interfering with model training or compromising its accuracy. Experiments demonstrate that DJ performs competitively compared to existing Bayesian and non-Bayesian regression baselines.
","['UCLA', 'University of Cambridge and UCLA']"
2020,The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks,"Jakub Swiatkowski, Kevin Roth, Bastiaan Veeling, Linh Tran, Joshua V Dillon, Jasper Snoek, Stephan Mandt, Tim Salimans, Rodolphe Jenatton, Sebastian Nowozin",https://icml.cc/Conferences/2020/Schedule?showEvent=6382,"Variational Bayesian Inference is a popular methodology for approximating posterior distributions over Bayesian neural network weights. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibit strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. Furthermore, we find that such factorized parameterizations improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.
","['University of Warsaw', 'ETH Zurich', 'University of Amsterdam', 'Imperial College London', 'Google', 'Google Brain', 'University of California, Irivine', 'Google', 'Google Research', 'Microsoft Research']"
2020,Monte-Carlo Tree Search as Regularized Policy Optimization,"Jean-Bastien Grill, Florent Altché, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis Antonoglou, Remi Munos",https://icml.cc/Conferences/2020/Schedule?showEvent=6380,"The combination of Monte-Carlo tree search (MCTS) with deep reinforcement learning has led to groundbreaking results in  artificial intelligence. However, AlphaZero, the current state-of-the-art MCTS algorithm still relies on handcrafted  heuristics that are only partially understood. In this paper, we show that AlphaZero's search heuristic, along with other common ones, can be interpreted as an approximation to the solution of a specific regularized policy optimization problem. With this insight, we propose a variant of AlphaZero which uses the exact solution to this policy optimization problem, and show experimentally that it reliably outperforms the original algorithm in multiple domains.
","['DeepMind', 'DeepMind', 'Columbia University', 'DeepMind', 'DeepMind', 'Deepmind', 'DeepMind']"
2020,Inter-domain Deep Gaussian Processes,"Tim G. J. Rudner, Dino Sejdinovic, Yarin Gal",https://icml.cc/Conferences/2020/Schedule?showEvent=6718,"Inter-domain Gaussian processes (GPs) allow for high flexibility and low computational cost when performing approximate inference in GP models. They are particularly suitable for modeling data exhibiting global structure but are limited to stationary covariance functions and thus fail to model non-stationary data effectively. We propose Inter-domain Deep Gaussian Processes, an extension of inter-domain shallow GPs that combines the advantages of inter-domain and deep Gaussian processes (DGPs), and demonstrate how to leverage existing approximate inference methods to perform simple and scalable approximate inference using inter-domain features in DGPs. We assess the performance of our method on a range of regression tasks and demonstrate that it outperforms inter-domain shallow GPs and conventional DGPs on challenging large-scale real-world datasets exhibiting both global structure as well as a high-degree of non-stationarity.
","['University of Oxford', 'University of Oxford', 'University of Oxford']"
2020,Online Multi-Kernel Learning with Graph-Structured Feedback,"Pouya M Ghari, Yanning Shen",https://icml.cc/Conferences/2020/Schedule?showEvent=6811,"Multi-kernel learning (MKL) exhibits reliable performance in nonlinear function approximation tasks. Instead of using one kernel, it learns the optimal kernel from a pre-selected dictionary of kernels.  The selection of the dictionary has crucial impact on both the performance and complexity of MKL. Specifically, inclusion of a large number of irrelevant kernels may impair the accuracy, and increase the complexity of MKL algorithms. To enhance the accuracy, and alleviate the computational burden, the present paper develops a novel scheme which actively chooses relevant kernels. The proposed framework models the pruned kernel combination as feedback collected from a graph, that is refined 'on the fly.' Leveraging the random feature approximation, we propose an online scalable multi-kernel learning approach with graph feedback, and prove that the proposed algorithm enjoys sublinear regret. Numerical tests on real datasets demonstrate the effectiveness of the novel approach.
","['University of California, Irvine', 'University of California, Irvine']"
2020,Graph-based Nearest Neighbor Search: From Practice to Theory,"Liudmila Prokhorenkova, Aleksandr Shekhovtsov",https://icml.cc/Conferences/2020/Schedule?showEvent=5956,"Graph-based approaches are empirically shown to be very successful for the nearest neighbor search (NNS). However, there has been very little research on their theoretical guarantees. We fill this gap and rigorously analyze the performance of graph-based NNS algorithms, specifically focusing on the low-dimensional ($d \ll \log n$) regime. In addition to the basic greedy algorithm on nearest neighbor graphs, we also analyze the most successful heuristics commonly used in practice: speeding up via adding shortcut edges and improving accuracy via maintaining a dynamic list of candidates. We believe that our theoretical insights supported by experimental analysis are an important step towards understanding the limits and benefits of graph-based NNS algorithms.","['Yandex', 'Yandex, Higher School of Economics']"
2020,Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,"Francesco Croce, Matthias Hein",https://icml.cc/Conferences/2020/Schedule?showEvent=6837,"The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient obfuscation.","['University of Tuebingen', 'University of Tübingen']"
2020,Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks,"Alexander Shevchenko, Marco Mondelli",https://icml.cc/Conferences/2020/Schedule?showEvent=6004,"The optimization of multilayer neural networks typically leads to a solution with zero training error, yet the landscape can exhibit spurious local minima and the minima can be disconnected. In this paper, we shed light on this phenomenon: we show that the combination of stochastic gradient descent (SGD) and over-parameterization makes the landscape of multilayer neural networks approximately connected and thus more favorable to optimization. More specifically, we prove that SGD solutions are connected via a piecewise linear path, and the increase in loss along this path vanishes as the number of neurons grows large. This result is a consequence of the fact that the parameters found by SGD are increasingly dropout stable as the network becomes wider. We show that, if we remove part of the neurons (and suitably rescale the remaining ones), the change in loss is independent of the total number of neurons, and it depends only on how many neurons are left. Our results exhibit a mild dependence on the input dimension: they are dimension-free for two-layer networks and require the number of neurons to scale linearly with the dimension for multilayer networks. We validate our theoretical findings with numerical experiments for different architectures and classification tasks.
","['IST Austria', 'IST Austria']"
2020,Missing Data Imputation using Optimal Transport,"Boris Muzellec, Julie Josse, Claire Boyer, Marco Cuturi",https://icml.cc/Conferences/2020/Schedule?showEvent=6455,"Missing data is a crucial issue when applying machine learning algorithms to real-world datasets. Starting from the simple assumption that two batches extracted randomly from the same dataset should share the same distribution, we leverage optimal transport distances to quantify that criterion and turn it into a loss function to impute missing data values. We propose practical methods to minimize these losses using end-to-end learning, that can exploit or not parametric assumptions on the underlying distributions of values. We evaluate our methods on datasets from the UCI repository, in MCAR, MAR and MNAR settings. These experiments show that OT-based methods match or out-perform state-of-the-art imputation methods, even for high percentages of missing values.
","['ENSAE, Institut Polytechnique de Paris', 'Polytechnique', 'LPSM, Sorbonne Université', 'Google']"
2020,Random extrapolation for primal-dual coordinate descent,"Ahmet Alacaoglu, Olivier Fercoq, Volkan Cevher",https://icml.cc/Conferences/2020/Schedule?showEvent=6151,"We introduce a randomly extrapolated primal-dual coordinate descent method that adapts to sparsity of the data matrix and the favorable structures of the objective function. 
Our method updates only a subset of primal and dual variables with sparse data, and it uses large step sizes with dense data, retaining the benefits of the specific methods designed for each case.
In addition to adapting to sparsity, our method attains fast convergence guarantees in favorable cases \textit{without any modifications}.
In particular, we prove linear convergence under metric subregularity, which applies to strongly convex-strongly concave problems, linear programs and piecewise linear quadratic functions. 
We show almost sure convergence of the sequence and optimal sublinear convergence rates for the primal-dual gap and objective values, in the general convex-concave case. 
Numerical evidence demonstrates the state-of-the-art empirical performance of our method in sparse and dense settings, matching and improving the existing methods.
","['EPFL', 'Telecom Paris', 'EPFL']"
2020,PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization,"Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter Liu",https://icml.cc/Conferences/2020/Schedule?showEvent=5953,"Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous  state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve  human performance on multiple datasets.
","['Imperial College London', 'Google', 'Google', 'Google Research, Brain team']"
2020,Influenza Forecasting Framework based on Gaussian Processes,"Christoph Zimmer, Reza Yaesoubi",https://icml.cc/Conferences/2020/Schedule?showEvent=5959,"The seasonal epidemic of influenza costs thousands
of lives each year in the US. While influenza
epidemics occur every year, timing and size of the
epidemic vary strongly from season to season.
This complicates the public health efforts to adequately
respond to such epidemics. Forecasting
techniques to predict the development of seasonal
epidemics such as influenza, are of great help to
public health decision making. Therefore, the
US Center for Disease Control and Prevention
(CDC) has initiated a yearly challenge to forecast
influenza-like illness. Here, we propose a
new framework based on Gaussian process (GP)
for seasonal epidemics forecasting and demonstrate
its capability on the CDC reference data
on influenza like illness: our framework leads to
accurate forecasts with small but reliable uncertainty
estimation. We compare our framework
to several state of the art benchmarks and show
competitive performance. We, therefore, believe
that our GP based framework for seasonal epidemics
forecasting will play a key role for future
influenza forecasting and, lead to further research
in the area.
","['Bosch Center for Artificial Intelligence BCAI', 'Health Policy and Management, Yale School of Public Health']"
2020,A Generative Model for Molecular Distance Geometry,"Gregor Simm, Jose Miguel Hernandez-Lobato",https://icml.cc/Conferences/2020/Schedule?showEvent=6383,"Computing equilibrium states for many-body systems, such as molecules, is a long-standing challenge. In the absence of methods for generating statistically independent samples, great computational effort is invested in simulating these systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates such samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties.
","['University of Cambridge', 'University of Cambridge']"
2020,Teaching with Limited Information on the Learner's Behaviour,"Ferdinando Cicalese, Francisco Sergio de Freitas Filho, Eduardo Laber, Marco Molinaro",https://icml.cc/Conferences/2020/Schedule?showEvent=5913,"Machine Teaching studies how efficiently a Teacher  can guide a Learner to a target hypothesis. We focus on the model of Machine Teaching with a black box learner introduced in [Dasgupta et al., ICML 2019], where the teaching is done  interactively   without having any knowledge of the Learner's   algorithm and  class of hypotheses, apart from the fact that it contains the target hypothesis $h^*$. 

We first refine some existing results for this   model and,  then, we study new variants of it. Motivated  by the realistic possibility that  $h^*$ is not available to the learner, we consider the case where the teacher can only aim at having the learner converge to a best available approximation of $h^*$. We also consider weaker black box learners, where, in each round, the choice of the consistent hypothesis returned to the Teacher is not adversarial, and in particular, we show that better provable bounds can be obtained for a type of Learner  that moves to the next  hypothesis smoothly, preferring hypotheses that are  close  to the current one; and for another type of Learner that can provide to the Teacher hypotheses chosen at random among those consistent with the examples received so far.  
Finally, we present an empirical evaluation of  our basic interactive teacher on real datasets.
","['University of Verona', 'PUC-RIO', 'PUC-RIO', 'PUC-RIO']"
2020,Preselection Bandits,"Viktor Bengs, Eyke Hüllermeier",https://icml.cc/Conferences/2020/Schedule?showEvent=6575,"In this paper, we introduce the Preselection Bandit problem, in which the learner preselects a subset of arms (choice alternatives) for a user, which then chooses the final arm from this subset. The learner is not aware of the user's preferences, but can learn them from observed choices. In our concrete setting, we allow these choices to be stochastic and model the user's actions by means of the Plackett-Luce model. The learner's main task is to preselect subsets that eventually lead to highly preferred choices. To formalize this goal, we introduce a reasonable notion of regret and derive lower bounds on the expected regret. Moreover, we propose algorithms for which the upper bound on expected regret matches the lower bound up to a logarithmic term of the time horizon. 
","['University of Paderborn', 'Paderborn University']"
2020,Towards non-parametric drift detection via Dynamic Adapting Window Independence Drift Detection (DAWIDD),"Fabian Hinder, André Artelt, CITEC Barbara Hammer",https://icml.cc/Conferences/2020/Schedule?showEvent=6126,"The notion of concept drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time; as a consequence machine learning models may become inaccurate and need adjustment. Many online learning schemes include drift detection to actively detect and react to observed changes. Yet, reliable drift detection constitutes a challenging problem in particular in the context of high dimensional data, varying drift characteristics, and the absence of a parametric model such as a classification scheme which reflects the drift. In this paper we present a novel concept drift detection method, Dynamic Adapting Window Independence Drift Detection (DAWIDD), which aims for non-parametric drift detection of diverse drift characteristics. For this purpose, we establish a mathematical equivalence of the presence of drift to the dependency of specific random variables in an according drift process. This allows us to rely on independence tests rather than parametric models or the classification loss, resulting in a fairly robust scheme to universally detect different types of drift, as it is also confirmed in experiments. 
","['CITEC, Bielefeld University', 'Bielefeld University', 'CITEC, Bielefeld University']"
2020,Latent Bernoulli Autoencoder,"Jiri Fajtl, Vasileios Argyriou, Dorothy Monekosso, Paolo Remagnino",https://icml.cc/Conferences/2020/Schedule?showEvent=6278,"In this work, we pose the question whether it is possible to design and train
an autoencoder model in an end-to-end fashion to learn representations in the
multivariate Bernoulli latent space, and achieve performance comparable with
the state-of-the-art variational methods.  Moreover, we investigate how to
generate novel samples and perform smooth interpolation and attributes
modification in the binary latent space.  To meet our objective, we propose a
simplified, deterministic model with a straight-through gradient estimator to
learn the binary latents and show its competitiveness with the latest VAE
methods.  Furthermore, we propose a novel method based on a random hyperplane
rounding for sampling and smooth interpolation in the latent space.  Our method
performs on a par or better than the current state-of-the-art methods on common
CelebA, CIFAR-10 and MNIST datasets.
","['Kingston University London', 'Kingston University', 'Leeds Beckett University', 'Kingston University']"
2020,Convolutional Kernel Networks for Graph-Structured Data,"Dexiong Chen, Laurent Jacob, Julien Mairal",https://icml.cc/Conferences/2020/Schedule?showEvent=6029,"We introduce a family of multilayer graph kernels and establish new links between graph convolutional neural networks and kernel methods. Our approach generalizes convolutional kernel networks to graph-structured data, by representing graphs as a sequence of kernel feature maps, where each node carries information about local graph substructures. On the one hand, the kernel point of view offers an unsupervised, expressive, and easy-to-regularize data representation, which is useful when limited samples are available. On the other hand, our model can also be trained end-to-end on large-scale data, leading to new types of graph convolutional neural networks.  We show that our method achieves state-of-the-art performance on several graph classification benchmarks, while offering simple model interpretation. Our code is freely available at https://github.com/claying/GCKN.
","['Inria', 'CNRS', 'Inria']"
2020,Towards Adaptive Residual Network Training: A Neural-ODE Perspective,"Chengyu Dong, Liyuan Liu, Zichao Li, Jingbo Shang",https://icml.cc/Conferences/2020/Schedule?showEvent=6808,"In pursuit of resource-economical machine learning, attempts have been made to dynamically adjust computation workloads in different training stages, i.e., starting with a shallow network and gradually increasing the model depth (and computation workloads) during training. However, there is neither guarantee nor guidance on designing such network grow, due to the lack of its theoretical underpinnings. In this work, to explore the theory behind, we conduct theoretical analyses from an ordinary differential equation perspective. Specifically, we illustrate the dynamics of network growth and propose a novel performance measure specific to the depth increase. Illuminated by our analyses, we move towards theoretically sound growing operations and schedulers, giving rise to an adaptive training algorithm for residual networks, LipGrow, which automatically increases network depth thus accelerates training. In our experiments, it achieves comparable performance while reducing ∼ 50% of training time.
","['UCSD', 'University of Illinois at Urbana Champaign', 'University of California, San Diego', 'University of California, San Diego']"
2020,Neural Topic Modeling with Continual Lifelong Learning,"Pankaj Gupta, Yatin Chaudhary, Thomas Runkler, Hinrich Schuetze",https://icml.cc/Conferences/2020/Schedule?showEvent=6644,"Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task. Code: https://github.com/pgcool/Lifelong-Neural-Topic-Modeling
","['Siemens AG', 'Siemens AG', 'Technical University of Munich', 'University of Munich (LMU)']"
2020,Stochastic Subspace Cubic Newton Method,"Filip Hanzely, Nikita Doikov, Yurii Nesterov, Peter Richtarik",https://icml.cc/Conferences/2020/Schedule?showEvent=5950,"In this paper, we propose a new randomized second-order optimization algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high dimensional convex function $f$. Our method can be seen both as a {\em stochastic} extension of the cubically-regularized Newton method of Nesterov and Polyak (2006), and a {\em second-order} enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function $\frac12 (x-x^*)^\top \nabla^2f(x^*)(x-x^*)$, where $x^*$ is the minimizer of $f$, and hence depends on the properties of $f$ at the optimum only. Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.","['KAUST', 'Université catholique de Louvain', 'Universite catholique de Louvain', 'KAUST']"
2020,Graph Filtration Learning,"Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, Roland Kwitt",https://icml.cc/Conferences/2020/Schedule?showEvent=6428,"We propose an approach to learning with graph-structured data in the problem domain of graph classification. In particular, we present a novel type of readout operation to aggregate node features into a graph-level representation. To this end, we leverage persistent homology computed via a real-valued, learnable, filter function. We establish the theoretical foundation for differentiating through the persistent homology computation. Empirically, we show that this type of readout operation compares favorably to previous techniques, especially when the graph connectivity structure is informative for the learning problem.
","['University of Salzburg', 'University of Salzburg', 'ETH Zurich', 'UNC', '""University of Salzburg, Austria""']"
2020,Efficient Proximal Mapping of the 1-path-norm of Shallow Networks,"Fabian Latorre, Paul Rolland, Shaul Nadav Hallak, Volkan Cevher",https://icml.cc/Conferences/2020/Schedule?showEvent=6496,"We demonstrate two new important properties of the 1-path-norm of shallow neural networks. First, despite its non-smoothness and non-convexity it allows a closed form proximal operator which can be efficiently computed, allowing the use of stochastic proximal-gradient-type methods for regularized empirical risk minimization. Second, when the activation functions is differentiable, it provides an upper bound on the Lipschitz constant of the network. Such bound is tighter than the trivial layer-wise product of Lipschitz constants, motivating its use for training networks robust to adversarial perturbations. In practical experiments we illustrate the advantages of using the proximal mapping and we compare the robustness-accuracy trade-off induced by the 1-path-norm, L1-norm and layer-wise constraints on the Lipschitz constant (Parseval networks).
","['EPFL', 'Ecole Polytechnique Fédérale de Lausanne', 'EPFL', 'EPFL']"
2020,Stochastic Latent Residual Video Prediction,"Jean-Yves Franceschi, Edouard Delasalles, Mickael Chen, Sylvain Lamprier, Patrick Gallinari",https://icml.cc/Conferences/2020/Schedule?showEvent=5773,"Designing video prediction models that account for the inherent uncertainty of the future is challenging. Most works in the literature are based on stochastic image-autoregressive recurrent networks, which raises several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and temporal dynamics. However, no such model for stochastic video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model whose dynamics are governed in a latent space by a residual update rule. This first-order scheme is motivated by discretization schemes of differential equations. It naturally models video dynamics as it allows our simpler, more interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.
","['Sorbonne Université', 'Sorbonne Université', 'Sorbonne Université', 'LIP6 - Sorbonne Universités', 'Sorbonne Universite, Criteo AI Lab']"
2020,A quantile-based approach for hyperparameter transfer learning,"David Salinas, Huibin Shen, Valerio Perrone",https://icml.cc/Conferences/2020/Schedule?showEvent=6483,"Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Traditionally, BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance objectives of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different datasets as well as different objectives. The main idea is to regress the mapping from hyperparameter to objective quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this estimation: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple objectives such as latency and accuracy, steering the optimization toward faster predictions for the same level of accuracy. Experiments on an extensive set of hyperparameter tuning tasks demonstrate significant improvements over state-of-the-art methods for both hyperparameter optimization and neural architecture search.
","['NAVER LABS Europe', 'Amazon', 'Amazon']"
2020,Projective Preferential Bayesian Optimization,"Petrus Mikkola, Milica Todorović, Jari Järvi, Patrick Rinke, Samuel Kaski",https://icml.cc/Conferences/2020/Schedule?showEvent=6141,"Bayesian optimization is an effective method for finding extrema of a black-box function. We propose a new type of Bayesian optimization for learning user preferences in high-dimensional spaces. The central assumption is that the underlying objective function cannot be evaluated directly, but instead a minimizer along a projection can be queried, which we call a projective preferential query. The form of the query allows for feedback that is natural for a human to give, and which enables interaction. This is demonstrated in a user experiment in which the user feedback comes in the form of optimal position and orientation of a molecule adsorbing to a surface. We demonstrate that our framework is able to find a global minimum of a high-dimensional black-box function, which is an infeasible task for existing preferential Bayesian optimization frameworks that are based on pairwise comparisons.
","['Aalto University', 'Aalto University', 'Aalto University', 'Aalto University', 'Aalto University and University of Manchester']"
2020, Topic Modeling via Full Dependence Mixtures,"Dan  Fisher, Mark Kozdoba, Shie Mannor",https://icml.cc/Conferences/2020/Schedule?showEvent=6209,"In this paper we introduce a new approach to topic modelling that scales to 
large datasets by  using  a compact representation of the data and by
leveraging the GPU architecture. 
In this approach, topics are learned directly from the 
co-occurrence data of the corpus. In particular, we introduce a novel
mixture model which we term the Full Dependence Mixture (FDM) model.
FDMs model  second moment under  general generative 
assumptions on the data. While there is previous work on topic 
modeling using second moments,  we develop a direct stochastic 
optimization procedure for fitting an FDM with a single Kullback 
Leibler objective. Moment methods in general have the benefit that 
an iteration no longer needs to scale with the size of the corpus. 
Our approach allows us to leverage standard 
optimizers and GPUs for the problem of topic modeling. In 
particular, we evaluate the approach on two large datasets, 
NeurIPS papers and a Twitter corpus, with a large number of 
topics, and show that the approach performs comparably or better than the standard benchmarks.
","['Technion', 'Technion', 'Technion']"
2020,Spectral Clustering with Graph Neural Networks for Graph Pooling,"Filippo Maria Bianchi, Daniele Grattarola, Cesare Alippi",https://icml.cc/Conferences/2020/Schedule?showEvent=6018,"Spectral clustering (SC) is a popular clustering technique to find strongly connected communities on a graph.
SC can be used in Graph Neural Networks (GNNs) to implement pooling operations that aggregate nodes belonging to the same cluster.
However, the eigendecomposition of the Laplacian is expensive and, since clustering results are graph-specific, pooling methods based on SC must perform a new optimization for each new sample.
In this paper, we propose a graph clustering approach that addresses these limitations of SC.
We formulate a continuous relaxation of the normalized minCUT problem and train a GNN to compute cluster assignments that minimize this objective. 
Our GNN-based implementation is differentiable, does not require to compute the spectral decomposition, and learns a clustering function that can be quickly evaluated on out-of-sample graphs.
From the proposed clustering method, we design a graph pooling operator that overcomes some important limitations of state-of-the-art graph pooling techniques and achieves the best performance in several supervised and unsupervised tasks.
","['NORCE the Norwegian Research Centre', 'Università della Svizzera italiana', 'Università della Svizzera Italiana']"
2020,Kernel interpolation with continuous volume sampling,"Ayoub Belhadji, Rémi Bardenet, Pierre Chainais",https://icml.cc/Conferences/2020/Schedule?showEvent=6612,"A fundamental task in kernel methods is to pick nodes and weights, so as to approximate a given function from an RKHS by the weighted sum of kernel translates located at the nodes. This is the crux of kernel density estimation, kernel quadrature, or interpolation from discrete samples. Furthermore, RKHSs offer a convenient mathematical and computational framework. We introduce and analyse continuous volume sampling (VS), the continuous counterpart -for choosing node locations- of a discrete distribution introduced in (Deshpande & Vempala, 2006).
Our contribution is theoretical: we prove almost optimal bounds for interpolation and quadrature under VS. While similar bounds already exist for some specific RKHSs using ad-hoc node constructions, VS offers bounds that apply to any Mercer kernel and depend on the spectrum of the associated integration operator. We emphasize that, unlike previous randomized approaches that rely on regularized leverage scores or determinantal point processes, evaluating the pdf of VS only requires pointwise evaluations of the kernel. VS is thus naturally amenable to MCMC samplers.
","['Ecole Centrale de Lille', 'CNRS and Univ. Lille', 'Centrale Lille / CRIStAL CNRS UMR 9189']"
2020,Predicting Choice with Set-Dependent Aggregation,"Nir Rosenfeld, Kojin Oshiba, Yaron Singer",https://icml.cc/Conferences/2020/Schedule?showEvent=6009,"Providing users with alternatives to choose from is an essential component of many online platforms, making the accurate prediction of choice vital to their success. A renewed interest in learning choice models has led to improved modeling power, but most current methods are either limited in the type of choice behavior they capture, cannot be applied to large-scale data, or both.
Here we propose a learning framework for predicting choice that is accurate, versatile, and theoretically grounded. Our key modeling point is that to account for how humans choose, predictive models must be expressive enough to accommodate complex choice patterns but structured enough to retain statistical efficiency. Building on recent results in economics, we derive a class of models that achieves this balance, and propose a neural implementation that allows for scalable end-to-end training. Experiments on three large choice datasets demonstrate the utility of our approach.
","['Harvard University', 'Harvard University', 'Harvard']"
2020,Amortised Learning by Wake-Sleep,"Li Kevin Wenliang, Theodore Moskovitz, Heishiro Kanagawa, Maneesh Sahani",https://icml.cc/Conferences/2020/Schedule?showEvent=6696,"Models that employ latent variables to capture structure in observed data lie at the heart of many current unsupervised learning algorithms, but exact maximum-likelihood learning for powerful and flexible latent-variable models is almost always intractable. Thus, state-of-the-art approaches either abandon the maximum-likelihood framework entirely, or else rely on a variety of variational approximations to the posterior distribution over the latents. Here, we propose an alternative approach that we call amortised learning. Rather than computing an approximation to the posterior over latents, we use a wake-sleep Monte-Carlo strategy to learn a function that directly estimates the maximum-likelihood parameter updates. Amortised learning is possible whenever samples of latents and observations can be simulated from the generative model, treating the model as a ``black box''. We demonstrate its effectiveness on a wide range of complex models, including those with latents that are discrete or supported on non-Euclidean spaces. 
","['Gatsby Unit, University College London', 'Gatsby Computational Neuroscience Unit', 'Gatsby Unit, UCL', 'Gatsby Unit, UCL']"
2020,Stochastic Differential Equations with Variational Wishart Diffusions,"Martin Jørgensen, Marc Deisenroth, Hugh Salimbeni",https://icml.cc/Conferences/2020/Schedule?showEvent=6149,"We present a Bayesian non-parametric way of inferring stochastic differential equations for both regression tasks and continuous-time dynamical modelling. The work has high emphasis on the stochastic part of the differential equation, also known as the diffusion, and modelling it by means of Wishart processes. Further, we present a semiparametric approach that allows the framework to scale to high dimensions. This successfully leads us onto how to model both latent and autoregressive temporal systems with conditional heteroskedastic noise. We provide experimental evidence that modelling diffusion often improves performance and that this randomness in the differential equation can be essential to avoid overfitting.
","['Technical University of Denmark', 'University College London', 'Imperial College London']"
2020,Learning to Branch for Multi-Task Learning,"Pengsheng Guo, Chen-Yu Lee, Daniel Ulbricht",https://icml.cc/Conferences/2020/Schedule?showEvent=6596,"Training multiple tasks jointly in one deep network yields reduced latency during inference and better performance over the single-task counterpart by sharing certain layers of a network. However, over-sharing a network could erroneously enforce over-generalization, causing negative knowledge transfer across tasks. Prior works rely on human intuition or pre-computed task relatedness scores for ad hoc branching structures. They provide sub-optimal end results and often require huge efforts for the trial-and-error process.
In this work, we present an automated multi-task learning algorithm that learns where to share or branch within a network, designing an effective network topology that is directly optimized for multiple objectives across tasks. Specifically, we propose a novel tree-structured design space that casts a tree branching operation as a gumbel-softmax sampling procedure. This enables differentiable network splitting that is end-to-end trainable. We validate the proposed method on controlled synthetic data, CelebA, and Taskonomy. 
","['Apple', 'Apple', 'Apple']"
2020,On Efficient Low Distortion Ultrametric Embedding,"Vincent Cohen-Addad, Karthik C. S., Guillaume Lagarde",https://icml.cc/Conferences/2020/Schedule?showEvent=6216,"A classic problem in unsupervised learning and data analysis is to find simpler and easy-to-visualize representations of the data that preserve its essential properties. A widely-used method to preserve the underlying hierarchical structure of the data while reducing its complexity is to find an embedding of the data into a tree or an ultrametric, but computing such an embedding on a data set of $n$ points in $\Omega(\log n)$ dimensions incurs a quite prohibitive running time of $\Theta(n^2)$.

In this paper, we provide a new algorithm which takes as input a set of points $P$ in $\R^d$, and for every $c\ge 1$,  runs in time $n^{1+\frac{\rho}{c^2}}$ (for some universal constant $\rho>1$) to output an ultrametric $\Delta$ such that for any two points $u,v$ in $P$, we have $\Delta(u,v)$ is within a multiplicative factor of $5c$ to the distance between $u$ and $v$ in the best ultrametric representation of $P$. Here, the best ultrametric is the ultrametric $\tilde\Delta$ that minimizes the maximum distance distortion with respect to the $\ell_2$ distance, namely that minimizes $\underset{u,v \in P}{\max}\ \nicefrac{\tilde\Delta(u,v)}{\|u-v\|_2}$.

We complement the above result by showing that under popular complexity theoretic assumptions, for every constant $\varepsilon>0$, no algorithm with running time $n^{2-\varepsilon}$ can distinguish between inputs in $\ell_\infty$-metric that admit isometric embedding and those that incur a distortion of $\nicefrac{3}{2}$.
   
Finally, we present empirical evaluation on classic machine learning datasets and show that the output of our algorithm is comparable to the output of the linkage algorithms while achieving a much faster running time.","['CNRS & Sorbonne Université', 'Tel Aviv University', 'LaBRI']"
2020,The Boomerang Sampler,"Joris Bierkens, Sebastiano Grazzi, Kengo Kamatani, Gareth Roberts",https://icml.cc/Conferences/2020/Schedule?showEvent=6487,"This paper introduces the boomerang sampler as a novel class of continuous-time non-reversible Markov chain Monte Carlo algorithms. The methodology begins by representing the target density as a density, $e^{-U}$, with respect to a prescribed (usually) Gaussian measure and constructs a continuous trajectory consisting of a piecewise circular path. The method moves from one circular orbit to another according to a rate function which can be written in terms of $U$. We demonstrate that the method is easy to implement and demonstrate empirically that it can out-perform existing benchmark piecewise deterministic Markov processes such as the bouncy particle sampler and the Zig-Zag. In the Bayesian statistics context, these competitor algorithms are of substantial interest in the large data context due to the fact that they can adopt data subsampling techniques which are exact (ie induce no error in the stationary distribution). We demonstrate theoretically and empirically that we can also construct a control-variate subsampling boomerang sampler which is also exact, and which possesses remarkable scaling properties in the large data limit. We furthermore illustrate a factorised version on the simulation of diffusion bridges.","['Technische Universiteit Delft', 'Technische Universiteit Delft', 'Osaka University', 'University of Warwick']"
2020,Robust Learning with the Hilbert-Schmidt Independence Criterion,"Daniel Greenfeld, Uri Shalit",https://icml.cc/Conferences/2020/Schedule?showEvent=6443,"We investigate the use of a non-parametric independence measure, the Hilbert-Schmidt Independence Criterion (HSIC), as a loss-function for learning robust regression and classification models. This loss-function encourages learning models where the distribution of the residuals between the label and the model prediction is statistically independent of the distribution of the instances themselves. This loss-function was first proposed by \citet{mooij2009regression} in the context of learning causal graphs. We adapt it to the task of learning for unsupervised covariate shift: learning on a source domain without access to any instances or labels from the unknown target domain, but with the assumption that $p(y|x)$  (the conditional probability of labels given instances) remains the same in the target domain. We show that the proposed loss is expected to give rise to models that generalize well on a class of target domains characterised by the complexity of their description within a reproducing kernel Hilbert space. Experiments on unsupervised covariate shift tasks  demonstrate that models learned with the proposed loss-function outperform models learned with standard loss functions, achieving state-of-the-art results on a challenging cell-microscopy unsupervised covariate shift task.","['Technion', 'Technion']"
2020,The FAST Algorithm for Submodular Maximization,"Adam Breuer, Eric Balkanski, Yaron Singer",https://icml.cc/Conferences/2020/Schedule?showEvent=6290,"In this paper we describe a new parallel algorithm called Fast Adaptive Sequencing Technique (FAST) for maximizing a monotone submodular function under a cardinality constraint k. This algorithm achieves the optimal 1-1/e approximation guarantee and is orders of magnitude faster than the state-of-the-art on a variety of experiments over real-world data sets. Following recent work by Balkanski and Singer (2018), there has been a great deal of research on algorithms whose theoretical parallel runtime is exponentially faster than algorithms used for submodular maximization over the past 40 years. However, while these new algorithms are fast in terms of asymptotic worst-case guarantees, it is computationally infeasible to use them in practice even on small data sets because the number of rounds and queries they require depend on  large constants and high-degree polynomials in terms of precision and confidence. The design principles behind the FAST algorithm we present here are a significant departure from those of recent theoretically fast algorithms. Rather than optimize for asymptotic theoretical guarantees, the design of FAST introduces several new techniques that achieve remarkable practical and theoretical parallel runtimes. The approximation guarantee obtained by FAST is arbitrarily close to 1 - 1/e, and its asymptotic parallel runtime (adaptivity) is O(log(n) log^2(log k)) using O(n log log(k)) total queries. We show that FAST is orders of magnitude faster than any algorithm for submodular maximization we are aware of, including hyper-optimized parallel versions of state-of-the-art serial algorithms, by running experiments on large data sets.
","['Harvard University', 'Harvard', 'Harvard']"
2020,Modulating Surrogates for Bayesian Optimization,"Erik Bodin, Markus Kaiser, Ieva Kazlauskaite, Zhenwen Dai, Neill Campbell, Carl Henrik Ek",https://icml.cc/Conferences/2020/Schedule?showEvent=6028,"Bayesian optimization (BO) methods often rely on the assumption that the objective function is well-behaved,
but in practice, this is seldom true for real-world objectives even if noise-free observations can be collected.
Common approaches, which try to model the objective as precisely as possible, often fail to make progress by spending too many evaluations modeling irrelevant details.
We address this issue by proposing surrogate models that focus on the well-behaved structure in the objective function, which is informative for search, while ignoring detrimental structure that is challenging to model from few observations.
First, we demonstrate that surrogate models with appropriate noise distributions can absorb challenging structures in the objective function by treating them as irreducible uncertainty.
Secondly, we show that a latent Gaussian process is an excellent surrogate for this purpose, comparing with Gaussian processes with standard noise distributions.
We perform numerous experiments on a range of BO benchmarks and find that our approach improves reliability and performance when faced with challenging objective functions.
","['University of Bristol', 'Technical University Munich', 'University of Bath', 'Spotify', 'University of Bath', 'University of Cambridge']"
2020,Efficiently sampling functions from Gaussian process posteriors,"James Wilson, Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, Marc Deisenroth",https://icml.cc/Conferences/2020/Schedule?showEvent=6461,"Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.
","['Imperial College London', 'St. Petersburg Department of Steklov Mathematical Institute of Russian Academy of Sciences (PDMI RAS)', 'Imperial College London', 'Saint Petersburg State University', 'University College London']"
2020,Anderson Acceleration of Proximal Gradient Methods,"Vien Mai, Mikael Johansson",https://icml.cc/Conferences/2020/Schedule?showEvent=6170,"Anderson acceleration is a well-established and simple technique for speeding up fixed-point computations with countless applications. This work introduces novel methods for adapting Anderson acceleration to proximal gradient algorithms. Under some technical conditions, we extend existing local convergence results of Anderson acceleration for smooth fixed-point mappings to the proposed non-smooth setting. We also prove analytically that it is in general, impossible to guarantee global convergence of native Anderson acceleration. We therefore propose a simple scheme for stabilization that combines the global worst-case guarantees of proximal gradient methods with the local adaptation and practical speed-up of Anderson acceleration. Finally, we provide the first applications of Anderson acceleration to non-Euclidean geometry.
","['KTH Royal Institute of Technology', 'KTH Royal Institute of Technology']"
2020,Extrapolation for Large-batch Training in Deep Learning,"Tao Lin, Lingjing Kong, Sebastian Stich, Martin Jaggi",https://icml.cc/Conferences/2020/Schedule?showEvent=6578,"Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.
To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.
","['EPFL', 'EPFL', 'EPFL', 'EPFL']"
2020,Unlabelled Data Improves Bayesian Uncertainty Calibration under Covariate Shift  ,"Alexander Chan, Ahmed Alaa, Zhaozhi Qian, Mihaela van der Schaar",https://icml.cc/Conferences/2020/Schedule?showEvent=6244,"Modern neural networks have proven to be powerful function approximators, providing state-of-the-art performance in a multitude of applications. They however fall short in their ability to quantify confidence in their predictions --- this is crucial in high-stakes applications that involve critical decision-making. Bayesian neural networks (BNNs) aim at solving this problem by placing a prior distribution over the network's parameters, thereby inducing a posterior distribution that encapsulates predictive uncertainty. While existing variants of BNNs based on Monte Carlo dropout produce reliable (albeit approximate) uncertainty estimates over in-distribution data, they tend to exhibit over-confidence in predictions made on target data whose feature distribution differs from the training data, i.e., the covariate shift setup. In this paper, we develop an approximate Bayesian inference scheme based on posterior regularisation, wherein unlabelled target data are used as ``pseudo-labels'' of model confidence that are used to regularise the model's loss on labelled source data. We show that this approach significantly improves the accuracy of uncertainty quantification on covariate-shifted data sets, with minimal modification to the underlying model architecture. We demonstrate the utility of our method in the context of transferring prognostic models of prostate cancer across globally diverse populations. 
","['University of Cambridge', 'UCLA', 'University of Cambridge', 'University of Cambridge and UCLA']"
2020,Reinforcement Learning for Molecular Design Guided by Quantum Mechanics,"Gregor Simm, Robert Pinsler, Jose Miguel Hernandez-Lobato",https://icml.cc/Conferences/2020/Schedule?showEvent=5972,"Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. Existing approaches work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MolGym, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space.
","['University of Cambridge', 'University of Cambridge', 'University of Cambridge']"
2020,From Sets to Multisets: Provable Variational  Inference for Probabilistic Integer Submodular Models,"Aytunc Sahin, Yatao Bian, Joachim Buhmann, Andreas Krause",https://icml.cc/Conferences/2020/Schedule?showEvent=6677,"Submodular functions have been studied extensively in machine learning and data mining. In particular, the optimization of submodular functions over  the integer lattice (integer submodular functions) has recently attracted much interest, because this domain relates naturally to many practical problem settings, such as multilabel graph cut,  budget allocation and revenue maximization with discrete assignments. In contrast, the use of these functions for probabilistic modeling has received surprisingly little attention so far. 
In this work, we firstly propose the Generalized Multilinear Extension, a continuous DR-submodular extension for integer submodular functions. We study central properties of this extension and formulate a new probabilistic model which is defined through integer submodular functions. Then, we introduce a block-coordinate ascent algorithm to perform approximate inference for this class of models and finally, we demonstrate its effectiveness and viability on several real-world social connection graph datasets with integer submodular objectives.
","['ETH Zurich', 'Tencent AI Lab', 'ETH Zurich', 'ETH Zurich']"
2020,Regularized Optimal Transport is Ground Cost Adversarial,"François-Pierre Paty, Marco Cuturi",https://icml.cc/Conferences/2020/Schedule?showEvent=6497,"Regularizing Wasserstein distances has proved to be the key in the recent advances of optimal transport (OT) in machine learning. Most prominent is the entropic regularization of OT, which not only allows for fast computations and differentiation using Sinkhorn algorithm, but also improves stability with respect to data and accuracy in many numerical experiments. Theoretical understanding of these benefits remains unclear, although recent statistical works have shown that entropy-regularized OT mitigates classical OT's curse of dimensionality. In this paper, we adopt a more geometrical point of view, and show using Fenchel duality that any convex regularization of OT can be interpreted as ground cost adversarial. This incidentally gives access to a robust dissimilarity measure on the ground space, which can in turn be used in other applications. We propose algorithms to compute this robust cost, and illustrate the interest of this approach empirically.
","['ENSAE Paris', 'Google']"
2020,Multi-Agent Determinantal Q-Learning,"Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, Weinan Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=5858,"Centralized training with decentralized execution has become an important paradigm in multi-agent learning. Though practical, current methods rely on restrictive assumptions to decompose the centralized  value function across agents for execution. In this paper, we eliminate this restriction by proposing multi-agent determinantal Q-learning. Our method is established on Q-DPP, a novel extension of determinantal point process (DPP) to multi-agent setting. Q-DPP promotes agents to acquire diverse behavioral models; this allows  a natural factorization of the joint Q-functions with no need for \emph{a priori} structural constraints on the value function or special network architectures. We demonstrate that Q-DPP generalizes major solutions including VDN, QMIX, and QTRAN on decentralizable cooperative tasks. To efficiently draw samples  from Q-DPP, we develop a linear-time sampler with theoretical approximation guarantee. Our sampler also benefits exploration by  coordinating agents to cover orthogonal directions in the state space during training. We evaluate our algorithm on multiple cooperative benchmarks; its effectiveness has been demonstrated when compared with the state-of-the-art. 
","['Huawei UK', 'UCL', 'UCL', 'Shanghai Jiao Tong University', ""Huawei Noah's Ark Lab"", ""Noah's Ark Laboratory, Huawei"", 'Shanghai Jiao Tong University']"
2020,Scalable Gaussian Process Separation for Kernels with a Non-Stationary Phase,"Jan Graßhoff, Alexandra Jankowski, Philipp Rostalski",https://icml.cc/Conferences/2020/Schedule?showEvent=6201,"The application of Gaussian processes (GPs) to large data sets is limited due to heavy memory and computational requirements. A variety of methods has been proposed to enable scalability, one of which is to exploit structure in the kernel matrix. Previous methods, however, cannot easily deal with mixtures of non-stationary processes. This paper investigates an efficient GP framework, that extends structured kernel interpolation methods to GPs with a non-stationary phase. We particularly treat the separation of nonstationary sources, which is a problem that commonly arises e.g. in spatio-temporal biomedical datasets. Our approach employs multiple sets of non-equidistant inducing points to account for the non-stationarity and retrieve Toeplitz and Kronecker structure in the kernel matrix allowing for efficient inference and kernel learning. Our approach is demonstrated on numerical examples and large spatio-temporal biomedical problems.
","['Universität zu Lübeck', 'Institute for Electrical Engineering in Medicine, Universität zu Lübeck', 'Universität zu Lübeck']"
2020,Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks,"Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Nir Shavit, Dan Alistarh",https://icml.cc/Conferences/2020/Schedule?showEvent=6840,"Optimizing convolutional neural networks for fast inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network.
Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions.
In this paper, we present an in-depth analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains.
To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss.
Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with little or no retraining cost.
","['Neural Magic', 'Neural Magic', 'Neural Magic', 'Neural Magic', 'Neural Magic', 'Neural Magic', 'Neural Magic', 'Neural Magic', 'Neural Magic', 'IST Austria & NeuralMagic']"
2020,Training Binary Neural Networks using the Bayesian Learning Rule,"Xiangming Meng, Roman Bachmann, Mohammad Emtiyaz Khan",https://icml.cc/Conferences/2020/Schedule?showEvent=6823,"Neural networks with binary weights are computation-efficient and hardware-friendly, but their training is challenging because it involves a discrete optimization problem. Surprisingly, ignoring the discrete nature of the problem and using gradient-based methods, such as Straight-Through Estimator, still works well in practice. This raises the question: are there principled approaches which justify such methods? In this paper, we propose such an approach using the Bayesian learning rule. The rule, when applied to estimate a Bernoulli distribution over the binary weights, results in an algorithm which justifies some of the algorithmic choices made by the previous approaches. The algorithm not only obtains state-of-the-art performance, but also enables uncertainty estimation and continual learning to avoid catastrophic forgetting. Our work provides a principled approach for training binary neural networks which also justifies and extends existing approaches. 
","['Riken', 'EPFL', 'RIKEN']"
2020,Linear bandits with Stochastic Delayed Feedback,"Claire Vernade, Alexandra Carpentier, Tor Lattimore, Giovanni Zappella, Beyza Ermis, Michael Brueckner",https://icml.cc/Conferences/2020/Schedule?showEvent=6130,"Stochastic linear bandits are a natural and well-studied model for structured exploration/exploitation problems and are widely used in applications such as on-line marketing and recommendation.
One of the main challenges faced by practitioners hoping to apply existing algorithms is that usually the feedback is randomly delayed and delays are only partially observable.
For example, while a purchase is usually observable some time after the display, the decision of not buying is never explicitly sent to the system.
In other words, the learner only observes delayed positive events.
We formalize this problem as a novel stochastic delayed linear bandit and propose OTFLinUCB and OTFLinTS, two computationally efficient algorithms able to integrate new information as it
becomes available and to deal with the permanently censored feedback. We prove optimal O(d\sqrt{T}) bounds on the regret of the first algorithm and study the dependency on delay-dependent parameters.
Our model, assumptions and results are validated by experiments on simulated and real data.
","['DeepMind', 'Otto-von-Guericke University', 'DeepMind', 'Amazon', 'Amazon Research', 'Amazon Research Berlin']"
2020,The continuous categorical: a novel simplex-valued exponential family,"Elliott Gordon-Rodriguez, Gabriel Loaiza-Ganem, John Cunningham",https://icml.cc/Conferences/2020/Schedule?showEvent=6542,"Simplex-valued data appear throughout statistics and machine learning, for example in the context of transfer learning and compression of deep networks. Existing models for this class of data rely on the Dirichlet distribution or other related loss functions; here we show these standard choices suffer systematically from a number of limitations, including bias and numerical issues that frustrate the use of flexible network models upstream of these distributions. We resolve these limitations by introducing a novel exponential family of distributions for modeling simplex-valued data – the continuous categorical, which arises as a nontrivial multivariate generalization of the recently discovered continuous Bernoulli. Unlike the Dirichlet and other typical choices, the continuous categorical results in a well-behaved probabilistic loss function that produces unbiased estimators, while preserving the mathematical simplicity of the Dirichlet. As well as exploring its theoretical properties, we introduce sampling methods for this distribution that are amenable to the reparameterization trick, and evaluate their performance. Lastly, we demonstrate that the continuous categorical outperforms standard choices empirically, across a simulation study, an applied example on multi-party elections, and a neural network compression task.
","['Columbia University', 'Layer 6 AI', 'Columbia']"
2020,Optimistic Policy Optimization with Bandit Feedback,"Lior Shani, Yonathan Efroni, Aviv Rosenberg, Shie Mannor",https://icml.cc/Conferences/2020/Schedule?showEvent=5890,"Policy optimization methods are one of the most widely used classes of Reinforcement Learning (RL) algorithms. Yet, so far, such methods have been mostly analyzed from an optimization perspective, without addressing the problem of exploration, or by making strong assumptions on the interaction with the environment. 
In this paper we consider model-based RL in the tabular finite-horizon MDP setting with unknown transitions and bandit feedback. For this setting, we propose an optimistic trust region policy optimization (TRPO) algorithm for which we establish $\tilde O(\sqrt{S^2 A H^4 K})$ regret for stochastic rewards. Furthermore, we prove $\tilde O( \sqrt{ S^2 A H^4 }  K^{2/3} ) $ regret for adversarial rewards. Interestingly, this result matches previous bounds derived for the bandit feedback case, yet with known transitions. To the best of our knowledge, the two results are the first sub-linear regret bounds obtained for policy optimization algorithms with unknown transitions and bandit feedback.","['Technion', 'Technion', 'Tel Aviv University', 'Technion']"
2020,Variational Autoencoders with Riemannian Brownian Motion Priors,"Dimitris Kalatzis, David Eklund, Georgios Arvanitidis, Søren Hauberg",https://icml.cc/Conferences/2020/Schedule?showEvent=6397,"Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.
","['Technical University of Denmark', 'Research Institutes of Sweden', 'MPI for Intelligent Systems, Tübingen', 'Technical University of Denmark']"
2020,Probing Emergent Semantics in Predictive Agents via Question Answering,"Abhishek Das, Federico Carnevale, Hamza Merzic, Laura Rimell, Rosalia Schneider, Josh Abramson, Alden Hung, Arun Ahuja, Stephen Clark, Greg Wayne, Feilx Hill",https://icml.cc/Conferences/2020/Schedule?showEvent=6531,"Recent work has shown how predictive modeling can endow agents with rich knowledge of their surroundings, improving their ability to act in complex environments. We propose question-answering as a general paradigm to decode and understand the representations that such agents develop, applying our method to two recent approaches to predictive modelling - action-conditional CPC (Guo et al., 2018) and SimCore (Gregor et al., 2019). After training agents with these predictive objectives in a visually-rich, 3D environment with an assortment of objects, colors, shapes, and spatial configurations, we probe their internal state representations with a host of synthetic (English) questions, without backpropagating gradients from the question-answering decoder into the agent. The performance of different agents when probed in this way reveals that they learn to encode factual, and seemingly compositional, information about objects, properties and spatial relations from their physical environment. Our approach is intuitive, i.e. humans can easily interpret the responses of the model as opposed to inspecting continuous vectors, and model-agnostic, i.e. applicable to any modeling approach. By revealing the implicit knowledge of objects, quantities, properties and relations acquired by agents as they learn, question-conditional agent probing can stimulate the design and development of stronger predictive learning objectives.
","['Facebook AI Research', 'Deepmind', 'DeepMind', '', 'DeepMind', 'DeepMind', 'DeepMind', 'DeepMind', 'University of Cambridge/Deepmind', 'DeepMind', 'Deepmind']"
2020,Forecasting Sequential Data Using Consistent Koopman Autoencoders,"Omri Azencot, N. Benjamin Erichson, Vanessa Lin, Michael Mahoney",https://icml.cc/Conferences/2020/Schedule?showEvent=6182,"Recurrent neural networks are widely used on time series data, yet such models often ignore the underlying physical structures in such sequences. A new class of physics-based methods related to Koopman theory has been introduced, offering an alternative for processing nonlinear dynamical systems. In this work, we propose a novel Consistent Koopman Autoencoder model which, unlike the majority of existing work, leverages the forward and backward dynamics. Key to our approach is a new analysis which explores the interplay between consistent dynamics and their associated Koopman operators. Our network is directly related to the derived analysis, and its computational requirements are comparable to other baselines. We evaluate our method on a wide range of high-dimensional and short-term dependent problems, and it achieves accurate estimates for significant prediction horizons, while also being robust to noise.
","['UCLA', 'University of California, Berkeley', 'University of California Berkeley', 'UC Berkeley']"
2020,Parallel Algorithm for Non-Monotone DR-Submodular Maximization,"Alina Ene, Huy Nguyen",https://icml.cc/Conferences/2020/Schedule?showEvent=6522,"In this work, we give a new parallel algorithm for the problem of maximizing a non-monotone diminishing returns submodular function subject to a cardinality constraint. For any desired accuracy $\epsilon$, our algorithm achieves a $1/e - \epsilon$ approximation using $O(\log{n} \log(1/\epsilon) / \epsilon^3)$ parallel rounds of function evaluations. The approximation guarantee nearly matches the best approximation guarantee known for the problem in the sequential setting and the number of parallel rounds is nearly-optimal for any constant $\epsilon$. Previous algorithms achieve worse approximation guarantees using $\Omega(\log^2{n})$ parallel rounds. Our experimental evaluation suggests that our algorithm obtains solutions whose objective value nearly matches the value obtained by the state of the art sequential algorithms, and it outperforms previous parallel algorithms in number of parallel rounds, iterations, and solution quality.","['Boston University', 'Northeastern University']"
2020,Randomization matters How to defend against strong adversarial attacks,"Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, Jamal Atif",https://icml.cc/Conferences/2020/Schedule?showEvent=6157,"\emph{Is there a classifier that ensures optimal robustness against all adversarial attacks?}
This paper tackles the question by adopting a game-theoretic point of view. We present the adversarial attacks and defenses problem as an \emph{infinite} zero-sum game where classical results (\emph{e.g.} Nash or Sion theorems) do not apply. We demonstrate the non-existence of a Nash equilibrium in our game when the classifier and the Adversary are both deterministic, hence giving a negative answer to the above question in the deterministic regime. Nonetheless, the question remains open in the randomized regime. We tackle this problem by showing that, under mild conditions on the dataset distribution, any deterministic classifier can be outperformed by a randomized one. This gives arguments for using randomization, and leads us to a simple method for building randomized classifiers that are robust to state-or-the-art adversarial attacks. Empirical results validate our theoretical analysis, and show that our defense method considerably outperforms Adversarial Training against strong adaptive attacks, by achieving 0.55 accuracy under adaptive PGD-attack on CIFAR10, compared to 0.42 for Adversarial training.
","['Dauphine University - CEA LIST', 'Université Paris-Dauphine', 'Université Paris Dauphine - PSL', 'Univ. Paris Dauphine', 'Université Paris-Dauphine']"
2020,Reliable Fidelity and Diversity Metrics for Generative Models,"Muhammad Ferjad Naeem, Seong Joon Oh, Yunjey Choi, Youngjung Uh, Jaejun Yoo",https://icml.cc/Conferences/2020/Schedule?showEvent=5832,"Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Frechet Inception Distance (FID) score. Since it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics.
","['Technical University of Munich', 'Clova AI Research, NAVER Corp.', 'Clova AI Research, NAVER Corp.', 'Clova AI Research, NAVER Corp.', 'EPFL']"
2020,Spectral Subsampling MCMC for Stationary Time Series,"Robert Salomone, Matias Quiroz, Robert kohn, Mattias Villani, Minh-Ngoc Tran",https://icml.cc/Conferences/2020/Schedule?showEvent=6744,"Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets has developed rapidly in recent years. However, the underlying methods are generally limited to relatively simple settings where the data have specific forms of independence. We propose a novel technique for speeding up MCMC for time series data by efficient data subsampling in the frequency domain. For several challenging time series models, we demonstrate a speedup of up to two orders of magnitude while incurring negligible bias compared to MCMC on the full dataset. We also propose alternative control variates for variance reduction based on data grouping and coreset constructions.
","['University of New South Wales', 'University of Technology Sydney', 'UNSW', 'Linkoeping University', 'U of Sydney']"
2020,Off-Policy Actor-Critic with Shared Experience Replay,"Simon Schmitt, Matteo Hessel, Karen Simonyan",https://icml.cc/Conferences/2020/Schedule?showEvent=5955,"We investigate the combination of actor-critic reinforcement learning algorithms with a uniform large-scale experience replay and propose solutions for two ensuing challenges: (a) efficient actor-critic learning with experience replay (b) the stability of off-policy learning where agents learn from other agents behaviour. To this end we analyze the bias-variance tradeoffs in V-trace, a form of importance sampling for actor-critic methods. Based on our analysis, we then argue for mixing experience sampled from replay with on-policy experience, and propose a new trust region scheme that scales effectively to data distributions where V-trace becomes unstable. We provide extensive empirical validation of the proposed solutions on DMLab-30 and further show the benefits of this setup in two training regimes for Atari: (1) a single agent is trained up until 200M environment frames per game (2) a population of agents is trained up until 200M environment frames each and may share experience. We demonstrate state-of-the-art data efficiency among model-free agents in both regimes.
","['DeepMind', 'Deep Mind', 'DeepMind']"
2020,Deep Streaming Label Learning,"Zhen Wang, Liu Liu, Dacheng Tao",https://icml.cc/Conferences/2020/Schedule?showEvent=5800,"In multi-label learning, each instance can be associated with multiple and non-exclusive labels. Previous studies assume that all the labels in the learning process are fixed and static; however, they ignore the fact that the labels will emerge continuously in changing environments. In order to fill in these research gaps, we propose a novel deep neural network (DNN) based framework, Deep Streaming Label Learning (DSLL), to classify instances with newly emerged labels effectively. DSLL can explore and incorporate the knowledge from past labels and historical models to understand and develop emerging new labels. DSLL consists of three components: 1) a streaming label mapping to extract deep relationships between new labels and past labels with a novel label-correlation aware loss; 2) a streaming feature distillation propagating feature-level knowledge from the historical model to a new model; 3) a senior student network to model new labels with the help of knowledge learned from the past. Theoretically, we prove that DSLL admits tight generalization error bounds for new labels in the DNN framework. Experimentally, extensive empirical results show that the proposed method performs significantly better than the existing state-of-the-art multi-label learning methods to handle the continually emerging new labels.
","['University of Sydney', 'The University of Sydney', 'The University of Sydney']"
2020,Curse of Dimensionality on Randomized Smoothing for Certifiable Robustness,"Aounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi",https://icml.cc/Conferences/2020/Schedule?showEvent=6282,"Randomized smoothing, using just a simple isotropic Gaussian distribution, has been shown to produce good robustness guarantees against $\ell_2$-norm bounded adversaries. In this work, we show that extending the smoothing technique to defend against other attack models can be challenging, especially in the high-dimensional regime.  In particular, for a vast class of i.i.d.~smoothing distributions, we prove that the largest $\ell_p$-radius that can be certified decreases as $O(1/d^{\frac{1}{2} - \frac{1}{p}})$ with dimension $d$ for $p > 2$. Notably, for $p \geq 2$, this dependence on $d$ is no better than that of the $\ell_p$-radius that can be certified using isotropic Gaussian smoothing, essentially putting a matching lower bound on the robustness radius.
When restricted to {\it generalized} Gaussian smoothing, these two bounds can be shown to be within a constant factor of each other in an asymptotic sense, establishing that Gaussian smoothing provides the best possible results, up to a constant factor, when $p \geq 2$. We present experimental results on CIFAR to validate our theory.
For other smoothing distributions, such as, a uniform distribution within an $\ell_1$ or an $\ell_\infty$-norm ball, we show upper bounds of the form $O(1 / d)$ and $O(1 / d^{1 - \frac{1}{p}})$ respectively, which have an even worse dependence on $d$. 
","['University of Maryland, College Park', 'University of Maryland', 'University of Maryland', 'University of Maryland']"
2020,Stochastic Optimization for Regularized Wasserstein Estimators,"Marin Ballu, Quentin Berthet, Francis Bach",https://icml.cc/Conferences/2020/Schedule?showEvent=6517,"Optimal transport is a foundational problem in optimization, that allows to compare probability distributions while taking into account geometric aspects. Its optimal objective value, the Wasserstein distance, provides an important loss between distributions that has been used in many applications throughout machine learning and statistics. Recent algorithmic progress on this problem and its regularized versions have made these tools increasingly popular. However, existing techniques require solving an optimization problem to obtain a single gradient of the loss, thus slowing down first-order methods to minimize the sum of losses, that require many such gradient computations. In this work, we introduce an algorithm to solve a regularized version of this problem of Wasserstein estimators, with a time per step which is sublinear in the natural dimensions of the problem. We introduce a dual formulation, and optimize it with stochastic gradient steps that can be computed directly from samples, without solving additional optimization problems at each step. Doing so, the estimation and computation tasks are performed jointly. We show that this algorithm can be extended to other tasks, including estimation of Wasserstein barycenters. We provide theoretical guarantees and illustrate the performance of our algorithm with experiments on synthetic data.
","['University of Cambridge', 'Google Brain', 'INRIA - Ecole Normale Supérieure']"
2020,Learning Factorized Weight Matrix for Joint Filtering,"Xiangyu Xu, Yongrui Ma, Wenxiu Sun",https://icml.cc/Conferences/2020/Schedule?showEvent=5833,"Joint filtering is a fundamental problem in computer vision with applications in many different areas. Most existing algorithms solve this problem with a weighted averaging process to aggregate input pixels. However, the weight matrix of this process is often empirically designed and not robust to complex input. In this work, we propose to learn the weight matrix for joint image filtering. This is a challenging problem, as directly learning a large weight matrix is computationally intractable. To address this issue, we introduce the correlation of deep features to approximate the aggregation weights. However, this strategy only uses inner product for the weight matrix estimation, which limits the performance of the proposed algorithm. Therefore, we further propose to learn a nonlinear function to predict sparse residuals of the feature correlation matrix. Note that the proposed method essentially factorizes the weight matrix into a low-rank and a sparse matrix and then learn both of them simultaneously with deep neural networks. Extensive experiments show that the proposed algorithm compares favorably against the state-of-the-art approaches on a wide variety of joint filtering tasks.
","['Carnegie Mellon University', 'Sensetime Research', 'SenseTime Research']"
2020,Neural Network Control Policy Verification With Persistent Adversarial Perturbation,"Yuh-Shyang Wang, Tsui-Wei Weng, Luca Daniel",https://icml.cc/Conferences/2020/Schedule?showEvent=6716,"Deep neural networks are known to be fragile to small adversarial perturbations, which raises serious concerns when a neural network policy is interconnected with a physical system in a closed loop. In this paper, we show how to combine recent works on static neural network certification tools with robust control theory to certify a neural network policy in a control loop. We give a sufficient condition and an algorithm to ensure that the closed loop state and control constraints are satisfied when the persistent adversarial perturbation is l-infinity norm bounded. Our method is based on finding a positively invariant set of the closed loop dynamical system, and thus we do not require the continuity of the neural network policy. Along with the verification result, we also develop an effective attack strategy for neural network control systems that outperforms exhaustive Monte-Carlo search significantly. We show that our certification algorithm works well on learned models and could achieve 5 times better result than the traditional Lipschitz-based method to certify the robustness of a neural network policy on the cart-pole balance control problem.
","['Argo AI', 'MIT', 'MIT']"
2020,Sample Complexity Bounds for 1-bit Compressive Sensing and Binary Stable Embeddings with Generative Priors,"Zhaoqiang Liu, Selwyn Gomes, Avtansh Tiwari, Jonathan Scarlett",https://icml.cc/Conferences/2020/Schedule?showEvent=5822,"The goal of standard 1-bit compressive sensing is to accurately recover an unknown sparse vector from binary-valued measurements, each indicating the sign of a linear function of the vector.  Motivated by recent advances in compressive sensing with generative models, where a generative modeling assumption replaces the usual sparsity assumption, we study the problem of 1-bit compressive sensing with generative models. We first consider noiseless 1-bit measurements, and provide sample complexity bounds for approximate recovery under i.i.d.~Gaussian measurements and a Lipschitz continuous generative prior, as well as a near-matching algorithm-independent lower bound. Moreover, we demonstrate that the Binary $\epsilon$-Stable Embedding property, which characterizes the robustness of the reconstruction to measurement errors and noise, also holds for 1-bit compressive sensing with Lipschitz continuous generative models with sufficiently many Gaussian measurements.  In addition, we apply our results to neural network generative models, and provide a proof-of-concept numerical experiment demonstrating significant improvements over sparsity-based approaches.","['NUS', 'National University of Singapore', 'IIT Kanpur ', 'National University of Singapore']"
2020,Do We Need Zero Training Loss After Achieving Zero Training Error?,"Takashi Ishida, Ikko Yamane, Tomoya Sakai, Gang Niu, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6799,"Overparameterized deep networks have the capacity to memorize training data with zero \emph{training error}. Even after memorization, the \emph{training loss} continues to approach zero, making the model overconfident and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, it is hard to tune their hyperparameters in order to maintain a fixed/preset level of training loss. We propose a direct solution called \emph{flooding} that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the \emph{flood level}. Our approach makes the loss float around the flood level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flood level. This can be implemented with one line of code and is compatible with any stochastic optimizer and other regularizers. With flooding, the model will continue to ``random walk'' with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization. We experimentally show that flooding improves performance and, as a byproduct, induces a double descent curve of the test loss.
","['The University of Tokyo / RIKEN', 'The University of Tokyo', 'NEC', 'RIKEN', 'RIKEN / The University of Tokyo']"
2020,On the (In)tractability of Computing Normalizing Constants for the Product of Determinantal Point Processes,"Naoto Ohsaka, Tatsuya Matsuoka",https://icml.cc/Conferences/2020/Schedule?showEvent=6379,"We consider the product of determinantal point processes (DPPs), a point process whose probability mass is proportional to the product of principal minors of multiple matrices as a natural, promising generalization of DPPs.
We study the computational complexity of computing its normalizing constant, which is among the most essential probabilistic inference tasks.
Our complexity-theoretic results (almost) rule out the existence of efficient algorithms for this task, unless input matrices are forced to have favorable structures.
In particular, we prove the following:
(1) Computing $\sum_{S} \det(\mat{A}_{S,S})^p$ exactly for every (fixed) positive even integer $p$ is UP-hard and Mod3P-hard, which gives a negative answer to an open question posed by Kulesza and Taskar (2012).
(2) $\sum_{S} \det(\mat{A}_{S,S}) \det(\mat{B}_{S,S}) \det(\mat{C}_{S,S})$ is NP-hard to approximate within a factor of $ 2^{O(|I|^{1-\epsilon})} $ for any $\epsilon > 0$, where $|I|$ is the input size. This result is stronger than #P-hardness for the case of two matrices by Gillenwater (2014).
(3) There exists a $ k^{O(k)} |I|^{O(1)} $-time algorithm for computing $\sum_{S} \det(\mat{A}_{S,S}) \det(\mat{B}_{S,S})$, where $k$ is ``the maximum rank of $\mat{A}$ and $\mat{B}$'' or ``the treewidth of the graph formed by nonzero entries of $\mat{A}$ and $\mat{B}$.'' Such parameterized algorithms are said to be fixed-parameter tractable.","['NEC Corporation', 'NEC Corporation']"
2020,A Tree-Structured Decoder for Image-to-Markup Generation,"Jianshu Zhang, Jun Du, Yongxin Yang, Yi-Zhe Song, Si Wei, Lirong Dai",https://icml.cc/Conferences/2020/Schedule?showEvent=6332,"Recent encoder-decoder approaches typically employ string decoders to convert images into serialized strings for image-to-markup. However, for tree-structured representational markup, string representations can hardly cope with the structural complexity. In this work, we first show via a set of toy problems that string decoders struggle to decode tree structures, especially as structural complexity increases, we then propose a tree-structured decoder that specifically aims at generating a tree-structured markup. Our decoders works sequentially, where at each step a child node and its parent node are simultaneously generated to form a sub-tree. This sub-tree is consequently used to construct the final tree structure in a recurrent manner. Key to the success of our tree decoder is twofold, (i) it strictly respects the parent-child relationship of trees, and (ii) it explicitly outputs trees as oppose to a linear string. Evaluated on both math formula recognition and chemical formula recognition, the proposed tree decoder is shown to greatly outperform strong string decoder baselines.
","['University of Science and Technology of China', 'University of Science and Technology of China', 'University of Surrey', 'University of Surrey', 'iFLYTEK', 'N/A']"
2020,Does the Markov Decision Process Fit the Data: Testing for the Markov Property in Sequential Decision Making,"Chengchun Shi, Runzhe Wan, Rui Song, Wenbin Lu, Ling Leng",https://icml.cc/Conferences/2020/Schedule?showEvent=5961,"The Markov assumption (MA) is fundamental to the empirical validity of reinforcement learning. In this paper, we propose a novel Forward-Backward Learning procedure to test MA in sequential decision making. The proposed test does not assume any parametric form on the joint distribution of the observed data and plays an important role for identifying the optimal policy in high-order Markov decision processes (MDPs) and partially observable MDPs. Theoretically, we establish the validity of our test. Empirically, we apply our test to both synthetic datasets and a real data example from mobile health studies to illustrate its usefulness.
","['London School of Economics and Political Science', 'North Carolina State University', 'North Carolina State University', 'North Carolina State University', 'Amazon']"
2020,Sparse Subspace Clustering with Entropy-Norm,"Liang Bai, Jiye Liang",https://icml.cc/Conferences/2020/Schedule?showEvent=6088,"In this paper, we provide an explicit theoretical connection between Sparse subspace clustering (SSC) and spectral clustering (SC) from the perspective of learning a data similarity matrix. We show that spectral clustering with Gaussian kernel can be viewed as sparse subspace clustering with entropy-norm (SSC+E). Compared to SSC, SSC+E can obtain an analytical, symmetrical, nonnegative and nonlinearly-representational similarity matrix. Besides, SSC+E makes use of Gaussian kernel to compute the sparse similarity matrix of objects, which can avoid the complex computation of the sparse optimization program of SSC. Finally, we provide the experimental analysis to compare the efficiency and effectiveness of sparse subspace clustering and spectral clustering on ten benchmark data sets. The theoretical and experimental analysis can well help users for the selection of high-dimensional data clustering algorithms.
","['Shanxi University, China', 'Shanxi University']"
2020,Learning Autoencoders with Relational Regularization,"Hongteng Xu, Dixin Luo, Ricardo Henao, Svati Shah, Lawrence Carin",https://icml.cc/Conferences/2020/Schedule?showEvent=6186,"We propose a new algorithmic framework for learning autoencoders of data distributions. 
In this framework, we minimize the discrepancy between the model distribution and the target one, with relational regularization on learnable latent prior. 
This regularization penalizes the fused Gromov-Wasserstein (FGW) distance between the latent prior and its corresponding posterior, which allows us to learn a structured prior distribution associated with the generative model in a flexible way. 
Moreover, it helps us co-train multiple autoencoders even if they are with heterogeneous architectures and incomparable latent spaces. 
We implement the framework with two scalable algorithms, making it applicable for both probabilistic and deterministic autoencoders. 
Our relational regularized autoencoder (RAE) outperforms existing methods, e.g., variational autoencoder, Wasserstein autoencoder, and their variants, on generating images. 
Additionally, our relational co-training strategy of autoencoders achieves encouraging results in both synthesis and real-world multi-view learning tasks.
","['InfiniaML, Inc.', 'Duke University', 'Duke University', 'Duke University', 'Duke']"
2020,Representation Learning via Adversarially-Contrastive Optimal Transport,"Anoop Cherian, Shuchin Aeron",https://icml.cc/Conferences/2020/Schedule?showEvent=6759,"In this paper, we study the problem of learning compact (low-dimensional) representations for sequential data that captures its implicit spatio-temporal cues. To maximize extraction of such informative cues from the data, we set the problem within the context of contrastive representation learning and to that end propose a novel objective via optimal transport. Specifically, our formulation seeks a low-dimensional subspace representation of the data that jointly (i) maximizes the distance of the data (embedded in this subspace) from an adversarial data distribution under the optimal transport, a.k.a. the Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the data distortion. To generate the adversarial distribution, we propose a novel framework connecting Wasserstein GANs with a classifier, allowing a principled mechanism for producing good negative distributions for contrastive learning, which is currently a challenging problem. Our full objective is cast as a subspace learning problem on the Grassmann manifold and solved via Riemannian optimization. To empirically study our formulation, we provide experiments on the task of human action recognition in video sequences. Our results demonstrate competitive performance against challenging baselines.
","['MERL', 'Tufts University']"
2020,Projection-free Distributed Online Convex Optimization with $O(\sqrt{T})$ Communication Complexity,"Yuanyu Wan, Wei-Wei Tu, Lijun Zhang",https://icml.cc/Conferences/2020/Schedule?showEvent=6112,"To deal with complicated constraints via locally light computation in distributed online learning, a recent study has presented a projection-free algorithm called distributed online conditional gradient (D-OCG), and achieved an $O(T^{3/4})$ regret bound, where $T$ is the number of prediction rounds. However, in each round, the local learners of D-OCG need to communicate with their neighbors to share the local gradients, which results in a high communication complexity of $O(T)$. In this paper, we first propose an improved variant of D-OCG, namely D-BOCG, which enjoys an $O(T^{3/4})$ regret bound with only $O(\sqrt{T})$ communication complexity. The key idea is to divide the total prediction rounds into $\sqrt{T}$ equally-sized blocks, and only update the local learners at the beginning of each block by performing iterative linear optimization steps. Furthermore, to handle the more challenging bandit setting, in which only the loss value is available, we incorporate the classical one-point gradient estimator into D-BOCG, and obtain similar theoretical guarantees.","['Nanjing University', '4Paradigm Inc.', 'Nanjing University']"
2020,Dual-Path Distillation: A Unified Framework to Improve Black-Box Attacks,"Yonggang Zhang, Ya Li, Tongliang Liu, Xinmei Tian",https://icml.cc/Conferences/2020/Schedule?showEvent=6318,"We study the problem of constructing black-box adversarial attacks, where no model information is revealed except for the feedback knowledge of the given inputs. To obtain sufficient knowledge for crafting adversarial examples, previous methods query the target model with inputs that are perturbed with different searching directions. However, these methods suffer from poor query efficiency since the employed searching directions are sampled randomly. To mitigate this issue, we formulate the goal of mounting efficient attacks as an optimization problem in which the adversary tries to fool the target model with a limited number of queries. Under such settings, the adversary has to select appropriate searching directions to reduce the number of model queries. By solving the efficient-attack problem, we find that we need to distill the knowledge in both the path of the adversarial examples and the path of the searching directions. Therefore, we propose a novel framework, dual-path distillation, that utilizes the feedback knowledge not only to craft adversarial examples but also to alter the  searching directions to achieve efficient attacks.  Experimental results suggest that our framework can significantly increase the query efficiency.
","['University of Science and Technology of China', 'IFLYTEK Research', 'The University of Sydney', 'USTC']"
2020,On the Power of Compressed Sensing with Generative Models ,"Akshay Kamath, Eric Price, Sushrut Karmalkar",https://icml.cc/Conferences/2020/Schedule?showEvent=6479,"The goal of compressed sensing is to learn a structured signal $x$ from a limited number of noisy linear measurements $y \approx Ax$. In traditional compressed sensing, ``structure'' is represented by sparsity in some known basis.  Inspired by the success of deep learning in modeling images, recent work starting with Bora-Jalal-Price-Dimakis'17 has instead considered structure to come from a generative model $G: \mathbb{R}^k \to \mathbb{R}^n$. We present two results establishing the difficulty and strength of this latter task, showing that existing bounds are tight: First, we provide a lower bound matching the Bora et.al upper  bound for compressed sensing with $L$-Lipschitz generative models $G$ which holds even for the more relaxed goal of \emph{non-uniform} recovery. Second, we show that generative models generalize sparsity as a representation of structure by constructing a ReLU-based neural network with $2$ hidden layers and $O(n)$ activations per layer whose range is precisely the set of all $k$-sparse vectors.","['University of Texas at Austin', 'UT-Austin', 'University of Texas at Austin']"
2020,Multinomial Logit Bandit with Low Switching Cost,"Kefan Dong, Yingkai Li, Qin Zhang, Yuan Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=5867,"We study multinomial logit bandit with limited adaptivity, where the algorithms change their exploration actions as infrequently as possible when achieving almost optimal minimax regret. We propose two measures of adaptivity: the assortment switching cost and the more fine-grained item switching cost. We present an anytime algorithm (AT-DUCB) with $O(N \log T)$ assortment switches, almost matching the lower bound $\Omega(\frac{N \log T}{ \log \log T})$. In the fixed-horizon setting, our algorithm FH-DUCB incurs $O(N \log \log T)$ assortment switches, matching the asymptotic lower bound. We also present the ESUCB algorithm with item switching cost $O(N \log^2 T)$.","['Tsinghua University', 'Northwestern University', 'Indiana University Bloomington', 'UIUC']"
2020,Transformer Hawkes Process,"Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, Hongyuan Zha",https://icml.cc/Conferences/2020/Schedule?showEvent=6202,"Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.
","['Georgia Institute of Technology', 'Georgia Tech', 'University of Science and technology of China', 'Georgia Tech', 'Georgia Institute of Technology']"
2020,Cost-effectively Identifying Causal Effects When Only Response Variable is Observable,"Tian-Zuo Wang, Xi-Zhu Wu, Sheng-Jun Huang, Zhi-Hua Zhou",https://icml.cc/Conferences/2020/Schedule?showEvent=6083,"In many real tasks, we care about how to make decisions rather than mere predictions on an event, e.g. how to increase the revenue next month instead of merely knowing it will drop. The key is to identify the causal effects on the desired event. It is achievable with do-calculus if the causal structure is known; however, in many real tasks it is not easy to infer the whole causal structure with the observational data. Introducing external interventions is needed to achieve it. In this paper, we study the situation where only the response variable is observable under intervention. We propose a novel approach which is able to cost-effectively identify the causal effects, by an active strategy introducing limited interventions, and thus guide decision-making. Theoretical analysis and empirical studies validate the effectiveness of the proposed approach.
","['Nanjing University', 'Nanjing University', 'Nanjing University of Aeronautics and Astronautics', 'Nanjing University']"
2020,Self-Attentive Associative Memory,"Hung Le, Truyen Tran, Svetha Venkatesh",https://icml.cc/Conferences/2020/Schedule?showEvent=5984,"Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering. 
","['Deakin University', 'Deakin University', 'Deakin University']"
2020,Non-autoregressive Machine Translation with Disentangled Context Transformer,"Jungo Kasai, James Cross, Marjan Ghazvininejad, Jiatao Gu",https://icml.cc/Conferences/2020/Schedule?showEvent=5827,"State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 translation directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average. 
","['University of Washington', 'Facebook', 'Facebook AI Research (FAIR)', 'Facebook AI Research']"
2020,Multi-objective Bayesian Optimization using Pareto-frontier Entropy,"Shinya Suzuki, Shion Takeno, Tomoyuki Tamura, Kazuki Shitara, Masayuki Karasuyama",https://icml.cc/Conferences/2020/Schedule?showEvent=6773,"This paper studies an entropy-based multi-objective Bayesian optimization (MBO). Existing entropy-based MBO methods need complicated approximations to evaluate entropy or employ over-simplification that ignores trade-off among objectives. We propose a novel entropy-based MBO called Pareto-frontier entropy search (PFES), which is based on the information gain of Pareto-frontier. We show that our entropy evaluation can be reduced to a closed form whose computation is quite simple while capturing the trade-off relation in Pareto-frontier. We further propose an extension for the ``decoupled'' setting, in which each objective function can be observed separately, and show that the PFES-based approach derives a natural extension of the original acquisition function which can also be evaluated simply. Our numerical experiments show effectiveness of PFES through several benchmark datasets, and real-word datasets from materials science.
","['Nagoya Institute of Technology', 'Nagoya Institute of Technology', 'National Institute for Material Science', 'Osaka University', 'Nagoya Institute of Technology']"
2020,DropNet: Reducing Neural Network Complexity via Iterative Pruning,"Chong Min John Tan, Mehul Motani",https://icml.cc/Conferences/2020/Schedule?showEvent=6092,"Modern deep neural networks require a significant amount of computing time and power to train and deploy, which limits their usage on edge devices. Inspired by the iterative weight pruning in the Lottery Ticket Hypothesis, we propose DropNet, an iterative pruning method which prunes nodes/filters to reduce network complexity. DropNet iteratively removes nodes/filters with the lowest average post-activation value across all training samples. Empirically, we show that DropNet is robust across a wide range of scenarios, including MLPs and CNNs using the MNIST and CIFAR datasets. We show that up to 90% of the nodes/filters can be removed without any significant loss of accuracy. The final pruned network performs well even with reinitialisation of the weights and biases. DropNet also achieves similar accuracy to an oracle which greedily removes nodes/filters one at a time to minimise training loss, highlighting its effectiveness.
","['National University of Singapore', 'NUS']"
2020,Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup,"Jang-Hyun Kim, Wonho Choo, Hyun Oh Song",https://icml.cc/Conferences/2020/Schedule?showEvent=6827,"While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets, and the source code is available at https://github.com/snu-mllab/PuzzleMix.
","['Seoul National University', 'Seoul National University', 'Seoul National University']"
2020,More Information Supervised Probabilistic Deep Face Embedding Learning,"Ying Huang, Shangfeng Qiu, Wenwei Zhang, Xianghui Luo, Jinzhuo Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=5828,"Researches using margin based comparison loss demonstrate the effectiveness of penalizing the distance between face feature and their corresponding class centers.
Despite their popularity and excellent performance, they do not explicitly encourage the generic embedding learning for an open set recognition problem.
In this paper, we analyse margin based softmax loss in probability view.
With this perspective, we propose two general principles: 1) monotonically decreasing and 2) margin probability penalty, for designing new margin loss functions.
Unlike methods optimized with single comparison metric, we provide a new perspective to treat open set face recognition as a problem of information transmission. 
And the generalization capability for face embedding is gained with more clean information.
An auto-encoder architecture called Linear-Auto-TS-Encoder(LATSE) is proposed to corroborate this finding.
Extensive experiments on several benchmarks demonstrate that LATSE help face embedding to gain more generalization capability and it boost the single model performance with open training dataset to more than 99% on MegaFace test.
","['Guangzhou Huya Technology Co., Ltd.', 'Guangzhou Huya Technology Co., Ltd', 'Guangzhou Huya Information Technologies Co., Limited', 'Guangzhou Huya Technology Co., Ltd', 'University of Oxford']"
2020,A Graph to Graphs Framework for Retrosynthesis Prediction,"Chence Shi, Minkai Xu, Hongyu Guo, Ming Zhang, Jian Tang",https://icml.cc/Conferences/2020/Schedule?showEvent=6449,"A fundamental problem in computational chemistry is to find a set of reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction. Existing state-of-the-art methods rely on matching the target molecule with a large set of reaction templates, which are very computationally expensive and also suffer from the problem of coverage. In this paper, we propose a novel template-free approach called G2Gs by transforming a target molecular graph into a set of reactant molecular graphs. G2Gs first splits the target molecular graph into a set of synthons by identifying the reaction centers, and then translates the synthons to the final reactant graphs via a variational graph translation framework. 
Experimental results show that G2Gs significantly outperforms existing template-free approaches by up to 63% in terms of the top-1 accuracy and achieves a performance close to that of state-of-the-art template-based approaches, but does not require domain knowledge and is much more scalable. 
","['Peking University', 'Shanghai Jiao Tong university', 'National Research Council Canada', 'Peking University', 'HEC Montreal & MILA']"
2020,Minimax Weight and Q-Function Learning for Off-Policy Evaluation,"Masatoshi Uehara, Jiawei Huang, Nan Jiang",https://icml.cc/Conferences/2020/Schedule?showEvent=5860,"We provide theoretical investigations into off-policy evaluation in reinforcement learning using function approximators for (marginalized) importance weights and value functions. Our contributions include: (1) A new estimator, MWL, that directly estimates importance ratios over the state-action distributions, removing the reliance on knowledge of the behavior policy as in prior work  (Liu et.al, 2018), (2) Another new estimator, MQL, obtained by swapping the roles of importance weights and value-functions in MWL. MQL has an intuitive interpretation of minimizing average Bellman errors and can be combined with MWL in a doubly robust manner, (3) Several additional results that offer further insights, including the sample complexities of MWL and MQL, their asymptotic optimality in the tabular setting, how the learned importance weights depend the choice of the discriminator class, and how our methods provide a unified view of some old and new algorithms in RL.
","['Harvard University', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']"
2020,On Layer Normalization in the Transformer Architecture,"Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu",https://icml.cc/Conferences/2020/Schedule?showEvent=5811,"The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.
","['Institute of Computing Technology, Chinese Academy of Sciences', 'Center for Data Science, Peking University', 'Microsoft Research', 'Peking University', 'microsoft.com', 'Nankai University', 'Microsoft', 'Institute of Computing Technology', 'Peking University', 'Microsoft Research Asia']"
2020,Learning Efficient Multi-agent Communication: An Information Bottleneck Approach,"Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, Zinovi Rabinovich",https://icml.cc/Conferences/2020/Schedule?showEvent=5864,"We consider the problem of the limited-bandwidth communication for multi-agent reinforcement learning, where agents cooperate with the assistance of a communication protocol and a scheduler. The protocol and scheduler jointly determine which agent is communicating what message and to whom. Under the limited bandwidth constraint, a communication protocol is required to generate informative messages. Meanwhile, an unnecessary communication connection should not be established because it occupies limited resources in vain. In this paper, we develop an Informative Multi-Agent Communication (IMAC) method to learn efficient communication protocols as well as scheduling. First, from the perspective of communication theory, we prove that the limited bandwidth constraint requires low-entropy messages throughout the transmission. Then inspired by the information bottleneck principle, we learn a valuable and compact communication protocol and a weight-based scheduler. To demonstrate the efficiency of our method, we conduct extensive experiments in various cooperative and competitive multi-agent tasks with different numbers of agents and different bandwidths. We show that IMAC converges faster and leads to efficient communication among agents under the limited bandwidth as compared to many baseline methods.
","['Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University', 'Nanyang Technological University']"
2020,Few-shot Domain Adaptation by Causal Mechanism Transfer,"Takeshi Teshima, Issei Sato, Masashi Sugiyama",https://icml.cc/Conferences/2020/Schedule?showEvent=5935,"We study few-shot supervised domain adaptation (DA) for regression problems, where only a few labeled target domain data and many labeled source domain data are available. Many of the current DA methods base their transfer assumptions on either parametrized distribution shift or apparent distribution similarities, e.g., identical conditionals or small distributional discrepancies. However, these assumptions may preclude the possibility of adaptation from intricately shifted and apparently very different distributions. To overcome this problem, we propose mechanism transfer, a meta-distributional scenario in which a data generating mechanism is invariant among domains. This transfer assumption can accommodate nonparametric shifts resulting in apparently different distributions while providing a solid statistical basis for DA. We take the structural equations in causal modeling as an example and propose a novel DA method, which is shown to be useful both theoretically and experimentally. Our method can be seen as the first attempt to fully leverage the invariance of structural causal models for DA.
","['The University of Tokyo / RIKEN', 'University of Tokyo / RIKEN', 'RIKEN / The University of Tokyo']"
2020,Self-PU: Self Boosted and Calibrated Positive-Unlabeled Training,"Xuxi Chen, Wuyang Chen, Tianlong Chen, Ye Yuan, Chen Gong, Kewei Chen, Zhangyang Wang",https://icml.cc/Conferences/2020/Schedule?showEvent=5946,"Many real-world applications have to tackle the Positive-Unlabeled (PU) learning problem, i.e., learning binary classifiers from a large amount of unlabeled data and a few labeled positive examples. While current state-of-the-art methods employ importance reweighting to design various biased or unbiased risk estimators, they completely ignored the learning capability of the model itself, which could provide reliable supervision. This motivates us to propose a novel Self-PU learning framework, which seamlessly integrates PU learning and self-training. Self-PU highlights three ``self''-oriented building blocks: a self-paced training algorithm that adaptively discovers and augments confident positive/negative examples as the training proceeds; a self-reweighted, instance-aware loss; and a self-distillation scheme that introduces teacher-students learning as an effective regularization for PU learning. We demonstrate the state-of-the-art performance of Self-PU on common PU learning benchmarks (MNIST and CIFAR10), which compare favorably against the latest competitors. Moreover, we study a real-world application of PU learning, i.e., classifying brain images of Alzheimer's Disease. Self-PU obtains significantly improved results on the renowned Alzheimer's Disease Neuroimaging Initiative (ADNI) database over existing methods.
","['University of Science and Technology of China', 'Texas A&M University', 'Texas A&M University', 'Texas A&M University', 'Nanjing University of Science and Technology', 'Green Valley Pharmaceutical LLC', 'University of Texas at Austin']"
